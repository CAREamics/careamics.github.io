{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>CAREamics is a PyTorch library aimed at simplifying the use of Noise2Void and its many variants and cousins (CARE, Noise2Noise, N2V2, P(P)N2V, HDN, muSplit etc.).</p>"},{"location":"#why-careamics","title":"Why CAREamics?","text":"<p>Noise2Void is a widely used denoising algorithm, and is readily available from the <code>n2v</code> python package. However, n2v is based on TensorFlow and Keras and we found it  increasingly hard to maintain. In addition, more recent methods (PPN2V, DivNoising, HDN) are all implemented in PyTorch, but are lacking the extra features that would make them usable by the community.</p> <p>The aim of CAREamics is to provide a PyTorch library reuniting all the latest methods in one package, while providing a simple and consistent API. The library relies on  PyTorch Lightning as a back-end. In addition, we will provide extensive documentation and  tutorials on how to best apply these methods in a scientific context.</p> <p>Work in progress</p> <p>These pages are still under construction.</p>"},{"location":"#getting-started","title":"Getting Started","text":"Installation <p>                                     Get started with CAREamics installation.                                 </p> Current State <p>                                     Check out where we stand and where we want to go.                                 </p> Guides <p>                                     In-depth guides on CAREamics usage and features.                                 </p> Applications <p>                                     Examples of CAREamics in action on various datasets.                                 </p> Algorithms <p>                                     Dive into the various CAREamics algorithms.                                 </p> Code Reference <p>                                     Code documentation for all CAREamics libraries.                                 </p>"},{"location":"#feedback","title":"Feedback","text":"<p>We are always welcoming feedback on what to improve of what features could be useful, therefore do not hesitate to open an issue on the Github repository!</p>"},{"location":"current_state/","title":"Current state","text":"<p>CAREamics is an on-going project and will include new algorithms in the next releases.  We are currently reaching Milestone 1. Here is a list of the current features:</p>"},{"location":"current_state/#algorithms","title":"Algorithms","text":"<ul> <li> Noise2Void 2D/3D/Channels</li> <li> structN2V 2D/3D/Channels</li> <li> N2V2 2D/3D/Channels</li> <li> CARE 2D/3D/Channels</li> <li> Noise2Noise 2D/3D/Channels</li> <li> CryoCARE 2D/3D</li> <li> PN2V 2D/3D/Channels</li> <li> PPN2V 2D/3D/Channels</li> <li> DivNoising/HDN 2D/3D/Channels</li> <li> muSplit 2D/3D</li> <li> denoiSplit 2D/3D</li> <li> EmbedSeg</li> </ul>"},{"location":"current_state/#features","title":"Features","text":"<ul> <li> Pydantic configuration</li> <li> TIFF dataloader</li> <li> In memory dataset</li> <li> Training/prediction</li> <li> Tiled prediction</li> <li> Checkpoint saving/loading</li> <li> Save/load bioimage.io format</li> <li> Zarr dataset</li> <li> Automated Mixed-Precision</li> <li> torch.compile</li> <li> napari plugin</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>CAREamics is a deep-learning library and we therefore recommend having GPU support as training the algorithms on the CPU can be very slow. MacOS users can also benefit from GPU-acceleration if they have the new chip generations (M1, M2, etc.).</p> <p>Support is provided directly from PyTorch, and is still experimental for macOS.</p>"},{"location":"installation/#step-by-step","title":"Step-by-step","text":"<p>We recommend using mamba (miniforge)  to install all packages in a virtual environment. As an alternative, you can use conda  (miniconda). </p> Linux and WindowsmacOS <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Verify that the GPU is available:</p> <pre><code>python -c \"import torch; print([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\"\n</code></pre> <p>This should show a list of available GPUs. If the list is empty, then you will need to change the <code>pytorch</code> and <code>pytorch-cuda</code> versions to match your hardware (linux and windows).</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install --pre \"careamics[examples]\"\n</code></pre> </li> </ol> <p>These instructions were tested on a linux virtual machine (RedHat 8.6) with a  NVIDIA A40-8Q GPU.</p> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch::pytorch torchvision torchaudio -c pytorch\n</code></pre> <p> Note that accelerated-training is only available on macOS silicon.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install --pre \"careamics[examples]\"\n</code></pre> </li> </ol>"},{"location":"installation/#extra-dependencies","title":"Extra dependencies","text":"<p>CAREamics extra dependencies can be installed by specifying them in brackets. In the previous section we installed <code>careamics[examples]</code>. You can add other extra dependencies, for instance <code>wandb</code> by doing:</p> <pre><code>pip install --pre \"careamics[examples, wandb]\"\n</code></pre> <p>Here is a list of the extra dependencies:</p> <ul> <li><code>examples</code>: Dependencies required to run the example notebooks.</li> <li><code>wandb</code>: Dependencies to use WandB as a logger.</li> <li><code>tensorboard</code>: Dependencies to use TensorBoard as a logger.</li> <li><code>dev</code>: Dependencies required to run all the tooling necessary to develop with CAREamics.</li> </ul>"},{"location":"installation/#quickstart","title":"Quickstart","text":"<p>Once you have installed CAREamics, the easiest way to get started is to look at the applications for full examples and the  guides for in-depth tweaking.</p>"},{"location":"algorithms/","title":"Algorithms","text":"<p>Work in progress</p> <p>These pages are still under construction and we expect a lot more details  descriptions of each algorithm in the near future.</p>"},{"location":"algorithms/#self-supervised-restoration","title":"Self-supervised restoration","text":"Noise2Void <p>                                     A self-supervised denoising algorithm based on a                                      pixel masking scheme.                                 </p> N2V2 <p>                                     A variant of Noise2Void capable of removing                                      checkboard artefacts.                                 </p> StructN2V <p>                                     A variant of Noise2Void that uses an enhanced mask                                     to remove structured noise.                                 </p>"},{"location":"algorithms/#supervised-restoration","title":"Supervised restoration","text":"CARE <p>                                     The original supervised method to restore microscopy                                     images.                                 </p> Noise2Noise <p>                                     A supervised methods that can denoise images without                                     corresponding clean data.                                 </p>"},{"location":"algorithms/care/","title":"Content-Aware image Restoration (CARE)","text":""},{"location":"algorithms/care/#overview","title":"Overview","text":""},{"location":"algorithms/care/#various-applications","title":"Various applications","text":""},{"location":"algorithms/care/#reference","title":"Reference","text":""},{"location":"algorithms/n2n/","title":"Noise2Noise","text":""},{"location":"algorithms/n2n/#overview","title":"Overview","text":""},{"location":"algorithms/n2n/#reference","title":"Reference","text":""},{"location":"algorithms/n2v/","title":"Noise2Void","text":""},{"location":"algorithms/n2v/#overview","title":"Overview","text":"<p>Noise2Void (N2V) is a self-supervised denoising method. It trains by randomly masking pixels in the input image and predicting their masked value from the surrounding pixels.</p> <p>N2V relies on two fundamental hypotheses:</p> <ul> <li>The underlying structures are continuous</li> <li>The noise is pixel-wise independent</li> </ul> <p>The corollory from these hypotheses is that if we consider the value of a pixel being the sum of the true signal value and a certain amount of noise, then:</p> <ul> <li>The true signal value can be estimated from the surrounding pixels</li> <li>The noise cannot be estimated from the surrounding pixels</li> </ul> <p>Therefore, in cases where the hypotheses hold, N2V can be use to estimate the true signal and thereby removing the noise.</p>"},{"location":"algorithms/n2v/#masking-scheme","title":"Masking scheme","text":""},{"location":"algorithms/n2v/#interpreting-the-loss","title":"Interpreting the loss","text":""},{"location":"algorithms/n2v/#artefacts","title":"Artefacts","text":"<p>(todo)</p> <ul> <li>checkboard</li> <li>structured noise</li> </ul>"},{"location":"algorithms/n2v/#references","title":"References","text":"<p>Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, 2019.</p> <p>Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\"  International Conference on Machine Learning. Proceedings of Machine Learning Research, 2019.</p>"},{"location":"algorithms/n2v2/","title":"N2V2","text":""},{"location":"algorithms/n2v2/#overview","title":"Overview","text":""},{"location":"algorithms/n2v2/#changes-to-the-model-architecture","title":"Changes to the model architecture","text":""},{"location":"algorithms/n2v2/#masking-scheme","title":"Masking scheme","text":""},{"location":"algorithms/n2v2/#comparison-with-noise2void","title":"Comparison with Noise2Void","text":""},{"location":"algorithms/n2v2/#reference","title":"Reference","text":""},{"location":"algorithms/structn2v/","title":"structN2V","text":""},{"location":"algorithms/structn2v/#overview","title":"Overview","text":""},{"location":"algorithms/structn2v/#masking-scheme","title":"Masking scheme","text":""},{"location":"algorithms/structn2v/#reference","title":"Reference","text":""},{"location":"applications/","title":"Applications","text":"<p>This section contains a list of example applications. These pages were automatically generated from the notebooks in careamics-example</p>"},{"location":"applications/#care","title":"CARE","text":"<ul> <li>2D denoising U2OS</li> </ul>"},{"location":"applications/#noise2noise","title":"Noise2Noise","text":"<ul> <li>2D SEM</li> </ul>"},{"location":"applications/#noise2void","title":"Noise2Void","text":"<ul> <li>2D SEM</li> <li>2D BSD68</li> </ul>"},{"location":"applications/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Applications</li> <li>CARE<ul> <li>2D denoising U2OS</li> </ul> </li> <li>Noise2Noise<ul> <li>2D SEM</li> </ul> </li> <li>Noise2Void<ul> <li>2D SEM</li> <li>2D BSD68</li> </ul> </li> </ul>"},{"location":"applications/CARE/2D_denoising_U2OS/","title":"2D denoising U2OS","text":"<p>The U2OS dataset is composed of pairs of noisy and high SNR nuclei images acquired in fluorescence microscopy. They were originally used in Weigert et al (2018) to showcase CARE denoising.</p> In\u00a0[25]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_care_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_care_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.CARE_U2OS.download(root_path)\n\n# path to the training data\nroot_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\"\ntrain_path = root_path / \"train\" / \"low\"\ntarget_path = root_path / \"train\" / \"GT\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.CARE_U2OS.download(root_path)  # path to the training data root_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\" train_path = root_path / \"train\" / \"low\" target_path = root_path / \"train\" / \"GT\" In\u00a0[27]: Copied! <pre># load training image and target, and show them side by side\ntrain_files = list(train_path.rglob(\"*.tif\"))\ntrain_files.sort()\n\ntarget_files = list(target_path.rglob(\"*.tif\"))\ntarget_files.sort()\n\n# select random example\nind = np.random.randint(len(train_files))\ntrain_image = tifffile.imread(train_files[ind])\ntrain_target = tifffile.imread(target_files[ind])\n\n# plot the two images and a crop\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[00].set_title(\"Training image\")\n\nax[1].imshow(train_target, cmap=\"gray\")\nax[1].set_title(\"Target image\")\n</pre> # load training image and target, and show them side by side train_files = list(train_path.rglob(\"*.tif\")) train_files.sort()  target_files = list(target_path.rglob(\"*.tif\")) target_files.sort()  # select random example ind = np.random.randint(len(train_files)) train_image = tifffile.imread(train_files[ind]) train_target = tifffile.imread(target_files[ind])  # plot the two images and a crop fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[00].set_title(\"Training image\")  ax[1].imshow(train_target, cmap=\"gray\") ax[1].set_title(\"Target image\") Out[27]: <pre>Text(0.5, 1.0, 'Target image')</pre> In\u00a0[28]: Copied! <pre>config = create_care_configuration(\n    experiment_name=\"care_n2n\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=(128, 128),\n    batch_size=32,\n    num_epochs=100,\n)\n\nprint(config)\n</pre> config = create_care_configuration(     experiment_name=\"care_n2n\",     data_type=\"tiff\",     axes=\"YX\",     patch_size=(128, 128),     batch_size=32,     num_epochs=100, )  print(config) <pre>{'algorithm_config': {'algorithm': 'care',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': False,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'tiff',\n                 'patch_size': [128, 128],\n                 'transforms': [{'mean': 0.485,\n                                 'name': 'Normalize',\n                                 'std': 0.229},\n                                {'name': 'XYFlip'},\n                                {'name': 'XYRandomRotate90'}]},\n 'experiment_name': 'care_n2n',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 100},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    train_target=target_path,\n    val_percentage=0.01,\n    val_minimum_split=20,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     train_target=target_path,     val_percentage=0.01,     val_minimum_split=20, ) In\u00a0[\u00a0]: remove_output Copied! <pre>test_path = root_path / \"test\" / \"low\"\n\nprediction = careamist.predict(source=test_path)\n</pre> test_path = root_path / \"test\" / \"low\"  prediction = careamist.predict(source=test_path) In\u00a0[31]: Copied! <pre># Show two images\ntest_GT_path = root_path / \"test\" / \"GT\"\ntest_GT_files = list(test_GT_path.rglob(\"*.tif\"))\ntest_GT_files.sort()\n\ntest_low_path = root_path / \"test\" / \"low\"\ntest_low_files = list(test_low_path.rglob(\"*.tif\"))\ntest_low_files.sort()\n\ntest_GT = [tifffile.imread(f) for f in test_GT_files]\ntest_low = [tifffile.imread(f) for f in test_low_files]\n\n# images to show\nimages = np.random.choice(range(len(test_GT)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 17))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    \n    psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])\n    psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)\n\n    ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images test_GT_path = root_path / \"test\" / \"GT\" test_GT_files = list(test_GT_path.rglob(\"*.tif\")) test_GT_files.sort()  test_low_path = root_path / \"test\" / \"low\" test_low_files = list(test_low_path.rglob(\"*.tif\")) test_low_files.sort()  test_GT = [tifffile.imread(f) for f in test_GT_files] test_low = [tifffile.imread(f) for f in test_low_files]  # images to show images = np.random.choice(range(len(test_GT)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 17)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()          psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])     psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)      ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\")  In\u00a0[32]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, test_GT)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, test_GT)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") <pre>PSNR: 31.51 +/- 3.70\n</pre>"},{"location":"applications/CARE/2D_denoising_U2OS/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The CARE U2OS dataset is composed of thousands of examples organized in train and test:</p> <ul> <li>train<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> <li>test<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> </ul>"},{"location":"applications/CARE/2D_denoising_U2OS/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/CARE/2D_denoising_U2OS/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/CARE/2D_denoising_U2OS/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/CARE/2D_denoising_U2OS/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/CARE/2D_denoising_U2OS/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training, and we can use the test set.</p>"},{"location":"applications/CARE/2D_denoising_U2OS/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/CARE/2D_denoising_U2OS/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Noise/2D_SEM/","title":"2D SEM","text":"<p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Noise on this particular dataset!</p> In\u00a0[8]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2n_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import tifffile from careamics import CAREamist from careamics.config import create_n2n_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.N2N_SEM.download(root_path)\nfiles = [f for f in download if f.endswith(\"tif\")]\nfiles.sort()\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.N2N_SEM.download(root_path) files = [f for f in download if f.endswith(\"tif\")] files.sort() In\u00a0[3]: Copied! <pre># load training and target image and show them side by side\ntrain_stack = tifffile.imread(files[1])\n\n# use the 1 us scan time to perform Noise2Noise\ntrain_image = train_stack[2]\ntrain_target = train_stack[3]\n\n# plot the two images and a crop\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].set_title(\"Training image (1 us)\")\n\nax[0, 1].imshow(train_target, cmap=\"gray\")\nax[0, 1].set_title(\"Target image (1 us)\")\n\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 0].set_title(\"Training crop (1 us)\")\n\nax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].set_title(\"Target crop (1 us)\")\n</pre> # load training and target image and show them side by side train_stack = tifffile.imread(files[1])  # use the 1 us scan time to perform Noise2Noise train_image = train_stack[2] train_target = train_stack[3]  # plot the two images and a crop fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].set_title(\"Training image (1 us)\")  ax[0, 1].imshow(train_target, cmap=\"gray\") ax[0, 1].set_title(\"Target image (1 us)\")  x_start, x_end = 600, 850 y_start, y_end = 200, 450 ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 0].set_title(\"Training crop (1 us)\")  ax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].set_title(\"Target crop (1 us)\") Out[3]: <pre>Text(0.5, 1.0, 'Target crop (1 us)')</pre> In\u00a0[4]: Copied! <pre>config = create_n2n_configuration(\n    experiment_name=\"sem_n2n\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=50,\n)\n\nprint(config)\n</pre> config = create_n2n_configuration(     experiment_name=\"sem_n2n\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=50, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2n',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': False,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 64,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'mean': 0.485,\n                                 'name': 'Normalize',\n                                 'std': 0.229},\n                                {'name': 'XYFlip'},\n                                {'name': 'XYRandomRotate90'}]},\n 'experiment_name': 'sem_n2n',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 50},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    train_target=train_target,\n    val_minimum_split=5,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     train_target=train_target,     val_minimum_split=5, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[9]: Copied! <pre># get pseudo ground-truth from the 5 us averaged scan time\npseudo_gt = train_stack[-1]\npsnr_noisy = scale_invariant_psnr(pseudo_gt, train_image)\npsnr_pred = scale_invariant_psnr(pseudo_gt, prediction.squeeze())\n\n# Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")\n\nax[0, 1].imshow(prediction.squeeze(), cmap=\"gray\")\nax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")\n\nax[0, 2].imshow(pseudo_gt, cmap=\"gray\")\nax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")\n\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction.squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # get pseudo ground-truth from the 5 us averaged scan time pseudo_gt = train_stack[-1] psnr_noisy = scale_invariant_psnr(pseudo_gt, train_image) psnr_pred = scale_invariant_psnr(pseudo_gt, prediction.squeeze())  # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 3, figsize=(15, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")  ax[0, 1].imshow(prediction.squeeze(), cmap=\"gray\") ax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")  ax[0, 2].imshow(pseudo_gt, cmap=\"gray\") ax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")  ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction.squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[9]: <pre>&lt;matplotlib.image.AxesImage at 0x7ff7e3264a90&gt;</pre>"},{"location":"applications/Noise2Noise/2D_SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The N2N SEM dataset consists of EM images with 7 different levels of noise:</p> <ul> <li>Image 0 is recorded with 0.2 us scan time</li> <li>Image 1 is recorded with 0.5 us scan time</li> <li>Image 2 is recorded with 1 us scan time</li> <li>Image 3 is recorded with 1 us scan time</li> <li>Image 4 is recorded with 2.1 us scan time</li> <li>Image 5 is recorded with 5.0 us scan time</li> <li>Image 6 is recorded with 5.0 us scan time and is the avg. of 4 images</li> </ul>"},{"location":"applications/Noise2Noise/2D_SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Noise/2D_SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Noise/2D_SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Noise/2D_SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Noise/2D_SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Noise/2D_SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/2D_BSD68/","title":"2D BSD68","text":"<p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of Noise2Void.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import psnr\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import psnr from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[3]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[4]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"bsd68_n2v\",\n    data_type=\"tiff\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=100,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"bsd68_n2v\",     data_type=\"tiff\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=100, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 64,\n                 'data_type': 'tiff',\n                 'patch_size': [64, 64],\n                 'transforms': [{'mean': 0.485,\n                                 'name': 'Normalize',\n                                 'std': 0.229},\n                                {'name': 'NDFlip'},\n                                {'name': 'XYRandomRotate90'},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'bsd68_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 100},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    val_source=val_path,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     val_source=val_path, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=test_path,\n    axes=\"YX\",\n    tile_size=(256, 256),\n    tile_overlap=(48, 48),\n)\n</pre> prediction = careamist.predict(     source=test_path,     axes=\"YX\",     tile_size=(256, 256),     tile_overlap=(48, 48), ) In\u00a0[23]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]]\n    psnr_noisy = psnr(gts[images[i]], noises[images[i]])\n    psnr_result = psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]]     psnr_noisy = psnr(gts[images[i]], noises[images[i]])     psnr_result = psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[24]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = psnr(gt, pred)\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = psnr(gt, pred)  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 27.09 +/- 2.90\nReported PSNR: 27.71\n</pre>"},{"location":"applications/Noise2Void/2D_BSD68/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/2D_BSD68/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/2D_BSD68/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/2D_BSD68/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/2D_BSD68/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/2D_BSD68/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training.</p>"},{"location":"applications/Noise2Void/2D_BSD68/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/2D_BSD68/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Void/2D_SEM/","title":"2D SEM","text":"<p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Void on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_SEM.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_SEM.download(root_path) In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(files[0])\nval_image = tifffile.imread(files[1])\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(files[0]) val_image = tifffile.imread(files[1])  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[3]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[4]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"sem_n2v\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=30,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"sem_n2v\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=30, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'mean': 0.485,\n                                 'name': 'Normalize',\n                                 'std': 0.229},\n                                {'name': 'NDFlip'},\n                                {'name': 'XYRandomRotate90'},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'sem_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 30},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_source=val_image,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_source=val_image, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[7]: Copied! <pre># Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 1].imshow(prediction.squeeze(), cmap=\"gray\")\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction.squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 1].imshow(prediction.squeeze(), cmap=\"gray\") ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction.squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[7]: <pre>&lt;matplotlib.image.AxesImage at 0x7fdd842a75b0&gt;</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>careamist.export_to_bmz(\n    path=\"sem_n2v_model.zip\",\n    name=\"SEM_N2V\",\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n)\n</pre> careamist.export_to_bmz(     path=\"sem_n2v_model.zip\",     name=\"SEM_N2V\",     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}], )"},{"location":"applications/Noise2Void/2D_SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/2D_SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/2D_SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/2D_SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/2D_SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/2D_SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/2D_SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/2D_SEM/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"guides/","title":"Guides","text":"<p>The basic usage of CAREamics follows this pattern:</p> CAREamics workflow<pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(  # (1)!\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (2)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)  # (3)!\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256))  # (4)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128))\npredction = careamist.predict(source=pred_data)\n\n# export to BMZ format\ncareamist.export_to_bmz(  # (5)!\n    path=\"my_model.bmz\", name=\"N2V 2D\", authors=[{\"name\": \"CAREamics authors\"}]\n)\n</code></pre> <ol> <li> <p>There are several convenience functions to create a configuration, but you can also create it entirely manually. Head to the configuration section to know more!</p> </li> <li> <p>Obviously, one should choose a more reasonable number of epochs for training.</p> </li> <li> <p>The CAREamist allows training, predicting and exporting the model. Refer to the  CAREamist section to learn more about it. There is also an alternative for more advance users, which we call the Lightning API.</p> </li> <li> <p>One should use real data for training!</p> </li> <li> <p>Models can be exported to the BioImage Model Zoo format.</p> </li> </ol> <p>Work in progress</p> <p>These pages are still under construction.</p> Configuration <p>                                     The configuration is at the heart of CAREamics, it                                      allow users to define how and which algorithm will be                                     trained.                                 </p> Using CAREAmics <p>                                     The CAREamist is the core element allowing training                                     and prediction using the model defined in the configuration.                                 </p> Lightning API <p>                                     Advanced users can re-use part of CAREamics in their                                     Lightning pipeline, with more customization potential                                     available.                                 </p> Developer resources <p>                                     More insights on how CAREamics is organized and how                                     to tweak it to your needs.                                 </p>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>The configuration summarizes all the parameters used internally by CAREamics. It is  used to create a <code>CAREamist</code> instance and is saved together with the checkpoints and  saved models.</p> <p>It is composed of four members:</p> Anatomy of the configuration<pre><code>from careamics import Configuration\n\nconfig_as_dict = {\n    \"experiment_name\": \"my_experiment\",  # (1)!\n    \"algorithm_config\": {  # (2)!\n        \"algorithm\": \"n2v\",\n        \"loss\": \"n2v\",\n        \"model\": {  # (3)!\n            \"architecture\": \"UNet\",\n        },\n    },\n    \"data_config\": {  # (4)!\n        \"data_type\": \"array\",\n        \"patch_size\": [128, 128],\n        \"axes\": \"YX\",\n    },\n    \"training_config\": {\n        \"num_epochs\": 1,\n    },\n}\nconfig = Configuration(**config_as_dict)  # (5)!\n</code></pre> <ol> <li>The name of the experiment, used to differentiate trained models.</li> <li>Configuration specific to the model.</li> <li>Configuration related to the data.</li> <li>Training parameters.</li> <li>The configuration is an object! </li> </ol> <p>If the number of parameters looks too limited, it is because the configuration is hiding a lot of default values! But don't be afraid, we have designed convenience functions to help you create a configuration for each of the algorithm CAREamics offers.</p> <p>In the next sections, you can dive deeper on how to use CAREamics  configuration with different levels of expertise.</p> <ul> <li>(beginner) Convenience functions</li> <li>(beginner) Save and load configurations</li> <li>(intermediate) Build the configuration from scratch</li> <li>(intermediate) Full specification</li> <li>(intermediate) Algorithm requirements</li> <li>(advanced) Advanced configuration</li> <li>(all) Understanding the errors</li> </ul>"},{"location":"guides/configuration/advanced_configuration/","title":"Advanced configuration","text":"<p>We have implemented several mechanism to allow users to use CAREamics in contexts we  do not explicitly support. In this section, we describe several of these mechanisms.</p> <p>In the future, we hope to add more depending on user requests.</p>"},{"location":"guides/configuration/advanced_configuration/#custom-data-type","title":"Custom data type","text":"<p>The <code>data_type</code> parameter of the <code>DataConfig</code> class is a string that is used to choose the data loader within CAREamics. We currently only support <code>array</code> and <code>tiff</code> explicitly.</p> <p>However, users can set the <code>data_type</code> to <code>custom</code> and use their own read function.</p> Custom data type<pre><code>from careamics.config import DataConfig\n\ndata_config = DataConfig(\n    data_type=\"custom\",  # (1)!\n    axes=\"YX\",\n    patch_size=[128, 128],\n    batch_size=8,\n    num_epochs=20,\n)\n</code></pre> <ol> <li>As far as the configuration is concerned, you only set the <code>data_type</code> to <code>custom</code>. The     rest happens in the <code>CAREamist</code> instance.</li> </ol> <p>Full example in other sections</p> <p>A full example of the use of a custom data type is available in the CAREamist  and Applications sections.</p>"},{"location":"guides/configuration/advanced_configuration/#custom-ai-model","title":"Custom AI model","text":"<p>CAREamics currently only support UNet models, but users can create their own model and use it in CAREamics. First, the model needs to be registered with the  <code>register_model</code> decorator, then both the <code>algorithm</code> of <code>AlgorithmConfig</code> and the  <code>architecture</code> of the <code>model</code> need to be set to custom.</p> Custom AI model<pre><code>from careamics.config import AlgorithmConfig, register_model\nfrom torch import nn, ones\n\n\n@register_model(name=\"linear_model\")  # (1)!\nclass LinearModel(nn.Module):\n    def __init__(self, in_features, out_features, *args, **kwargs):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(ones(in_features, out_features))\n        self.bias = nn.Parameter(ones(out_features))\n\n    def forward(self, input):\n        return (input @ self.weight) + self.bias\n\n\nconfig = AlgorithmConfig(\n    algorithm=\"custom\",  # (2)!\n    loss=\"mse\",\n    model={\n        \"architecture\": \"Custom\",  # (3)!\n        \"name\": \"linear_model\",  # (4)!\n        \"in_features\": 10,\n        \"out_features\": 5,\n    },\n)\n</code></pre> <ol> <li>Register your model using the decorator and indicates its <code>name</code>.</li> <li>Set the <code>algorithm</code> to <code>custom</code>.</li> <li>In the <code>model</code>, set the <code>architecture</code> to <code>Custom</code>. Watch the capital letter!</li> <li>Indicate the name of the model.</li> </ol> <p>Full example in other sections</p> <p>A full example of the use of a custom data type is available in the CAREamist  and Applications sections.</p>"},{"location":"guides/configuration/algorithm_requirements/","title":"Algorithm requirements","text":"<p>In this section we detail the constraints of each algorithm on the configuration.</p>"},{"location":"guides/configuration/algorithm_requirements/#noise2void-family","title":"Noise2Void family","text":"<p>This is valid for <code>Noise2Void</code>, <code>N2V2</code> and <code>structN2V</code>.</p>"},{"location":"guides/configuration/algorithm_requirements/#algorithm-configuration","title":"Algorithm configuration","text":"<ul> <li><code>algorithm=\"n2v\"</code></li> <li><code>loss=\"n2v\"</code></li> <li><code>model</code>: <ul> <li>must be a UNet (<code>architecture=\"UNet\"</code>)</li> <li><code>in_channels</code> and <code>num_classes</code> must be equal</li> </ul> </li> </ul>"},{"location":"guides/configuration/algorithm_requirements/#data-configuration","title":"Data configuration","text":"<ul> <li><code>transforms</code>: must contain <code>N2VManipulateModel</code> as the last transform</li> </ul>"},{"location":"guides/configuration/algorithm_requirements/#care","title":"CARE","text":""},{"location":"guides/configuration/algorithm_requirements/#algorithm-configuration_1","title":"Algorithm configuration","text":"<ul> <li><code>algorithm=\"care\"</code></li> <li><code>loss</code>: any but <code>n2v</code></li> </ul>"},{"location":"guides/configuration/algorithm_requirements/#data-configuration_1","title":"Data configuration","text":"<ul> <li><code>transforms</code>: must not contain <code>N2VManipulateModel</code></li> </ul>"},{"location":"guides/configuration/algorithm_requirements/#noise2noise","title":"Noise2Noise","text":""},{"location":"guides/configuration/algorithm_requirements/#algorithm-configuration_2","title":"Algorithm configuration","text":"<ul> <li><code>algorithm=\"care\"</code></li> <li><code>loss</code>: any but <code>n2v</code></li> <li><code>model</code>: <code>in_channels</code> and <code>num_classes</code> must be equal</li> </ul>"},{"location":"guides/configuration/algorithm_requirements/#data-configuration_2","title":"Data configuration","text":"<ul> <li><code>transforms</code>: must not contain <code>N2VManipulateModel</code></li> </ul>"},{"location":"guides/configuration/build_configuration/","title":"Build the configuration","text":"<p>Beginner vs Intermediate</p> <p>This is an intermediate level way to create CAREamics configuration. Do check the convenience functions if you are looking for a simpler way to create CAREanics configurations!</p> <p>CAREamics configuration is validated using Pydantic,  a library that allows you to define schemas and automatically check the types of the  input data you provide. </p> <p>In addition, it allows great flexibility in writing custom validators. In turns this ensures that the configuration is always valid and coherent, protecting against errors deep in the library.</p> <p>As shown in the introduction, CAREamics configuration is composed of four main elements:</p> <ol> <li>Experiment name, a simple string</li> <li>Algorithm configuration, also a Pydantic model</li> <li>Data configuration, also a Pydantic model</li> <li>Training configuration, also a Pydantic model</li> </ol> <p>Each of the parameters and models are validated independently, and the configuration as a whole is validated at the end.</p> <p>There are two ways to build Pydantic models: by passing a dictionary that reproduces the model structure, or by calling all Pydantic models explicitly. While the first method is  more concise, the second method is less error prone and allow you to explore all parameters available easily if you are using an IDE (e.g. VSCode, JupyterLab etc.).</p>"},{"location":"guides/configuration/build_configuration/#using-nested-dictionaries","title":"Using nested dictionaries","text":"<p>In the introduction, we have seen a minimum example on how to build the configuration with a dictionary, reproduced here:</p> Building the configuration with a dictionary<pre><code>from careamics import Configuration\n\nconfig_as_dict = {\n    \"experiment_name\": \"my_experiment\",  # (1)!\n    \"algorithm_config\": {  # (2)!\n        \"algorithm\": \"n2v\",\n        \"loss\": \"n2v\",\n        \"model\": {  # (3)!\n            \"architecture\": \"UNet\",\n        },\n    },\n    \"data_config\": {  # (4)!\n        \"data_type\": \"array\",\n        \"patch_size\": [128, 128],\n        \"axes\": \"YX\",\n    },\n    \"training_config\": {\n        \"num_epochs\": 1,\n    },\n}\nconfig = Configuration(**config_as_dict)  # (5)!\n</code></pre> <ol> <li>The first parameter is just a string!</li> <li>But this one is itself a Pydantic model, so we need to pass a dictionary that     respects the structure of the model.</li> <li>Don't be surprised, the deep neural network model is also a Pydantic model.</li> <li>Same here, and so on...</li> <li>The configuration is instantiated by passing keywords arguments rather than a dictionary,      and Pydantic knows how to interpret the sub-dictionaries to correctly instantiate the member      models.</li> </ol> <p>While this is neat, because you are dealing with nested dictionaries, it is easy to add the parameters at the wrong level and you need to constantly refer to the code documentation to know which parameters are available.</p> <p>Finally, because you are validating the configuration at once, you will get all the validation errors in one go.</p>"},{"location":"guides/configuration/build_configuration/#using-pydantic-models-preferred","title":"Using Pydantic models (preferred)","text":"<p>The preferred way to build the configuration is to call the Pydantic models directly. This allows you to explore the parameters via your IDE, but also to get the validation errors closer to the source of the error.</p> Building the configuration using Pydantic models<pre><code>from careamics import Configuration\nfrom careamics.config import (  # (1)!\n    AlgorithmConfig,\n    DataConfig,\n    TrainingConfig,\n)\nfrom careamics.config.architectures import UNetModel\nfrom careamics.config.support import (\n    SupportedAlgorithm,\n    SupportedArchitecture,\n    SupportedData,\n    SupportedLogger,\n    SupportedLoss,\n    SupportedTransform,\n)\nfrom careamics.config.transformations import N2VManipulateModel\n\nexperiment_name = \"Pydantic N2V2 example\"\n\n# build AlgorithmConfig\nalgorithm_model = AlgorithmConfig(  # (2)!\n    algorithm=SupportedAlgorithm.N2V.value,  # (3)!\n    loss=SupportedLoss.N2V.value,\n    model=UNetModel(  # (4)!\n        architecture=SupportedArchitecture.UNET.value,\n        in_channels=1,\n        num_classes=1,\n    ),\n)\n\n# then the DataConfig\ndata_model = DataConfig(\n    data_type=SupportedData.ARRAY.value,\n    patch_size=(256, 256),\n    batch_size=8,\n    axes=\"YX\",\n    transforms=[\n        {  # (5)!\n            \"name\": SupportedTransform.XY_FLIP.value,\n        },\n        N2VManipulateModel(  # (6)!\n            masked_pixel_percentage=0.15,\n        ),\n    ],\n    dataloader_params={  # (7)!\n        \"num_workers\": 4,\n    },\n)\n\n# then the TrainingConfig\ntraining_model = TrainingConfig(\n    num_epochs=30,\n    logger=SupportedLogger.WANDB.value,\n)\n\n# finally, build the Configuration\nconfig = Configuration(  # (8)!\n    experiment_name=experiment_name,\n    algorithm_config=algorithm_model,\n    data_config=data_model,\n    training_config=training_model,\n)\n</code></pre> <ol> <li>The main Pydantic models are imported from the <code>careamics</code> and <code>careamics.config</code>      submodules, the others are organized in different submodules.</li> <li>A Pydantic model is instantiated like any other class.</li> <li>In CAREamics, we store constant values in <code>enum</code> classes in the <code>careamics.config.support</code>      submodule. This allows to have a single source of truth for the values. But you have     to remember to use the <code>.value</code> attribute to get the string value.</li> <li>You can instantiate the nested models directly in the parent model or outside (outside     is better to track down the errors!).</li> <li>The <code>transforms</code> parameter is a list, you can mix and match the different transformations,     but also use dictionaries or the Pydantic model classes directly. Make sure to pass     the <code>name</code> as it is used to identify the correct Pydantic model to instantiate for the     transformation.</li> <li>Here for instance, we use the Pydantic model directly.</li> <li>The <code>dataloader_params</code> is a dictionary, you can pass any parameter that is accepted by     the <code>torch.utils.data.DataLoader</code> class.</li> <li>Finally, the configuration is instantiated by passing the Pydantic models directly.</li> </ol>"},{"location":"guides/configuration/convenience_functions/","title":"Convenience functions","text":"<p>As building a full CAREamics configuration requires a complete understanding of the  various parameters and experience with Pydantic, we provide convenience functions to create configurations with a only few parameters related to the algorithm users want to train.</p> <p>All convenience methods can be found in the <code>careamics.config</code> modules. CAREamics  currently supports Noise2Void and its variants,  CARE and Noise2Noise. </p> Import convenience functions<pre><code>from careamics.config import (\n    create_care_configuration,  # CARE\n    create_n2n_configuration,  # Noise2Noise\n    create_n2v_configuration,  # Noise2Void, N2V2, structN2V\n)\n</code></pre> <p>Each method does all the heavy lifting to make the configuration coherent. They share a certain numbers of mandatory parameters:</p> <ul> <li><code>experiment_name</code>: The name of the experiment, used to differentiate trained models.</li> <li><code>data_type</code>: One of the types supported by CAREamics (<code>array</code>, <code>tiff</code> or <code>custom</code>).</li> <li><code>axes</code>: Axes of the data (e.g. SYX), can only the following letters: <code>STCZYX</code>.</li> <li><code>patch_size</code>: Size of the patches along the spatial dimensions (e.g. [64, 64]).</li> <li><code>batch_size</code>: Batch size to use during training (e.g. 8). This parameter affects the     memory footprint on the GPU.</li> <li><code>num_epochs</code>: Number of epochs.</li> </ul> <p>Additional optional parameters can be passed to tweak the configuration. </p>"},{"location":"guides/configuration/convenience_functions/#noise2void","title":"Noise2Void","text":""},{"location":"guides/configuration/convenience_functions/#training-with-channels","title":"Training with channels","text":"<p>When training with multiple channels, the <code>axes</code> parameter should contain <code>C</code> (e.g. <code>YXC</code>). An error will be then thrown if the optional parameter <code>n_channels</code> is not specified!  Likewise if <code>n_channels</code> is specified but <code>C</code> is not in <code>axes</code>.</p> <p>The correct way is to specify them both at the same time.</p> Configuration with multiple channels<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,  # (2)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> </ol> <p>Independent channels</p> <p>By default, the channels are trained independently: that means that they have no influence on each other. As they might have completely different noise models, this can lead to better results.</p> <p>However, in some cases, you might want to train the channels together to get more structural information.</p> <p>To control whether the channels are trained independently, you can use the  <code>independent_channels</code> parameter:</p> Training channels together<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol>"},{"location":"guides/configuration/convenience_functions/#using-augmentations","title":"Using augmentations","text":"<p>By default CAREamics configuration uses augmentations that are specific to the algorithm (e.g. Noise2Void) and that are compatible with microscopy images (e.g. flip and 90 degrees rotations).</p> <p>However in certain cases, users might want to disable augmentations. For instance if you have structures that are always oriented in the same direction. To do so there is a single <code>use_agumentations</code> parameter:</p> Configuration without augmentations<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_augmentations=False,  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization and N2V pixel manipulation are still there!).</li> </ol>"},{"location":"guides/configuration/convenience_functions/#choosing-a-logger","title":"Choosing a logger","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p> Configuration with WandB<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol>"},{"location":"guides/configuration/convenience_functions/#advanced-passing-model-specific-parameters","title":"(Advanced) Passing model specific parameters","text":"<p>By default, the convenience functions use the default UNet model parameters. But if  you are feeling brave, you can pass model specific parameters in the <code>model_kwargs</code> dictionary. </p> Configuration with model specific parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_kwargs={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> <p>Model parameters overwriting</p> <p>Some values of the model parameters are not compatible with certain algorithms.  Therefore, these are overwritten by the convenience functions. For instance, if you pass <code>in_channels</code> or <code>independent_channels</code> in the <code>model_kwargs</code> dictionary,  they will be ignored and replaced by the explicit parameters passed to the convenience function.</p>"},{"location":"guides/configuration/convenience_functions/#noise2void-specific-parameters","title":"Noise2Void specific parameters","text":"<p>Noise2Void has a few additional parameters that can be set, including for using its  variants N2V2 and structN2V.</p> <p>Understanding Noise2Void and its variants</p> <p>Before deciding which variant to use, and how to modify the parameters, we recommend to die a little a bit on how each algorithm works!</p>"},{"location":"guides/configuration/convenience_functions/#noise2void-parameters","title":"Noise2Void parameters","text":"<p>There are two Noise2Void parameters that influence how the patches are manipulated during training:</p> <ul> <li><code>roi_size</code>: This parameter specifies the size of the area used to replace the masked pixel value.</li> <li><code>masked_pixel_percentage</code>: This parameter specifies how many pixels per patch will be manipulated.</li> </ul> <p>While the default values are usually fine, they can be tweaked to improve the training in certain cases.</p> Configuration with N2V parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    roi_size=7,\n    masked_pixel_percentage=0.5,\n)\n</code></pre>"},{"location":"guides/configuration/convenience_functions/#n2v2","title":"N2V2","text":"<p>To use N2V2, the <code>use_n2v2</code> parameter should simply be set to <code>True</code>.</p> Configuration with N2V2<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v2_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_n2v2=True,  # (1)!\n)\n</code></pre> <ol> <li>What it does is modifying the architecture of the UNet model and the way the masked     pixels are replaced.</li> </ol>"},{"location":"guides/configuration/convenience_functions/#structn2v","title":"structN2V","text":"<p>StructN2V has two parameters that can be set:</p> <ul> <li><code>struct_n2v_axis</code>: The axis along which the structN2V mask will be applied. By default it     is set to <code>none</code> (structN2V is disabled), you can set it to either <code>horizontal</code> or <code>vertical</code>.</li> <li><code>struct_n2v_span</code>: The size of the structN2V mask.</li> </ul> Configuration with structN2V<pre><code>config = create_n2v_configuration(\n    experiment_name=\"structn2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    struct_n2v_axis=\"horizontal\",\n    struct_n2v_span=5,\n)\n</code></pre>"},{"location":"guides/configuration/convenience_functions/#noise2noise","title":"Noise2Noise","text":""},{"location":"guides/configuration/convenience_functions/#training-with-channels_1","title":"Training with channels","text":"<p>When training with multiple channels, the <code>axes</code> parameter should contain <code>C</code> (e.g. <code>YXC</code>). An error will be then thrown if the optional parameter <code>n_channels</code> is not specified!  Likewise if <code>n_channels</code> is specified but <code>C</code> is not in <code>axes</code>.</p> <p>The correct way is to specify them both at the same time.</p> Configuration with multiple channels<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,  # (2)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> </ol> <p>Independent channels</p> <p>By default, the channels are trained independently: that means that they have no influence on each other. As they might have completely different noise models, this can lead to better results.</p> <p>However, in some cases, you might want to train the channels together to get more structural information.</p> <p>To control whether the channels are trained independently, you can use the  <code>independent_channels</code> parameter:</p> Training channels together<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol>"},{"location":"guides/configuration/convenience_functions/#using-augmentations_1","title":"Using augmentations","text":"<p>By default CAREamics configuration uses augmentations that are specific to the algorithm (e.g. Noise2Void) and that are compatible with microscopy images (e.g. flip and 90 degrees rotations).</p> <p>However in certain cases, users might want to disable augmentations. For instance if you have structures that are always oriented in the same direction. To do so there is a single <code>use_agumentations</code> parameter:</p> Configuration without augmentations<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_augmentations=False,  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization and N2V pixel manipulation are still there!).</li> </ol>"},{"location":"guides/configuration/convenience_functions/#choosing-a-logger_1","title":"Choosing a logger","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p> Configuration with WandB<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol>"},{"location":"guides/configuration/convenience_functions/#advanced-passing-model-specific-parameters_1","title":"(Advanced) Passing model specific parameters","text":"<p>By default, the convenience functions use the default UNet model parameters. But if  you are feeling brave, you can pass model specific parameters in the <code>model_kwargs</code> dictionary. </p> Configuration with model specific parameters<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_kwargs={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> <p>Model parameters overwriting</p> <p>Some values of the model parameters are not compatible with certain algorithms.  Therefore, these are overwritten by the convenience functions. For instance, if you pass <code>in_channels</code> or <code>independent_channels</code> in the <code>model_kwargs</code> dictionary,  they will be ignored and replaced by the explicit parameters passed to the convenience function.</p>"},{"location":"guides/configuration/convenience_functions/#noise2noise-with-another-loss","title":"Noise2Noise with another loss","text":"<p>As opposed to Noise2Void, CARE and Noise2Noise can be trained with different loss functions. This can be set using the <code>loss</code> parameter (surprise, surprise!).</p> Configuration with different loss<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol>"},{"location":"guides/configuration/convenience_functions/#care","title":"CARE","text":""},{"location":"guides/configuration/convenience_functions/#training-with-channels_2","title":"Training with channels","text":"<p>When training with multiple channels, the <code>axes</code> parameter should contain <code>C</code> (e.g. <code>YXC</code>). An error will be then thrown if the optional parameter <code>n_channels_in</code> is not specified!  Likewise if <code>n_channels_in</code> is specified but <code>C</code> is not in <code>axes</code>.</p> <p>The correct way is to specify them both at the same time.</p> Configuration with multiple channels<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,  # (2)!\n    n_channels_out=2,  # (3)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> <li>Depending on the CARE task, you also see to set <code>n_channels_out</code></li> </ol> <p>Independent channels</p> <p>By default, the channels are trained independently: that means that they have no influence on each other. As they might have completely different noise models, this can lead to better results.</p> <p>However, in some cases, you might want to train the channels together to get more structural information.</p> <p>To control whether the channels are trained independently, you can use the  <code>independent_channels</code> parameter:</p> Training channels together<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,\n    n_channels_out=2,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol>"},{"location":"guides/configuration/convenience_functions/#using-augmentations_2","title":"Using augmentations","text":"<p>By default CAREamics configuration uses augmentations that are specific to the algorithm (e.g. Noise2Void) and that are compatible with microscopy images (e.g. flip and 90 degrees rotations).</p> <p>However in certain cases, users might want to disable augmentations. For instance if you have structures that are always oriented in the same direction. To do so there is a single <code>use_agumentations</code> parameter:</p> Configuration without augmentations<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_augmentations=False,  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization and N2V pixel manipulation are still there!).</li> </ol>"},{"location":"guides/configuration/convenience_functions/#choosing-a-logger_2","title":"Choosing a logger","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p> Configuration with WandB<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol>"},{"location":"guides/configuration/convenience_functions/#advanced-passing-model-specific-parameters_2","title":"(Advanced) Passing model specific parameters","text":"<p>By default, the convenience functions use the default UNet model parameters. But if  you are feeling brave, you can pass model specific parameters in the <code>model_kwargs</code> dictionary. </p> Configuration with model specific parameters<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_kwargs={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> <p>Model parameters overwriting</p> <p>Some values of the model parameters are not compatible with certain algorithms.  Therefore, these are overwritten by the convenience functions. For instance, if you pass <code>in_channels</code> or <code>independent_channels</code> in the <code>model_kwargs</code> dictionary,  they will be ignored and replaced by the explicit parameters passed to the convenience function.</p>"},{"location":"guides/configuration/convenience_functions/#noise2noise-with-another-loss_1","title":"Noise2Noise with another loss","text":"<p>CARE can be trained with different loss functions. This can be set using the <code>loss</code> parameter (surprise, surprise!).</p> Configuration with different loss<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol>"},{"location":"guides/configuration/full_spec/","title":"Full specification","text":"<p>The full specification of the configuration is a detailed description of all the parameters  that can be used to configure CAREamics. It is useful for advanced users who want to  have full control over the training process.</p> <p>You can also explore all the Pydantic models using the reference documentation.</p> Full specification<pre><code>from careamics import Configuration\nfrom careamics.config import (\n    AlgorithmConfig,\n    DataConfig,\n    TrainingConfig,\n)\nfrom careamics.config.architectures import UNetModel\nfrom careamics.config.callback_model import EarlyStoppingModel\nfrom careamics.config.optimizer_models import LrSchedulerModel, OptimizerModel\nfrom careamics.config.support import (\n    SupportedActivation,\n    SupportedAlgorithm,\n    SupportedArchitecture,\n    SupportedData,\n    SupportedLogger,\n    SupportedLoss,\n    SupportedOptimizer,\n    SupportedPixelManipulation,\n    SupportedScheduler,\n    SupportedStructAxis,\n)\nfrom careamics.config.transformations import (\n    N2VManipulateModel,\n    XYFlipModel,\n    XYRandomRotate90Model,\n)\n\nexperiment_name = \"Full example\"\n\n# Algorithm\nmodel = UNetModel(  # (1)!\n    architecture=SupportedArchitecture.UNET.value,\n    in_channels=1,\n    num_classes=1,\n    depth=2,\n    num_channels_init=32,\n    final_activation=SupportedActivation.NONE.value,\n    n2v2=False,\n)\n\noptimizer = OptimizerModel(\n    name=SupportedOptimizer.ADAM.value, parameters={\"lr\": 0.0001}\n)\n\nscheduler = LrSchedulerModel(\n    name=SupportedScheduler.REDUCE_LR_ON_PLATEAU.value,\n)\n\nalgorithm_model = AlgorithmConfig(\n    algorithm=SupportedAlgorithm.N2V.value,\n    loss=SupportedLoss.N2V.value,\n    model=model,\n    optimizer=optimizer,\n    lr_scheduler=scheduler,\n)\n\n# Data\nxyflip = XYFlipModel()\nrotate = XYRandomRotate90Model()\nn2vmanipulate = N2VManipulateModel(\n    roi_size=11,\n    masked_pixel_percentage=0.2,\n    strategy=SupportedPixelManipulation.MEDIAN.value,\n    struct_mask_axis=SupportedStructAxis.NONE.value,\n    struct_mask_span=7,\n)\n\ndata_model = DataConfig(\n    data_type=SupportedData.ARRAY.value,\n    patch_size=(256, 256),\n    batch_size=8,\n    axes=\"YX\",\n    transforms=[xyflip, rotate, n2vmanipulate],\n    dataloader_params={\n        \"num_workers\": 4,\n        # (2)!\n    },\n)\n\n# Traning\nearlystopping = EarlyStoppingModel(\n    # (3)!\n)\n\ntraining_model = TrainingConfig(\n    num_epochs=30,\n    logger=SupportedLogger.WANDB.value,\n    early_stopping_callback=earlystopping,\n)\n\nconfig = Configuration(\n    experiment_name=experiment_name,\n    algorithm_config=algorithm_model,\n    data_config=data_model,\n    training_config=training_model,\n)\n</code></pre> <ol> <li>Currently, we only support the UNet architecture and custom models (see advanced     configuration). But in the future, there will be more     models to use here.</li> <li>Here the parameters are those from Pytorch DataLoaders.</li> <li>Normalization is also a transformation, but it is always applied before these      augmentations.</li> <li>The <code>EarlyStoppingModel</code> has a lot of parameters not reproduced here.</li> </ol> <p>However, not all algorithms are compatible with all parameters. The configuration does some heavy lifting to correct the obvious incompatibilities, but some are left to the user. In the next section, we will see the constraints on each algorithm.</p>"},{"location":"guides/configuration/save_load/","title":"Save and load","text":"<p>CAREamics configurations can be saved to the disk as <code>.yml</code> file and loaded easily to start similar experiments.</p>"},{"location":"guides/configuration/save_load/#save-a-configuration","title":"Save a configuration","text":"Save a configuration<pre><code>from careamics import save_configuration\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"Config_to_save\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=(8, 64, 64),\n    batch_size=8,\n    num_epochs=20,\n)\nsave_configuration(config, \"config.yml\")\n</code></pre> <p>In the resulting file, you can see all the parameters that are defaults and hidden from you.</p> resulting config.yml file <pre><code>version: 0.1.0\nexperiment_name: N2V 3D\nalgorithm_config:\n  algorithm: n2v\n  loss: n2v\n  model:\n    architecture: UNet\n    conv_dims: 3\n    num_classes: 1\n    in_channels: 1\n    depth: 2\n    num_channels_init: 32\n    final_activation: None\n    n2v2: false\n    independent_channels: true\n  optimizer:\n    name: Adam\n    parameters:\n      lr: 0.0001\n  lr_scheduler:\n    name: ReduceLROnPlateau\n    parameters: {}\ndata_config:\n  data_type: tiff\n  patch_size:\n  - 8\n  - 64\n  - 64\n  batch_size: 8\n  axes: ZYX\n  transforms:\n  - name: XYFlip\n    flip_x: true\n    flip_y: true\n    p: 0.5\n  - name: XYRandomRotate90\n    p: 0.5\n  - name: N2VManipulate\n    roi_size: 11\n    masked_pixel_percentage: 0.2\n    strategy: uniform\n    struct_mask_axis: none\n    struct_mask_span: 5\ntraining_config:\n  num_epochs: 20\n  checkpoint_callback:\n    monitor: val_loss\n    verbose: false\n    save_weights_only: false\n    mode: min\n    auto_insert_metric_name: false\n    save_last: true\n    save_top_k: 3\n</code></pre>"},{"location":"guides/configuration/save_load/#load-a-configuration","title":"Load a configuration","text":"Load a configuration<pre><code>from careamics import load_configuration\n\nconfig = load_configuration(\"config.yml\")\n</code></pre>"},{"location":"guides/configuration/understanding_errors/","title":"Configuration errors","text":"<p>Work in progress</p> <p>These pages are still under construction and will come soon.</p>"},{"location":"guides/dev_resources/","title":"Developer's guide","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <ul> <li>How to contribute to the project?</li> <li>How does the website work?</li> <li>CAREamics docstring conventions</li> </ul>"},{"location":"guides/dev_resources/contribute/","title":"Contribute to CAREamics","text":"<p>CAREamics is a free and open-source software and we are happy to receive contributions from the community! There are several ways to contribute to the project:</p> <ul> <li>Contribute applications</li> <li>Contribute methods and features</li> <li>Contribute to the documentation</li> </ul>"},{"location":"guides/dev_resources/contribute/#contribute-applications","title":"Contribute applications","text":"<p>Microscopy is a vast field and there are many different types of data and projects that can benefit from denoising. We only have applications that were brought to us or used in the publications.</p> <p>We would love to know more about different types of microscopy data, and whether  CAREamics helped analyse them or not. In the future, we want to illustrate how to scientifically apply the methods in CAREamics to research data and any example, working or failing, is of great help to the community!</p> <p>To contribute applications, open an issue so that we can discuss the problems you encountered or have a look at the results!</p> <p>Once an application is accepted, you can create a <code>jupyter notebook</code> and add it to the careamics-example repository. Finally, the website guide explains how to add the notebook to the  website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-methods-and-features","title":"Contribute methods and features","text":"<p>CAREamics is a growing project and we are always looking for new methods and features to better help the community. We are interested in any method that proved to be  valuable for microscopy data denoising and restoration.</p> <p>What about data formats?</p> <p>We currently only support <code>numpy arrays</code> and <code>tif</code> files. In the near future, we will focus on <code>zarr</code> data. We do not intend in maintaining support for other data formats.</p> <p>For particular data formats, we advise to use the custom data loading, which is described in the training guide.</p> <p>To contribute methods, open an issue so that we can discuss the method and its implementation. Same for new features!</p>"},{"location":"guides/dev_resources/contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request, make sure that you installed the <code>dev</code> optional dependencies of CAREamics:</p> <pre><code>pip install careamics[dev]\n</code></pre> <p>In particular, make sure that you use pre-commit before committing changes:</p> <pre><code>pre-commit install\n</code></pre> <p>The PR need to pass the tests and the pre-commit checks! Make sure to also fill in the  PR template and make a PR to the documentation website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>If you find any typo, mistakes or missing information in the documentation, feel free to make a PR to the documentation repository.</p> <p>Read the website guide to know how to better understand the various mechanisms implemented in the website.</p>"},{"location":"guides/dev_resources/docstring/","title":"Docstring conventions","text":"<p>CAREamics follows the numpydoc  docstring conventions. The is enforced by the use of the <code>numpydoc</code> pre-commit hook.</p> <p>On top of the numpy conventions, we try to build a more human readable docstring by adapting principles from Pandas docstring:</p> <pre><code>param : Union[str, int] \u274c\nparam : str or int \u2705\nparam : str | int \u2705\n\nchoice : Literal[\"a\", \"b\", \"c\"] \u274c\nchoice : {\"a\", \"b\", \"c\"} \u2705\n\nparam : Tuple[int, int] \u274c\nparam : (int, int) \u2705\n\nsequence: List[int] \u274c\nsequence: list of int \u2705\n\nparam : int \n    The default is 1. \u274c\nparam : int, default=1 \u2705\n\nparam : Optional[int] \u274c\nparam : int, optional \u2705\n</code></pre>"},{"location":"guides/dev_resources/website/","title":"Github pages","text":"<p>The Github pages are built using mkdocs, more specifically the  mkdocs-material theme. Modifications to the theme were greatly inspired from pydev-guide.</p> <p>In this page, we describe some of the technical details on how to maintain this website.</p>"},{"location":"guides/dev_resources/website/#environment","title":"Environment","text":"<p>The <code>requirements.txt</code> file contains all the packages used to generate this website.</p>"},{"location":"guides/dev_resources/website/#build-the-pages-locally","title":"Build the pages locally","text":"<p>In order to build the pages locally, follow these steps:</p> <ol> <li>Fork this repository and clone it.</li> <li>Create a new environment and install the dependencies:     <pre><code>conda create -n careamics-docs python=3.11\npip install -r requirements.txt\n</code></pre></li> <li>Run the following scripts (which are normally run by the CI):     <pre><code>python scripts/check_out_repos.sh\npython scripts/check_out_notebooks.sh\n</code></pre></li> <li>Build the pages:     <pre><code>mkdocs serve\n</code></pre></li> <li>Open the local link in your browser.</li> </ol> <p>Note: This will not show you the version mechanism. For this, check out the  Version release section.</p>"},{"location":"guides/dev_resources/website/#code-snippets","title":"Code snippets","text":"<p>Code snippets are all automatically tested in careamics-example and added automaticallt to the Github pages in the guides section.</p> <p>The script <code>scripts/check_out_examples.sh</code> clone the examples repository locally and upon building the pages, the code snippets are automatically added to the markdown by the PyMdown Snippets extension.</p> <p>It works as follows, first create a code snippet in a python file on  careamics-example:</p> careamics-example/some_example/some_file.py<pre><code># code necessary for running successfully the snippet\n# but not shown on the Github pages\nimport numpy as np\n\narray = np.ones((32, 32))\n\n# we add a snippet section:\n# --8&lt;-- [start:my_snippet]\nsum_array = np.sum(array) # snippets appearing on the website (can be multi-lines)\n# --8&lt;-- [end:my_snippet]\n\n# then more code or snippets sections\n...\n</code></pre> <p>Then, in the CAREamics Github pages source, the corresponding markdown file has the following content telling <code>PyMdown.Snippets</code> to  include the snippet section:</p> careamics.github.io/guides/some_example/some_file.py<pre><code>    To sum the array, simply do:\n\n    ```python\n    --8&lt;-- \"careamics-example/some_example/some_file.py:my_snippet\"\n    ```\n</code></pre>"},{"location":"guides/dev_resources/website/#jupyter-notebooks-applications","title":"Jupyter notebooks applications","text":"<p>The pages in the application section are automatically generated from the Jupyter notebooks in careamics-example  using mkdocs-jupyter. A bash script (<code>scripts/check_out_examples.sh</code>) checks out the repository and copies  all the notebooks referenced in a text files into the correct path in the application  folder. Finally, the script <code>scripts/gen_jupyter_nav.py</code> creates entries for each notebook  in the navigation file of mkdocs.</p>"},{"location":"guides/dev_resources/website/#adding-a-new-notebook","title":"Adding a new notebook","text":"<ol> <li>Add the notebook to <code>scripts/notebooks.csv</code>, without using spaces. The second column   specifies the path to the page in the application section, while the last column is used    as title.</li> <li>You can test the notebook by running <code>sh scripts/notebooks.sh</code> then <code>mkdocs serve</code>.</li> </ol> <p>!!! info title=\"Cell tags\"</p> <pre><code>By default, all cell outputs are shown. To hide the output of a particular cell,\nadd the tag `remove_output` to the cell. The `mkdocs.ynml` specifies that this \ntag is used to hide cell outputs.\n</code></pre> <p>!!! info title=\"CSV ending on a new line\"</p> <pre><code>In is important to end the `.csv` file with a new line, otherwise the last line might\nbe ignored.\n</code></pre>"},{"location":"guides/dev_resources/website/#code-reference","title":"Code reference","text":"<p>The code reference is generated using mkdocstrings,  the script <code>scripts/checkout_repos.sh</code> and the page building script <code>scripts/gen_ref_pages.py</code>.  To include a new package, simply add it to the <code>scripts/git_repositories.txt</code> file.</p> <pre><code>https://github.com/CAREamics/careamics\nhttps://github.com/CAREamics/careamics-portfolio\n&lt;new project here&gt;\n</code></pre>"},{"location":"guides/dev_resources/website/#updating-the-website-version","title":"Updating the website version","text":"<p>In principle, when a new release of CAREamics is made, the state of the documentation is saved into the corresponding version, and the documentation is tagged with the next (ongoing) version.</p> <p>For instance, the documentation is showing version <code>0.4</code>, upon release of version  <code>0.4</code>, the state of the documentation is saved. The latest documentation is then  tagged with version <code>0.5</code> (the next version) until this one is released.</p> <p>In order to keep track of versions, we use mike.  We apply the following procedure:</p> <ol> <li>Release version MAJOR.MINOR of CAREamics</li> <li>Tag the latest documentation with version MAJOR.(MINOR+1)   <pre><code>git tag MAJOR.(MINOR+1)\ngit push --tags\n</code></pre></li> </ol> <p>To visualize the pages with the versions, you can use:</p> <pre><code>mike serve\n</code></pre>"},{"location":"guides/dev_resources/website/#correcting-a-version-error","title":"Correcting a version error","text":"<p>All the versions are stored in the <code>gh-pages</code> branch. If you made a mistake in the version tagging, you can correct it by deleting the tag and pushing the changes.</p>"},{"location":"guides/lightning_api/","title":"Lightning API","text":"<p>Work in progress</p> <p>These pages are still under construction.</p>"},{"location":"guides/usage/","title":"Using CAREamics","text":"<p>In this section, we will explore the many facets of the <code>CAREamist</code> class, which allors training and predicting using the various algorithms in CAREamics.</p> <p>The workflow in CAREamics has five steps: creating a configuration, instantiating a <code>CAREamist</code> object, training, prediction, and model export.</p> Basic CAREamics usage<pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256))  # (2)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128))\npredction = careamist.predict(source=pred_data)\n\n# export to BMZ format\ncareamist.export_to_bmz(\n    path=\"n2v_model.bmz\", name=\"N2V 2D\", authors=[{\"name\": \"CAREamics authors\"}]\n)\n</code></pre> <ol> <li> <p>Obviously, one should choose a more realistic number of epochs for training.</p> </li> <li> <p>One should use real data for training!</p> </li> </ol>"},{"location":"guides/usage/careamist/","title":"CAREamist","text":"<p>The <code>CAREamist</code> is the central class in CAREamics, it provides the API to train, predict and save models. There are three ways to create a <code>CAREamist</code> object: with a configuration,  with a path to a configuration, or with a path to a trained model.</p>"},{"location":"guides/usage/careamist/#instantiating-with-a-configuration","title":"Instantiating with a configuration","text":"<p>When passing a configuration to the <code>CAREamist</code> constructor, the model is initialized with random weights and prediction will not be possible until the model is trained.</p> Instantiating CAREamist with a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)  # (1)!\n\ncareamist = CAREamist(config)\n</code></pre> <ol> <li>Any valid configuration will do!</li> </ol>"},{"location":"guides/usage/careamist/#instantiating-with-a-path-to-a-configuration","title":"Instantiating with a path to a configuration","text":"<p>This is similar to the previous section, except that the configuration is loaded from a file on disk.</p> Instantiating CAREamist with a path to a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration, save_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)\n\nsave_configuration(config, \"configuration_example.yml\")\n\ncareamist = CAREamist(\"configuration_example.yml\")\n</code></pre>"},{"location":"guides/usage/careamist/#instantiating-with-a-path-to-a-model","title":"Instantiating with a path to a model","text":"<p>There are two types of models exported from CAREamics. During training, the model is saved as checkpoints (<code>.ckpt</code>). After training, users can export the model to the  bioimage model zoo format (saved as a<code>.zip</code>). Both can be loaded into CAREamics to either retrain or predict. Alternatively, a checkpoint can be loaded in order to  export it as a bioimage model zoo model.</p> <p>In any case, both types of pre-trained models can be loaded into CAREamics by passing the path to the model file. The instantiated CAREamist is then ready to predict on new images!</p> Instantiating CAREamist with a path to a model<pre><code>from careamics import CAREamist\n\npath_to_model = \"model.zip\"  # (1)!\n\ncareamist = CAREamist(path_to_model)\n</code></pre> <ol> <li>Any valid path to a model, as a string or a <code>Path.path</code> object, will work.</li> </ol>"},{"location":"guides/usage/careamist/#experiment-name","title":"Experiment name","text":"<p>When loading a pre-trained model, the experiment name, used in the loggers (e.g. WandB), or to name the checkpoints, is automatically set to <code>CAREamics</code>. But you can change that by passing it to the <code>CAREamist</code> constructor.</p> Changing the experiment name<pre><code>careamist = CAREamist(path_to_model, experiment_name=\"a_new_experiment\")\n</code></pre>"},{"location":"guides/usage/careamist/#setting-the-working-directory","title":"Setting the working directory","text":"<p>By default, CAREamics will save the checkpoints in the current working directory. When creating a new CAREamist, you can indicate a different working directory in which to save the logs and checkpoints during training.</p> Changing the working directory<pre><code>careamist = CAREamist(config, work_dir=\"work_dir\")\n</code></pre>"},{"location":"guides/usage/datasets/","title":"Datasets","text":"<p>Datasets are the internal classes providing the individual patches for training,  validation and prediction. In CAREamics, we provide a <code>CAREamicsTrainData</code> class that  creates the datasets for training and validation (there is a class for prediction as well, which is simpler and shares some parameters with the training one). In most cases, it is created internally. In this section, we describe what it does and shed light on some of its parameters that are passed to the train methods.</p>"},{"location":"guides/usage/datasets/#overview","title":"Overview","text":"<p>The <code>CAREamicsTrainData</code> receives both data configuration and data itself. The data can be passed a path to a folder, to a file or as <code>numpy</code> array. </p> Simplest way to instantiate CAREamicsTrainData<pre><code>import numpy as np\nfrom careamics import CAREamicsTrainData\nfrom careamics.config import create_n2v_configuration\n\ntrain_array = np.random.rand(128, 128)\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = CAREamicsTrainData(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n</code></pre> <p>It has the following parameters:</p> <ul> <li><code>data_config</code>: data configuration</li> <li><code>train_data</code>: training data (array or path)</li> <li><code>(optional) val_data</code>: validation data, if not provided, the validation data is taken from the training data</li> <li><code>(optional) train_data_target</code>: target data for training (if applicable)</li> <li><code>(optional) val_data_target</code>: target data for validation (if applicable)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> <li><code>(optional) val_percentage</code>: percentage of validation data to extract from the training     (see splitting validation)</li> <li><code>(optional) val_minimum_split</code>: minimum validation split      (see splitting validation)</li> <li><code>(optional) use_in_memory</code>: whether to use in-memory dataset if possible (Default is <code>True</code>),      not applicable to mnumpy arrays.</li> </ul> <p>Depending on the type of the data, which is specified in the <code>data_config</code> and is compared to the type of <code>train_data</code>, the <code>CAREamicsTrainData</code> will create the appropriate dataset for both training and validation data.</p> <p>In the absence of validation, validation data is extracted from training data (see splitting validation).</p>"},{"location":"guides/usage/datasets/#available-datasets","title":"Available datasets","text":"<p>CAREamics currently support two datasets:</p> <ul> <li>InMemoryDataset: used when the data fits in memory.</li> <li>IterableDataset: used when the data is too large to fit in memory.</li> </ul> <p>If the data is a <code>numpy</code> array, the <code>InMemoryDataset</code> is used automatically. Otherwise, we list the files contained in the path, compute the size of the data and instantiate an <code>InMemoryDataset</code> if the data is less than 80% of the total RAM size. If not, CAREamics instantiate an <code>IterableDataset</code>.</p> <p>Both datasets work differently, and the main differences can be summarized as follows:</p> Feature <code>InMemoryDataset</code> <code>IterableDataset</code> Used with arrays  Yes  No Patch extraction Sequential Random Data loading All in memory One file at a time <p>In the next sections, we describe the different steps they perform.</p>"},{"location":"guides/usage/datasets/#in-memory-dataset","title":"In-memory dataset","text":"<p>As the name implies, the in-memory dataset loads all the data in memory. It is used when the data on the disk seems to fit in memory, or when the data is already in memory and  passed as a numpy array. The advantage of the dataset is that is allows faster access to the patches, and therefore faster training time.</p> <p>It performs the following steps:</p>  On numpy arrays On Paths <ol> <li>Compute the <code>mean</code> and <code>std</code> of the dataset over all images.</li> <li>Reshape the array so that the axes are ordered following the convention <code>SC(Z)YX</code>.</li> <li>Extract patches sequentially so that they cover all images and keep them     in memory.</li> <li>Update the <code>mean</code> and <code>std</code> in the configuration if they were not provided. This step     also updates the <code>mean</code> and <code>std</code> of the normalization transform.</li> <li>Get the transforms from the configuration.</li> <li>Each time a patch is requested:<ol> <li>A patch is extracted from the in-memory patches, it has dimensions <code>(1, C, (Z), Y, X)</code>,      where <code>C</code> is the number of channels, <code>Z</code> is present only if the data is 3D, and     <code>Z, Y, X</code> are the patch sizes in each dimension.</li> <li>The transformations are applied to the patch (see transforms).</li> <li>The result of the transformation is returned.</li> </ol> </li> </ol> <ol> <li>For each file in the path, the corresponding image is loaded.</li> <li>The <code>mean</code> and <code>std</code> are computed for the loaded image.</li> <li>The image is reshaped so that the axes are ordered following the convention <code>SC(Z)YX</code>.</li> <li>Extract patches sequentially so that they cover the whole image and kept them     in memory.</li> <li>Once all files have been processed, the average <code>mean</code> and <code>std</code> are computed.</li> <li>Update the <code>mean</code> and <code>std</code> in the configuration if they were not provided. This step     also updates the <code>mean</code> and <code>std</code> of the normalization transform.</li> <li>All patches are concatenated together.</li> <li>Get the transforms from the configuration.</li> <li>Each time a patch is requested:<ol> <li>A patch is extracted from the in-memory patches, it has dimensions <code>(1, C, (Z), Y, X)</code>,      where <code>C</code> is the number of channels, <code>Z</code> is present only if the data is 3D, and     <code>Z, Y, X</code> are the patch sizes in each dimension.</li> <li>The transformations are applied to the patch (see transforms).</li> <li>The result of the transformation is returned.</li> </ol> </li> </ol> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/usage/datasets/#iterable-dataset","title":"Iterable dataset","text":"<p>The iterable dataset is used to load patches from a single file at a time, one file after another. This allows training on datasets that are too large to fit in memory. This dataset is exclusively used with files input (data passed as paths).</p> <p>It performs the following steps:</p> <ol> <li>The dataset does a first pass of all the data to compute the average <code>mean</code> and <code>std</code>,     if these have not been specified in the configuration.</li> <li>Update the configuration and the transforms with the computed <code>mean</code> and <code>std</code>.</li> <li>Get the list of transforms from the configuration.</li> <li>Each time a patch is requested:<ol> <li>If there is no more patches (see point 6), the next image is loaded.</li> <li>The image is reshaped so that the axes are ordered following the convention <code>SC(Z)YX</code>.</li> <li>Random patches are extracted from the image, they have dimensions <code>(N, C, (Z), Y, X)</code>,      where <code>C</code> is the number of channels, <code>Z</code> is present only if the data is 3D,     <code>Z, Y, X</code> are the patch sizes in each dimension, and <code>N</code> is the number of patches.</li> <li>The transformations are applied to the patches (see transforms).</li> <li>The next patch is yielded. The patches are yielded one at a time until there are no more patches     in the image, at which point the next image is loaded when the next patch is requested (see point 1).</li> </ol> </li> </ol> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/usage/datasets/#intermediate-transforms","title":"(Intermediate) Transforms","text":"<p>Transforms are augmentations and any operation applied to the patches before feeding them into the network. CAREamics supports the following transforms (see  configuration full spec for an example on how to configure them):</p> Transform Description Notes <code>Normalize</code> Normalize (zero mean, unit variance) Necessary <code>XYFlip</code> Flip the image along X and Y, one at a time Can flip a single axis, optional <code>XYRandomRotate90Model</code> Rotate by 90 degrees the XY axes Optional <code>N2VManipulateModel</code> N2V pixel manipulation Only for N2V, in which case it is necessary <p>The <code>Normalize</code> transform is always applied, and the rest are optional. The exception is <code>N2VManipulateModel</code>, which is only applied when training with N2V (see Noise2Void).</p> <p>When to turn off transforms?</p> <p>The configuration allows turning off transforms. In this case, only normalization (and potentially the <code>N2VManipulateModel</code> for N2V) is applied. This is useful when the structures in your sample are always in the same orientation, and flipping and rotation do not make sense.</p>"},{"location":"guides/usage/datasets/#advanced-custom-data-types","title":"(Advanced) Custom data types","text":"<p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You should also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.npy\") to filter the files extension using <code>extension_filter</code>.</p> Read custom data types<pre><code>from pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom careamics import CAREamicsTrainData\nfrom careamics.config import create_n2v_configuration\n\n\ndef read_npy(  # (1)!\n    path: Path,  # (2)!\n    *args: Any,\n    **kwargs: Any,  # (3)!\n) -&gt; np.ndarray:\n    return np.load(path)  # (4)!\n\n\n# example data\ntrain_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"custom\",  # (5)!\n    axes=\"YX\",\n    patch_size=[32, 32],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = CAREamicsTrainData(\n    data_config=config.data_config,\n    train_data=\"train_array.npy\",  # (6)!\n    read_source_func=read_npy,  # (7)!\n    extension_filter=\"*.npy\",  # (8)!\n)\ndata_module.prepare_data()\ndata_module.setup()  # (9)!\n\n# check dataset output\ndataloader = data_module.train_dataloader()\nprint(dataloader.dataset[0][0].shape)  # (10)!\n</code></pre> <ol> <li> <p>We define a function that reads the custom data type.</p> </li> <li> <p>It takes a path as argument!</p> </li> <li> <p>But it also need to receive <code>*args</code> and <code>**kwargs</code> to be compatible with the <code>read_source_func</code> signature.</p> </li> <li> <p>It simply returns a <code>numpy</code> array.</p> </li> <li> <p>The data type must be <code>custom</code>!</p> </li> <li> <p>And we pass a <code>Path | str</code>.</p> </li> <li> <p>Simply pass the method by name.</p> </li> <li> <p>We also need to provide an extension filter that is compatible with <code>fnmatch</code> and <code>Path.rglob</code>.</p> </li> <li> <p>These two lines are necessary to instantiate the training dataset that we call at the end. They are     called automatically by PyTorch Lightning during training.</p> </li> <li> <p>The dataloader gives access to the dataset, we choose the first element, and since     we configured CAREamics to use N2V, the output is a tuple whose first element is our     first patch!</p> </li> </ol>"},{"location":"guides/usage/datasets/#prediction-datasets","title":"Prediction datasets","text":"<p>The prediction data module, <code>CAREamicsPredictData</code> works similarly to <code>CAREamicsTrainData</code>, albeit with fewer parameters:</p> <ul> <li><code>pred_config</code>: data configuration</li> <li><code>pred_data</code>: prediction data (array or path)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> </ul> <p>It uses <code>InMemoryPredictionDataset</code> for arrays and <code>IterablePredictionDataset</code> for paths. These are similar to their training counterparts, but they have simpler transforms and offer the possibility to run test-time augmentation. For more details, refer to the prediction section.</p>"},{"location":"guides/usage/model_export/","title":"Export to BMZ","text":"<p>The BioImage Model Zoo is a zoo of models that can be run in a variety of software thanks to the BMZ format. CAREamics is compatible with the BMZ format and can export and load (CAREamics) models in this format.</p> Export to BMZ format<pre><code>tada\n</code></pre> <p>example of generated readme</p>"},{"location":"guides/usage/prediction/","title":"Prediction","text":""},{"location":"guides/usage/prediction/#predict-on-arrays-or-paths","title":"Predict on arrays or paths","text":"On numpy arrays On Paths <p>blah</p> <p>blah</p>"},{"location":"guides/usage/prediction/#tiling","title":"Tiling","text":"<p>tile size and overlaps</p>"},{"location":"guides/usage/prediction/#test-time-augmentation","title":"Test time augmentation","text":"<p>tta</p>"},{"location":"guides/usage/prediction/#transforms","title":"Transforms","text":"<p>normalization</p>"},{"location":"guides/usage/prediction/#intermediate-predict-from-a-checkpoint","title":"(Intermediate) Predict from a checkpoint","text":""},{"location":"guides/usage/prediction/#advanced-predict-on-custom-data-type","title":"(Advanced) Predict on custom data type","text":""},{"location":"guides/usage/training/","title":"Training","text":"<p>You can provide data in various way to train your model: as a <code>numpy</code> array, using a path to a folder or files, or by using CAREamics data module class for more control (advanced).</p> <p>The details of how CAREamics deals with the loading and patching is detailed in the dataset section.</p> <p>Data type</p> <p>The data type of the source and targets must be the same as the one specified in the configuration. That is to say <code>array</code> in the case of <code>np.ndarray</code>, and <code>tiff</code> in the case of paths.</p>"},{"location":"guides/usage/training/#training-by-passing-an-array","title":"Training by passing an array","text":"<p>CAREamics can be trained by simply passing numpy arrays.</p> Training by passing an array<pre><code>import numpy as np\n\ntrain_array = np.random.rand(256, 256)\nval_array = np.random.rand(256, 256)\n\ncareamist.train(\n    train_source=train_array,  # (1)!\n    val_source=val_array,  # (2)!\n)\n</code></pre> <ol> <li>All parameters to the <code>train</code> method must be specified by keyword.</li> <li>If you don't provide a validation source, CAREamics will use a fraction of the training data    to validate the model.</li> </ol> <p>Supervised training</p> <p>If you are training a supervised model, you must provide the target data as well.</p> <pre><code>careamist_supervised.train(\n    train_source=train_array,\n    train_target=target_array,\n    val_source=val_array,\n    val_target=val_target_array,\n)\n</code></pre>"},{"location":"guides/usage/training/#training-by-passing-a-path","title":"Training by passing a path","text":"<p>The same thing can be done by passing a path to a folder or files.</p> Training by passing a path<pre><code>careamist.train(\n    train_source=path_to_train_data,  # (1)!\n    val_source=path_to_val_data,\n)\n</code></pre> <ol> <li>The path can point to a single file, or contain multiple files.</li> </ol> <p>Training from path</p> <p>To train from a path, the data type must be set to <code>tiff</code> or <code>custom</code> in the  configuration.</p>"},{"location":"guides/usage/training/#splitting-validation-from-training-data","title":"Splitting validation from training data","text":"<p>If you only provide training data, CAREamics will extract the validation data directly from the training set. There are two parameters controlling that behaviour: <code>val_percentage</code> and <code>val_minimum_split</code>.</p> <p><code>val_percentage</code> is the fraction of the training data that will be used for validation, and <code>val_minimum_split</code> is the minimum number of iamges used. If the percentage leads to a  number of patches smaller than <code>val_minimum_split</code>, CAREamics will use <code>val_minimum_split</code>.</p> Splitting validation from training data<pre><code>careamist.train(\n    train_source=train_array,\n    val_percentage=0.1,  # (1)!\n    val_minimum_split=5,  # (2)!\n)\n</code></pre> <ol> <li>10% of the training data will be used for validation.</li> <li>If the number of images is less than 5, CAREamics will use 5 images for validation.</li> </ol> <p>Arrays vs files</p> <p>The behaviour of <code>val_percentage</code> and <code>val_minimum_split</code> is based different depending on whether the source data is an array or a path. If the source is an array, the split is done on the patches (<code>N</code> patches are used for validation). If the source is a path, the split is done on the files (<code>N</code> files are used for validation).</p>"},{"location":"guides/usage/training/#training-by-passing-a-careamicstraindata-object","title":"Training by passing a CAREamicsTrainData object","text":"<p>CAREamics provides a class to handle the data loading of custom data type. We will dive  in more details in the next section into what this class can be used for. Here is a  brief overview of how it passed to the <code>train</code> method.</p> Training by passing a CAREamicsTrainData object<pre><code>from careamics import CAREamicsTrainData\n\ndata_module = CAREamicsTrainData(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n\ncareamist.train(datamodule=data_module)\n</code></pre> <ol> <li>Here this does the same thing as passing the <code>train_source</code> directly into the <code>train</code> method.     In the next section, we will see a more useful example.</li> </ol>"},{"location":"guides/usage/training/#callbacks","title":"Callbacks","text":"<p>CAREamics currently allows two different callbacks from PyTorch Lightning:</p> <ul> <li><code>ModelCheckpoint</code>: to save the model at different points during the training.</li> <li><code>EarlyStopping</code>: to stop the training based on a few parameters.</li> </ul> <p>The parameters for the callbacks are the same as the ones from PyTorch Lightning, and can be set in the configuration.</p>"},{"location":"guides/usage/training/#logging-the-training","title":"Logging the training","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>To decide on the logger, check out the Configuration section.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p>"}]}