{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Documentation for CAREamics v0.0.16.</p> <p>CAREamics is a PyTorch library aimed at simplifying the use of state of the art image restoration deep-learning algorithms, such as CARE, Noise2Void, HDN, MicroSplit, etc.</p>"},{"location":"#getting-started","title":"Getting Started","text":"Installation <p>                                         Get started with CAREamics installation.                                     </p> Features <p>                                         What you can do with CAREamics.                                     </p> Guides <p>                                         In-depth guides on CAREamics usage and features.                                     </p> Applications <p>                                         Examples of CAREamics in action on various datasets.                                     </p> Algorithms <p>                                         Dive into the various CAREamics algorithms.                                     </p> API Documentation <p>                                         Code documentation for all CAREamics libraries.                                     </p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you are having trouble using the library or the napari plugin, contact us via the  Image.sc forum.</p> <p>We are always welcoming feedback on what to improve of what features could be useful, therefore do not hesitate to open an issue on the Github repository!</p>"},{"location":"features/","title":"Features","text":"<p>CAREamics is an on-going project and the API is not yet stable.</p>"},{"location":"features/#officially-supported-features-v0015","title":"Officially supported features (v0.0.15)","text":"<ul> <li>Installation from conda and mamba</li> <li>Data formats<ul> <li>in-memory arrays</li> <li>tiff files</li> </ul> </li> <li>Training<ul> <li>training in memory or from multiple files</li> <li>choose number of epochs and steps per epochs</li> <li>WandB and TensorBoard monitoring</li> </ul> </li> <li>Prediction<ul> <li>Tiled prediction</li> </ul> </li> <li>Algorithms<ul> <li>Content-aware image restoration (CARE) [ref]</li> <li>Noise2Noise [ref]</li> <li>Noise2Void [ref1, ref2]</li> <li>N2V2 [ref]</li> </ul> </li> <li>napari UI</li> </ul>"},{"location":"features/#current-work","title":"Current work","text":"<ul> <li>Next-generation dataset<ul> <li>data formats: <ul> <li>Zarr</li> <li>CZI</li> </ul> </li> <li>flexible, modular</li> <li>arbitrary data organization</li> <li>on-line statistics estimation</li> <li>data masking</li> <li>background patch rejection</li> </ul> </li> <li>Prediction<ul> <li>direct tile writing (e.g. Zarr)</li> </ul> </li> <li>Algorithms<ul> <li>P(P)N2V [ref1, ref2]</li> <li>Hierarchical DivNoising [ref1, ref2]</li> <li>MicroSplit [ref]</li> </ul> </li> <li>Stand-alone UI</li> </ul>"},{"location":"algorithms/","title":"Algorithms","text":"<p>Work in progress</p> <p>These pages are still under construction and we expect a lot more details  descriptions of each algorithm in the near future.</p> <p>In these pages, you will find explanations and illustrations of how the various algorithms used in CAREamics work. These algorithms are divided into different  sections and a few keywords help you to understand the main characteristics of each algorithm. </p>"},{"location":"algorithms/#keywords","title":"Keywords","text":"<ul> <li>no ground-truth: The algorithm trains without clean images.</li> <li>single image: The algorithm can train on a single image.</li> <li>pairs of noisy images: The algorithm requires pairs of noisy images.</li> <li>ground-truth: The algorithm requires pairs of clean and noisy images.</li> </ul>"},{"location":"algorithms/#self-supervised-restoration","title":"Self-supervised restoration","text":"Noise2Void <p>                                         A self-supervised denoising algorithm based on a                                          pixel masking scheme.                                     </p> no ground-truth single image N2V2 <p>                                         A variant of Noise2Void capable of removing                                          checkboard artefacts.                                     </p> no ground-truth single image StructN2V <p>                                         A variant of Noise2Void that uses an enhanced mask                                         to remove structured noise.                                     </p> no ground-truth single image"},{"location":"algorithms/#noise-models","title":"Noise Models","text":"Noise Models <p>                                         Methods to estimate the noise models of                                         microscopes.                                     </p> no ground-truth"},{"location":"algorithms/#supervised-restoration-without-ground-truth","title":"Supervised restoration without ground-truth","text":"Noise2Noise <p>                                         A supervised methods that can denoise images without                                         corresponding clean data.                                     </p> no ground-truth pairs of noisy images"},{"location":"algorithms/#supervised-restoration","title":"Supervised restoration","text":"CARE <p>                                         The original supervised method to restore microscopy                                         images.                                     </p> ground-truth"},{"location":"algorithms/N2V2/","title":"N2V2","text":"<p>Fig 3.: Max pool vs max blur pool downsampling. An input (left) is passed through a max pool layer (center) and through a max blur pool layer (right) for comparison. The blur pool layer introduces blurring, avoiding aliasing-related artefacts. CC-BY.</p> <p>Fig 4.: Noise2Void vs N2V2 manipulation. In both algorithms, randomly selected pixels in the input patch (left) are selected and their value replaced. Noise2Void (center) replaces the values by the value of one of the neighboring pixel, while N2V2 (right) uses the median of the neighborhood. CC-BY.</p>"},{"location":"algorithms/N2V2/#n2v2","title":"N2V2\u00b6","text":""},{"location":"algorithms/N2V2/#overview","title":"Overview\u00b6","text":"<p>N2V2 [1] is a self-supervised denoising method based on Noise2Void [2,3]. The method was developed to remove pixel-noises from images without leaving unwanted checkerboard artefacts which can occasionally happen with Noise2Void.</p> <p>It retains the same training scheme (i.e. training on noisy images by randomly masking pixels), and differs from Noise2Void by the network architecture and pixel masking procedure. More specifically, N2V2 removes the first skip connection and introduces max blurpool layers in the UNet model [4], and the N2V2 masking scheme uses the median of each local image patch as replacement/masking value, rather than a randomly chosen neighboring pixel intensity.</p>"},{"location":"algorithms/N2V2/#checkerboard-artefacts","title":"Checkerboard artefacts\u00b6","text":"<p>Checkerboard artefacts [1] sometimes occur with Noise2Void. This effect is most prominent in the presence of salt and pepper noise or hot pixels in sCMOS cameras. They appear as little crosses, as illustrated in the figure below.</p> <p> </p> <p>Fig 1.: Checkerboard artefacts. The figure shows a noisy image crop (left), a noisy close-up (center) and the corresponding Noise2Void prediction (right). While the denoising was effective, a distinctive checkerboard pattern appears nonetheless in the Noise2Void prediction. CC-BY.</p>"},{"location":"algorithms/N2V2/#noise2void-vs-n2v2","title":"Noise2Void vs N2V2\u00b6","text":""},{"location":"algorithms/N2V2/#changes-to-the-architecture","title":"Changes to the architecture\u00b6","text":"<p>The model architecture used in Noise2Void and its siblings (N2V2, structN2V, P(P)N2V etc.) is a UNet model [4], a fully convolutional architecture.  Its main parts are an encoder, which downsamples and compresses the information into a bottleneck, and a decoder, which upsamples the information back to the original image size (see figure 2).</p> <p>In order to compress the information, the UNet architecture can perform multiple downsampling operations, defining multiple levels of spatial resolution. For each downsampling operation in the encoder, there is a corresponding upsampling operation in the decoder.</p> <p>One of the trick of the UNet architecture is to use skip connections between the encoder and the decoder. These skip connections allow the decoder to access the information from the encoder at the same spatial resolution. This is often useful to recover fine details of the image.</p> <p>N2V2 changes the architecture used in Noise2Void in two ways: the top skip connection is removed, and the downsampling is done using a different operation.</p> <p> </p> <p>Fig 2.: UNet architecture as used by Noise2Void. CC-BY.</p>"},{"location":"algorithms/N2V2/#skip-connections","title":"Skip connections\u00b6","text":"<p>In N2V2, the first skip connection is removed to constrain the among of high-frequency information that can be passed from the encoder to the decoder. Since the checkerboard artefacts are high-frequency artefacts, this change is expected to reduce their occurrence.</p>"},{"location":"algorithms/N2V2/#max-blur-pooling-layers","title":"Max blur pooling layers\u00b6","text":"<p>The second change is the introduction of max blur pooling as downsampling layers in the encoder to avoid aliasing-related artefacts, as opposed to the max pool layer used in Noise2Void. As with Noise2Void, the downsampling step starts from computing the maximum value in the patch (i.e. the region over which the filter is applied for each pixel), before applying a blur kernel (which is a convolutional operation) [5]. Since the output is smoothed, sharp transitions between pixel values are avoided and the aliasing artefacts are reduced.</p>"},{"location":"algorithms/N2V2/#changes-to-the-masking-scheme","title":"Changes to the masking scheme\u00b6","text":"<p>In Noise2Void, the masking scheme consists in replacing central pixels by a randomly chosen neighboring pixel (see the example). N2V2 changes the strategy for pixel selection by replacing the central pixel with the median of the neighborhood. This leads to a masking with a lower difference between masked pixels and their surrounding, preventing aliasing artefacts arising from sharp transitions between masked and unmasked pixels.</p>"},{"location":"algorithms/N2V2/#comparison","title":"Comparison\u00b6","text":"<p>These changes allow suppression of the checkerboard artefacts, as illustrated in the figure below. N2V2 does not take longer than Noise2Void to train, and you can easily compare the results of both methods.</p> <p>To train Noise2Void and N2V2 on the same dataset, refer to their respective examples: SEM Noise2Void and SEM N2V2.</p> <p> </p> <p>Fig 5.: Noise2Void vs N2V2 in the presence of checkerboard artefacts. The figure shows a noisy image crop (left), the Noise2Void prediction of the region delimited by a red square (center) and the corresponding N2V2 prediction (right). The checkerboard artefacts are clearly visible in the Noise2Void prediction (center), while they are much less severe with N2V2 (left). CC-BY.</p>"},{"location":"algorithms/N2V2/#limitations","title":"Limitations\u00b6","text":"<p>Beyond the checkerboard artefacts, N2V2 suffers from the same limitations as Noise2Void regarding pixel-wise independent noise.</p>"},{"location":"algorithms/N2V2/#references","title":"References\u00b6","text":"<p>[1] Eva H\u00f6ck, Tim-Oliver Buchholz, Anselm Brachmann, Florian Jug, and Alexander Freytag. \"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture.\" ECCV, 2022. link</p> <p>[2] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[3] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p> <p>[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" MICCAI, 2015. link</p> <p>[5] Richard Zhang. \"Making convolutional networks shift-invariant again.\" MLR, 2019. link</p>"},{"location":"algorithms/Noise2Void/","title":"Noise2Void","text":"<p>Fig 1.: Noise2Void masking scheme. Top row: a toy example showcasing the pixel value replacement of masked pixels (left), and the resulting mask used for supervision (right). The mask contains the original values of the masked pixels. Bottom row: realistic example with the default masking parameters (data from 3). CC-BY.</p> <p>Fig 3.: Structured noise. Top row: image with uncorrelated noise (left) and its autocorrelation (close-up on the zero-shift component). Bottom row: same images with correlations introduced in the noise component. CC-BY.</p>"},{"location":"algorithms/Noise2Void/#noise2void","title":"Noise2Void\u00b6","text":""},{"location":"algorithms/Noise2Void/#overview","title":"Overview\u00b6","text":"<p>Noise2Void [1, 2] is a self-supervised denoising method. It trains by randomly masking pixels in the input image and predicting their masked value from the surrounding pixels.</p> <p>N2V relies on two fundamental hypotheses:</p> <ul> <li>The underlying structures are smooths (i.e. continuous)</li> <li>The noise is pixel-wise independent (i.e. the noise in one pixel is not correlated with the noise in a neighboring pixel)</li> </ul> <p>The corollary from these hypotheses is that if we consider the value of a pixel, $x$, as being the sum of the true signal, $s$, and a ceertain amount of noise, $\\epsilon$, such that $x = s + \\epsilon$, then:</p> <ul> <li>The true signal value $s$ can be estimated from the surrounding pixels</li> <li>The noise $\\epsilon$ cannot be estimated from the surrounding pixels</li> </ul> <p>Therefore, in cases where the hypotheses hold, if a pixel is masked, its noise cannot be estimated from the surrounding pixels, while the true signal can. This is exactly how N2V is trained!</p>"},{"location":"algorithms/Noise2Void/#training-noise2void","title":"Training Noise2Void\u00b6","text":"<p>Noise2Void is self-supervised, meaning that it trains on the data itself. It relies on a classical UNet architecture [4], and applies a special augmentation called <code>N2V manipulate</code> that replaces the value of randomly-selected pixels with the value of one of their neighbours (see the left column in the next figure).</p> <p>During training, the output of the network is compared to an image that is only comprised of the original pixels that were obfuscated during the augmentation. The loss is only computed on these obfuscated pixels, which means that the network is trained specifically to predict the masked pixels values!</p> <p>Below you can see two examples of the input to the network (left) and the masked pixels used to compute the loss (right). The first example is a toy image that allows visualizing the pixel replacement (it masks $2\\%$ of the pixels). The second example is a realistic image with the default N2V pixel manipulation parameters (e.g. $0.2\\%$ of masked pixels).</p>"},{"location":"algorithms/Noise2Void/#interpreting-the-loss","title":"Interpreting the loss\u00b6","text":"<p>As described above, the loss is computed by calculating the mean squared error over the masked pixels:</p> <p>$$loss = \\dfrac{1}{N_{masked}}\\sum_{i, masked}(x'_{i}-x_{i})^{2}$$</p> <p>where $N_{masked}$ is the total number of masked pixels, $x'_{i}$ is the network prediction for the masked pixel $i$, and $x_{i}$ is the original noisy value of pixel $i$.</p> <p>As opposed to supervised approaches, where the loss is computed with respect to a known ground truth, N2V computes the loss based on a noisy signal. At the beginning of the training the network usually learns within a few epochs an approximation of the structures in the image and the loss decreases sharply. Then, it rapidly reaches a plateau and oscillates around a particular loss value.</p> <p>Because the loss is computed between prediction and noisy signal, its absolute value is not informative for whether the network is properly trained. Likewise, oscillation on the plateau does not indicate that it does not learn anymore.</p> <p>The best way to assess the quality of the training is to look at the denoised images, potentially at different points during training, using the checkpoints. In practice, we often simply train long enough for the image to look properly denoised!</p> <p>If the training loss is not too informative, what about the validation loss? It is basically the same. If your validation loss increases, however, that does mean you are overfitting and might need to add more images to the training data.</p>"},{"location":"algorithms/Noise2Void/#predicting-on-the-training-set","title":"Predicting on the training set\u00b6","text":"<p>We are usually told that training and validation images should not be used to assess the performance (even qualitatively) of the network, because it was trained specifically to perform well on them.</p> <p>With Noise2Void, however, the network is trained without biasing it towards a ground-truth as it is trained on noisy pixels only. Therefore, it is perfectly fine to predict on the training images.</p>"},{"location":"algorithms/Noise2Void/#re-using-a-trained-model","title":"Re-using a trained model\u00b6","text":"<p>Noise2Void learns both the noise distribution in the image and a structural prior, that is to say how the structures in the image look. As with most deep learning approaches, if you try applying a trained neural network on images that are different from the training set, the network will most likely fail and produce results of lesser quality.</p> <p>This will happen with N2V, for example, if the noise distribution is different in the new images, or if they contain different structures. Fortunately, N2V is quick to train, so you can train a single network for each new experiment!</p>"},{"location":"algorithms/Noise2Void/#why-do-n2v-predictions-sometimes-look-blurry","title":"Why do N2V predictions sometimes look blurry?\u00b6","text":"<p>The absence of noise can by itself make images look slightly blurry. If some regions of your data look unreasonably blurry, you might be encountering the a case of regression to the mean.</p> <p>For each noisy image, we often say that there is a whole distribution of possible denoised images. In particular, for strongly degraded images, even different structures can produce the same noisy patch.</p> <p>Because of this, a network trained with the mean squared error, such as N2V, will tend to predict the average of all possible denoised images. This is why the prediction can often look blurry and washed out. For approaches that predict single instances from the distribution of possible denoised images, check out DivNoising or HDN.</p>"},{"location":"algorithms/Noise2Void/#assessing-quality-of-results","title":"Assessing quality of results\u00b6","text":"<p>Denoising is a difficult task to assess quantitatively. The most common and more accurate way to estimate the performances of a network is to compare its output with a ground truth image. That means that you need to acquire images with and without noise. This can be done by acquiring with more laser power, longer exposure time, or by averaging multiple images. That is, however, cumbersome and often impossible.</p> <p>There are other ways perform controls on the quality of the denoising.</p>"},{"location":"algorithms/Noise2Void/#quality-assessment-by-inspecting-the-residuals","title":"Quality assessment by inspecting the residuals\u00b6","text":"<p>The residuals are the pixel intensity removed from the original image by the network:</p> <p>$$res = image - pred$$</p> <p>They correspond to the subtraction of the prediction from the original image. If the residual show details of the image structure, then the training went wrong or the network did not train for long enough.</p> <p> </p> <p>Fig 2.: The residuals are a good indicator of successful training. Top row: input image (left), prediction (center) and corresponding residuals (right) after one training epoch. The residuals show structures. Bottom row: same after 50 epochs, there is no visible structure that is not pure noise. CC-BY.</p> <p>Note that in the presence of Poisson noise (a.k.a shot noise), the amplitude of noise scales with the square root of the signal. Therefore, the residuals can still be modulated by the structures in the image, especially the bright ones. Please note that a property of the residuals is that the sum of its pixels should always be close to zero: even if some areas of the residuals look brighter, on closer inspection, you should see a lot of very dark pixels in this very region.</p>"},{"location":"algorithms/Noise2Void/#assessing-quality-by-training-multiple-networks","title":"Assessing quality by training multiple networks\u00b6","text":"<p>As Noise2Void training is stochastic, in order to assess whether structures are really in your image, you can perform different types of experiments:</p> <ul> <li>Split your images in several subsets and train independent networks on each of them. Compare the results.</li> <li>Train multiple randomly initialized networks on your training data and compare the results.</li> </ul>"},{"location":"algorithms/Noise2Void/#limitations","title":"Limitations\u00b6","text":""},{"location":"algorithms/Noise2Void/#pixel-wise-independent-noise","title":"Pixel-wise independent noise\u00b6","text":"<p>It might happen that the noise in your images is not pixel-wise independent. In this case, the network can learn the amount of noise in the masked pixels from its neighboring pixels. Noise2Void might then introduce small artefacts or simply reinforce the correlated pattern in the denoised image.</p> <p>Correlations in the noise can sometimes be observed with sCMOS camera or point-scanning microscopy methods. To estimate whether the noise is correlated, one can perform image autocorrelation and inspect the shape of the central peak in the distribution. The next figure shows two examples of the same image with artificial Gaussian noise. In the second one, we introduced a correlation in the Gaussian noise, leading to an horizontal line in the autocorrelation image.</p>"},{"location":"algorithms/Noise2Void/#checkerboard-artefacts","title":"Checkerboard artefacts\u00b6","text":"<p>Checkerboard artefacts [5] sometimes occur with Noise2Void. This effect is most prominent in the presence of salt and pepper noise or hot pixels in sCMOS cameras. They are characterized by small cross patterns in the denoised image (see figure 4). To reduce the presence of these artefacts, you can use N2V2.</p> <p> </p> <p>Fig 4.: Checkerboard artefacts. The figure shows a noisy image crop (left), a noisy close-up (center) and the corresponding Noise2Void prediction (right). While the denoising was effective, a distinctive checkerboard pattern nonetheless appears in the Noise2Void prediction. CC-BY.</p>"},{"location":"algorithms/Noise2Void/#references","title":"References\u00b6","text":"<p>[1] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[2] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p> <p>[3] Tim-Oliver Buchholz, Mangal Prakash, Deborah Schmidt, Alexander Krull, and Florian Jug. \"Denoiseg: joint denoising and segmentation.\" ECCV, 2020. link</p> <p>[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" MICCAI, 2015. link</p> <p>[5] Eva H\u00f6ck, Tim-Oliver Buchholz, Anselm Brachmann, Florian Jug, and Alexander Freytag. \"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture.\" ECCV, 2022. link</p>"},{"location":"algorithms/NoiseModels/","title":"Noise Models","text":""},{"location":"algorithms/NoiseModels/#noise-models","title":"Noise Models\u00b6","text":"<p>Noise models are an important aspect of denoising images using P(P)N2V, HDN or microSplit.</p> <p> </p>"},{"location":"algorithms/structN2V/","title":"structN2V","text":"<p>Fig 1.: Structured noise. Top row: image with uncorrelated noise (left) and its autocorrelation (close-up on the zero-shift component). Bottom row: same images with correlations introduced in the noise component. CC-BY.</p> <p>Fig 2.: Noise2Void vs structN2V manipulation, with a toy example (top row) and a real-world one (bottom row). In both algorithms, randomly selected pixels in the input patch (left) are selected and their value replaced. Noise2Void (center) replaces the values by the value of one of the neighboring pixel, while structN2V (right) additionally masks neighboring pixels using a uniformly sampled value comprised between the min and max patch value to prevent the network from learning from the noise correlations. CC-BY.</p>"},{"location":"algorithms/structN2V/#structn2v","title":"structN2V\u00b6","text":""},{"location":"algorithms/structN2V/#overview","title":"Overview\u00b6","text":"<p>structN2V [1] is a self-supervised denoising method based on Noise2Void [2,3]. The method was developed to remove pixel-noises from images, even in the presence of spatial noise correlations.</p> <p>It retains the same training scheme (i.e. training on noisy images by randomly masking pixels), and differs from Noise2Void by the use of an additional mask covering the extent of the spatial noise correlation.</p>"},{"location":"algorithms/structN2V/#pixel-wise-independent-noise","title":"Pixel-wise independent noise\u00b6","text":"<p>It might happen that the noise in your images is not pixel-wise independent. In this case, the network can learn the amount of noise in the masked pixels from its neighboring pixels. Noise2Void might then introduce small artefacts or simply reinforce the correlated pattern in the denoised image.</p> <p>Correlations in the noise can sometimes be observed with sCMOS camera or point-scanning microscopy methods. To estimate whether the noise is correlated, one can perform image autocorrelation and inspect the shape of the central peak in the distribution. The next figure shows two examples of the same image with artificial Gaussian noise. In the second one, we introduced a correlation in the Gaussian noise, leading to an horizontal line in the autocorrelation image.</p>"},{"location":"algorithms/structN2V/#n2v-vs-structn2v-masking","title":"N2V vs structN2V masking\u00b6","text":"<p>In order to obfuscate the pixel values whose noise might be correlated, structN2V introduces a mask spanning the extent of the correlation. In order to estimate the size and direction of the correlation, one simply needs to examine the autocorrelation image (see section above).</p> <p>However, as in Noise2Void, masking pixels by setting their values to zero will confuse the network by introducing unrealistic values. To circumvent this issue, structN2V replaces the masked pixels by values uniformly sampled between the minimum and maximum pixel values of the patch.</p>"},{"location":"algorithms/structN2V/#references","title":"References\u00b6","text":"<p>[1] Coleman Broaddus, Alexander Krull, Martin Weigert, Uwe Schmidt, and Gene Myers. \" Removing structured noise with self-supervised blind-spot networks.\" ISBI, 2020. link</p> <p>[2] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[3] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p>"},{"location":"applications/","title":"Applications","text":"<p>Click on your algorithm of choice to explore various applications. We collected the  algorithms based on the type of training data they require! </p>"},{"location":"applications/#keywords","title":"Keywords","text":"<ul> <li>no ground-truth: The algorithm trains without clean images.</li> <li>single image: The algorithm can train on a single image.</li> <li>pairs of noisy images: The algorithm requires pairs of noisy images.</li> <li>ground-truth: The algorithm requires pairs of clean and noisy images.</li> </ul>"},{"location":"applications/#denoising-noisy-images-without-clean-data","title":"Denoising noisy images without clean data","text":"<p>You have noisy images and no clean images? No problem! These algorithms can help you, as they do not require any ground-truth data. You can also train on a single image of reasonable size.</p> Noise2Void <p>                                         A self-supervised denoising algorithm based on a                                          pixel masking scheme.                                     </p> no ground-truth single image <p>If you have multiple noisy instances of the same structure (e.g. a noisy time-lapse),  then Noise2Noise might be the right choice for you.</p> Noise2Noise <p>                                         A supervised methods that can denoise images without                                         corresponding clean data.                                     </p> no ground-truth pairs of noisy images"},{"location":"applications/#supervised-restoration-with-clean-images","title":"Supervised restoration with clean images","text":"<p>If you have pairs of clean (e.g. high SNR, long exposure or high laser power) and noisy images, then CARE might be the right choice for you.</p> <p>Note that CARE can be used for a variety of tasks, such as denoising, deconvolution, isotropic resolution restoration or projection.</p> CARE <p>                                         The original supervised method to restore microscopy                                         images.                                     </p> ground-truth"},{"location":"applications/#using-the-lightning-api","title":"Using the Lightning API","text":"<p>If you need more control on the algorithm training, for instance to implement or replace features, you can use the Lightning API. </p> <p>It uses PyTorch Lightning and the  CAREamics Lightning components.</p> Lightning API <p>                                         Get full control of the training and prediction                                         pipelines by using CAREamics Lightning components.                                     </p>"},{"location":"applications/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Noise2Void<ul> <li>BSD68</li> <li>Flywing</li> <li>SEM</li> <li>Mouse Nuclei</li> </ul> </li> <li>Noise2Noise<ul> <li>SEM</li> </ul> </li> <li>CARE<ul> <li>denoising U2OS</li> </ul> </li> <li>Lightning_API<ul> <li>BSD68 N2V</li> </ul> </li> </ul>"},{"location":"applications/CARE/","title":"CARE","text":"<p>For more details on the algorithm, check out its description.</p> denoising U2OS <p>                                   Supervised denoising of nuclei imaged in 2D fluorescence with CARE.                               </p> 2D fluorescence nuclei <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/CARE/denoising_U2OS/","title":"denoising U2OS","text":"<p> Find me on Github </p> In\u00a0[\u00a0]: Copied! <pre># /// script\n# requires-python = \"&gt;=3.12\"\n# dependencies = [\n#     \"careamics[examples]&gt;=0.0.15\",\n# ]\n# ///\n</pre> # /// script # requires-python = \"&gt;=3.12\" # dependencies = [ #     \"careamics[examples]&gt;=0.0.15\", # ] # /// <p>The U2OS dataset is composed of pairs of noisy and high SNR nuclei images acquired in fluorescence microscopy. They were originally used in Weigert et al (2018) to showcase CARE denoising.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_care_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_care_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.CARE_U2OS.download(root_path)\n\n# path to the training data\nroot_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\"\ntrain_path = root_path / \"train\" / \"low\"\ntarget_path = root_path / \"train\" / \"GT\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.CARE_U2OS.download(root_path)  # path to the training data root_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\" train_path = root_path / \"train\" / \"low\" target_path = root_path / \"train\" / \"GT\" In\u00a0[5]: Copied! <pre># load training image and target, and show them side by side\ntrain_files = list(train_path.rglob(\"*.tif\"))\ntrain_files.sort()\n\ntarget_files = list(target_path.rglob(\"*.tif\"))\ntarget_files.sort()\n\n# select random example\nind = np.random.randint(len(train_files))\ntrain_image = tifffile.imread(train_files[ind])\ntrain_target = tifffile.imread(target_files[ind])\n\n# plot the two images and a crop\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[00].set_title(\"Training image\")\n\nax[1].imshow(train_target, cmap=\"gray\")\nax[1].set_title(\"Target image\")\n</pre> # load training image and target, and show them side by side train_files = list(train_path.rglob(\"*.tif\")) train_files.sort()  target_files = list(target_path.rglob(\"*.tif\")) target_files.sort()  # select random example ind = np.random.randint(len(train_files)) train_image = tifffile.imread(train_files[ind]) train_target = tifffile.imread(target_files[ind])  # plot the two images and a crop fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[00].set_title(\"Training image\")  ax[1].imshow(train_target, cmap=\"gray\") ax[1].set_title(\"Target image\") Out[5]: <pre>Text(0.5, 1.0, 'Target image')</pre> In\u00a0[6]: Copied! <pre>config = create_care_configuration(\n    experiment_name=\"care_U20S\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=(128, 128),\n    batch_size=32,\n    num_epochs=100,\n)\n\nprint(config)\n</pre> config = create_care_configuration(     experiment_name=\"care_U20S\",     data_type=\"tiff\",     axes=\"YX\",     patch_size=(128, 128),     batch_size=32,     num_epochs=100, )  print(config) <pre>{'algorithm_config': {'algorithm': 'care',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'tiff',\n                 'patch_size': [128, 128],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'care_U20S',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 100,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    train_target=target_path,\n    val_percentage=0.01,\n    val_minimum_split=20,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     train_target=target_path,     val_percentage=0.01,     val_minimum_split=20, ) <p>Show loss curves.</p> In\u00a0[13]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[13]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>test_path = root_path / \"test\" / \"low\"\n\nprediction = careamist.predict(source=test_path)\n</pre> test_path = root_path / \"test\" / \"low\"  prediction = careamist.predict(source=test_path) In\u00a0[9]: Copied! <pre># Show two images\ntest_GT_path = root_path / \"test\" / \"GT\"\ntest_GT_files = list(test_GT_path.rglob(\"*.tif\"))\ntest_GT_files.sort()\n\ntest_low_path = root_path / \"test\" / \"low\"\ntest_low_files = list(test_low_path.rglob(\"*.tif\"))\ntest_low_files.sort()\n\ntest_GT = [tifffile.imread(f) for f in test_GT_files]\ntest_low = [tifffile.imread(f) for f in test_low_files]\n\n# images to show\nimages = np.random.choice(range(len(test_GT)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 17))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n\n    psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])\n    psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)\n\n    ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images test_GT_path = root_path / \"test\" / \"GT\" test_GT_files = list(test_GT_path.rglob(\"*.tif\")) test_GT_files.sort()  test_low_path = root_path / \"test\" / \"low\" test_low_files = list(test_low_path.rglob(\"*.tif\")) test_low_files.sort()  test_GT = [tifffile.imread(f) for f in test_GT_files] test_low = [tifffile.imread(f) for f in test_low_files]  # images to show images = np.random.choice(range(len(test_GT)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 17)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()      psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])     psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)      ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[10]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, test_GT)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, test_GT)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") <pre>PSNR: 31.52 +/- 3.71\n</pre> In\u00a0[11]: Copied! <pre># create a cover image\nim_idx = 17\ncv_image_noisy = test_low[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros_like(cv_image_noisy, dtype=np.float32)\nwidth = cover.shape[1]\n\n# # normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : width // 2] = norm_noise[:, : width // 2]\ncover[:, width // 2 :] = norm_pred[:, width // 2 :]\n\n# plot the single image\nplt.imshow(cover)\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"U2OS_CARE.jpeg\")\n</pre> # create a cover image im_idx = 17 cv_image_noisy = test_low[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros_like(cv_image_noisy, dtype=np.float32) width = cover.shape[1]  # # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : width // 2] = norm_noise[:, : width // 2] cover[:, width // 2 :] = norm_pred[:, width // 2 :]  # plot the single image plt.imshow(cover)  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"U2OS_CARE.jpeg\")"},{"location":"applications/CARE/denoising_U2OS/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The CARE U2OS dataset is composed of thousands of examples organized in train and test:</p> <ul> <li>train<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> <li>test<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> </ul>"},{"location":"applications/CARE/denoising_U2OS/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/CARE/denoising_U2OS/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/CARE/denoising_U2OS/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/CARE/denoising_U2OS/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training, and we can use the test set.</p>"},{"location":"applications/CARE/denoising_U2OS/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Lightning_API/","title":"Lightning_API","text":"BSD68 N2V <p>                                   Example of Noise2Void using the Lightning API with 2D natural images and synthetic noise.                               </p> 2D benchmark N2V <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/","title":"BSD68 N2V","text":"<p> Find me on Github </p> <p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of Noise2Void using the Lightning API of CAREamics.</p> <p>This API gives you more freedom to customize the training by using wrappers around the main elements of CAREamics: the datasets and the lightning module.</p> In\u00a0[30]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom PIL import Image\nfrom careamics.lightning import (\n    create_careamics_module,\n    create_predict_datamodule,\n    create_train_datamodule,\n)\nfrom careamics.config.support import SupportedTransform\nfrom careamics.prediction_utils import convert_outputs\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom careamics.utils.lightning_utils import read_csv_logger\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import numpy as np import tifffile from PIL import Image from careamics.lightning import (     create_careamics_module,     create_predict_datamodule,     create_train_datamodule, ) from careamics.config.support import SupportedTransform from careamics.prediction_utils import convert_outputs from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from careamics.utils.lightning_utils import read_csv_logger from pytorch_lightning import Trainer from pytorch_lightning.callbacks import ModelCheckpoint In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[14]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[14]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[15]: Copied! <pre>model = create_careamics_module(\n    algorithm=\"n2v\",\n    loss=\"n2v\",\n    architecture=\"UNet\",\n    model_parameters={\"n2v2\": False},\n)\n</pre> model = create_careamics_module(     algorithm=\"n2v\",     loss=\"n2v\",     architecture=\"UNet\",     model_parameters={\"n2v2\": False}, ) In\u00a0[16]: Copied! <pre>train_data_module = create_train_datamodule(\n    train_data=train_path,\n    val_data=val_path,\n    data_type=\"tiff\",\n    patch_size=(64, 64),\n    axes=\"SYX\",\n    batch_size=64,\n    transforms=[\n        { # you can delete a transform here to not apply it\n            \"name\": SupportedTransform.XY_FLIP.value,\n            \"flip_x\": True, # you can set parameters\n            \"flip_y\": True,\n        },\n        {\n            \"name\": SupportedTransform.XY_RANDOM_ROTATE90.value,\n        },\n    ],\n)\n</pre> train_data_module = create_train_datamodule(     train_data=train_path,     val_data=val_path,     data_type=\"tiff\",     patch_size=(64, 64),     axes=\"SYX\",     batch_size=64,     transforms=[         { # you can delete a transform here to not apply it             \"name\": SupportedTransform.XY_FLIP.value,             \"flip_x\": True, # you can set parameters             \"flip_y\": True,         },         {             \"name\": SupportedTransform.XY_RANDOM_ROTATE90.value,         },     ], ) In\u00a0[\u00a0]: remove_output Copied! <pre># Create Callbacks\nroot = Path(\"bsd68_n2v\")\ncallbacks = [\n    ModelCheckpoint(\n        dirpath=root / \"checkpoints\",\n        filename=\"bsd68_lightning_api\",\n        save_last=True,\n    )\n]\n\n# Create a Lightning Trainer\ntrainer = Trainer(max_epochs=100, default_root_dir=root, callbacks=callbacks)\n\n# Train the model\ntrainer.fit(model, datamodule=train_data_module)\n</pre> # Create Callbacks root = Path(\"bsd68_n2v\") callbacks = [     ModelCheckpoint(         dirpath=root / \"checkpoints\",         filename=\"bsd68_lightning_api\",         save_last=True,     ) ]  # Create a Lightning Trainer trainer = Trainer(max_epochs=100, default_root_dir=root, callbacks=callbacks)  # Train the model trainer.fit(model, datamodule=train_data_module) <p>Plot losses.</p> In\u00a0[\u00a0]: remove_output Copied! <pre>means, stds = train_data_module.get_data_statistics()\npred_data_module = create_predict_datamodule(\n    pred_data=test_path,\n    data_type=\"tiff\",\n    axes=\"YX\",\n    batch_size=1,\n    tta_transforms=True,\n    image_means=means,\n    image_stds=stds,\n    tile_size=(128, 128),\n    tile_overlap=(32, 32),\n)\n</pre> means, stds = train_data_module.get_data_statistics() pred_data_module = create_predict_datamodule(     pred_data=test_path,     data_type=\"tiff\",     axes=\"YX\",     batch_size=1,     tta_transforms=True,     image_means=means,     image_stds=stds,     tile_size=(128, 128),     tile_overlap=(32, 32), ) In\u00a0[\u00a0]: remove_output Copied! <pre># Predict\nprediction = trainer.predict(model, datamodule=pred_data_module)\n\n# Convert the outputs to the original format, mostly useful if tiling is used\nprediction = convert_outputs(prediction, tiled=True)\n</pre> # Predict prediction = trainer.predict(model, datamodule=pred_data_module)  # Convert the outputs to the original format, mostly useful if tiling is used prediction = convert_outputs(prediction, tiled=True) In\u00a0[31]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])\n    psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()     psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])     psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[33]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 26.30 +/- 2.64\nReported PSNR: 27.71\n</pre> In\u00a0[34]: Copied! <pre># create a cover image\nim_idx = 3\ncv_image_noisy = noises[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))   \n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min())\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())\n\n# fill in halves\ncover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2]\ncover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"BSD68_Noise2Void_lightning_api.jpeg\")\n</pre> # create a cover image im_idx = 3 cv_image_noisy = noises[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256))    (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min()) norm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())  # fill in halves cover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2] cover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"BSD68_Noise2Void_lightning_api.jpeg\")"},{"location":"applications/Lightning_API/BSD68_N2V/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#train-with-the-careamics-lightning-api","title":"Train with the CAREamics Lightning API\u00b6","text":"<p>Using the Lightning API of CAREamics, you need to instantiate the lightning module, the data module and the trainer yourself.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-lightning-module","title":"Create the Lightning module\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-data-module","title":"Create the data module\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-trainer","title":"Create the trainer\u00b6","text":"<p>Note that here we modify the prediction loop, but this will be  changed in the near future.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#predict-with-careamics-lightning-api","title":"Predict with CAREamics Lightning API\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#define-the-prediction-datamodule","title":"Define the prediction datamodule\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Noise/","title":"Noise2Noise","text":"<p>For more details on the algorithm, check out its description.</p> SEM <p>                                   Denoising of 2D scanning electron microscopy with Noise2Noise.                               </p> 2D electron microscopy <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Noise2Noise/SEM/","title":"SEM","text":"<p> Find me on Github </p> <p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Noise on this particular dataset!</p> In\u00a0[2]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport tifffil\nimport numpy as np\nfrom PIL import Image\n\nfrom careamics import CAREamist\nfrom careamics.config import create_n2n_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import tifffil import numpy as np from PIL import Image  from careamics import CAREamist from careamics.config import create_n2n_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.N2N_SEM.download(root_path)\nfiles = [f for f in download if f.endswith(\"tif\")]\nfiles.sort()\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.N2N_SEM.download(root_path) files = [f for f in download if f.endswith(\"tif\")] files.sort() In\u00a0[5]: Copied! <pre># load training and target image and show them side by side\ntrain_stack = tifffile.imread(files[1])\n\n# use the 1 us scan time to perform Noise2Noise\ntrain_image = train_stack[2]\ntrain_target = train_stack[3]\n\n# plot the two images and a crop\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].set_title(\"Training image (1 us)\")\n\nax[0, 1].imshow(train_target, cmap=\"gray\")\nax[0, 1].set_title(\"Target image (1 us)\")\n\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 0].set_title(\"Training crop (1 us)\")\n\nax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].set_title(\"Target crop (1 us)\")\n</pre> # load training and target image and show them side by side train_stack = tifffile.imread(files[1])  # use the 1 us scan time to perform Noise2Noise train_image = train_stack[2] train_target = train_stack[3]  # plot the two images and a crop fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].set_title(\"Training image (1 us)\")  ax[0, 1].imshow(train_target, cmap=\"gray\") ax[0, 1].set_title(\"Target image (1 us)\")  x_start, x_end = 600, 850 y_start, y_end = 200, 450 ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 0].set_title(\"Training crop (1 us)\")  ax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].set_title(\"Target crop (1 us)\") Out[5]: <pre>Text(0.5, 1.0, 'Target crop (1 us)')</pre> In\u00a0[6]: Copied! <pre>config = create_n2n_configuration(\n    experiment_name=\"sem_n2n\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=50,\n)\n\nprint(config)\n</pre> config = create_n2n_configuration(     experiment_name=\"sem_n2n\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=50, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2n',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 64,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'sem_n2n',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 50,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    train_target=train_target,\n    val_minimum_split=5,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     train_target=train_target,     val_minimum_split=5, ) <p>Plot the losses.</p> In\u00a0[8]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[8]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[10]: Copied! <pre># get pseudo ground-truth from the 5 us averaged scan time\npseudo_gt = train_stack[-1]\npsnr_noisy = scale_invariant_psnr(pseudo_gt, train_image)\npsnr_pred = scale_invariant_psnr(pseudo_gt, prediction[0].squeeze())\n\n# Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")\n\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")\n\nax[0, 2].imshow(pseudo_gt, cmap=\"gray\")\nax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")\n\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # get pseudo ground-truth from the 5 us averaged scan time pseudo_gt = train_stack[-1] psnr_noisy = scale_invariant_psnr(pseudo_gt, train_image) psnr_pred = scale_invariant_psnr(pseudo_gt, prediction[0].squeeze())  # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 3, figsize=(15, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")  ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")  ax[0, 2].imshow(pseudo_gt, cmap=\"gray\") ax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")  ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[10]: <pre>&lt;matplotlib.image.AxesImage at 0x7f566aa20d70&gt;</pre> In\u00a0[11]: Copied! <pre># create a cover image\nx_start, width = 500, 512\ny_start, height = 1400, 512\n\n# create image\ncover = np.zeros((height, width))   \n\n# normalize train and prediction\nnorm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())\n\npred = prediction[0].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2]\ncover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"SEM_Noise2Noise.jpeg\")\n</pre> # create a cover image x_start, width = 500, 512 y_start, height = 1400, 512  # create image cover = np.zeros((height, width))     # normalize train and prediction norm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())  pred = prediction[0].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2] cover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"SEM_Noise2Noise.jpeg\")"},{"location":"applications/Noise2Noise/SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The N2N SEM dataset consists of EM images with 7 different levels of noise:</p> <ul> <li>Image 0 is recorded with 0.2 us scan time</li> <li>Image 1 is recorded with 0.5 us scan time</li> <li>Image 2 is recorded with 1 us scan time</li> <li>Image 3 is recorded with 1 us scan time</li> <li>Image 4 is recorded with 2.1 us scan time</li> <li>Image 5 is recorded with 5.0 us scan time</li> <li>Image 6 is recorded with 5.0 us scan time and is the avg. of 4 images</li> </ul>"},{"location":"applications/Noise2Noise/SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Noise/SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Noise/SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Noise/SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Noise/SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Noise/SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Noise/SEM/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/","title":"Noise2Void","text":"<p>For more details on the algorithm, check out its description.</p> Mouse Nuclei <p>                                   Denoising of 2D fluorescence images of nuclei with Noise2Void.                               </p> 2D fluorescence SEM <p>                                   Denoising of 2D scanning electron microscopy with N2V(2).                               </p> 2D electron microscopy Flywing <p>                                   Denoising of 3D tissue using Noise2Void.                               </p> 3D fluorescence tissue BSD68 <p>                                   Benchmark Noise2Void in 2D using natural images and synthetic noise.                               </p> 2D benchmark <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Noise2Void/BSD68/","title":"BSD68","text":"<p> Find me on Github </p> <p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of Noise2Void and N2V2.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[2]: Copied! <pre>use_gpu  = \"yes\" if len([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]) &gt; 0 else \"no\"\nprint(f\"Using GPU: {use_gpu}\")\n</pre> use_gpu  = \"yes\" if len([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]) &gt; 0 else \"no\" print(f\"Using GPU: {use_gpu}\") <pre>Using GPU: yes\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[4]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[4]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[5]: Copied! <pre>use_n2v2 = False\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=f\"bsd68_{algo}\",\n    data_type=\"tiff\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=100,\n    use_n2v2=use_n2v2\n)\n\nprint(config)\n</pre> use_n2v2 = False algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=f\"bsd68_{algo}\",     data_type=\"tiff\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=100,     use_n2v2=use_n2v2 )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'n2v_config': {'masked_pixel_percentage': 0.2,\n                                     'name': 'N2VManipulate',\n                                     'remove_center': True,\n                                     'roi_size': 11,\n                                     'strategy': 'uniform',\n                                     'struct_mask_axis': 'none',\n                                     'struct_mask_span': 5},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 64,\n                 'data_type': 'tiff',\n                 'patch_size': [64, 64],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'bsd68_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 100,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    val_source=val_path,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     val_source=val_path, ) <p>Plot loss.</p> In\u00a0[7]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[7]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=test_path,\n    axes=\"YX\",\n    tile_size=(128, 128),\n    tile_overlap=(48, 48),\n    batch_size=1,\n)\n</pre> prediction = careamist.predict(     source=test_path,     axes=\"YX\",     tile_size=(128, 128),     tile_overlap=(48, 48),     batch_size=1, ) In\u00a0[11]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])\n    psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()     psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])     psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[12]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 26.30 +/- 2.65\nReported PSNR: 27.71\n</pre> In\u00a0[13]: Copied! <pre># create a cover image\nim_idx = 3\ncv_image_noisy = noises[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))\n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : 256 // 2] = norm_noise[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2\n]\ncover[:, 256 // 2 :] = norm_pred[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(f\"BSD68_{algo}.jpeg\")\n</pre> # create a cover image im_idx = 3 cv_image_noisy = noises[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256)) (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : 256 // 2] = norm_noise[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2 ] cover[:, 256 // 2 :] = norm_pred[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2 ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(f\"BSD68_{algo}.jpeg\") In\u00a0[15]: Copied! <pre>general_description = (\n    \"This model is a UNet trained using the N2V algorithm to denoise images. The\"\n    \"data is a benchmark used in the original paper.\"\n    \"https://careamics.github.io/0.1/applications/Noise2Void/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained using the N2V algorithm to denoise images. The\"     \"data is a benchmark used in the original paper.\"     \"https://careamics.github.io/0.1/applications/Noise2Void/.\" ) print(general_description) <pre>This model is a UNet trained using the N2V algorithm to denoise images. Thedata is a benchmark used in the original paper.https://careamics.github.io/0.1/applications/Noise2Void/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=f\"bsd68{algo}_model.zip\",\n    friendly_model_name=f\"BSD68_{algo}\",\n    input_array=noises[im_idx][np.newaxis, ..., :256, :256],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=portfolio.denoising.N2V_BSD68.description\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=f\"bsd68{algo}_model.zip\",     friendly_model_name=f\"BSD68_{algo}\",     input_array=noises[im_idx][np.newaxis, ..., :256, :256],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=portfolio.denoising.N2V_BSD68.description )"},{"location":"applications/Noise2Void/BSD68/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/BSD68/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/BSD68/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions. The switch between Noise2Void and N2V2 is done via the <code>use_n2v2</code> parameter.</p>"},{"location":"applications/Noise2Void/BSD68/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/BSD68/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training.</p>"},{"location":"applications/Noise2Void/BSD68/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/","title":"Flywing","text":"<p> Find me on Github </p> <p>The Flywing dataset is composed of a single 3D fluorescence microscopy stack. Here, we demonstrate the performances of Noise2Void on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils import autocorrelation\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils import autocorrelation from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfile = portfolio.denoising.Flywing.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") file = portfolio.denoising.Flywing.download(root_path) In\u00a0[8]: Copied! <pre># load stack\ntrain_image = tifffile.imread(file[0])\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(train_image[10])\nax[0].set_title(\"Slice 10\")\nax[1].imshow(train_image[20])\nax[1].set_title(\"Slice 20\")\nax[2].imshow(train_image[30])\nax[2].set_title(\"Slice 30\")\n</pre> # load stack train_image = tifffile.imread(file[0]) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].imshow(train_image[10]) ax[0].set_title(\"Slice 10\") ax[1].imshow(train_image[20]) ax[1].set_title(\"Slice 20\") ax[2].imshow(train_image[30]) ax[2].set_title(\"Slice 30\") <pre>Image shape: (35, 520, 692)\n</pre> Out[8]: <pre>Text(0.5, 1.0, 'Slice 30')</pre> In\u00a0[9]: Copied! <pre>slice_idx = 10\nautocorr = autocorrelation(train_image[slice_idx])\n\n# crop the correlation around (0, 0)\nmidpoint = train_image[slice_idx].shape[0] // 2\ncrop_size = 128\nslices = (\n    slice(midpoint - crop_size, midpoint + crop_size),\n    slice(midpoint - crop_size, midpoint + crop_size),\n)\n# plot autocorrelation\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nax[0].imshow(train_image[slice_idx])\nax[0].set_title(\"Image 1\")\nax[1].imshow(autocorr[slices], cmap=\"gray\")\nax[1].set_title(\"Autocorrelation\")\n</pre> slice_idx = 10 autocorr = autocorrelation(train_image[slice_idx])  # crop the correlation around (0, 0) midpoint = train_image[slice_idx].shape[0] // 2 crop_size = 128 slices = (     slice(midpoint - crop_size, midpoint + crop_size),     slice(midpoint - crop_size, midpoint + crop_size), ) # plot autocorrelation fig, ax = plt.subplots(1, 2, figsize=(15, 5)) ax[0].imshow(train_image[slice_idx]) ax[0].set_title(\"Image 1\") ax[1].imshow(autocorr[slices], cmap=\"gray\") ax[1].set_title(\"Autocorrelation\") Out[9]: <pre>Text(0.5, 1.0, 'Autocorrelation')</pre> In\u00a0[11]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"flywing_n2v\",\n    data_type=\"array\",\n    axes=\"ZYX\",\n    patch_size=(16, 64, 64),\n    batch_size=2,\n    num_epochs=50,\n    augmentations=[],  # remove augmentations\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"flywing_n2v\",     data_type=\"array\",     axes=\"ZYX\",     patch_size=(16, 64, 64),     batch_size=2,     num_epochs=50,     augmentations=[],  # remove augmentations )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 3,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'n2v_config': {'masked_pixel_percentage': 0.2,\n                                     'name': 'N2VManipulate',\n                                     'remove_center': True,\n                                     'roi_size': 11,\n                                     'strategy': 'uniform',\n                                     'struct_mask_axis': 'none',\n                                     'struct_mask_span': 5},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'ZYX',\n                 'batch_size': 2,\n                 'data_type': 'array',\n                 'patch_size': [16, 64, 64],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'flywing_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'lightning_trainer_config': {'max_epochs': 50}},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[13]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[13]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(32, 128, 128),\n    tile_overlap=(8, 48, 48),\n    batch_size=1,\n    tta=False,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(32, 128, 128),     tile_overlap=(8, 48, 48),     batch_size=1,     tta=False, ) In\u00a0[15]: Copied! <pre>pred_folder = Path(\"results_n2v\")\npred_folder.mkdir(exist_ok=True, parents=True)\n\ntifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0])\n</pre> pred_folder = Path(\"results_n2v\") pred_folder.mkdir(exist_ok=True, parents=True)  tifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0]) In\u00a0[16]: Copied! <pre># Show multiple slices\nzs = [5, 10, 15, 20, 25, 30]\n\nfig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs)))\nfor i, z in enumerate(zs):\n    ax[i, 0].imshow(train_image[z])\n    ax[i, 0].set_title(f\"Noisy - Plane {z}\")\n\n    ax[i, 1].imshow(prediction[0].squeeze()[z])\n    ax[i, 1].set_title(f\"Prediction - Plane {z}\")\n</pre> # Show multiple slices zs = [5, 10, 15, 20, 25, 30]  fig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs))) for i, z in enumerate(zs):     ax[i, 0].imshow(train_image[z])     ax[i, 0].set_title(f\"Noisy - Plane {z}\")      ax[i, 1].imshow(prediction[0].squeeze()[z])     ax[i, 1].set_title(f\"Prediction - Plane {z}\") In\u00a0[17]: Copied! <pre># create a cover image\nim_idx = 20\nx_min = 0\ny_min = 0\nsize = 256\n\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[0].squeeze()[im_idx]\n\n# create image\ncover = np.zeros((size, size))\n(height, width) = cv_image_noisy.shape\nassert height &gt; size\nassert width &gt; size\nassert y_min + size &lt; height\nassert x_min + size &lt; width\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : size // 2] = norm_noise[y_min : y_min + size, x_min : x_min + size // 2]\ncover[:, size // 2 :] = norm_pred[y_min : y_min + size, x_min : x_min + size // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image (saturate for rendering)\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\n\nim.save(\"Flywing_N2V.jpeg\")\n</pre> # create a cover image im_idx = 20 x_min = 0 y_min = 0 size = 256  cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[0].squeeze()[im_idx]  # create image cover = np.zeros((size, size)) (height, width) = cv_image_noisy.shape assert height &gt; size assert width &gt; size assert y_min + size &lt; height assert x_min + size &lt; width  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : size // 2] = norm_noise[y_min : y_min + size, x_min : x_min + size // 2] cover[:, size // 2 :] = norm_pred[y_min : y_min + size, x_min : x_min + size // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image (saturate for rendering) im = Image.fromarray(cover * 255) im = im.convert(\"L\")  im.save(\"Flywing_N2V.jpeg\")"},{"location":"applications/Noise2Void/Flywing/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/Flywing/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#compute-autocorrelation","title":"Compute autocorrelation\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/Flywing/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/Flywing/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/Flywing/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/Flywing/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/","title":"Mouse Nuclei","text":"<p> Find me on Github </p> <p>The mouse nuclei dataset is composed of a train and test dataset.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code import torch import matplotlib.pyplot as plt import numpy as np from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[2]: Copied! <pre>use_gpu  = \"yes\" if len([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]) &gt; 0 else \"no\"\nprint(f\"Using GPU: {use_gpu}\")\n</pre> use_gpu  = \"yes\" if len([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())]) &gt; 0 else \"no\" print(f\"Using GPU: {use_gpu}\") <pre>Using GPU: yes\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># download file\nportfolio = PortfolioManager()\nfiles = portfolio.denoiseg.MouseNuclei_n20.download()\nfiles.sort()\n\n# load images\ntrain_data = np.load(files[1])[\"X_train\"]\nprint(f\"Train data shape: {train_data.shape}\")\n</pre> # download file portfolio = PortfolioManager() files = portfolio.denoiseg.MouseNuclei_n20.download() files.sort()  # load images train_data = np.load(files[1])[\"X_train\"] print(f\"Train data shape: {train_data.shape}\") In\u00a0[4]: Copied! <pre>portfolio.denoiseg.MouseNuclei_n20.description\n</pre> portfolio.denoiseg.MouseNuclei_n20.description Out[4]: <pre>'A dataset depicting diverse and non-uniformly clustered nuclei in the mouse skull, consisting of 908 training and 160 validation patches. The test set counts 67 additional images'</pre> In\u00a0[5]: Copied! <pre>indices = [34, 293, 571, 783]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].imshow(train_data[indices[0]], cmap=\"gray\")\nax[0, 0].set_title(f\"Image {indices[0]}\")\nax[0, 0].set_xticks([])\nax[0, 0].set_yticks([])\n\nax[0, 1].imshow(train_data[indices[1]], cmap=\"gray\")\nax[0, 1].set_title(f\"Image {indices[1]}\")\nax[0, 1].set_xticks([])\nax[0, 1].set_yticks([])\n\nax[1, 0].imshow(train_data[indices[2]], cmap=\"gray\")\nax[1, 0].set_title(f\"Image {indices[2]}\")\nax[1, 0].set_xticks([])\nax[1, 0].set_yticks([])\n\nax[1, 1].imshow(train_data[indices[3]], cmap=\"gray\")\nax[1, 1].set_title(f\"Image {indices[3]}\")\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\n\nplt.show()\n</pre> indices = [34, 293, 571, 783]  fig, ax = plt.subplots(2, 2, figsize=(8, 8)) ax[0, 0].imshow(train_data[indices[0]], cmap=\"gray\") ax[0, 0].set_title(f\"Image {indices[0]}\") ax[0, 0].set_xticks([]) ax[0, 0].set_yticks([])  ax[0, 1].imshow(train_data[indices[1]], cmap=\"gray\") ax[0, 1].set_title(f\"Image {indices[1]}\") ax[0, 1].set_xticks([]) ax[0, 1].set_yticks([])  ax[1, 0].imshow(train_data[indices[2]], cmap=\"gray\") ax[1, 0].set_title(f\"Image {indices[2]}\") ax[1, 0].set_xticks([]) ax[1, 0].set_yticks([])  ax[1, 1].imshow(train_data[indices[3]], cmap=\"gray\") ax[1, 1].set_title(f\"Image {indices[3]}\") ax[1, 1].set_xticks([]) ax[1, 1].set_yticks([])  plt.show() In\u00a0[6]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"mouse_nuclei_n2v\",\n    data_type=\"array\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=16,\n    num_epochs=10,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"mouse_nuclei_n2v\",     data_type=\"array\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=16,     num_epochs=10, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'n2v_config': {'masked_pixel_percentage': 0.2,\n                                     'name': 'N2VManipulate',\n                                     'remove_center': True,\n                                     'roi_size': 11,\n                                     'strategy': 'uniform',\n                                     'struct_mask_axis': 'none',\n                                     'struct_mask_span': 5},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 16,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'mouse_nuclei_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 10,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_data,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_data, ) In\u00a0[8]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[8]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_data)\n</pre> prediction = careamist.predict(source=train_data) In\u00a0[10]: Copied! <pre># download ground truth\nfiles = portfolio.denoiseg.MouseNuclei_n0.download()\nfiles.sort()\n\n# load images\ngt_data = np.load(files[1])[\"X_train\"]\nprint(f\"GT data shape: {gt_data.shape}\")\n</pre> # download ground truth files = portfolio.denoiseg.MouseNuclei_n0.download() files.sort()  # load images gt_data = np.load(files[1])[\"X_train\"] print(f\"GT data shape: {gt_data.shape}\") <pre>GT data shape: (908, 128, 128)\n</pre> In\u00a0[11]: Copied! <pre>indices = [389, 621]\n\nfor i in indices:\n    # compute psnr\n    psnr_noisy = scale_invariant_psnr(gt_data[i], train_data[i])\n    psnr_denoised = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())\n\n    # plot images\n    fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n    ax[0].imshow(train_data[i], cmap=\"gray\")\n    ax[0].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}\")\n    ax[0].set_xticks([])\n    ax[0].set_yticks([])\n\n    ax[1].imshow(prediction[i].squeeze(), cmap=\"gray\")\n    ax[1].set_title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}\")\n    ax[1].set_xticks([])\n    ax[1].set_yticks([])\n\n    ax[2].imshow(gt_data[i], cmap=\"gray\")\n    ax[2].set_title(\"GT Image\")\n    ax[2].set_xticks([])\n    ax[2].set_yticks([])\n\n    plt.show()\n</pre> indices = [389, 621]  for i in indices:     # compute psnr     psnr_noisy = scale_invariant_psnr(gt_data[i], train_data[i])     psnr_denoised = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())      # plot images     fig, ax = plt.subplots(1, 3, figsize=(10, 10))     ax[0].imshow(train_data[i], cmap=\"gray\")     ax[0].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}\")     ax[0].set_xticks([])     ax[0].set_yticks([])      ax[1].imshow(prediction[i].squeeze(), cmap=\"gray\")     ax[1].set_title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}\")     ax[1].set_xticks([])     ax[1].set_yticks([])      ax[2].imshow(gt_data[i], cmap=\"gray\")     ax[2].set_title(\"GT Image\")     ax[2].set_xticks([])     ax[2].set_yticks([])      plt.show() In\u00a0[12]: Copied! <pre>psnrs = np.zeros(gt_data.shape[0])\n\nfor i in range(gt_data.shape[0]):\n    psnrs[i] = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())\n\nprint(f\"PSNR: {np.mean(psnrs):.2f} \u00b1 {np.std(psnrs):.2f}\")\n</pre> psnrs = np.zeros(gt_data.shape[0])  for i in range(gt_data.shape[0]):     psnrs[i] = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())  print(f\"PSNR: {np.mean(psnrs):.2f} \u00b1 {np.std(psnrs):.2f}\") <pre>PSNR: 32.03 \u00b1 2.15\n</pre> In\u00a0[13]: Copied! <pre># create a cover image\nindex_cover = 5\nheight, width = 128, 128\n\n# create image\ncover = np.zeros((height, width))\n\n# normalize train and prediction\nnorm_train = (train_data[index_cover] - train_data[index_cover].min()) / (\n    train_data[index_cover].max() - train_data[index_cover].min()\n)\n\npred = prediction[index_cover].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, : width // 2] = norm_train[:height, : width // 2]\ncover[:, width // 2 :] = norm_pred[:height, width // 2 :]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"MouseNuclei_Noise2Void.jpeg\")\n</pre> # create a cover image index_cover = 5 height, width = 128, 128  # create image cover = np.zeros((height, width))  # normalize train and prediction norm_train = (train_data[index_cover] - train_data[index_cover].min()) / (     train_data[index_cover].max() - train_data[index_cover].min() )  pred = prediction[index_cover].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, : width // 2] = norm_train[:height, : width // 2] cover[:, width // 2 :] = norm_pred[:height, width // 2 :]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"MouseNuclei_Noise2Void.jpeg\") In\u00a0[14]: Copied! <pre>general_description = (\n    \"This model is a UNet trained with the Noise2Void algorithm to denoise images. \"\n    \"The training data consists of images of non-uniformly clustered nuclei in the \"\n    \"mouse skull. The notebook used to train this model is available on the CAREamics \"\n    \"documentation website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained with the Noise2Void algorithm to denoise images. \"     \"The training data consists of images of non-uniformly clustered nuclei in the \"     \"mouse skull. The notebook used to train this model is available on the CAREamics \"     \"documentation website at the following link: \"     \"https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\" ) print(general_description) <pre>This model is a UNet trained with the Noise2Void algorithm to denoise images. The training data consists of images of non-uniformly clustered nuclei in the mouse skull. The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"mousenuclei_n2v_model.zip\",\n    friendly_model_name=\"MouseNuclei_N2V\",\n    input_array=train_data[[indices[0]]].astype(np.float32),\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    data_description=portfolio.denoiseg.MouseNuclei_n20.description,\n    general_description=general_description\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"mousenuclei_n2v_model.zip\",     friendly_model_name=\"MouseNuclei_N2V\",     input_array=train_data[[indices[0]]].astype(np.float32),     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     data_description=portfolio.denoiseg.MouseNuclei_n20.description,     general_description=general_description )"},{"location":"applications/Noise2Void/Mouse_Nuclei/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"applications/Noise2Void/SEM/","title":"SEM","text":"<p> Find me on Github </p> In\u00a0[12]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import torch import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_SEM.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_SEM.download(root_path) In\u00a0[15]: Copied! <pre>portfolio.denoising.N2V_SEM.description\n</pre> portfolio.denoising.N2V_SEM.description Out[15]: <pre>'Cropped images from a SEM dataset from T.-O. Buchholz et al (Methods Cell Biol, 2020).'</pre> In\u00a0[16]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(files[0])\nval_image = tifffile.imread(files[1])\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(files[0]) val_image = tifffile.imread(files[1])  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[16]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[17]: Copied! <pre>use_n2v2 = False\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=f\"sem_{algo}\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=30,\n    use_n2v2=use_n2v2,\n)\n\nprint(config)\n</pre> use_n2v2 = False algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=f\"sem_{algo}\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=30,     use_n2v2=use_n2v2, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1,\n                                'use_batch_norm': True},\n                      'n2v_config': {'masked_pixel_percentage': 0.2,\n                                     'name': 'N2VManipulate',\n                                     'remove_center': True,\n                                     'roi_size': 11,\n                                     'strategy': 'uniform',\n                                     'struct_mask_axis': 'none',\n                                     'struct_mask_span': 5},\n                      'optimizer': {'name': 'Adam', 'parameters': {}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'train_dataloader_params': {'num_workers': 4,\n                                             'pin_memory': True,\n                                             'shuffle': True},\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}],\n                 'val_dataloader_params': {'num_workers': 4,\n                                           'pin_memory': True}},\n 'experiment_name': 'sem_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 30,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_source=val_image,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_source=val_image, ) <p>Show loss.</p> In\u00a0[19]: Copied! <pre>loss_dict = careamist.get_losses()\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</pre> loss_dict = careamist.get_losses() plt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"]) plt.legend([\"Train loss\", \"Val loss\"]) plt.title(\"Losses\") <pre>{'epoch': [], 'learning_rate': [], 'step': [], 'train_loss_epoch': [], 'train_loss_step': [], 'val_loss': []}\n</pre> Out[19]: <pre>Text(0.5, 1.0, 'Losses')</pre> In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[9]: Copied! <pre># Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[9]: <pre>&lt;matplotlib.image.AxesImage at 0x7fd04b230980&gt;</pre> In\u00a0[20]: Copied! <pre># create a cover image\nx_start, width = 500, 512\ny_start, height = 1400, 512\n\n# create image\ncover = np.zeros((height, width))\n\n# normalize train and prediction\nnorm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())\n\npred = prediction[0].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, : width // 2] = norm_train[\n    y_start : y_start + height, x_start : x_start + width // 2\n]\ncover[:, width // 2 :] = norm_pred[\n    y_start : y_start + height, x_start + width // 2 : x_start + width\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(f\"SEM_{algo}.jpeg\")\n</pre> # create a cover image x_start, width = 500, 512 y_start, height = 1400, 512  # create image cover = np.zeros((height, width))  # normalize train and prediction norm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())  pred = prediction[0].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, : width // 2] = norm_train[     y_start : y_start + height, x_start : x_start + width // 2 ] cover[:, width // 2 :] = norm_pred[     y_start : y_start + height, x_start + width // 2 : x_start + width ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(f\"SEM_{algo}.jpeg\") In\u00a0[2]: Copied! <pre>general_description = (\n    \"This model is a UNet trained using the Noise2Void algorithm to denoise images.\"\n    \"The training data consists of crops \"\n    \"from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The \"\n    \"notebook used to train this model is available on the CAREamics documentation \"\n    \"website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/N2V2/SEM/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained using the Noise2Void algorithm to denoise images.\"     \"The training data consists of crops \"     \"from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The \"     \"notebook used to train this model is available on the CAREamics documentation \"     \"website at the following link: \"     \"https://careamics.github.io/0.1/applications/N2V2/SEM/.\" ) print(general_description) <pre>This model is a UNet trained using the Noise2Void algorithm to denoise images.The training data consists of crops from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/N2V2/SEM/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\",\ncareamist.export_to_bmz(\n    path_to_archive=f\"sem_{algo}_model.zip\",\n    friendly_model_name=f\"SEM_{algo}\",\n    input_array=train_image[1400:1656, 500:756],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=portfolio.denoising.N2V_SEM.description\n)\n</pre> # Export the model\", careamist.export_to_bmz(     path_to_archive=f\"sem_{algo}_model.zip\",     friendly_model_name=f\"SEM_{algo}\",     input_array=train_image[1400:1656, 500:756],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=portfolio.denoising.N2V_SEM.description )"},{"location":"applications/Noise2Void/SEM/#the-sem-dataset-is-composed-of-a-training-and-a-validation-images-acquired-on-a-scanning","title":"The SEM dataset is composed of a training and a validation images acquired on a scanning\u00b6","text":"<p>electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Void and N2V2, an extension of Noise2Void, on this particular dataset!</p>"},{"location":"applications/Noise2Void/SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/SEM/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p> <p>There the switch between Noise2Void and N2V2 is done by changing the <code>use_n2v2</code> parameter.</p>"},{"location":"applications/Noise2Void/SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"guides/","title":"Guides","text":"<p>CAREamics can be used in two ways, the recommended way is to use the CAREamist API, as it made to be simple, intuitive and minimize potential errors. Advanced users might want to only use parts of CAREamics in their own pipeline, for this they should refer to the Lightning API.</p> <p>We also provide a command-line interface and developer resources!</p> CAREamist API <p>                                         The recommended way to use CAREamics in a few lines                                         of code to apply various methods in the most                                          comfortable way possible.                                     </p> Lightning API <p>                                         Advanced users can re-use parts of CAREamics in their                                         Lightning pipeline, with more customization potential                                         available.                                     </p> napari plugin <p>                                         A UI interface for CAREamics in the popular                                         napari image viewer, allowing training,                                          predicting and saving models on your                                          own data.                                     </p> Tutorials <p>                                         Thematic tutorials on various ways to use                                         and apply CAREamics algorithms, from data                                         inspection to using CAREamics in other                                         tools.                                     </p> Command-line interface <p>                                         Want to run CAREamics from the command line, on your                                         machine, remotely or on a cluster? Head this way!                                     </p> Developer resources <p>                                         More details on how the documentation is set up,                                         and guidelines for modifying and contributing to                                         CAREamics.                                     </p>"},{"location":"guides/careamist_api/","title":"CAREamist API","text":"<p>The CAREamist API is the recommended way to use CAREamics, it is a two stage process, in which users first define a configuration and then use a the <code>CAREamist</code> to run their  training and prediction.</p>"},{"location":"guides/careamist_api/#quick-start","title":"Quick start","text":"<p>The simplest way to use CAREamics is to create a configuration using the convenience functions. Checkout the applications section for real-world examples of the various algorithms.</p> Noise2VoidCARENoise2Noise <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256)).astype(np.float32)  # (2)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data)\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training!</p> </li> </ol> <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_care_configuration\n\n# create a configuration\nconfig = create_care_configuration(\n    experiment_name=\"care_2D\",\n    data_type=\"array\",\n    axes=\"SYX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (5, 256, 256)).astype(np.float32)  # (2)!\ntrain_target = np.random.randint(0, 255, (5, 256, 256)).astype(np.float32)\nval_data = np.random.randint(0, 255, (2, 256, 256)).astype(np.float32)\nval_target = np.random.randint(0, 255, (2, 256, 256)).astype(np.float32)\ncareamist.train(\n    train_source=train_data,\n    train_target=train_target,\n    val_source=val_data,\n    val_target=val_target,\n)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data, axes=\"YX\")\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training! Here, we added validation data as well.</p> </li> </ol> <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2n_configuration\n\n# create a configuration\nconfig = create_n2n_configuration(\n    experiment_name=\"n2n_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256)).astype(np.float32)  # (2)!\ntrain_target = np.random.randint(0, 255, (256, 256)).astype(np.float32)\ncareamist.train(\n    train_source=train_data,\n    train_target=train_target,\n)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data)\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training!</p> </li> </ol>"},{"location":"guides/careamist_api/#documentation","title":"Documentation","text":"<p>There are many features that can be useful for your application, explore the documentation to learn all the various aspects of CAREamics.</p> Configuration <p>                                         The configuration is at the heart of CAREamics, it                                          allow users to define how and which algorithm will be                                         trained.                                     </p> Using CAREAmics <p>                                         The CAREamist is the core element allowing training                                         and prediction using the model defined in the configuration.                                     </p>"},{"location":"guides/careamist_api/faq/","title":"Frequently asked questions","text":""},{"location":"guides/careamist_api/faq/#training","title":"Training","text":""},{"location":"guides/careamist_api/faq/#notimplementederror-the-operator-is-not-currently-implemented-for-the-mps-device","title":"<code>NotImplementedError: The operator [...] is not currently implemented for the MPS device</code>","text":"<p>Unfortunately, not all the operations in PyTorch are implemented for the Metal architecture  of recent Apple devices (MPS stands for Metal Performance Shaders). This error occurs when you are trying to run CAREamics on such a device without forcing training on CPU.</p> <pre><code>NotImplementedError: The operator 'aten::max_pool3d_with_indices' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n</code></pre> <p>Solution</p> <p>In your environment (e.g. terminal before you start the jupyter notebook), set the environment variable <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code>. This will force the training to run on the CPU, which is slower but will work.</p> <p>This can be done for instance by running the following command in your terminal:</p> <pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1\n</code></pre>"},{"location":"guides/careamist_api/faq/#prediction","title":"Prediction","text":""},{"location":"guides/careamist_api/faq/#sizes-of-tensors-must-match","title":"Sizes of tensors must match","text":"<p>If you get an error similar to:</p> <pre><code>RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 320 but got size 321 for tensor number 1 in the list.\n</code></pre> <p>This is likely because the input data size is not compatible with the model (e.g. the model has layers that downsample the data, then upsample it, and this is only  compatible with dimensions that are power of 2!).</p> <p>Solution</p> <p>By tiling the prediction, you can ensure that every input to the model has the correct shape. Then upon stitching back the tiles, you recover your whole image!</p>"},{"location":"guides/careamist_api/faq/#bmz-model-export","title":"BMZ model export","text":""},{"location":"guides/careamist_api/faq/#pydantic_core_pydantic_corevalidationerror","title":"<code>pydantic_core._pydantic_core.ValidationError</code>","text":"<p>The BMZ format is also validated by the pydantic library. If some of the metadata given to the <code>export_to_bmz</code> function is not correct,  you might get such errors:</p> Code<pre><code>train_data = np.random.randint(0, 255, (256, 256)).\ncareamist.export_to_bmz( \n    path=\"n2v_models\", \n    name=\"n2v_model_example.bmz\", \n    input_array=train_data, \n    authors=[{ \"name\": \"nobody\", \"email\": \"nobody@nobody\", }]\n)\n</code></pre> Error message<pre><code>pydantic_core._pydantic_core.ValidationError: 2 validation errors for bioimage.io model specification\nname\n  Value error, 'n2v_model_example.bmz' is not restricted to 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_- ()' [type=value_error, input_value='n2v_model_example.bmz', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/value_error\nauthors.0.email\n  value is not a valid email address: The part after the @-sign is not valid. It should have a period. [type=value_error, input_value='nobody@nobody', input_type=str]\n</code></pre> <p>If we look closely at the error message, Pydantic is telling us that there are actually two errors: - <code>name</code> should only contain letters, numbers, hyphens, underscores, spaces and parentheses. - <code>authors.0.email</code> is not a valid email address, as it is missing a period after the @-sign.</p> <p>Solution</p> <p>Look at the input <code>name</code>, it contains a forbidden character: the dot. The <code>email</code> should have something like <code>.com</code> after the <code>@</code> sign. Fixing these two issues will solve the problem.</p>"},{"location":"guides/careamist_api/configuration/","title":"Configuration","text":"<p>The configuration summarizes all the parameters used internally by CAREamics. It is  used to create a <code>CAREamist</code> instance and is saved together with the checkpoints and  saved models.</p> <p>Configurations are created for specific algorithms using convenience functions. They are Python objects, but can be exported to a dictionary, and contain a hierarchy of parameters.</p> Configuration example <p>Here is an example of a configuration for the <code>Noise2Void</code> algorithm.</p> Noise2Void configuration<pre><code>version: 0.1.0\nexperiment_name: N2V 3D\nalgorithm_config:\n  algorithm: n2v\n  loss: n2v\n  model:\n    architecture: UNet\n    conv_dims: 3\n    num_classes: 1\n    in_channels: 1\n    depth: 2\n    num_channels_init: 32\n    final_activation: None\n    n2v2: false\n    independent_channels: true\n  optimizer:\n    name: Adam\n    parameters:\n      lr: 0.0001\n  lr_scheduler:\n    name: ReduceLROnPlateau\n    parameters: {}\n  n2v_config:\n    name: N2VManipulate\n    roi_size: 11\n    masked_pixel_percentage: 0.2\n    remove_center: true\n    strategy: uniform\n    struct_mask_axis: none\n    struct_mask_span: 5\ndata_config:\n  data_type: tiff\n  axes: ZYX\n  patch_size:\n  - 8\n  - 64\n  - 64\n  batch_size: 8\n  transforms:\n  - name: XYFlip\n    flip_x: true\n    flip_y: true\n    p: 0.5\n  - name: XYRandomRotate90\n    p: 0.5\n  train_dataloader_params:\n    shuffle: true\n  val_dataloader_params: {}\ntraining_config:\n  num_epochs: 20\n  precision: '32'\n  max_steps: -1\n  check_val_every_n_epoch: 1\n  enable_progress_bar: true\n  accumulate_grad_batches: 1\n  gradient_clip_algorithm: norm\n  checkpoint_callback:\n    monitor: val_loss\n    verbose: false\n    save_weights_only: false\n    save_last: true\n    save_top_k: 3\n    mode: min\n    auto_insert_metric_name: false\n</code></pre> <p>The number of parameters might appear overwhelming, but in practice users only call a function with few parameters. The configuration is designed to hide the complexity of the algorithms and provide a simple interface to the user.</p> <pre><code>from careamics.config import (\n    create_care_configuration,  # CARE\n    create_n2n_configuration,  # Noise2Noise\n    create_n2v_configuration,  # Noise2Void\n)\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_experiment\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=50,\n)\n</code></pre> <p>In the next sections, you can dive deeper on how to create a configuration and interact with the configuration with different levels of expertise.</p> <p> Beginner</p> <p> Intermediate</p> <p> Advanced</p> <ul> <li> Convenience functions</li> <li> Save and load configurations</li> <li> Custom types</li> <li> Understanding configuration errors</li> <li> Build the configuration from scratch</li> <li> Algorithm requirements</li> </ul>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/","title":"Algorithm requirements","text":"<p>In this section we detail the constraints of each algorithm on the configuration and their differences. </p>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#parent-configuration","title":"Parent configuration","text":"<p>The parent configuration is <code>Configuration</code> and it is the same for all algorithms. It  should not be used to train any of the algorithms as the child classes ensure coherence across the parameters.</p>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#noise2void-family","title":"Noise2Void family","text":"<p>Noise2Void algorithms (N2V, N2V2, structN2V) are configured using <code>N2VConfiguration</code>. It enforces the following constraints:</p> <ul> <li><code>algorithm_config</code>: must be a <code>N2VAlgorithm</code></li> <li><code>algorithm_config.algorithm=\"n2v\"</code></li> <li><code>algorithm_config.loss=\"n2v\"</code></li> <li><code>algorithm_config.model</code>: <ul> <li>must be a UNet (<code>architecture=\"UNet\"</code>)</li> <li><code>in_channels</code> and <code>num_classes</code> must be equal</li> </ul> </li> <li><code>data_config</code>: must be a <code>N2VDataConfiguration</code></li> <li><code>data_config.transforms</code>: must contain <code>N2VManipulateModel</code> as the last transform</li> </ul>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#care-and-noise2noise","title":"CARE and Noise2Noise","text":"<p>The two algorithms are very similar and therefore their constraints are sensibly the same. They are configured using <code>CAREConfiguration</code> and <code>N2NConfiguration</code> respectively.</p> <ul> <li><code>algorithm_config</code>: must be a <code>CAREAlgorithm</code> (CARE) or <code>N2NAlgorithm</code> (Noise2Noise)</li> <li><code>algorithm_config.algorithm</code>: <code>care</code> (CARE) or <code>n2n</code> (Noise2Noise)</li> <li><code>algorithm_config.loss</code>: <code>mae</code> or <code>mse</code></li> <li><code>algorithm_config.model</code>: must be a UNet (<code>architecture=\"UNet\"</code>)</li> <li><code>data_config</code>: must be a <code>DataConfiguration</code></li> <li><code>data_config.transforms</code>: must not contain <code>N2VManipulateModel</code></li> </ul>"},{"location":"guides/careamist_api/configuration/build_configuration/","title":"Build the configuration","text":"<p>Beginner vs Advanced</p> <p>This is an advanced level way to create CAREamics configuration. Do check the convenience functions if you are looking for a simpler way to create CAREanics configurations!</p> <p>CAREamics configuration is validated using Pydantic,  a library that allows you to define schemas and automatically check the types of the  input data you provide. </p> <p>In addition, it allows great flexibility in writing custom validators. In turns this ensures that the configuration is always valid and coherent, protecting against errors deep in the library.</p> <p>As shown in the introduction, a CAREamics configuration is composed of four main elements:</p> <ol> <li>Experiment name, a simple string</li> <li>Algorithm configuration, also a Pydantic model</li> <li>Data configuration, also a Pydantic model</li> <li>Training configuration, also a Pydantic model</li> </ol> <p>Each of the parameters and models are validated independently, and the configuration as a whole is validated at the end.</p> <p>There are two ways to build Pydantic models: by passing a dictionary that reproduces the model structure, or by calling all Pydantic models explicitly. While the second method is  more concise, the first approach is less error prone and allow you to explore all parameters available easily if you are using an IDE (e.g. VSCode, JupyterLab etc.).</p> <p>Each algorithm has a specific configuration that may or may not be similar to the others. For differences between algorithms, refer to the algorithm requirements section.</p> <p>Complete list of parameters</p> <p>A complete list of all parameters would be very long and difficult to showcase, as it also depends on the algorithm.</p> <p>The preferred way to check the parameters of the various configurations and subconfigurations is to read directly the code reference or the source code.</p>"},{"location":"guides/careamist_api/configuration/build_configuration/#using-pydantic-models","title":"Using Pydantic models","text":"<p>The preferred way to build the configuration is to call the Pydantic models directly. This allows you to explore the parameters via your IDE, but also to get the validation errors closer to the source of the error.</p> Building the configuration using Pydantic models<pre><code>from careamics.config import (  # (1)!\n    Configuration,\n    DataConfig,\n    N2VAlgorithm,\n    TrainingConfig,\n)\nfrom careamics.config.architectures import UNetModel\nfrom careamics.config.callback_model import CheckpointModel, EarlyStoppingModel\nfrom careamics.config.support import (\n    SupportedData,\n    SupportedLogger,\n)\nfrom careamics.config.transformations import XYFlipModel\n\nexperiment_name = \"N2V_example\"\n\n# build the model and algorithm configurations\nmodel = UNetModel(\n    architecture=\"UNet\",  # (2)!\n    num_channels_init=64,  # (3)!\n    depth=3,\n    # (4)!\n)\n\nalgorithm_model = N2VAlgorithm(  # (5)!\n    model=model,\n    # (6)!\n)\n\n# then the data configuration for N2V\ndata_model = DataConfig(  # (7)!\n    data_type=SupportedData.ARRAY.value,\n    patch_size=(256, 256),\n    batch_size=8,\n    axes=\"YX\",\n    transforms=[XYFlipModel(flip_y=False)],  # (8)!\n    dataloader_params={  # (9)!\n        \"num_workers\": 4,\n        \"shuffle\": True,\n    },\n)\n\n# then the TrainingConfig\nearlystopping = EarlyStoppingModel(\n    # (10)!\n)\n\ncheckpoints = CheckpointModel(every_n_epochs=10)  # (11)!\n\ntraining_model = TrainingConfig(\n    num_epochs=30,\n    logger=SupportedLogger.WANDB.value,\n    early_stopping_callback=earlystopping,\n    checkpoint_callback=checkpoints,\n    # (12)!\n)\n\n# finally, build the Configuration\nconfig = Configuration(  # (13)!\n    experiment_name=experiment_name,\n    algorithm_config=algorithm_model,\n    data_config=data_model,\n    training_config=training_model,\n)\n</code></pre> <ol> <li>The Pydantic models are imported from the <code>careamics.config</code>      submodules, and its various submodules.</li> <li>Because the <code>architecture</code> parameter is used to discriminate between different models     within CAREamics, there is no default value and it is therefore necessary to set it.</li> <li>You can change here the model parameters.</li> <li>There are more parameters available, but keep in mind that by creating the models     directly, you are responsible for setting the correct parameters and you might get     errors when assembling the final configuration. For instance, Noise2Void requires     the same number of input and output channels, which is not checked here.</li> <li>Noise2Void requires a specific algorithm configuration.</li> <li>Here once can modify also the N2V specific parameters related to the pixel replacement (see     N2V algorithm description).</li> <li>As opposed to CARE and Noise2Noise, Noise2Void requires a specific data configuration.</li> <li>You can decide on the augmentations and their parameters.</li> <li>The dataloader parameters are the general parameters that can be passed to PyTorch's     <code>DataLoader</code> class (see documentation). If     you modify these, you need to add the <code>shuffle</code> parameter to the dictionary. We strongly     recommend to keep the default value (<code>True</code>) for the shuffling.</li> <li>The early stopping callback has many parameters, which are those available in the     PyTorch Lightning callback (see documentation).</li> <li>Similarly, the <code>CheckpointModel</code> parameters are those of the corresponding PyTorch     Lightning callback (see documentation).</li> <li>The training configuration is general and has many parameters, checkout the code     reference or the code base for more information.</li> <li>Finally the Noise2Void configuration can be instantiated.</li> </ol>"},{"location":"guides/careamist_api/configuration/build_configuration/#using-nested-dictionaries","title":"Using nested dictionaries","text":"<p>An alternative to working with Pydantic models is to assemble the configuration using a dictionary. While this is neat, because you are dealing with nested dictionaries, it is easy to add the parameters at the wrong level and you need to constantly refer to the code documentation to know which parameters are available. Finally, because you are validating the configuration at once, you will get all the validation errors in one go.</p> <p>Here, we reproduce the same configuration as previously, but as a dictionary this time:</p> Building the configuration with a dictionary<pre><code>from careamics.config import Configuration\n\nconfig_dict = {\n    \"experiment_name\": \"N2V_example\",\n    \"algorithm_config\": {\n        \"algorithm\": \"n2v\",  # (1)!\n        \"loss\": \"n2v\",\n        \"model\": {\n            \"architecture\": \"UNet\",  # (2)!\n            \"num_channels_init\": 64,\n            \"depth\": 3,\n        },\n        # (3)!\n    },\n    \"data_config\": {\n        \"data_type\": \"array\",\n        \"patch_size\": [256, 256],\n        \"batch_size\": 8,\n        \"axes\": \"YX\",\n        \"transforms\": [\n            {\n                \"name\": \"XYFlip\",\n                \"flip_y\": False,\n            },\n        ],\n        \"dataloader_params\": {\n            \"num_workers\": 4,\n        },\n    },\n    \"training_config\": {\n        \"num_epochs\": 30,\n        \"logger\": \"wandb\",\n        \"early_stopping_callback\": {},  # (4)!\n        \"checkpoint_callback\": {\n            \"every_n_epochs\": 10,\n        },\n    },\n}\n\n# instantiate specific configuration\nconfig_as_dict = Configuration(**config_dict)  # (5)!\n</code></pre> <ol> <li>In order to correctly instantiate the N2V configuration via a dictionary, we have     to explicitly specify certain parameters that otherwise have a default value when     using Pydantic. This is the case for the <code>algorithm</code> parameter.</li> <li>As previously, we also specify the architecture.</li> <li>In parctice, we do not change the <code>n2v_config</code>, but this is were one could. The <code>n2v_config</code>     is related to the parameters of the pixel replacement in N2V.</li> <li>Since many parameters have default values, we don't need to specify them but still     need to pass an empty dictionary to make sure that there is an early stopping     callback.</li> <li>Here we are using the Pydantic class with the unpacked dictionary.</li> </ol>"},{"location":"guides/careamist_api/configuration/configuration_errors/","title":"Configuration errors","text":"<p>Because the CAREamics configuration is validated using Pydantic, all the configuration errors are delegated to the validation library.</p> <p>Here we showcase a certain number of confguration errors that can appear when using the convenience functions. It is impossible to cover all possibilities, so these examples are here to highlight how to read Pydantic errors.</p> <p>Under construction</p> <p>This page is under construction and will be updated soon.</p>"},{"location":"guides/careamist_api/configuration/configuration_errors/#pydantic-error","title":"Pydantic error","text":"<pre><code>ValidationError: 1 validation errors for union[function-after[validate_n2v2(), function-after[validate_3D(), N2VConfiguration]],function-after[validate_3D(), N2NConfiguration],function-after[validate_3D(), CAREConfiguration]]\nfunction-after[validate_n2v2(), function-after[validate_3D(), N2VConfiguration]].data_config.data_type\n  Input should be 'array', 'tiff' or 'custom' [type=literal_error, input_value='arrray', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\n...\n</code></pre> <p>In this case, the input to <code>data_config.data_type</code> should be either <code>'array'</code>, <code>'tiff'</code> or <code>'custom'</code>. The error message is telling us that the input value <code>'arrray'</code> is not a valid option.</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/","title":"Convenience functions","text":"<p>As building a full CAREamics configuration requires a complete understanding of the  various parameters and experience with Pydantic, we provide convenience functions to create configurations with a only few parameters related to the algorithm users want to train.</p> <p>All convenience methods can be found in the <code>careamics.config</code> modules. CAREamics  currently supports Noise2Void and its variants,  CARE and Noise2Noise. </p> Import convenience functions<pre><code>from careamics.config import (\n    create_care_configuration,  # CARE\n    create_n2n_configuration,  # Noise2Noise\n    create_n2v_configuration,  # Noise2Void\n)\n</code></pre> <p>Each method does all the heavy lifting to make the configuration coherent. They share a certain numbers of mandatory parameters:</p> <ul> <li><code>experiment_name</code>: The name of the experiment, used to differentiate trained models.</li> <li><code>data_type</code>: One of the types supported by CAREamics (<code>array</code>, <code>tiff</code> or <code>custom</code>).</li> <li><code>axes</code>: Axes of the data (e.g. SYX), can only the following letters: <code>STCZYX</code>.</li> <li><code>patch_size</code>: Size of the patches along the spatial dimensions (e.g. [64, 64]).</li> <li><code>batch_size</code>: Batch size to use during training (e.g. 8). This parameter affects the     memory footprint on the GPU.</li> <li><code>num_epochs</code>: Number of epochs.</li> </ul> <p>Additional optional parameters can be passed to tweak the configuration (e.g. the number of steps per epoch). </p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#settings-the-number-of-steps-per-epoch","title":"Settings the number of steps per epoch","text":"<p>For large images, the number of patches may be very large as well. A consequence are long epochs, and a sparser sampling in time of the network performances during training. In such cases, it is useful to set the number of step per epochs. By setting a number of steps samller than the number of total patches, the epoch length is reduced and metrics calculation over the validation set occurs more often.</p> Noise2VoidCARENoise2Noise Configuration with maximum number of steps<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_steps\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    num_steps=50,  # (1)!\n)\n</code></pre> <ol> <li>Set the maximum number of steps. If the number of patches is smaller than that number, then it simply trains as if we would not have set the number of steps.</li> </ol> Configuration with maximum number of steps<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_steps\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    num_steps=50,  # (1)!\n)\n</code></pre> <ol> <li>Set the maximum number of steps. If the number of patches is smaller than that number, then it simply trains as if we would not have set the number of steps.</li> </ol> Configuration with maximum number of steps<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_steps\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    num_steps=50,  # (1)!\n)\n</code></pre> <ol> <li>Set the maximum number of steps. If the number of patches is smaller than that number, then it simply trains as if we would not have set the number of steps.</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#training-with-channels","title":"Training with channels","text":"<p>When training with multiple channels, the <code>axes</code> parameter should contain <code>C</code> (e.g. <code>YXC</code>). An error will be then thrown if the optional parameter <code>n_channels</code> (or <code>n_channel_in</code> for  CARE and Noise2Noise) is not specified! Likewise if <code>n_channels</code> is specified but <code>C</code> is not in <code>axes</code>.</p> <p>The correct way is to specify them both at the same time.</p> Noise2VoidCARENoise2Noise Configuration with multiple channels<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,  # (2)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> </ol> Configuration with multiple channels<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,  # (2)!\n    n_channels_out=2,  # (3)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> <li>Depending on the CARE task, you also see to set <code>n_channels_out</code> (optional).</li> </ol> Configuration with multiple channels<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,  # (2)!\n    n_channels_out=2,  # (3)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> <li>Depending on the CARE task, you also see to set <code>n_channels_out</code> (optional).</li> </ol> <p>Independent channels</p> <p>By default, the channels are trained independently: that means that they have no influence on each other. As they might have completely different noise models, this can lead to better results.</p> <p>However, in some cases, you might want to train the channels together to get more structural information.</p> <p>To control whether the channels are trained independently, you can use the  <code>independent_channels</code> parameter:</p> Noise2VoidCARENoise2Noise Training channels together<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol> Training channels together<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,\n    n_channels_out=2,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels_in</code>.</li> <li>This ensures that the channels are trained together!</li> </ol> Training channels together<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#augmentations","title":"Augmentations","text":"<p>By default CAREamics configuration uses augmentations that are specific to the algorithm (e.g. Noise2Void) and that are compatible with microscopy images (e.g. flip and 90 degrees rotations).</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#disable-augmentations","title":"Disable augmentations","text":"<p>However in certain cases, users might want to disable augmentations. For instance if you have structures that are always oriented in the same direction. To do so there is a single <code>augmentations</code> parameter:</p> Noise2VoidCARENoise2Noise Configuration without augmentations<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization and N2V pixel manipulation will still be added by CAREamics!).</li> </ol> Configuration without augmentations<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization will still be added!).</li> </ol> Configuration without augmentations<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization will still be added!).</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#non-default-augmentations","title":"Non-default augmentations","text":"<p>Default augmentations apply a random flip along X or Y and a 90 degrees rotation (note that there is always for each patch and for each augmentation a 0.5 probability that no augmentation is applied). For samples that contain objects that are never flipped or rotated (e.g. objects with always the same orientation, or with patterns along a certain direction), it will be beneficial to apply non-default augmentations.</p> <p>For instance, in a case where the objects can only be flipped horizontally, we would only apply flipping along the <code>X</code> axis and not apply any rotation.</p> Noise2VoidCARENoise2Noise Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_care_configuration(\n    experiment_name=\"care_2D_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_n2n_configuration(\n    experiment_name=\"n2n_2D_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> <p>Available augmentations</p> <p>The available augmentations are the following:</p> <ul> <li><code>XYFlipModel</code>, which can be along <code>X</code>, <code>Y</code> or both.</li> <li><code>XYRandomRotate90Model</code></li> </ul>"},{"location":"guides/careamist_api/configuration/convenience_functions/#choosing-a-logger","title":"Choosing a logger","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p> Noise2VoidCARENoise2Noise Configuration with WandB<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol> Configuration with WandB<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol> Configuration with WandB<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#advanced-passing-data-loader-parameters","title":"(Advanced) Passing data loader parameters","text":"<p>The convenience functions allow passing data loader parameters directly through the <code>train_dataloader_params</code> or <code>val_dataloader_params</code> parameters. These are the same parameters as those accepted by the <code>torch.utils.data.DataLoader</code> class (see PyTorch documentation).</p> Noise2VoidCARENoise2Noise Configuration with data loader parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol> Configuration with data loader parameters<pre><code>config = create_care_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol> Configuration with data loader parameters<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#advanced-passing-model-specific-parameters","title":"(Advanced) Passing model specific parameters","text":"<p>By default, the convenience functions use the default UNet model parameters. But if  you are feeling brave, you can pass model specific parameters in the <code>model_params</code> dictionary. </p> Noise2VoidCARENoise2Noise Configuration with model specific parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> Configuration with model specific parameters<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> Configuration with model specific parameters<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> <p>Model parameters overwriting</p> <p>Some values of the model parameters are not compatible with certain algorithms.  Therefore, these are overwritten by the convenience functions. For instance, if you pass <code>in_channels</code> or <code>independent_channels</code> in the <code>model_kwargs</code> dictionary,  they will be ignored and replaced by the explicit parameters passed to the convenience function.</p> <p>Model parameters</p> <p>The model parameters are the following:</p> <ul> <li><code>conv_dims</code></li> <li><code>num_classes</code></li> <li><code>in_channels</code></li> <li><code>depth</code></li> <li><code>num_channels_init</code></li> <li><code>final_activation</code> </li> <li><code>n2v2</code></li> <li><code>independent_channels</code></li> </ul> <p>Description for each parameter can be found in the code reference.</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#noise2void-specific-parameters","title":"Noise2Void specific parameters","text":"<p>Noise2Void has a few additional parameters that can be set, including for using its  variants N2V2 and structN2V.</p> <p>Understanding Noise2Void and its variants</p> <p>Before deciding which variant to use, and how to modify the parameters, we recommend to die a little a bit on how each algorithm works!</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#noise2void-parameters","title":"Noise2Void parameters","text":"<p>There are two Noise2Void parameters that influence how the patches are manipulated during training:</p> <ul> <li><code>roi_size</code>: This parameter specifies the size of the area used to replace the masked pixel value.</li> <li><code>masked_pixel_percentage</code>: This parameter specifies how many pixels per patch will be manipulated.</li> </ul> <p>While the default values are usually fine, they can be tweaked to improve the training in certain cases.</p> Configuration with N2V parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    roi_size=7,\n    masked_pixel_percentage=0.5,\n)\n</code></pre>"},{"location":"guides/careamist_api/configuration/convenience_functions/#n2v2","title":"N2V2","text":"<p>To use N2V2, the <code>use_n2v2</code> parameter should simply be set to <code>True</code>.</p> Configuration with N2V2<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v2_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_n2v2=True,  # (1)!\n)\n</code></pre> <ol> <li>What it does is modifying the architecture of the UNet model and the way the masked     pixels are replaced.</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#structn2v","title":"structN2V","text":"<p>StructN2V has two parameters that can be set:</p> <ul> <li><code>struct_n2v_axis</code>: The axis along which the structN2V mask will be applied. By default it     is set to <code>none</code> (structN2V is disabled), you can set it to either <code>horizontal</code> or <code>vertical</code>.</li> <li><code>struct_n2v_span</code>: The size of the structN2V mask.</li> </ul> Configuration with structN2V<pre><code>config = create_n2v_configuration(\n    experiment_name=\"structn2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    struct_n2v_axis=\"horizontal\",\n    struct_n2v_span=5,\n)\n</code></pre>"},{"location":"guides/careamist_api/configuration/convenience_functions/#care-and-noise2noise-parameters","title":"CARE and Noise2Noise parameters","text":""},{"location":"guides/careamist_api/configuration/convenience_functions/#using-another-loss-function","title":"Using another loss function","text":"<p>As opposed to Noise2Void, CARE and Noise2Noise  can be trained with different loss functions. This can be set using the <code>loss</code> parameter  (surprise, surprise!).</p> CARENoise2Noise Configuration with different loss<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol> Configuration with different loss<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol>"},{"location":"guides/careamist_api/configuration/custom_types/","title":"Custom types","text":"<p>The <code>data_type</code> parameter of the <code>DataConfig</code> class is a string that is used to choose the data loader within CAREamics. We currently only support <code>array</code> and <code>tiff</code> explicitly.</p> <p>However, users can set the <code>data_type</code> to <code>custom</code> and use their own read function.</p> Custom data type<pre><code>from careamics.config import DataConfig\n\ndata_config = DataConfig(\n    data_type=\"custom\",  # (1)!\n    axes=\"YX\",\n    patch_size=[128, 128],\n    batch_size=8,\n    num_epochs=20,\n)\n</code></pre> <ol> <li>As far as the configuration is concerned, you only set the <code>data_type</code> to <code>custom</code>. The     rest happens in the <code>CAREamist</code> instance.</li> </ol> <p>Full example in other sections</p> <p>A full example of the use of a custom data type is available in the CAREamist and Applications sections.</p>"},{"location":"guides/careamist_api/configuration/save_load/","title":"Save and load","text":"<p>CAREamics configurations can be saved to the disk as <code>.yml</code> file and loaded easily to start similar experiments.</p>"},{"location":"guides/careamist_api/configuration/save_load/#save-a-configuration","title":"Save a configuration","text":"Save a configuration<pre><code>from careamics import save_configuration\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"Config_to_save\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=(8, 64, 64),\n    batch_size=8,\n    num_epochs=20,\n)\nsave_configuration(config, \"config.yml\")\n</code></pre> <p>In the resulting file, you can see all the parameters that are defaults and hidden from you.</p> resulting config.yml file <pre><code>version: 0.1.0\nexperiment_name: N2V 3D\nalgorithm_config:\n  algorithm: n2v\n  loss: n2v\n  model:\n    architecture: UNet\n    conv_dims: 3\n    num_classes: 1\n    in_channels: 1\n    depth: 2\n    num_channels_init: 32\n    final_activation: None\n    n2v2: false\n    independent_channels: true\n  optimizer:\n    name: Adam\n    parameters:\n      lr: 0.0001\n  lr_scheduler:\n    name: ReduceLROnPlateau\n    parameters: {}\n  n2v_config:\n    name: N2VManipulate\n    roi_size: 11\n    masked_pixel_percentage: 0.2\n    remove_center: true\n    strategy: uniform\n    struct_mask_axis: none\n    struct_mask_span: 5\ndata_config:\n  data_type: tiff\n  axes: ZYX\n  patch_size:\n  - 8\n  - 64\n  - 64\n  batch_size: 8\n  transforms:\n  - name: XYFlip\n    flip_x: true\n    flip_y: true\n    p: 0.5\n  - name: XYRandomRotate90\n    p: 0.5\n  train_dataloader_params:\n    shuffle: true\n  val_dataloader_params: {}\ntraining_config:\n  num_epochs: 20\n  precision: '32'\n  max_steps: -1\n  check_val_every_n_epoch: 1\n  enable_progress_bar: true\n  accumulate_grad_batches: 1\n  gradient_clip_algorithm: norm\n  checkpoint_callback:\n    monitor: val_loss\n    verbose: false\n    save_weights_only: false\n    save_last: true\n    save_top_k: 3\n    mode: min\n    auto_insert_metric_name: false\n</code></pre>"},{"location":"guides/careamist_api/configuration/save_load/#load-a-configuration","title":"Load a configuration","text":"Load a configuration<pre><code>from careamics import load_configuration\n\nconfig = load_configuration(\"config.yml\")\n</code></pre>"},{"location":"guides/careamist_api/usage/","title":"Using CAREamics","text":"<p>In this section, we will explore the many facets of the <code>CAREamist</code> class, which allors training and predicting using the various algorithms in CAREamics.</p> <p>The workflow in CAREamics has five steps: creating a configuration, instantiating a <code>CAREamist</code> object, training, prediction, and model export.</p> Basic CAREamics usage<pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256))  # (2)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\npredction = careamist.predict(source=pred_data)\n\n# export to BMZ format\ncareamist.export_to_bmz(\n    path_to_archive=\"n2v_model.zip\",\n    friendly_model_name=\"N2V 2D\",\n    input_array=pred_data,\n    authors=[{\"name\": \"CAREamics authors\"}],\n    general_description=\"This model was trained to denoise 2D images.\",\n    data_description=\"The data was acquired on a confocal microscope [...]\",\n)\n</code></pre> <ol> <li> <p>Obviously, one should choose a more realistic number of epochs for training.</p> </li> <li> <p>One should use real data for training!</p> </li> </ol>"},{"location":"guides/careamist_api/usage/careamist/","title":"CAREamist","text":"<p>The <code>CAREamist</code> is the central class in CAREamics, it provides the API to train, predict and save models. There are three ways to create a <code>CAREamist</code> object: with a configuration,  with a path to a configuration, or with a path to a trained model.</p>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-configuration","title":"Instantiating with a configuration","text":"<p>When passing a configuration to the <code>CAREamist</code> constructor, the model is initialized with random weights and prediction will not be possible until the model is trained.</p> Instantiating CAREamist with a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)  # (1)!\n\ncareamist = CAREamist(config)\n</code></pre> <ol> <li>Any valid configuration will do!</li> </ol> <p>Creating configurations</p> <p>Check out the configuration guide to learn how to create configurations.</p>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-path-to-a-configuration","title":"Instantiating with a path to a configuration","text":"<p>This is similar to the previous section, except that the configuration is loaded from a file on disk.</p> Instantiating CAREamist with a path to a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration, save_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)\n\nsave_configuration(config, \"configuration_example.yml\")\n\ncareamist = CAREamist(\"configuration_example.yml\")\n</code></pre>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-path-to-a-model","title":"Instantiating with a path to a model","text":"<p>There are two types of models exported from CAREamics. During training, the model is saved as checkpoints (<code>.ckpt</code>). After training, users can export the model to the  bioimage model zoo format (saved as a<code>.zip</code>). Both can be loaded into CAREamics to either retrain or predict. Alternatively, a checkpoint can be loaded in order to  export it as a bioimage model zoo model.</p> <p>In any case, both types of pre-trained models can be loaded into CAREamics by passing the path to the model file. The instantiated CAREamist is then ready to predict on new images!</p> Instantiating CAREamist with a path to a model<pre><code>from careamics import CAREamist\n\npath_to_model = \"model.zip\"  # (1)!\n\ncareamist = CAREamist(path_to_model)\n</code></pre> <ol> <li>Any valid path to a model, as a string or a <code>Path.path</code> object, will work.</li> </ol>"},{"location":"guides/careamist_api/usage/careamist/#setting-the-working-directory","title":"Setting the working directory","text":"<p>By default, CAREamics will save the checkpoints in the current working directory. When creating a new CAREamist, you can indicate a different working directory in which to save the logs and checkpoints during training.</p> Changing the working directory<pre><code>careamist = CAREamist(config, work_dir=\"work_dir\")\n</code></pre>"},{"location":"guides/careamist_api/usage/careamist/#custom-callbacks","title":"Custom callbacks","text":"<p>CAREamics uses two different callbacks from PyTorch Lightning:</p> <ul> <li><code>ModelCheckpoint</code>: to save the model at different points during the training.</li> <li><code>EarlyStopping</code>: to stop the training based on a few parameters.</li> </ul> <p>The parameters for the callbacks are the same as the ones from PyTorch Lightning, and can be set in the configuration.</p> <p>Custom callbacks can be passed to the <code>CAREamist</code> constructor. The callbacks must inherit from the PyTorch Lightning <code>Callback</code> class.</p> Custom callbacks<pre><code>from pytorch_lightning.callbacks import Callback\n\n\n# define a custom callback\nclass MyCallback(Callback):  # (1)!\n    def __init__(self):\n        super().__init__()\n\n        self.has_started = False\n        self.has_ended = False\n\n    def on_train_start(self, trainer, pl_module):\n        self.has_started = True  # (2)!\n\n    def on_train_end(self, trainer, pl_module):\n        self.has_ended = True\n\n\nmy_callback = MyCallback()  # (3)!\n\ncareamist = CAREamist(config, callbacks=[my_callback])  # (4)!\n</code></pre> <ol> <li> <p>The callbacks must inherit from the PyTorch Lightning <code>Callback</code> class.</p> </li> <li> <p>This is just an example to test that the callback was called!</p> </li> <li> <p>Create your callback.</p> </li> <li> <p>Pass the callback to the CAREamist constructor as a list.</p> </li> </ol>"},{"location":"guides/careamist_api/usage/datasets/","title":"(Intermediate) Datasets","text":"<p>Datasets are the internal classes providing the individual patches for training,  validation and prediction. In CAREamics, we provide a <code>TrainDataModule</code> class that  creates the datasets for training and validation (there is a class for prediction as well, which is simpler and shares some parameters with the training one). In most cases, it is created internally. In this section, we describe what it does and shed light on some of its parameters that are passed to the train methods.</p> <p>Datasets in practice</p> <p>This section contains descriptions of the internal working of CAREamics. In practice, most users will never have to instantiate the datasets themselves, as they are created from within the <code>careamist.train</code> or <code>careamist.predict</code> methods.</p>"},{"location":"guides/careamist_api/usage/datasets/#overview","title":"Overview","text":"<p>The <code>TrainDataModule</code> receives both data configuration and data itself. The data can be passed a path to a folder, to a file or as <code>numpy</code> array. </p> Simplest way to instantiate TrainDataModule<pre><code>import numpy as np\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\ntrain_array = np.random.rand(128, 128)\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = TrainDataModule(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n</code></pre> <p>It has the following parameters:</p> <ul> <li><code>data_config</code>: data configuration</li> <li><code>train_data</code>: training data (array or path)</li> <li><code>(optional) val_data</code>: validation data, if not provided, the validation data is taken from the training data</li> <li><code>(optional) train_data_target</code>: target data for training (if applicable)</li> <li><code>(optional) val_data_target</code>: target data for validation (if applicable)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> <li><code>(optional) val_percentage</code>: percentage of validation data to extract from the training     (see splitting validation)</li> <li><code>(optional) val_minimum_split</code>: minimum validation split      (see splitting validation)</li> <li><code>(optional) use_in_memory</code>: whether to use in-memory dataset if possible (Default is <code>True</code>),      not applicable to mnumpy arrays.</li> </ul> <p>Depending on the type of the data, which is specified in the <code>data_config</code> and is compared to the type of <code>train_data</code>, the <code>TrainDataModule</code> will create the appropriate dataset for both training and validation data.</p> <p>In the absence of validation, validation data is extracted from training data (see splitting validation).</p>"},{"location":"guides/careamist_api/usage/datasets/#available-datasets","title":"Available datasets","text":"<p>CAREamics currently support two datasets:</p> <ul> <li>InMemoryDataset: used when the data fits in memory.</li> <li>IterableDataset: used when the data is too large to fit in memory.</li> </ul> <p>If the data is a <code>numpy</code> array, the <code>InMemoryDataset</code> is used automatically. Otherwise, we list the files contained in the path, compute the size of the data and instantiate an <code>InMemoryDataset</code> if the data is less than 80% of the total RAM size. If not, CAREamics instantiate an <code>IterableDataset</code>.</p> <p>Both datasets work differently, and the main differences can be summarized as follows:</p> Feature <code>InMemoryDataset</code> <code>IterableDataset</code> Used with arrays  Yes  No Patch extraction Sequential Random Data loading All in memory One file at a time <p>In the next sections, we describe the different steps they perform.</p>"},{"location":"guides/careamist_api/usage/datasets/#in-memory-dataset","title":"In-memory dataset","text":"<p>As the name implies, the in-memory dataset loads all the data in memory. It is used when the data on the disk seems to fit in memory, or when the data is already in memory and  passed as a numpy array. The advantage of the dataset is that is allows faster access to the patches, and therefore faster training time.</p> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/careamist_api/usage/datasets/#iterable-dataset","title":"Iterable dataset","text":"<p>The iterable dataset is used to load patches from a single file at a time, one file after another. This allows training on datasets that are too large to fit in memory. This dataset is exclusively used with files input (data passed as paths).</p> <p>Iterable dataset and splitting validation</p> <p>The iterable dataset does not split patches from the training data, but files!  (see splitting validation).</p> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/careamist_api/usage/datasets/#intermediate-transforms","title":"(Intermediate) Transforms","text":"<p>Transforms are augmentations and any operation applied to the patches before feeding them into the network. CAREamics supports the following transforms (see  configuration full spec for an example on how to configure them):</p> Transform Description Notes <code>Normalize</code> Normalize (zero mean, unit variance) Necessary <code>XYFlip</code> Flip the image along X and Y, one at a time Can flip a single axis, optional <code>XYRandomRotate90Model</code> Rotate by 90 degrees the XY axes Optional <code>N2VManipulateModel</code> N2V pixel manipulation Only for N2V, in which case it is necessary <p>The <code>Normalize</code> transform is always applied, and the rest are optional. The exception is <code>N2VManipulateModel</code>, which is only applied when training with N2V (see Noise2Void).</p> <p>When to turn off transforms?</p> <p>The configuration allows turning off transforms. In this case, only normalization (and potentially the <code>N2VManipulateModel</code> for N2V) is applied. This is useful when the structures in your sample are always in the same orientation, and flipping and rotation do not make sense.</p>"},{"location":"guides/careamist_api/usage/datasets/#advanced-custom-data-types","title":"(Advanced) Custom data types","text":"<p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You should also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.npy\") to filter the files extension using <code>extension_filter</code>.</p> Read custom data types<pre><code>from pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\n\ndef read_npy(  # (1)!\n    path: Path,  # (2)!\n    *args: Any,\n    **kwargs: Any,  # (3)!\n) -&gt; np.ndarray:\n    return np.load(path)  # (4)!\n\n\n# example data\ntrain_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"custom\",  # (5)!\n    axes=\"YX\",\n    patch_size=[32, 32],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = TrainDataModule(\n    data_config=config.data_config,\n    train_data=\"train_array.npy\",  # (6)!\n    read_source_func=read_npy,  # (7)!\n    extension_filter=\"*.npy\",  # (8)!\n)\ndata_module.prepare_data()\ndata_module.setup()  # (9)!\n\n# check dataset output\ndataloader = data_module.train_dataloader()\nprint(dataloader.dataset[0][0].shape)  # (10)!\n</code></pre> <ol> <li> <p>We define a function that reads the custom data type.</p> </li> <li> <p>It takes a path as argument!</p> </li> <li> <p>But it also need to receive <code>*args</code> and <code>**kwargs</code> to be compatible with the <code>read_source_func</code> signature.</p> </li> <li> <p>It simply returns a <code>numpy</code> array.</p> </li> <li> <p>The data type must be <code>custom</code>!</p> </li> <li> <p>And we pass a <code>Path | str</code>.</p> </li> <li> <p>Simply pass the method by name.</p> </li> <li> <p>We also need to provide an extension filter that is compatible with <code>fnmatch</code> and <code>Path.rglob</code>.</p> </li> <li> <p>These two lines are necessary to instantiate the training dataset that we call at the end. They are     called automatically by PyTorch Lightning during training.</p> </li> <li> <p>The dataloader gives access to the dataset, we choose the first element, and since     we configured CAREamics to use N2V, the output is a tuple whose first element is our     first patch!</p> </li> </ol> <p>In practice, you should not access the dataloader directly (except for testing). Using  custom types for training should be done as follows:</p> <pre><code>from pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\n\ndef read_npy(\n    path: Path,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return np.load(path)\n\n\n# example data\ntrain_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"custom\",\n    axes=\"YX\",\n    patch_size=[32, 32],\n    batch_size=1,\n    num_epochs=1,\n)\n\n# Data module for custom types\ndata_module = TrainDataModule(\n    data_config=config.data_config,\n    train_data=\"train_array.npy\",\n    read_source_func=read_npy,\n    extension_filter=\"*.npy\",\n)\n\n# CAREamist\ncareamist = CAREamist(source=config)\n\n# Train\ncareamist.train(datamodule=data_module)\n</code></pre>"},{"location":"guides/careamist_api/usage/datasets/#prediction-datasets","title":"Prediction datasets","text":"<p>The prediction data module, <code>PredictDataModule</code> works similarly to <code>TrainDataModule</code>, albeit with different parameters:</p> <ul> <li><code>pred_config</code>: inference configuration</li> <li><code>pred_data</code>: prediction data (array or path)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> </ul>"},{"location":"guides/careamist_api/usage/datasets/#advanced-subclass-traindatamodule","title":"(Advanced) Subclass TrainDataModule","text":"<p>The data module used in CAREamics have only a limited number of parameters, and they  make use of the CAREamics datasets. If you need to have a different dataset, then you can subclass <code>TrainDataModule</code> and override the <code>setup</code> method to use your own datasets.</p>"},{"location":"guides/careamist_api/usage/model_export/","title":"Export to BMZ","text":"<p>The BioImage Model Zoo is a zoo of models that can be run in  a variety of software thanks to the  BMZ format. CAREamics is compatible  with the BMZ format and can export and load (CAREamics) models in this format.</p> <p>To export a trained model, you can simply call <code>careamist.export_to_bmz</code>:</p> Export to BMZ format<pre><code>careamist.export_to_bmz(\n    path_to_archive=export_path / \"my_model.zip\",  # (1)!\n    friendly_model_name=\"CARE_mito\",  # (2)!\n    input_array=my_array,  # (3)!\n    authors=[\n        {\n            \"name\": \"Ignatius J. Reilly\",\n            \"affiliation\": \"Levy Pants\",\n            \"email\": \"ijr@levy.com\",\n        },\n        {\"name\": \"Myrna Minkoff\", \"orcid\": \"0000-0002-3291-8524\"},  # (4)!\n    ],\n    general_description=\"This model was trained to denoise 2D images of mitochondria.\",  # (5)!\n    data_description=\"The data was acquired on a confocal microscope [...]\",  # (6)!\n)\n</code></pre> <ol> <li> <p>The model export should be a <code>.zip</code> file, if not CAREamics will add the extension.</p> </li> <li> <p>Give the model a name that is informative! It should consist of letters, numbers, hyphens and underscores.</p> </li> <li> <p>We need an input array to verify the export, the input and the prediction will be  packaged in the BMZ model. They will also be used to create the cover of the model if it is uploaded to the BioImage Model Zoo.</p> </li> <li> <p>You can have multiple authors, and <code>affiliation</code>, <code>email</code>, <code>orcid</code> and  <code>github _user</code>.</p> </li> <li> <p>A README will automatically be generated by CAREamics, containing information on how the model was trained. The general description should be a short description of what the model is used for.</p> </li> <li> <p>The <code>data_description</code> should contain precise information on the type of data the  model was trained on, this can include the type of data (specimen, modality), the dimensions (physical, number of pixels) and the content (type of structures).</p> </li> </ol>"},{"location":"guides/careamist_api/usage/model_export/#optional-parameters","title":"Optional parameters","text":"<p>The <code>export_to_bmz</code> function has an optional parameter:</p> <pre><code>careamist.export_to_bmz(\n    path_to_archive=export_path / \"my_model.zip\",\n    friendly_model_name=\"CARE_mito\",\n    input_array=my_array,\n    authors=[\n        {\n            \"name\": \"Ignatius J. Reilly\",\n            \"affiliation\": \"Levy Pants\",\n            \"email\": \"ijr@levy.com\",\n        },\n        {\"name\": \"Myrna Minkoff\", \"orcid\": \"0000-0002-3291-8524\"},\n    ],\n    general_description=\"This model was trained to denoise 2D images of mitochondria.\",\n    data_description=\"The data was acquired on a confocal microscope [...]\",\n    channel_names=[\"mito\", \"nucleus\"],  # (1)!\n)\n</code></pre> <ol> <li>If your data has channels, then you should add their name!</li> </ol>"},{"location":"guides/careamist_api/usage/model_export/#examples-of-careamics-models","title":"Examples of CAREamics models","text":"<p>In progress</p> <p>This page is still in construction.</p>"},{"location":"guides/careamist_api/usage/prediction/","title":"Prediction","text":"<p>Prediction is done by calling <code>careamist.predict</code> on either path or arrays. By default, the prediction function will expect the same type of data (e.g. array or path) as it was trained on, but it is possible to predict on a different type of data.</p> <p>Prediction is performed using the current weights.</p>"},{"location":"guides/careamist_api/usage/prediction/#predict-on-arrays-or-paths","title":"Predict on arrays or paths","text":"On numpy arrays On Paths <pre><code>import numpy as np\n\narray = np.random.rand(256, 256)\n\ncareamist.predict(\n    source=array,\n)\n</code></pre> <pre><code>careamist.predict(\n    source=path_to_data,\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/prediction/#tiling","title":"Tiling","text":"<p>Often, an image will be too large to fit on the GPU memory, or will have dimensions that are incompatible with the model (e.g. odd dimensions). To solve this, the image can be tiled into smaller overlapping patches, predicted upon and ultimately recombined.</p> <p>In <code>careamist.predict</code>, this is done by passing two parameters:</p> <pre><code>careamist.predict(\n    source=array,\n    tile_size=[64, 64],  # (1)!\n    tile_overlap=[32, 32],  # (2)!\n)\n</code></pre> <ol> <li> <p>The tile sizes correspond to each spatial dimensions. A good start is the patch size used during training.</p> </li> <li> <p>The overlap is the number of pixels that each patch will overlap with its neighbors.</p> </li> </ol> <p>After prediction, each tile is cropped to half of the overlap in each of the directions it overlaps with a neighboring tile. The reason is to minimize edge artifacts.</p> <p>Tile and overlap model constraints</p> <p>Some models, e.g. UNet model, impose constraints on the tile and overlap sizes. This is a direct consequence of the model architecture.</p> <p>For instance, when using a UNet model, the pooling and upsampling operations are not compatible with any tile size:</p> <ul> <li>tile sizes must be equal to \\(k2^n\\), where \\(n\\) is the number of pooling layers (equal to the model depth) and \\(k\\) is an integer.</li> <li>overlaps must be even and larger than twice the receptive field.</li> </ul>"},{"location":"guides/careamist_api/usage/prediction/#test-time-augmentation","title":"Test time augmentation","text":"<p>Test-time augmentation applies augmentations to the prediction input and averages the  (de-augmented) predictions. This can improve the quality of the prediction. The TTA generates all possible flipped and rotated versions of the image.</p> <p>By default, test-time augmentation is applied by CAREamics. In order to deactivate TTA,  you can set <code>tta</code> to <code>False</code>.</p> <pre><code>careamist.predict(\n    source=array,\n    tta=False,  # (1)!\n)\n</code></pre> <ol> <li>By default, TTA is activated!</li> </ol> <p>Transforms and TTA</p> <p>If you have turned off transforms, or used the non default ones, then you should  turn off TTA, as it would otherwise create images that do not correspond to your training data.</p>"},{"location":"guides/careamist_api/usage/prediction/#using-batches","title":"Using batches","text":"<p>To potentially predict faster, you can predict on batches of images.</p> <pre><code>careamist.predict(\n    source=array,\n    batch_size=2,  # (1)!\n)\n</code></pre> <ol> <li>Each prediction step will be performed on 2 images or tiles.</li> </ol> <p>Batch and TTA</p> <p>Having <code>batch_size&gt;1</code> is compatible with the TTA.</p>"},{"location":"guides/careamist_api/usage/prediction/#changing-the-data-type","title":"Changing the data type","text":"<p>You can use a different type (in the sense <code>path</code> vs <code>array</code>) of data at prediction time by changing the <code>data_type</code> parameter.</p> <pre><code>careamist.predict(\n    source=path_to_data,\n    data_type=\"tiff\",  # (1)!\n)\n</code></pre> <ol> <li>As in the rest of CAREamics, the supported value are <code>tiff</code>, <code>array</code> and <code>custom</code>. </li> </ol>"},{"location":"guides/careamist_api/usage/prediction/#changing-the-axes","title":"Changing the axes","text":"<p>Similarly, if you want to predict on data that has different axes than the training data, as long as those have the same number of channels and spatial dimensions, then you can change the <code>axes</code> parameter.</p> <pre><code>careamist.predict(\n    source=other_array,\n    axes=\"SYX\",  # (1)!\n)\n</code></pre> <ol> <li>Obviously, this need to match <code>source</code>.</li> </ol>"},{"location":"guides/careamist_api/usage/prediction/#advanced-predict-on-custom-data-type","title":"(Advanced) Predict on custom data type","text":"<p>As for the training, one can predict on custom data types by providing a function that reads the data from a path and a function to filter the requested extension.</p> <pre><code>def read_npy(\n    path: Path,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return np.load(path)\n\n\n# example data\npredict_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# Train\ncareamist.predict(\n    source=predict_array,\n    read_source_func=read_npy,\n    extension_filter=\"*.npy\",\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/training/","title":"Training","text":"<p>You can provide data in various way to train your model: as a <code>numpy</code> array, using a path to a folder or files, or by using CAREamics data module class for more control (advanced).</p> <p>The details of how CAREamics deals with the loading and patching is detailed in the dataset section.</p> <p>Data type</p> <p>The data type of the source and targets must be the same as the one specified in the configuration. That is to say <code>array</code> in the case of <code>np.ndarray</code>, and <code>tiff</code> in the case of paths.</p>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-an-array","title":"Training by passing an array","text":"<p>CAREamics can be trained by simply passing numpy arrays.</p> Training by passing an array<pre><code>import numpy as np\n\ntrain_array = np.random.rand(256, 256)\nval_array = np.random.rand(256, 256)\n\ncareamist.train(\n    train_source=train_array,  # (1)!\n    val_source=val_array,  # (2)!\n)\n</code></pre> <ol> <li>All parameters to the <code>train</code> method must be specified by keyword.</li> <li>If you don't provide a validation source, CAREamics will use a fraction of the training data    to validate the model.</li> </ol> <p>Supervised training</p> <p>If you are training a supervised model, you must provide the target data as well.</p> <pre><code>careamist_supervised.train(\n    train_source=train_array,\n    train_target=target_array,\n    val_source=val_array,\n    val_target=val_target_array,\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-a-path","title":"Training by passing a path","text":"<p>The same thing can be done by passing a path to a folder or files.</p> Training by passing a path<pre><code>careamist.train(\n    train_source=path_to_train_data,  # (1)!\n    val_source=path_to_val_data,\n)\n</code></pre> <ol> <li>The path can point to a single file, or contain multiple files.</li> </ol> <p>Training from path</p> <p>To train from a path, the data type must be set to <code>tiff</code> or <code>custom</code> in the  configuration.</p>"},{"location":"guides/careamist_api/usage/training/#splitting-validation-from-training-data","title":"Splitting validation from training data","text":"<p>If you only provide training data, CAREamics will extract the validation data directly from the training set. There are two parameters controlling that behaviour: <code>val_percentage</code> and <code>val_minimum_split</code>.</p> <p><code>val_percentage</code> is the fraction of the training data that will be used for validation, and <code>val_minimum_split</code> is the minimum number of images used. If the percentage leads to a  number of patches smaller than <code>val_minimum_split</code>, CAREamics will use <code>val_minimum_split</code>.</p> Splitting validation from training data<pre><code>careamist.train(\n    train_source=train_array,\n    val_percentage=0.1,  # (1)!\n    val_minimum_split=5,  # (2)!\n)\n</code></pre> <ol> <li>10% of the training data will be used for validation.</li> <li>If the number of images is less than 5, CAREamics will use 5 images for validation.</li> </ol> <p>Arrays vs files</p> <p>The behaviour of <code>val_percentage</code> and <code>val_minimum_split</code> is based different depending on whether the source data is an array or a path. If the source is an array, the split is done on the patches (<code>N</code> patches are used for validation). If the source is a path, the split is done on the files (<code>N</code> files are used for validation).</p>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-a-traindatamodule-object","title":"Training by passing a <code>TrainDataModule</code> object","text":"<p>CAREamics provides a class to handle the data loading of custom data type. We will dive  in more details in the next section into what this class can be used for. Here is a  brief overview of how it is passed to the <code>train</code> method.</p> Training by passing a TrainDataModule object<pre><code>from careamics.lightning import TrainDataModule\n\ndata_module = TrainDataModule(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n\ncareamist.train(datamodule=data_module)\n</code></pre> <ol> <li>Here this does the same thing as passing the <code>train_source</code> directly into the <code>train</code> method.     In the next section, we will see a more useful example.</li> </ol>"},{"location":"guides/careamist_api/usage/training/#logging-the-training","title":"Logging the training","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>To decide on the logger, check out the Configuration section.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p>"},{"location":"guides/careamist_api/usage/training/#plotting-loss","title":"Plotting loss","text":"<p>To plot the loss curves, you can use the <code>CAREamist.get_losses</code> function:</p> Plotting losses<pre><code>loss_dict = careamist.get_losses()\n\nfrom matplotlib import pyplot as plt\nplt.plot(loss_dict[\"train_epoch\"], loss_dict[\"train_loss\"], loss_dict[\"val_epoch\"], loss_dict[\"val_loss\"])\nplt.legend([\"Train loss\", \"Val loss\"])\nplt.title(\"Losses\")\n</code></pre>"},{"location":"guides/cli/","title":"Command-line interface","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <pre><code>$ careamics --help\nUsage: careamics [OPTIONS] COMMAND [ARGS]...\n\nRun CAREamics algorithms from the command line, including Noise2Void and its many variants and cousins                                                      \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion  Install completion for the      \u2502 \n\u2502                       current shell.                  \u2502\n\u2502 --show-completion     Show completion for the current \u2502\n|                       shell, to copy it or customize  |\n|                       the installation.               \u2502\n\u2502 --help                Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 conf      Build and save CAREamics configuration files.                      \u2502\n\u2502 predict   Create and save predictions from CAREamics models.                 \u2502\n\u2502 train     Train CAREamics models.                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"guides/dev_resources/","title":"Developer resources","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <ul> <li>How to contribute to the project?</li> <li>How does the website work?</li> <li>CAREamics docstring conventions</li> <li>Conda-forge package maintenance</li> </ul>"},{"location":"guides/dev_resources/conda/","title":"Conda-forge package","text":"<p>CAREamics is available via conda-forge. Package maintenance is done through the careamics-feedstock.</p>"},{"location":"guides/dev_resources/conda/#new-careamics-version","title":"New CAREamics version","text":"<p>New version are automatically pulled by conda-forge from PyPi, and a PR is made to the feedstock repository.</p> <p>Typically, the PR updates the <code>version</code> number and the <code>hash</code> in the <code>meta.yaml</code> file.</p>"},{"location":"guides/dev_resources/conda/#updating-the-recipe","title":"Updating the recipe","text":"<p>When dependencies change, the conda recipe needs to be updated. This is done by making a PR (e.g. from careamics-feedstock fork).</p> <p>The best is to update the dependencies in the <code>meta.yaml</code> file, and push the changes to the automated PR on the official feedstock repository.</p>"},{"location":"guides/dev_resources/conda/#testing-the-recipe","title":"Testing the recipe","text":"<p>It may happen that the container build on the official feedstock repo fails, in which  case one needs to test locally what can go wrong. Here is how to do it:</p> <pre><code>cd careamics-feedstock\nconda create -n forge python=3.12\nconda activate forge\nconda install conda-build\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <p>Then create a local conda config:</p> <pre><code>vi recipe/conda_build_config.yaml\n</code></pre> <p>And add the following content:</p> <pre><code>python_min:\n  - \"3.11\"  # Set the minimum Python version required for your recipe\n</code></pre> <p>Finally, build the package:</p> <pre><code>conda build recipe\n</code></pre>"},{"location":"guides/dev_resources/contribute/","title":"Contribute to CAREamics","text":"<p>CAREamics is a free and open-source software and we are happy to receive contributions from the community! There are several ways to contribute to the project:</p> <ul> <li>Contribute applications</li> <li>Contribute methods and features</li> <li>Contribute to the documentation</li> </ul>"},{"location":"guides/dev_resources/contribute/#contribute-applications","title":"Contribute applications","text":"<p>Microscopy is a vast field and there are many different types of data and projects that can benefit from denoising. We only have applications that were brought to us or used in the publications.</p> <p>We would love to know more about different types of microscopy data, and whether  CAREamics helped analyse them or not. In the future, we want to illustrate how to scientifically apply the methods in CAREamics to research data and any example, working or failing, is of great help to the community!</p> <p>To contribute applications, open an issue so that we can discuss the problems you encountered or have a look at the results!</p> <p>Once an application is accepted, you can create a <code>jupyter notebook</code> and add it to the careamics-example repository. Finally, the website guide explains how to add the notebook to the  website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-methods-and-features","title":"Contribute methods and features","text":"<p>CAREamics is a growing project and we are always looking for new methods and features to better help the community. We are interested in any method that proved to be  valuable for microscopy data denoising and restoration.</p> <p>What about data formats?</p> <p>We currently only support <code>numpy arrays</code> and <code>tif</code> files. In the near future, we will focus on <code>zarr</code> data. We do not intend in maintaining support for other data formats.</p> <p>For particular data formats, we advise to use the custom data loading, which is described in the training guide.</p> <p>To contribute methods, open an issue so that we can discuss the method and its implementation. Same for new features!</p>"},{"location":"guides/dev_resources/contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request, make sure that you installed the <code>dev</code> optional dependencies of CAREamics:</p> <pre><code>pip install careamics[dev]\n</code></pre> <p>In particular, make sure that you use pre-commit before committing changes:</p> <pre><code>pre-commit install\n</code></pre> <p>The PR need to pass the tests and the pre-commit checks! Make sure to also fill in the  PR template and make a PR to the documentation website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>If you find any typo, mistakes or missing information in the documentation, feel free to make a PR to the documentation repository.</p> <p>Read the website guide to know how to better understand the various mechanisms implemented in the website.</p>"},{"location":"guides/dev_resources/docstring/","title":"Docstring conventions","text":"<p>CAREamics follows the numpydoc  docstring conventions. The is enforced by the use of the <code>numpydoc</code> pre-commit hook.</p> <p>On top of the numpy conventions, we try to build a more human readable docstring by adapting principles from Pandas docstring.</p>"},{"location":"guides/dev_resources/docstring/#parameter-declaration","title":"Parameter declaration","text":"<pre><code>\"\"\"\n    param : Union[str, int] \u274c\n    param : str or int \u2705\n    param : str | int \u2705\n\n    choice : Literal[\"a\", \"b\", \"c\"] \u274c\n    choice : {\"a\", \"b\", \"c\"} \u2705\n\n    param : Tuple[int, int] \u274c\n    param : (int, int) \u2705\n\n    sequence: List[int] \u274c\n    sequence: list of int \u2705\n\n    param : int \n        The default is 1. \u274c\n    param : int, default=1 \u2705\n\n    param : Optional[int] \u274c\n    param : int, optional \u2705\n\"\"\"\n</code></pre>"},{"location":"guides/dev_resources/docstring/#third-party-types","title":"Third-party types","text":"<pre><code>\"\"\"\n    param : pandas.DataFrame\n    param : NDArray\n    param : torch.Tensor\n    param : tensorflow.Tensor\n\"\"\"\n</code></pre>"},{"location":"guides/dev_resources/website/","title":"Github pages","text":"<p>The Github pages are built using mkdocs, more specifically the  mkdocs-material theme. Modifications to the theme were greatly inspired from pydev-guide.</p> <p>In this page, we describe some of the technical details on how to maintain this website.</p>"},{"location":"guides/dev_resources/website/#environment","title":"Environment","text":"<p>The <code>requirements.txt</code> file contains all the packages used to generate this website.</p>"},{"location":"guides/dev_resources/website/#build-the-pages-locally","title":"Build the pages locally","text":"<p>In order to build the pages locally, follow these steps:</p> <ol> <li>Fork this repository and clone it.</li> <li>Create a new environment and install the dependencies:     <pre><code>conda create -n careamics-docs python=3.12\npip install -r requirements.txt\n</code></pre></li> <li> <p>Run the following scripts (which are normally run by the CI):     <pre><code>sh scripts/check_out_repos.sh\nsh scripts/check_out_examples.sh\n</code></pre></p> <p>Note</p> <p>To run the scripts locally, you will need to install <code>jq</code>. For instance with <code>brew install jq</code>.</p> </li> <li> <p>Build the pages:     <pre><code>mkdocs serve\n</code></pre></p> </li> <li>Open the local link in your browser.</li> </ol> <p>Note: This will not show you the versioning mechanism. For this, check out the  Version release section.</p>"},{"location":"guides/dev_resources/website/#code-snippets","title":"Code snippets","text":"<p>Code snippets are all automatically tested in careamics-example and added automaticallt to the Github pages in the guides section.</p> <p>The script <code>scripts/check_out_examples.sh</code> clone the examples repository locally and upon building the pages, the code snippets are automatically added to the markdown by the PyMdown Snippets extension.</p> <p>It works as follows, first create a code snippet in a python file on  careamics-example:</p> careamics-example/some_example/some_file.py<pre><code># code necessary for running successfully the snippet\n# but not shown on the Github pages\nimport numpy as np\n\narray = np.ones((32, 32))\n\n# we add a snippet section:\nsum_array = np.sum(array) # snippets appearing on the website (can be multi-lines)\n\n# then more code or snippets sections\n...\n</code></pre> <p>Then, in the CAREamics Github pages source, the corresponding markdown file has the following content telling <code>PyMdown.Snippets</code> to  include the snippet section:</p> careamics.github.io/guides/some_example/some_file.py<pre><code>    To sum the array, simply do:\n\n    ```python\n    --8&lt;-- \"careamics-example/some_example/some_file.py:my_snippet\"\n    ```\n</code></pre> <p>The examples in the guide are automatically tested in the CI. To make sure it runs on the correct branches in the pull request, use the same branch name for the PR to the CAREamics source code and to the examples repository.</p>"},{"location":"guides/dev_resources/website/#jupyter-notebook-applications","title":"Jupyter notebook applications","text":"<p>The pages in the application section are automatically generated from the Jupyter notebooks in careamics-example  using mkdocs-jupyter. A bash script (<code>scripts/check_out_examples.sh</code>) checks out the repository and copies  all the notebooks referenced into the correct path in the application  folder. Finally, the script <code>scripts/gen_jupyter_nav.py</code> creates entries for each notebook  in the navigation file of mkdocs.</p>"},{"location":"guides/dev_resources/website/#adding-a-new-notebook","title":"Adding a new notebook","text":"<ol> <li>Add the notebook to <code>scripts/notebooks.json</code>. The structure is as follows:     <pre><code>{\n    \"applications\": [\n        ...,\n        {\n            \"name\": \"Name_of_the_Notebook\", // (1)!\n            \"description\": \"Some description of the notebook.\", // (2)!\n            \"cover\": \"File_name.jpeg\", // (3)!\n            \"source\": \"path/in/careamics-examples/repo/File_Name.ipynb\", // (4)!\n            \"destination\": \"Category_in_Applications\", // (5)!\n            \"tags\": \"3D, channels, fluorescence\" // (6)!\n        },\n        // (7)!\n    ]\n    ...\n}\n</code></pre><ol> <li><code>name</code> will be used as the name of the notebook, without the <code>_</code> when used as title and with <code>Name_of_the_Notebook.ipynb</code> as the file name after copy.</li> <li><code>description</code> will be shown in the cards corresponding to the notebook on the website.</li> <li><code>cover</code> is the name of the image file that will be used as the cover in the notebook card, the actual image should be placed in <code>docs/assets/notebook_covers</code>.</li> <li><code>source</code> is the path to the notebook in the <code>careamics-examples</code> repository.</li> <li><code>destination</code> is the category in the <code>applications</code> folder where the notebook will be copied to (e.g. all notebooks using <code>Noise2Void</code> are in the <code>Noise2Void</code> category).</li> <li><code>tags</code> are shown on the notebook card and inform on the characteristic of the data. These are comma separated values.</li> <li>You can add multiple notebooks!</li> </ol> </li> <li>You can test that the notebook was correctly added by running <code>sh scripts/notebooks.sh</code> then <code>mkdocs serve</code>.</li> </ol> <p>Cell tags</p> <p>By default, all notebook cell outputs are shown. To hide the output of a particular cell, add the tag <code>remove_output</code> to the cell. The <code>mkdocs.ynml</code> specifies that this  tag is used to hide cell outputs.</p> <p>For instance, this is useful for the training cell, which output hundreds of lines.</p>"},{"location":"guides/dev_resources/website/#code-reference","title":"Code reference","text":"<p>The code reference is generated using mkdocstrings,  the script <code>scripts/checkout_repos.sh</code> and the page building script <code>scripts/gen_ref_pages.py</code>.  To include a new package, simply add it to the <code>scripts/git_repositories.txt</code> file.</p> <pre><code>https://github.com/CAREamics/careamics\nhttps://github.com/CAREamics/careamics-portfolio\n&lt;new project here&gt;\n</code></pre>"},{"location":"guides/dev_resources/website/#updating-the-website-version","title":"Updating the website version","text":"<p>In principle, when a new release of CAREamics is made, the state of the documentation is saved into the corresponding version, and the documentation is tagged with the next (ongoing) version.</p> <p>For instance, the documentation is showing version <code>0.4</code>, upon release of version  <code>0.4</code>, the state of the documentation is saved. The latest documentation is then  tagged with version <code>0.5</code> (the next version) until this one is released.</p> <p>In order to keep track of versions, we use mike.  We apply the following procedure:</p> <ol> <li>Release version MAJOR.MINOR of CAREamics</li> <li>Tag the latest documentation with version MAJOR.(MINOR+1)   <pre><code>git tag MAJOR.(MINOR+1)\ngit push --tags\n</code></pre></li> </ol> <p>To visualize the pages with the versions, you can use:</p> <pre><code>mike serve\n</code></pre>"},{"location":"guides/dev_resources/website/#correcting-a-version-error","title":"Correcting a version error","text":"<p>All the versions are stored in the <code>gh-pages</code> branch. If you made a mistake in the version tagging, you can correct it by deleting the tag and pushing the changes.</p>"},{"location":"guides/lightning_api/","title":"Lightning API","text":"<p>The so-called \"Lightning API\" is how we refer to using the lightning modules  from CAREamics in a PyTorch Ligthning  pipeline. In our high-level API, these modules are  hidden from users and many checks, validations, error handling, and other  features are provided. However, if you want to have increased flexibility, for instance to use your own dataset, model or a different training loop, you can re-use many of  CAREamics modules in your own PyTorch Lightning pipeline.</p> Basic Usage<pre><code>import numpy as np\nfrom careamics.lightning import (  # (1)!\n    create_careamics_module,\n    create_predict_datamodule,\n    create_train_datamodule,\n)\nfrom careamics.prediction_utils import convert_outputs\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import (\n    ModelCheckpoint,\n)\n\n# training data\nrng = np.random.default_rng(42)\ntrain_array = rng.integers(0, 255, (32, 32)).astype(np.float32)\nval_array = rng.integers(0, 255, (32, 32)).astype(np.float32)\n\n# create lightning module\nmodel = create_careamics_module(  # (2)!\n    algorithm=\"n2v\",\n    loss=\"n2v\",\n    architecture=\"UNet\",\n)\n\n# create data module\ndata = create_train_datamodule(\n    train_data=train_array,\n    val_data=val_array,\n    data_type=\"array\",\n    patch_size=(16, 16),\n    axes=\"YX\",\n    batch_size=2,\n)\n\n# create trainer\ntrainer = Trainer(  # (3)!\n    max_epochs=1,\n    default_root_dir=mypath,\n    callbacks=[\n        ModelCheckpoint(  # (4)!\n            dirpath=mypath / \"checkpoints\",\n            filename=\"basic_usage_lightning_api\",\n        )\n    ],\n)\n\n# train\ntrainer.fit(model, datamodule=data)\n\n# predict\nmeans, stds = data.get_data_statistics()\npredict_data = create_predict_datamodule(\n    pred_data=val_array,\n    data_type=\"array\",\n    axes=\"YX\",\n    image_means=means,\n    image_stds=stds,\n    tile_size=(8, 8),  # (5)!\n    tile_overlap=(2, 2),\n)\n\n# predict\npredicted = trainer.predict(model, datamodule=predict_data)\npredicted_stitched = convert_outputs(predicted, tiled=True)  # (6)!\n</code></pre> <ol> <li> <p>We provide convenience functions to create the various Lightning modules.</p> </li> <li> <p>Each convenience function will have a set of algorithms. Often, these correspond  to the parameters in the CAREamics configuration. You can check the next pages for more details.</p> </li> <li> <p>As for any Lightning pipeline, you need to instantiate a <code>Trainer</code>.</p> </li> <li> <p>This way, you have all freedom to set your own callbacks.</p> </li> <li> <p>Our prediction Lightning data module has the possibility to break the images into overlapping tiles.</p> </li> <li> <p>If you predicted using tiled images, you need to recombine the tiles into images. We provide a general function to take care of this.</p> </li> </ol> <p>There are three types of Lightning modules in CAREamics:</p> <ul> <li>Lightning Module</li> <li>Training Lightning Datamodule</li> <li>Prediction Lightning Datamodule</li> </ul> <p>In the next pages, we give more details on the various parameters of the convenience  functions. For the rest, refer the the PyTorch Lightning documentation.</p>"},{"location":"guides/napari_plugin/","title":"CAREamics napari plugin","text":"<p>CAREamics provides a napari plugin for running our algorithms with a friendly GUI. Check out the installation instructions to get started.</p> <p> </p>"},{"location":"guides/napari_plugin/#overview","title":"Overview","text":"<p>The CAREamics napari plugin currently supports CARE, Noise2Noise, and Noise2Void algorithms. Users are guided from top to bottom through the following steps:</p> <ol> <li>Loading data: Load data by using napari layers or from a folder.</li> <li>Training: Train a model.</li> <li>Monitor training progress: Monitor the training progress via a plot or TensorBoard.</li> <li>Prediction: Predict on a layer or on files from a folder.</li> <li>Saving models: Save the trained model for later use as BioImage.io or PyTorch model.</li> </ol>"},{"location":"guides/napari_plugin/#quick-start","title":"Quick start","text":"<p>The easiest way to try out the plugin, is to start the plugin and load a sample dataset. You can do this by running the following command in your terminal:</p> <pre><code>napari\n</code></pre> <p>Then, in the napari UI, select \"Train CAREamics (CAREamics)\" from the plugin menu:</p> <p> </p> <p>The sample data can be downloaded from: File &gt; Open Sample &gt; CAREamics:</p> <p> </p>"},{"location":"guides/napari_plugin/#documentation","title":"Documentation","text":"<p>In this section, we describe in more details the different elements of the UI.</p> <p>Having an issue?</p> <p>In the header of the plugin, click on the Github icon to open an issue, we will be happy to help you!</p>"},{"location":"guides/napari_plugin/#algorithm","title":"Algorithm","text":"<p>The first step is to select the algorithm you want to use via the drop-down menu.</p> <p> </p> <p>GPU - CPU indicator</p> <p>Next to the algorithm name, you can see a GPU - CPU indicator. This indicates whether the algorithm is running on the GPU or CPU. If you have a compatible GPU, the algorithm will run on it by default. If not, it will run on the CPU.</p> <p>GPU not available</p> <p>If you expected the GPU-CPU indicator to be showing a green \"GPU\", but it isn't, then the problem is likely in the way your <code>conda</code> environment was set up.</p> <p>Make sure that you have a CUDA version that is compatible with your GPU and the PyTorch version you installed.</p>"},{"location":"guides/napari_plugin/#loading-data","title":"Loading data","text":"<p>Loading data can be done either by using images already loaded in napari (<code>From layers</code>), or by loading images from a folder (<code>From disk</code>). The plugin will automatically detect the file format and load the images accordingly. The tab that is selected tells the plugin the source of the data.</p> <p>Unsurprisingly, the <code>Train</code> data is used for training, and the <code>Val</code> one for validation. When using CARE, additional target train and validation data are required.</p> <p> </p> <p>Size of the validation data</p> <p>If your validation data is too large, this will slow down training.</p> <p>Split validation from training data</p> <p>If you set the <code>Val</code> data to the same layers or folder as the <code>Train</code> data, the plugin will automatically split the data. There are two parameters used to control the amount of data used for validation, these are found in the advanced settings.</p> <p>Loading from a folder</p> <p>Loading from a folder will not show the images in napari.</p>"},{"location":"guides/napari_plugin/#training","title":"Training","text":"<p>Once you have selected where the data is, nothing has happened yet! You can then set the training parameters.</p> <p> </p> <p>Here are the meaning of each of the parameters:</p> <ul> <li><code>Enable 3D</code>: runs the training in 3D. This is only available if you set a <code>Z</code> axis in <code>Axes</code>.</li> <li><code>Axes</code>: enter here the axes of your data. Accepted values are <code>C</code>, <code>Z</code>, <code>Y</code>, <code>X</code>, <code>T</code> and <code>S</code>. </li> <li><code>N epochs</code>: number of epochs for which to train the model. This parameter influences the total training time.</li> <li><code>Batch size</code>: number of images to use in each training step. This parameter influences the total training time, but also the GPU memory used.</li> <li><code>Patch XY</code>: size of the patches to use in the XY plane. This parameter impacts the GPU memory used.</li> <li><code>Patch Z</code>: size of the patches to use in the Z plane. This parameter impacts the GPU memory used. It is only available if you set a <code>Z</code> axis in <code>Axes</code>.</li> </ul> <p>Click on <code>Train</code> to start training. Training can be stopped at any time by clicking on the <code>Stop</code> button. The training will be saved. You can then<code>Reset</code> the model to start training again from scratch.</p> <p>What to do when GPU memory is limited?</p> <p>Often times, you will run out of GPU memory when training a model. This is especially true for 3D data. In this case, you can try to reduce the <code>Patch XY</code> and <code>Patch Z</code> parameters. This will reduce the size of the images used for training, and thus reduce the GPU memory used. </p> <p>If this is not sufficient, you can also try to reduce the <code>Batch size</code> parameter. This will reduce the number of images used for training at each step, and thus reduce the GPU memory used. </p>"},{"location":"guides/napari_plugin/#advanced-settings","title":"Advanced settings","text":"<p>If you click on the gear icon in the top right corner of the training tab, you will see a list of advanced settings. These settings are not necessary to change for most users, but can be useful for advanced users.</p> <p> </p> <p>The parameters are the following:</p> <ul> <li><code>Experiment name</code>: name of the experiment used in the logs.</li> <li><code>Validation</code><ul> <li><code>Percentage</code>: percentage of the training data (number of patches or images, depending on whether the data is loaded fully in memory) used for validation.</li> <li><code>Minimum split</code>: minimum number of patches or images used for validation.</li> </ul> </li> <li><code>Augmentations</code><ul> <li><code>X Flip</code>: uncheck to disable the X flip augmentation.</li> <li><code>Y Flip</code>: uncheck to disable the Y flip augmentation.</li> <li><code>90 Rotations</code>: uncheck to disable the 90 degree rotations augmentation.</li> </ul> </li> <li><code>N2V2</code>: use N2V2 rather than Noise2Void. Only available when training using Noise2Void (see algorithms).</li> <li><code>UNet parameters</code><ul> <li><code>Depth</code>: depth of the UNet. Larger depth means more layers, and potentially a more powereful model. But it may also lead to overfitting, slower learning, and will take more memory on GPU.</li> <li><code>N filters</code>: number of filters in the first layer of the UNet. Larger number means more filters, and potentially a more powereful model. But it may also lead to overfitting, slower learning, and will take more memory on GPU.</li> </ul> </li> </ul>"},{"location":"guides/napari_plugin/#monitoring-training-progress","title":"Monitoring training progress","text":"<p>While training, you can monitor the training progress via a plot or TensorBoard. Click on <code>Open in TensorBoard</code> to access the TensorBoard UI. </p> <p> </p>"},{"location":"guides/napari_plugin/#prediction","title":"Prediction","text":"<p>Once the training is finished, you can predict on a layer or on files from a folder. The tab that is selected tells the plugin the source of the data.</p> <p> </p> <p>Large image can be tiled for prediction. If you select <code>Tile prediction</code>, the following parameters will be available:</p> <ul> <li><code>XY tile size</code>: size of the tiles in the XY plane. This parameter impacts the GPU memory used (less memory used per prediction for smaller sizes) and the duration of the prediction (longer prediction time for smaller sizes).</li> <li><code>Z tile size</code>: size of the tiles in the Z plane. This parameter impacts the GPU memory used (less memory used per prediction for smaller sizes) and the duration of the prediction (longer prediction time for smaller sizes). It is only available if you set a <code>Z</code> axis in <code>Axes</code>.</li> <li><code>Batch size</code>: number of images to use in each prediction step.</li> </ul> <p>Image is too large</p> <p>If your image is too large and causes the prediction to fail, you can use tiling to break it into smaller pieces that are manageable by your GPU memory.</p> <p>Too many tiles</p> <p>Currently, we have no way to stop prediction (other than stopping napari). If you have a large number of tiles (e.g. XY tile size too small), the prediction will take a long time!</p>"},{"location":"guides/napari_plugin/#saving-models","title":"Saving models","text":"<p>Finally, you can save the trained model for later use. The plugin supports saving the model as a BioImage.io model or a PyTorch model. </p> <p> </p>"},{"location":"guides/tutorials/","title":"Tutorials","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <ul> <li>Using WandB and TensorBoard</li> <li>Predicting on a validation image throughout training (Callback)</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>CAREamics is a deep-learning library and we therefore recommend having GPU support as training the algorithms on the CPU can be very slow. For macOS silicon-acceleration (MPS), please refer to the specific sections!</p> <ul> <li>Install using mamba/conda</li> <li>Install using uv</li> </ul>"},{"location":"installation/conda_mamba/","title":"Install using mamba/conda","text":"<p>We recommend using mamba (miniforge)  to install all packages in a virtual environment. As an alternative, you can use conda  (miniconda) with the same commands (replacing <code>mamba</code> by <code>conda</code>). </p> <p>This section install CAREamics for use in your own library or tool, via jupyter notebook or as scripts. For the napari plugin, refer to the next section.</p> Linux and WindowsmacOS (no GPU) <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Verify that the GPU is available:</p> <pre><code>python -c \"import torch; print([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\"\n</code></pre> <p>This should show a list of available GPUs. If the list is empty, then you will need to change the <code>pytorch</code> and <code>pytorch-cuda</code> versions to match your hardware (linux and windows).</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol> <p>These instructions were tested on a linux virtual machine (RedHat 8.6) with a  NVIDIA A40-8Q GPU.</p> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch::pytorch torchvision -c pytorch\n</code></pre> <p> Note that this will probably not install silicon GPU acceleration. If you want GPU acceleration, please refer to the relevant section to ensure that you install packages for the correct platform.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol>"},{"location":"installation/conda_mamba/#extra-dependencies","title":"Extra dependencies","text":"<p>CAREamics extra dependencies can be installed by specifying them in brackets. In the previous section we installed <code>careamics[examples]</code>. You can add other extra dependencies, for instance <code>wandb</code> by doing:</p> <pre><code>pip install \"careamics[examples, wandb]\"\n</code></pre> <p>Here is a list of the extra dependencies:</p> <ul> <li><code>examples</code>: Dependencies required to run the example notebooks.</li> <li><code>wandb</code>: Dependencies to use WandB as a logger.</li> <li><code>tensorboard</code>: Dependencies to use TensorBoard as a logger.</li> <li><code>dev</code>: Dependencies required to run all the tooling necessary to develop with CAREamics.</li> </ul>"},{"location":"installation/conda_mamba/#macos-silicon-gpu","title":"MacOS silicon GPU","text":"condamamba <ol> <li>Open the terminal and type <code>conda</code> to verify that conda is available.</li> <li> <p>Create a new environment:</p> <pre><code>CONDA_SUBDIR=osx-arm64 conda create -n careamics python=3.10\nconda activate careamics\nconda config --env --set subdir osx-arm64\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10 --platform osx-arm64\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol>"},{"location":"installation/conda_mamba/#quickstart","title":"Quickstart","text":"<p>Once you have installed CAREamics, the easiest way to get started is to look at the applications for full examples and the  guides for in-depth tweaking.</p>"},{"location":"installation/conda_mamba/#careamics-napari-plugin","title":"CAREamics napari plugin","text":"Linux and WindowsmacOS (no GPU) <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Verify that the GPU is available:</p> <pre><code>python -c \"import torch; print([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\"\n</code></pre> <p>This should show a list of available GPUs. If the list is empty, then you will need to change the <code>pytorch</code> and <code>pytorch-cuda</code> versions to match your hardware (linux and windows).</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol> <p>These instructions were tested on a linux virtual machine (RedHat 8.6) with a  NVIDIA A40-8Q GPU.</p> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> <p> Note that this will probably not install silicon GPU acceleration. If you want GPU acceleration, please refer to the relevant section to ensure that you install packages for the correct platform.</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol>"},{"location":"installation/conda_mamba/#macos-silicon-gpu_1","title":"MacOS silicon GPU","text":"condamamba <ol> <li>Open the terminal and type <code>conda</code> to verify that conda is available.</li> <li> <p>Create a new environment:</p> <pre><code>CONDA_SUBDIR=osx-arm64 conda create -n careamics python=3.10\nconda activate careamics\nconda config --env --set subdir osx-arm64\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10 --platform osx-arm64\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official instructions     while specifying the platform. As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol>"},{"location":"installation/uv/","title":"Installing CAREamics with uv","text":"<p>Follow uv installation guidelines to use <code>uv</code>.</p> <p>Important</p> <p>Please refer to the installing PyTorch section of <code>uv</code> documentation should you encounter issues running on GPU.</p>"},{"location":"installation/uv/#using-notebooks","title":"Using notebooks","text":"<p>To run notebooks, install juv:</p> Installing juv<pre><code>uv tool install juv\n</code></pre>"},{"location":"installation/uv/#running-careamics-examples","title":"Running CAREamics examples","text":"<p>CAREamics-examples contains multiple notebook examples that can be directly run with <code>uv</code>:</p> Run a notebook<pre><code>juv run notebook.ipynb\n</code></pre>"},{"location":"installation/uv/#running-your-own-notebook","title":"Running your own notebook","text":"<p>To run your own notebook, we recommend adding PEP723 metadata:</p> Add CAREamics dependency<pre><code>juv add my_notebook.ipynb \"careamics[examples]\"\njuv run my_notebook.ipynb\n</code></pre> <p>You can also pin the CAREamics dependency in order to improve reproducibility:</p> Pin CAREamics dependency<pre><code>juv add my_notebook.ipynb \"careamics[examples]==0.0.16\"\n</code></pre>"},{"location":"installation/uv/#careamics-as-a-dependency","title":"CAREamics as a dependency","text":"<p>In this section, we will describe creating a project with CAREamics as a dependency, using standard <code>uv</code>:</p> <pre><code>uv init careamics_project\ncd careamics_project\nuv add \"careamics[examples]&gt;=0.0.16\"  # (1)!\n</code></pre> <ol> <li>Pin the dependency for reproducibility</li> </ol>"},{"location":"reference/","title":"Code Reference","text":"CAREamics CAREamics napari"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index.md</li> <li>careamics<ul> <li>index.md</li> <li>careamist</li> <li>cli<ul> <li>conf</li> <li>main</li> <li>utils</li> </ul> </li> <li>config<ul> <li>algorithms<ul> <li>care_algorithm_model</li> <li>hdn_algorithm_model</li> <li>microsplit_algorithm_model</li> <li>n2n_algorithm_model</li> <li>n2v_algorithm_model</li> <li>unet_algorithm_model</li> <li>vae_algorithm_model</li> </ul> </li> <li>architectures<ul> <li>architecture_model</li> <li>lvae_model</li> <li>unet_model</li> </ul> </li> <li>callback_model</li> <li>configuration</li> <li>configuration_factories</li> <li>configuration_io</li> <li>data<ul> <li>data_model</li> <li>ng_data_model</li> <li>patch_filter<ul> <li>filter_model</li> <li>mask_filter_model</li> <li>max_filter_model</li> <li>meanstd_filter_model</li> <li>shannon_filter_model</li> </ul> </li> <li>patching_strategies<ul> <li>_overlapping_patched_model</li> <li>_patched_model</li> <li>random_patching_model</li> <li>sequential_patching_model</li> <li>tiled_patching_model</li> <li>whole_patching_model</li> </ul> </li> </ul> </li> <li>inference_model</li> <li>likelihood_model</li> <li>loss_model</li> <li>nm_model</li> <li>optimizer_models</li> <li>support<ul> <li>supported_activations</li> <li>supported_algorithms</li> <li>supported_architectures</li> <li>supported_data</li> <li>supported_filters</li> <li>supported_loggers</li> <li>supported_losses</li> <li>supported_optimizers</li> <li>supported_patching_strategies</li> <li>supported_pixel_manipulations</li> <li>supported_struct_axis</li> <li>supported_transforms</li> </ul> </li> <li>tile_information</li> <li>training_model</li> <li>transformations<ul> <li>n2v_manipulate_model</li> <li>normalize_model</li> <li>transform_model</li> <li>transform_unions</li> <li>xy_flip_model</li> <li>xy_random_rotate90_model</li> </ul> </li> <li>validators<ul> <li>model_validators</li> <li>validator_utils</li> </ul> </li> </ul> </li> <li>conftest</li> <li>dataset<ul> <li>dataset_utils<ul> <li>dataset_utils</li> <li>file_utils</li> <li>iterate_over_files</li> <li>running_stats</li> </ul> </li> <li>in_memory_dataset</li> <li>in_memory_pred_dataset</li> <li>in_memory_tiled_pred_dataset</li> <li>iterable_dataset</li> <li>iterable_pred_dataset</li> <li>iterable_tiled_pred_dataset</li> <li>patching<ul> <li>patching</li> <li>random_patching</li> <li>sequential_patching</li> <li>validate_patch_dimension</li> </ul> </li> <li>tiling<ul> <li>collate_tiles</li> <li>lvae_tiled_patching</li> <li>tiled_patching</li> </ul> </li> </ul> </li> <li>dataset_ng<ul> <li>dataset</li> <li>demos<ul> <li>demo_patch_extractor</li> </ul> </li> <li>factory</li> <li>legacy_interoperability</li> <li>patch_extractor<ul> <li>demo_custom_image_stack_loader</li> <li>image_stack<ul> <li>czi_image_stack</li> <li>image_stack_protocol</li> <li>in_memory_image_stack</li> <li>zarr_image_stack</li> </ul> </li> <li>image_stack_loader</li> <li>patch_extractor</li> <li>patch_extractor_factory</li> </ul> </li> <li>patch_filter<ul> <li>coordinate_filter_protocol</li> <li>filter_factory</li> <li>mask_filter</li> <li>max_filter</li> <li>mean_std_filter</li> <li>patch_filter_protocol</li> <li>shannon_filter</li> </ul> </li> <li>patching_strategies<ul> <li>patching_strategy_protocol</li> <li>random_patching</li> <li>sequential_patching</li> <li>tiling_strategy</li> <li>whole_sample</li> </ul> </li> </ul> </li> <li>file_io<ul> <li>read<ul> <li>get_func</li> <li>tiff</li> </ul> </li> <li>write<ul> <li>get_func</li> <li>tiff</li> </ul> </li> </ul> </li> <li>lightning<ul> <li>callbacks<ul> <li>data_stats_callback</li> <li>hyperparameters_callback</li> <li>prediction_writer_callback<ul> <li>file_path_utils</li> <li>prediction_writer_callback</li> <li>write_strategy</li> <li>write_strategy_factory</li> </ul> </li> <li>progress_bar_callback</li> </ul> </li> <li>dataset_ng<ul> <li>data_module</li> <li>lightning_modules<ul> <li>care_module</li> <li>n2v_module</li> <li>unet_module</li> </ul> </li> </ul> </li> <li>lightning_module</li> <li>microsplit_data_module</li> <li>predict_data_module</li> <li>train_data_module</li> </ul> </li> <li>losses<ul> <li>fcn<ul> <li>losses</li> </ul> </li> <li>loss_factory</li> <li>lvae<ul> <li>loss_utils</li> <li>losses</li> </ul> </li> </ul> </li> <li>lvae_training<ul> <li>calibration</li> <li>dataset<ul> <li>config</li> <li>lc_dataset</li> <li>ms_dataset_ref</li> <li>multich_dataset</li> <li>multicrop_dset</li> <li>multifile_dataset</li> <li>types</li> <li>utils<ul> <li>data_utils</li> <li>empty_patch_fetcher</li> <li>index_manager</li> <li>index_switcher</li> </ul> </li> </ul> </li> <li>eval_utils</li> <li>get_config</li> <li>lightning_module</li> <li>metrics</li> <li>train_lvae</li> <li>train_utils</li> </ul> </li> <li>model_io<ul> <li>bioimage<ul> <li>_readme_factory</li> <li>bioimage_utils</li> <li>cover_factory</li> <li>model_description</li> </ul> </li> <li>bmz_io</li> <li>model_io_utils</li> </ul> </li> <li>models<ul> <li>activation</li> <li>layers</li> <li>lvae<ul> <li>layers</li> <li>likelihoods</li> <li>lvae</li> <li>noise_models</li> <li>stochastic</li> <li>utils</li> </ul> </li> <li>model_factory</li> <li>unet</li> </ul> </li> <li>prediction_utils<ul> <li>lvae_prediction</li> <li>lvae_tiling_manager</li> <li>prediction_outputs</li> <li>stitch_prediction</li> </ul> </li> <li>transforms<ul> <li>compose</li> <li>n2v_manipulate</li> <li>n2v_manipulate_torch</li> <li>normalize</li> <li>pixel_manipulation</li> <li>pixel_manipulation_torch</li> <li>struct_mask_parameters</li> <li>transform</li> <li>tta</li> <li>xy_flip</li> <li>xy_random_rotate90</li> </ul> </li> <li>utils<ul> <li>autocorrelation</li> <li>base_enum</li> <li>context</li> <li>lightning_utils</li> <li>logging</li> <li>metrics</li> <li>path_utils</li> <li>plotting</li> <li>ram</li> <li>receptive_field</li> <li>serializers</li> <li>torch_utils</li> <li>version</li> </ul> </li> </ul> </li> <li>careamics_napari<ul> <li>index.md</li> <li>_version</li> <li>base_plugin</li> <li>bmz<ul> <li>author_widget</li> <li>bmz_export_widget</li> </ul> </li> <li>care_plugin</li> <li>careamics_utils<ul> <li>algorithms</li> <li>callbacks</li> <li>care_configs</li> <li>configs</li> <li>free_memory</li> <li>n2n_configs</li> <li>n2v_configs</li> </ul> </li> <li>n2n_plugin</li> <li>n2v_plugin</li> <li>resources<ul> <li>resources</li> </ul> </li> <li>sample_data</li> <li>signals<ul> <li>prediction_status</li> <li>saving_signal</li> <li>saving_status</li> <li>training_status</li> </ul> </li> <li>utils<ul> <li>axes_utils</li> <li>gpu_utils</li> <li>workers</li> </ul> </li> <li>widgets<ul> <li>advanced_config</li> <li>axes_widget</li> <li>banner_widget</li> <li>care_config_window</li> <li>folder_widget</li> <li>gpu_widget</li> <li>magicgui_widgets</li> <li>n2n_config_window</li> <li>n2v_config_window</li> <li>predict_data_widget</li> <li>prediction_widget</li> <li>qt_widgets</li> <li>saving_widget</li> <li>scroll_wrapper</li> <li>tbplot_widget</li> <li>train_data_widget</li> <li>train_progress_widget</li> <li>training_configuration_widget</li> <li>training_widget</li> <li>utils</li> </ul> </li> <li>workers<ul> <li>prediction_worker</li> <li>training_worker</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/careamics/","title":"CAREamics","text":"<p>Use the navigation index on the left to explore the documentation.</p>"},{"location":"reference/careamics/careamist/","title":"careamist","text":"<p>A class to train, predict and export models in CAREamics.</p>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist","title":"<code>CAREamist</code>","text":"<p>Main CAREamics class, allowing training and prediction using various algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>pathlib.Path or str or CAREamics Configuration</code> <p>Path to a configuration file or a trained model.</p> required <code>work_dir</code> <code>str</code> <p>Path to working directory in which to save checkpoints and logs, by default None.</p> <code>None</code> <code>callbacks</code> <code>list of Callback</code> <p>List of callbacks to use during training and prediction, by default None.</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether a progress bar will be displayed during training, validation and prediction.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>CAREamicsModule</code> <p>CAREamics model.</p> <code>cfg</code> <code>Configuration</code> <p>CAREamics configuration.</p> <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> <code>experiment_logger</code> <code>TensorBoardLogger or WandbLogger</code> <p>Experiment logger, \"wandb\" or \"tensorboard\".</p> <code>work_dir</code> <code>Path</code> <p>Working directory.</p> <code>train_datamodule</code> <code>TrainDataModule</code> <p>Training datamodule.</p> <code>pred_datamodule</code> <code>PredictDataModule</code> <p>Prediction datamodule.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>class CAREamist:\n    \"\"\"Main CAREamics class, allowing training and prediction using various algorithms.\n\n    Parameters\n    ----------\n    source : pathlib.Path or str or CAREamics Configuration\n        Path to a configuration file or a trained model.\n    work_dir : str, optional\n        Path to working directory in which to save checkpoints and logs,\n        by default None.\n    callbacks : list of Callback, optional\n        List of callbacks to use during training and prediction, by default None.\n    enable_progress_bar : bool\n        Whether a progress bar will be displayed during training, validation and\n        prediction.\n\n    Attributes\n    ----------\n    model : CAREamicsModule\n        CAREamics model.\n    cfg : Configuration\n        CAREamics configuration.\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    experiment_logger : TensorBoardLogger or WandbLogger\n        Experiment logger, \"wandb\" or \"tensorboard\".\n    work_dir : pathlib.Path\n        Working directory.\n    train_datamodule : TrainDataModule\n        Training datamodule.\n    pred_datamodule : PredictDataModule\n        Prediction datamodule.\n    \"\"\"\n\n    @overload\n    def __init__(  # numpydoc ignore=GL08\n        self,\n        source: Union[Path, str],\n        work_dir: Union[Path, str] | None = None,\n        callbacks: list[Callback] | None = None,\n        enable_progress_bar: bool = True,\n    ) -&gt; None: ...\n\n    @overload\n    def __init__(  # numpydoc ignore=GL08\n        self,\n        source: Configuration,\n        work_dir: Union[Path, str] | None = None,\n        callbacks: list[Callback] | None = None,\n        enable_progress_bar: bool = True,\n    ) -&gt; None: ...\n\n    def __init__(\n        self,\n        source: Union[Path, str, Configuration],\n        work_dir: Union[Path, str] | None = None,\n        callbacks: list[Callback] | None = None,\n        enable_progress_bar: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Initialize CAREamist with a configuration object or a path.\n\n        A configuration object can be created using directly by calling `Configuration`,\n        using the configuration factory or loading a configuration from a yaml file.\n\n        Path can contain either a yaml file with parameters, or a saved checkpoint.\n\n        If no working directory is provided, the current working directory is used.\n\n        Parameters\n        ----------\n        source : pathlib.Path or str or CAREamics Configuration\n            Path to a configuration file or a trained model.\n        work_dir : str or pathlib.Path, optional\n            Path to working directory in which to save checkpoints and logs,\n            by default None.\n        callbacks : list of Callback, optional\n            List of callbacks to use during training and prediction, by default None.\n        enable_progress_bar : bool\n            Whether a progress bar will be displayed during training, validation and\n            prediction.\n\n        Raises\n        ------\n        NotImplementedError\n            If the model is loaded from BioImage Model Zoo.\n        ValueError\n            If no hyper parameters are found in the checkpoint.\n        ValueError\n            If no data module hyper parameters are found in the checkpoint.\n        \"\"\"\n        # select current working directory if work_dir is None\n        if work_dir is None:\n            self.work_dir = Path.cwd()\n            logger.warning(\n                f\"No working directory provided. Using current working directory: \"\n                f\"{self.work_dir}.\"\n            )\n        else:\n            self.work_dir = Path(work_dir)\n\n        # configuration object\n        if isinstance(source, Configuration):\n            self.cfg = source\n\n            # instantiate model\n            if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                self.model = FCNModule(\n                    algorithm_config=self.cfg.algorithm_config,\n                )\n            else:\n                raise NotImplementedError(\"Architecture not supported.\")\n\n        # path to configuration file or model\n        else:\n            # TODO: update this check so models can be downloaded directly from BMZ\n            source = check_path_exists(source)\n\n            # configuration file\n            if source.is_file() and (\n                source.suffix == \".yaml\" or source.suffix == \".yml\"\n            ):\n                # load configuration\n                self.cfg = load_configuration(source)\n\n                # instantiate model\n                if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                    self.model = FCNModule(\n                        algorithm_config=self.cfg.algorithm_config,\n                    )  # type: ignore\n                else:\n                    raise NotImplementedError(\"Architecture not supported.\")\n\n            # attempt loading a pre-trained model\n            else:\n                self.model, self.cfg = load_pretrained(source)\n\n        # define the checkpoint saving callback\n        self._define_callbacks(callbacks, enable_progress_bar)\n\n        # instantiate logger\n        csv_logger = CSVLogger(\n            name=self.cfg.experiment_name,\n            save_dir=self.work_dir / \"csv_logs\",\n        )\n\n        if self.cfg.training_config.has_logger():\n            if self.cfg.training_config.logger == SupportedLogger.WANDB:\n                experiment_logger: LOGGER_TYPES = [\n                    WandbLogger(\n                        name=self.cfg.experiment_name,\n                        save_dir=self.work_dir / Path(\"wandb_logs\"),\n                    ),\n                    csv_logger,\n                ]\n            elif self.cfg.training_config.logger == SupportedLogger.TENSORBOARD:\n                experiment_logger = [\n                    TensorBoardLogger(\n                        save_dir=self.work_dir / Path(\"tb_logs\"),\n                    ),\n                    csv_logger,\n                ]\n        else:\n            experiment_logger = [csv_logger]\n\n        # instantiate trainer\n        self.trainer = Trainer(\n            enable_progress_bar=enable_progress_bar,\n            callbacks=self.callbacks,\n            default_root_dir=self.work_dir,\n            logger=experiment_logger,\n            **self.cfg.training_config.lightning_trainer_config or {},\n        )\n\n        # place holder for the datamodules\n        self.train_datamodule: TrainDataModule | None = None\n        self.pred_datamodule: PredictDataModule | None = None\n\n    def _define_callbacks(\n        self, callbacks: list[Callback] | None, enable_progress_bar: bool\n    ) -&gt; None:\n        \"\"\"Define the callbacks for the training loop.\n\n        Parameters\n        ----------\n        callbacks : list of Callback, optional\n            List of callbacks to use during training and prediction, by default None.\n        enable_progress_bar : bool\n            Whether a progress bar will be displayed during training, validation and\n            prediction. It controls whether a `ProgressBarCallback` is added to the\n            callback list.\n        \"\"\"\n        self.callbacks = [] if callbacks is None else callbacks\n\n        # check that user callbacks are not any of the CAREamics callbacks\n        for c in self.callbacks:\n            if isinstance(c, ModelCheckpoint) or isinstance(c, EarlyStopping):\n                raise ValueError(\n                    \"ModelCheckpoint and EarlyStopping callbacks are already defined \"\n                    \"in CAREamics and should only be modified through the \"\n                    \"training configuration (see TrainingConfig).\"\n                )\n\n            if isinstance(c, HyperParametersCallback) or isinstance(\n                c, ProgressBarCallback\n            ):\n                raise ValueError(\n                    \"HyperParameter and ProgressBar callbacks are defined internally \"\n                    \"and should not be passed as callbacks.\"\n                )\n\n        # checkpoint callback saves checkpoints during training\n        self.callbacks.extend(\n            [\n                HyperParametersCallback(self.cfg),\n                ModelCheckpoint(\n                    dirpath=self.work_dir / Path(\"checkpoints\"),\n                    filename=f\"{self.cfg.experiment_name}_{{epoch:02d}}_step_{{step}}\",\n                    **self.cfg.training_config.checkpoint_callback.model_dump(),\n                ),\n            ]\n        )\n        if enable_progress_bar:\n            self.callbacks.append(ProgressBarCallback())\n\n        # early stopping callback\n        if self.cfg.training_config.early_stopping_callback is not None:\n            self.callbacks.append(\n                EarlyStopping(self.cfg.training_config.early_stopping_callback)\n            )\n\n    def stop_training(self) -&gt; None:\n        \"\"\"Stop the training loop.\"\"\"\n        # raise stop training flag\n        self.trainer.should_stop = True\n        self.trainer.limit_val_batches = 0  # skip  validation\n\n    # TODO: is there are more elegant way than calling train again after _train_on_paths\n    def train(\n        self,\n        *,\n        datamodule: TrainDataModule | None = None,\n        train_source: Union[Path, str, NDArray] | None = None,\n        val_source: Union[Path, str, NDArray] | None = None,\n        train_target: Union[Path, str, NDArray] | None = None,\n        val_target: Union[Path, str, NDArray] | None = None,\n        use_in_memory: bool = True,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data.\n\n        If a datamodule is provided, then training will be performed using it.\n        Alternatively, the training data can be provided as arrays or paths.\n\n        If `use_in_memory` is set to True, the source provided as Path or str will be\n        loaded in memory if it fits. Otherwise, training will be performed by loading\n        patches from the files one by one. Training on arrays is always performed\n        in memory.\n\n        If no validation source is provided, then the validation is extracted from\n        the training data using `val_percentage` and `val_minimum_split`. In the case\n        of data provided as Path or str, the percentage and minimum number are applied\n        to the number of files. For arrays, it is the number of patches.\n\n        Parameters\n        ----------\n        datamodule : TrainDataModule, optional\n            Datamodule to train on, by default None.\n        train_source : pathlib.Path or str or NDArray, optional\n            Train source, if no datamodule is provided, by default None.\n        val_source : pathlib.Path or str or NDArray, optional\n            Validation source, if no datamodule is provided, by default None.\n        train_target : pathlib.Path or str or NDArray, optional\n            Train target source, if no datamodule is provided, by default None.\n        val_target : pathlib.Path or str or NDArray, optional\n            Validation target source, if no datamodule is provided, by default None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n        val_percentage : float, optional\n            Percentage of validation extracted from training data, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of validation (patch or file) extracted from training data,\n            by default 1.\n\n        Raises\n        ------\n        ValueError\n            If both `datamodule` and `train_source` are provided.\n        ValueError\n            If sources are not of the same type (e.g. train is an array and val is\n            a Path).\n        ValueError\n            If the training target is provided to N2V.\n        ValueError\n            If neither a datamodule nor a source is provided.\n        \"\"\"\n        if datamodule is not None and train_source is not None:\n            raise ValueError(\n                \"Only one of `datamodule` and `train_source` can be provided.\"\n            )\n\n        # check that inputs are the same type\n        source_types = {\n            type(s)\n            for s in (train_source, val_source, train_target, val_target)\n            if s is not None\n        }\n        if len(source_types) &gt; 1:\n            raise ValueError(\"All sources should be of the same type.\")\n\n        # train\n        if datamodule is not None:\n            self._train_on_datamodule(datamodule=datamodule)\n\n        else:\n            # raise error if target is provided to N2V\n            if self.cfg.algorithm_config.algorithm == SupportedAlgorithm.N2V.value:\n                if train_target is not None:\n                    raise ValueError(\n                        \"Training target not compatible with N2V training.\"\n                    )\n\n            # dispatch the training\n            if isinstance(train_source, np.ndarray):\n                # mypy checks\n                assert isinstance(val_source, np.ndarray) or val_source is None\n                assert isinstance(train_target, np.ndarray) or train_target is None\n                assert isinstance(val_target, np.ndarray) or val_target is None\n\n                self._train_on_array(\n                    train_source,\n                    val_source,\n                    train_target,\n                    val_target,\n                    val_percentage,\n                    val_minimum_split,\n                )\n\n            elif isinstance(train_source, Path) or isinstance(train_source, str):\n                # mypy checks\n                assert (\n                    isinstance(val_source, Path)\n                    or isinstance(val_source, str)\n                    or val_source is None\n                )\n                assert (\n                    isinstance(train_target, Path)\n                    or isinstance(train_target, str)\n                    or train_target is None\n                )\n                assert (\n                    isinstance(val_target, Path)\n                    or isinstance(val_target, str)\n                    or val_target is None\n                )\n\n                self._train_on_path(\n                    train_source,\n                    val_source,\n                    train_target,\n                    val_target,\n                    use_in_memory,\n                    val_percentage,\n                    val_minimum_split,\n                )\n\n            else:\n                raise ValueError(\n                    f\"Invalid input, expected a str, Path, array or TrainDataModule \"\n                    f\"instance (got {type(train_source)}).\"\n                )\n\n    def _train_on_datamodule(self, datamodule: TrainDataModule) -&gt; None:\n        \"\"\"\n        Train the model on the provided datamodule.\n\n        Parameters\n        ----------\n        datamodule : TrainDataModule\n            Datamodule to train on.\n        \"\"\"\n        # register datamodule\n        self.train_datamodule = datamodule\n\n        # set defaults (in case `stop_training` was called before)\n        self.trainer.should_stop = False\n        self.trainer.limit_val_batches = 1.0  # 100%\n\n        # train\n        self.trainer.fit(self.model, datamodule=datamodule)\n\n    def _train_on_array(\n        self,\n        train_data: NDArray,\n        val_data: NDArray | None = None,\n        train_target: NDArray | None = None,\n        val_target: NDArray | None = None,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 5,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data arrays.\n\n        Parameters\n        ----------\n        train_data : NDArray\n            Training data.\n        val_data : NDArray, optional\n            Validation data, by default None.\n        train_target : NDArray, optional\n            Train target data, by default None.\n        val_target : NDArray, optional\n            Validation target data, by default None.\n        val_percentage : float, optional\n            Percentage of patches to use for validation, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of patches to use for validation, by default 5.\n        \"\"\"\n        # create datamodule\n        datamodule = TrainDataModule(\n            data_config=self.cfg.data_config,\n            train_data=train_data,\n            val_data=val_data,\n            train_data_target=train_target,\n            val_data_target=val_target,\n            val_percentage=val_percentage,\n            val_minimum_split=val_minimum_split,\n        )\n\n        # train\n        self.train(datamodule=datamodule)\n\n    def _train_on_path(\n        self,\n        path_to_train_data: Union[Path, str],\n        path_to_val_data: Union[Path, str] | None = None,\n        path_to_train_target: Union[Path, str] | None = None,\n        path_to_val_target: Union[Path, str] | None = None,\n        use_in_memory: bool = True,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data paths.\n\n        Parameters\n        ----------\n        path_to_train_data : pathlib.Path or str\n            Path to the training data.\n        path_to_val_data : pathlib.Path or str, optional\n            Path to validation data, by default None.\n        path_to_train_target : pathlib.Path or str, optional\n            Path to train target data, by default None.\n        path_to_val_target : pathlib.Path or str, optional\n            Path to validation target data, by default None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n        val_percentage : float, optional\n            Percentage of files to use for validation, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of files to use for validation, by default 1.\n        \"\"\"\n        # sanity check on data (path exists)\n        path_to_train_data = check_path_exists(path_to_train_data)\n\n        if path_to_val_data is not None:\n            path_to_val_data = check_path_exists(path_to_val_data)\n\n        if path_to_train_target is not None:\n            path_to_train_target = check_path_exists(path_to_train_target)\n\n        if path_to_val_target is not None:\n            path_to_val_target = check_path_exists(path_to_val_target)\n\n        # create datamodule\n        datamodule = TrainDataModule(\n            data_config=self.cfg.data_config,\n            train_data=path_to_train_data,\n            val_data=path_to_val_data,\n            train_data_target=path_to_train_target,\n            val_data_target=path_to_val_target,\n            use_in_memory=use_in_memory,\n            val_percentage=val_percentage,\n            val_minimum_split=val_minimum_split,\n        )\n\n        # train\n        self.train(datamodule=datamodule)\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self, source: PredictDataModule\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self,\n        source: Union[Path, str],\n        *,\n        batch_size: int = 1,\n        tile_size: tuple[int, ...] | None = None,\n        tile_overlap: tuple[int, ...] | None = (48, 48),\n        axes: str | None = None,\n        data_type: Literal[\"tiff\", \"custom\"] | None = None,\n        tta_transforms: bool = False,\n        dataloader_params: dict | None = None,\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self,\n        source: NDArray,\n        *,\n        batch_size: int = 1,\n        tile_size: tuple[int, ...] | None = None,\n        tile_overlap: tuple[int, ...] | None = (48, 48),\n        axes: str | None = None,\n        data_type: Literal[\"array\"] | None = None,\n        tta_transforms: bool = False,\n        dataloader_params: dict | None = None,\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    def predict(\n        self,\n        source: Union[PredictDataModule, Path, str, NDArray],\n        *,\n        batch_size: int = 1,\n        tile_size: tuple[int, ...] | None = None,\n        tile_overlap: tuple[int, ...] | None = (48, 48),\n        axes: str | None = None,\n        data_type: Literal[\"array\", \"tiff\", \"custom\"] | None = None,\n        tta_transforms: bool = False,\n        dataloader_params: dict | None = None,\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        **kwargs: Any,\n    ) -&gt; Union[list[NDArray], NDArray]:\n        \"\"\"\n        Make predictions on the provided data.\n\n        Input can be a CAREamicsPredData instance, a path to a data file, or a numpy\n        array.\n\n        If `data_type`, `axes` and `tile_size` are not provided, the training\n        configuration parameters will be used, with the `patch_size` instead of\n        `tile_size`.\n\n        Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n        parameter. The TTA augmentation applies all possible flip and 90 degrees\n        rotations to the prediction input and averages the predictions. TTA augmentation\n        should not be used if you did not train with these augmentations.\n\n        Note that if you are using a UNet model and tiling, the tile size must be\n        divisible in every dimension by 2**d, where d is the depth of the model. This\n        avoids artefacts arising from the broken shift invariance induced by the\n        pooling layers of the UNet. If your image has less dimensions, as it may\n        happen in the Z dimension, consider padding your image.\n\n        Parameters\n        ----------\n        source : PredictDataModule, pathlib.Path, str or numpy.ndarray\n            Data to predict on.\n        batch_size : int, default=1\n            Batch size for prediction.\n        tile_size : tuple of int, optional\n            Size of the tiles to use for prediction.\n        tile_overlap : tuple of int, default=(48, 48)\n            Overlap between tiles, can be None.\n        axes : str, optional\n            Axes of the input data, by default None.\n        data_type : {\"array\", \"tiff\", \"custom\"}, optional\n            Type of the input data.\n        tta_transforms : bool, default=True\n            Whether to apply test-time augmentation.\n        dataloader_params : dict, optional\n            Parameters to pass to the dataloader.\n        read_source_func : Callable, optional\n            Function to read the source data.\n        extension_filter : str, default=\"\"\n            Filter for the file extension.\n        **kwargs : Any\n            Unused.\n\n        Returns\n        -------\n        list of NDArray or NDArray\n            Predictions made by the model.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the configuration.\n        ValueError\n            If tile size is not divisible by 2**depth for UNet models.\n        ValueError\n            If tile overlap is not specified.\n        \"\"\"\n        if (\n            self.cfg.data_config.image_means is None\n            or self.cfg.data_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided in the configuration.\")\n\n        # tile size for UNets\n        if tile_size is not None:\n            model = self.cfg.algorithm_config.model\n\n            if model.architecture == SupportedArchitecture.UNET.value:\n                # tile size must be equal to k*2^n, where n is the number of pooling\n                # layers (equal to the depth) and k is an integer\n                depth = model.depth\n                tile_increment = 2**depth\n\n                for i, t in enumerate(tile_size):\n                    if t % tile_increment != 0:\n                        raise ValueError(\n                            f\"Tile size must be divisible by {tile_increment} along \"\n                            f\"all axes (got {t} for axis {i}). If your image size is \"\n                            f\"smaller along one axis (e.g. Z), consider padding the \"\n                            f\"image.\"\n                        )\n\n            # tile overlaps must be specified\n            if tile_overlap is None:\n                raise ValueError(\"Tile overlap must be specified.\")\n\n        # create the prediction\n        self.pred_datamodule = create_predict_datamodule(\n            pred_data=source,\n            data_type=data_type or self.cfg.data_config.data_type,  # type: ignore\n            axes=axes or self.cfg.data_config.axes,\n            image_means=self.cfg.data_config.image_means,\n            image_stds=self.cfg.data_config.image_stds,\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            batch_size=batch_size or self.cfg.data_config.batch_size,\n            tta_transforms=tta_transforms,\n            read_source_func=read_source_func,\n            extension_filter=extension_filter,\n            dataloader_params=dataloader_params,\n        )\n\n        # predict\n        predictions = self.trainer.predict(\n            model=self.model, datamodule=self.pred_datamodule\n        )\n        return convert_outputs(predictions, self.pred_datamodule.tiled)\n\n    def predict_to_disk(\n        self,\n        source: Union[PredictDataModule, Path, str],\n        *,\n        batch_size: int = 1,\n        tile_size: tuple[int, ...] | None = None,\n        tile_overlap: tuple[int, ...] | None = (48, 48),\n        axes: str | None = None,\n        data_type: Literal[\"tiff\", \"custom\"] | None = None,\n        tta_transforms: bool = False,\n        dataloader_params: dict | None = None,\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        write_type: Literal[\"tiff\", \"custom\"] = \"tiff\",\n        write_extension: str | None = None,\n        write_func: WriteFunc | None = None,\n        write_func_kwargs: dict[str, Any] | None = None,\n        prediction_dir: Union[Path, str] = \"predictions\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Make predictions on the provided data and save outputs to files.\n\n        The predictions will be saved in a new directory 'predictions' within the set\n        working directory. The directory stucture within the 'predictions' directory\n        will match that of the source directory.\n\n        The `source` must be from files and not arrays. The file names of the\n        predictions will match those of the source. If there is more than one sample\n        within a file, the samples will be saved to seperate files. The file names of\n        samples will have the name of the corresponding source file but with the sample\n        index appended. E.g. If the the source file name is 'images.tiff' then the first\n        sample's prediction will be saved with the file name \"image_0.tiff\".\n        Input can be a PredictDataModule instance, a path to a data file, or a numpy\n        array.\n\n        If `data_type`, `axes` and `tile_size` are not provided, the training\n        configuration parameters will be used, with the `patch_size` instead of\n        `tile_size`.\n\n        Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n        parameter. The TTA augmentation applies all possible flip and 90 degrees\n        rotations to the prediction input and averages the predictions. TTA augmentation\n        should not be used if you did not train with these augmentations.\n\n        Note that if you are using a UNet model and tiling, the tile size must be\n        divisible in every dimension by 2**d, where d is the depth of the model. This\n        avoids artefacts arising from the broken shift invariance induced by the\n        pooling layers of the UNet. If your image has less dimensions, as it may\n        happen in the Z dimension, consider padding your image.\n\n        Parameters\n        ----------\n        source : PredictDataModule or pathlib.Path, str\n            Data to predict on.\n        batch_size : int, default=1\n            Batch size for prediction.\n        tile_size : tuple of int, optional\n            Size of the tiles to use for prediction.\n        tile_overlap : tuple of int, default=(48, 48)\n            Overlap between tiles.\n        axes : str, optional\n            Axes of the input data, by default None.\n        data_type : {\"array\", \"tiff\", \"custom\"}, optional\n            Type of the input data.\n        tta_transforms : bool, default=True\n            Whether to apply test-time augmentation.\n        dataloader_params : dict, optional\n            Parameters to pass to the dataloader.\n        read_source_func : Callable, optional\n            Function to read the source data.\n        extension_filter : str, default=\"\"\n            Filter for the file extension.\n        write_type : {\"tiff\", \"custom\"}, default=\"tiff\"\n            The data type to save as, includes custom.\n        write_extension : str, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` an extension to save the data with must be passed.\n        write_func : WriteFunc, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` a function to save the data must be passed. See notes below.\n        write_func_kwargs : dict of {str: any}, optional\n            Additional keyword arguments to be passed to the save function.\n        prediction_dir : Path | str, default=\"predictions\"\n            The path to save the prediction results to. If `prediction_dir` is not\n            absolute, the directory will be assumed to be relative to the pre-set\n            `work_dir`. If the directory does not exist it will be created.\n        **kwargs : Any\n            Unused.\n\n        Raises\n        ------\n        ValueError\n            If `write_type` is custom and `write_extension` is None.\n        ValueError\n            If `write_type` is custom and `write_fun is None.\n        ValueError\n            If `source` is not `str`, `Path` or `PredictDataModule`\n        \"\"\"\n        if write_func_kwargs is None:\n            write_func_kwargs = {}\n\n        if Path(prediction_dir).is_absolute():\n            write_dir = Path(prediction_dir)\n        else:\n            write_dir = self.work_dir / prediction_dir\n        write_dir.mkdir(exist_ok=True, parents=True)\n\n        # guards for custom types\n        if write_type == SupportedData.CUSTOM:\n            if write_extension is None:\n                raise ValueError(\n                    \"A `write_extension` must be provided for custom write types.\"\n                )\n            if write_func is None:\n                raise ValueError(\n                    \"A `write_func` must be provided for custom write types.\"\n                )\n        else:\n            write_func = get_write_func(write_type)\n            write_extension = SupportedData.get_extension(write_type)\n\n        # extract file names\n        source_path: Union[Path, str, NDArray]\n        source_data_type: Literal[\"array\", \"tiff\", \"custom\"]\n        if isinstance(source, PredictDataModule):\n            source_path = source.pred_data\n            source_data_type = source.data_type  # type: ignore\n            extension_filter = source.extension_filter\n        elif isinstance(source, (str | Path)):\n            source_path = source\n            source_data_type = (\n                data_type or self.cfg.data_config.data_type  # type: ignore\n            )\n            extension_filter = SupportedData.get_extension_pattern(\n                SupportedData(source_data_type)\n            )\n        else:\n            raise ValueError(f\"Unsupported source type: '{type(source)}'.\")\n\n        if source_data_type == \"array\":\n            raise ValueError(\n                \"Predicting to disk is not supported for input type 'array'.\"\n            )\n        assert isinstance(source_path, (Path | str))  # because data_type != \"array\"\n        source_path = Path(source_path)\n\n        file_paths = list_files(source_path, source_data_type, extension_filter)\n\n        # predict and write each file in turn\n        for file_path in file_paths:\n            # source_path is relative to original source path...\n            # should mirror original directory structure\n            prediction = self.predict(\n                source=file_path,\n                batch_size=batch_size,\n                tile_size=tile_size,\n                tile_overlap=tile_overlap,\n                axes=axes,\n                data_type=data_type,\n                tta_transforms=tta_transforms,\n                dataloader_params=dataloader_params,\n                read_source_func=read_source_func,\n                extension_filter=extension_filter,\n                **kwargs,\n            )\n            # TODO: cast to float16?\n            write_data = np.concatenate(prediction)\n\n            # create directory structure and write path\n            if not source_path.is_file():\n                file_write_dir = write_dir / file_path.parent.relative_to(source_path)\n            else:\n                file_write_dir = write_dir\n            file_write_dir.mkdir(parents=True, exist_ok=True)\n            write_path = (file_write_dir / file_path.name).with_suffix(write_extension)\n\n            # write data\n            write_func(file_path=write_path, img=write_data)\n\n    def export_to_bmz(\n        self,\n        path_to_archive: Union[Path | str],\n        friendly_model_name: str,\n        input_array: NDArray,\n        authors: list[dict],\n        general_description: str,\n        data_description: str,\n        covers: list[Union[Path, str]] | None = None,\n        channel_names: list[str] | None = None,\n        model_version: str = \"0.1.0\",\n    ) -&gt; None:\n        \"\"\"Export the model to the BioImage Model Zoo format.\n\n        This method packages the current weights into a zip file that can be uploaded\n        to the BioImage Model Zoo. The archive consists of the model weights, the model\n        specifications and various files (inputs, outputs, README, env.yaml etc.).\n\n        `path_to_archive` should point to a file with a \".zip\" extension.\n\n        `friendly_model_name` is the name used for the model in the BMZ specs\n        and website, it should consist of letters, numbers, dashes, underscores and\n        parentheses only.\n\n        Input array must be of the same dimensions as the axes recorded in the\n        configuration of the `CAREamist`.\n\n        Parameters\n        ----------\n        path_to_archive : pathlib.Path or str\n            Path in which to save the model, including file name, which should end with\n            \".zip\".\n        friendly_model_name : str\n            Name of the model as used in the BMZ specs, it should consist of letters,\n            numbers, dashes, underscores and parentheses only.\n        input_array : NDArray\n            Input array used to validate the model and as example.\n        authors : list of dict\n            List of authors of the model.\n        general_description : str\n            General description of the model used in the BMZ metadata.\n        data_description : str\n            Description of the data the model was trained on.\n        covers : list of pathlib.Path or str, default=None\n            Paths to the cover images.\n        channel_names : list of str, default=None\n            Channel names.\n        model_version : str, default=\"0.1.0\"\n            Version of the model.\n        \"\"\"\n        # TODO: add in docs that it is expected that input_array dimensions match\n        # those in data_config\n\n        output_patch = self.predict(\n            input_array,\n            data_type=SupportedData.ARRAY.value,\n            tta_transforms=False,\n        )\n        output = np.concatenate(output_patch, axis=0)\n        input_array = reshape_array(input_array, self.cfg.data_config.axes)\n\n        export_to_bmz(\n            model=self.model,\n            config=self.cfg,\n            path_to_archive=path_to_archive,\n            model_name=friendly_model_name,\n            general_description=general_description,\n            data_description=data_description,\n            authors=authors,\n            input_array=input_array,\n            output_array=output,\n            covers=covers,\n            channel_names=channel_names,\n            model_version=model_version,\n        )\n\n    def get_losses(self) -&gt; dict[str, list]:\n        \"\"\"Return data that can be used to plot train and validation loss curves.\n\n        Returns\n        -------\n        dict of str: list\n            Dictionary containing the losses for each epoch.\n        \"\"\"\n        return read_csv_logger(self.cfg.experiment_name, self.work_dir / \"csv_logs\")\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.__init__","title":"<code>__init__(source, work_dir=None, callbacks=None, enable_progress_bar=True)</code>","text":"<pre><code>__init__(source: Union[Path, str], work_dir: Union[Path, str] | None = None, callbacks: list[Callback] | None = None, enable_progress_bar: bool = True) -&gt; None\n</code></pre><pre><code>__init__(source: Configuration, work_dir: Union[Path, str] | None = None, callbacks: list[Callback] | None = None, enable_progress_bar: bool = True) -&gt; None\n</code></pre> <p>Initialize CAREamist with a configuration object or a path.</p> <p>A configuration object can be created using directly by calling <code>Configuration</code>, using the configuration factory or loading a configuration from a yaml file.</p> <p>Path can contain either a yaml file with parameters, or a saved checkpoint.</p> <p>If no working directory is provided, the current working directory is used.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>pathlib.Path or str or CAREamics Configuration</code> <p>Path to a configuration file or a trained model.</p> required <code>work_dir</code> <code>str or Path</code> <p>Path to working directory in which to save checkpoints and logs, by default None.</p> <code>None</code> <code>callbacks</code> <code>list of Callback</code> <p>List of callbacks to use during training and prediction, by default None.</p> <code>None</code> <code>enable_progress_bar</code> <code>bool</code> <p>Whether a progress bar will be displayed during training, validation and prediction.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the model is loaded from BioImage Model Zoo.</p> <code>ValueError</code> <p>If no hyper parameters are found in the checkpoint.</p> <code>ValueError</code> <p>If no data module hyper parameters are found in the checkpoint.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def __init__(\n    self,\n    source: Union[Path, str, Configuration],\n    work_dir: Union[Path, str] | None = None,\n    callbacks: list[Callback] | None = None,\n    enable_progress_bar: bool = True,\n) -&gt; None:\n    \"\"\"\n    Initialize CAREamist with a configuration object or a path.\n\n    A configuration object can be created using directly by calling `Configuration`,\n    using the configuration factory or loading a configuration from a yaml file.\n\n    Path can contain either a yaml file with parameters, or a saved checkpoint.\n\n    If no working directory is provided, the current working directory is used.\n\n    Parameters\n    ----------\n    source : pathlib.Path or str or CAREamics Configuration\n        Path to a configuration file or a trained model.\n    work_dir : str or pathlib.Path, optional\n        Path to working directory in which to save checkpoints and logs,\n        by default None.\n    callbacks : list of Callback, optional\n        List of callbacks to use during training and prediction, by default None.\n    enable_progress_bar : bool\n        Whether a progress bar will be displayed during training, validation and\n        prediction.\n\n    Raises\n    ------\n    NotImplementedError\n        If the model is loaded from BioImage Model Zoo.\n    ValueError\n        If no hyper parameters are found in the checkpoint.\n    ValueError\n        If no data module hyper parameters are found in the checkpoint.\n    \"\"\"\n    # select current working directory if work_dir is None\n    if work_dir is None:\n        self.work_dir = Path.cwd()\n        logger.warning(\n            f\"No working directory provided. Using current working directory: \"\n            f\"{self.work_dir}.\"\n        )\n    else:\n        self.work_dir = Path(work_dir)\n\n    # configuration object\n    if isinstance(source, Configuration):\n        self.cfg = source\n\n        # instantiate model\n        if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n            self.model = FCNModule(\n                algorithm_config=self.cfg.algorithm_config,\n            )\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n\n    # path to configuration file or model\n    else:\n        # TODO: update this check so models can be downloaded directly from BMZ\n        source = check_path_exists(source)\n\n        # configuration file\n        if source.is_file() and (\n            source.suffix == \".yaml\" or source.suffix == \".yml\"\n        ):\n            # load configuration\n            self.cfg = load_configuration(source)\n\n            # instantiate model\n            if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                self.model = FCNModule(\n                    algorithm_config=self.cfg.algorithm_config,\n                )  # type: ignore\n            else:\n                raise NotImplementedError(\"Architecture not supported.\")\n\n        # attempt loading a pre-trained model\n        else:\n            self.model, self.cfg = load_pretrained(source)\n\n    # define the checkpoint saving callback\n    self._define_callbacks(callbacks, enable_progress_bar)\n\n    # instantiate logger\n    csv_logger = CSVLogger(\n        name=self.cfg.experiment_name,\n        save_dir=self.work_dir / \"csv_logs\",\n    )\n\n    if self.cfg.training_config.has_logger():\n        if self.cfg.training_config.logger == SupportedLogger.WANDB:\n            experiment_logger: LOGGER_TYPES = [\n                WandbLogger(\n                    name=self.cfg.experiment_name,\n                    save_dir=self.work_dir / Path(\"wandb_logs\"),\n                ),\n                csv_logger,\n            ]\n        elif self.cfg.training_config.logger == SupportedLogger.TENSORBOARD:\n            experiment_logger = [\n                TensorBoardLogger(\n                    save_dir=self.work_dir / Path(\"tb_logs\"),\n                ),\n                csv_logger,\n            ]\n    else:\n        experiment_logger = [csv_logger]\n\n    # instantiate trainer\n    self.trainer = Trainer(\n        enable_progress_bar=enable_progress_bar,\n        callbacks=self.callbacks,\n        default_root_dir=self.work_dir,\n        logger=experiment_logger,\n        **self.cfg.training_config.lightning_trainer_config or {},\n    )\n\n    # place holder for the datamodules\n    self.train_datamodule: TrainDataModule | None = None\n    self.pred_datamodule: PredictDataModule | None = None\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.export_to_bmz","title":"<code>export_to_bmz(path_to_archive, friendly_model_name, input_array, authors, general_description, data_description, covers=None, channel_names=None, model_version='0.1.0')</code>","text":"<p>Export the model to the BioImage Model Zoo format.</p> <p>This method packages the current weights into a zip file that can be uploaded to the BioImage Model Zoo. The archive consists of the model weights, the model specifications and various files (inputs, outputs, README, env.yaml etc.).</p> <p><code>path_to_archive</code> should point to a file with a \".zip\" extension.</p> <p><code>friendly_model_name</code> is the name used for the model in the BMZ specs and website, it should consist of letters, numbers, dashes, underscores and parentheses only.</p> <p>Input array must be of the same dimensions as the axes recorded in the configuration of the <code>CAREamist</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_archive</code> <code>Path or str</code> <p>Path in which to save the model, including file name, which should end with \".zip\".</p> required <code>friendly_model_name</code> <code>str</code> <p>Name of the model as used in the BMZ specs, it should consist of letters, numbers, dashes, underscores and parentheses only.</p> required <code>input_array</code> <code>NDArray</code> <p>Input array used to validate the model and as example.</p> required <code>authors</code> <code>list of dict</code> <p>List of authors of the model.</p> required <code>general_description</code> <code>str</code> <p>General description of the model used in the BMZ metadata.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to the cover images.</p> <code>None</code> <code>channel_names</code> <code>list of str</code> <p>Channel names.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Version of the model.</p> <code>\"0.1.0\"</code> Source code in <code>src/careamics/careamist.py</code> <pre><code>def export_to_bmz(\n    self,\n    path_to_archive: Union[Path | str],\n    friendly_model_name: str,\n    input_array: NDArray,\n    authors: list[dict],\n    general_description: str,\n    data_description: str,\n    covers: list[Union[Path, str]] | None = None,\n    channel_names: list[str] | None = None,\n    model_version: str = \"0.1.0\",\n) -&gt; None:\n    \"\"\"Export the model to the BioImage Model Zoo format.\n\n    This method packages the current weights into a zip file that can be uploaded\n    to the BioImage Model Zoo. The archive consists of the model weights, the model\n    specifications and various files (inputs, outputs, README, env.yaml etc.).\n\n    `path_to_archive` should point to a file with a \".zip\" extension.\n\n    `friendly_model_name` is the name used for the model in the BMZ specs\n    and website, it should consist of letters, numbers, dashes, underscores and\n    parentheses only.\n\n    Input array must be of the same dimensions as the axes recorded in the\n    configuration of the `CAREamist`.\n\n    Parameters\n    ----------\n    path_to_archive : pathlib.Path or str\n        Path in which to save the model, including file name, which should end with\n        \".zip\".\n    friendly_model_name : str\n        Name of the model as used in the BMZ specs, it should consist of letters,\n        numbers, dashes, underscores and parentheses only.\n    input_array : NDArray\n        Input array used to validate the model and as example.\n    authors : list of dict\n        List of authors of the model.\n    general_description : str\n        General description of the model used in the BMZ metadata.\n    data_description : str\n        Description of the data the model was trained on.\n    covers : list of pathlib.Path or str, default=None\n        Paths to the cover images.\n    channel_names : list of str, default=None\n        Channel names.\n    model_version : str, default=\"0.1.0\"\n        Version of the model.\n    \"\"\"\n    # TODO: add in docs that it is expected that input_array dimensions match\n    # those in data_config\n\n    output_patch = self.predict(\n        input_array,\n        data_type=SupportedData.ARRAY.value,\n        tta_transforms=False,\n    )\n    output = np.concatenate(output_patch, axis=0)\n    input_array = reshape_array(input_array, self.cfg.data_config.axes)\n\n    export_to_bmz(\n        model=self.model,\n        config=self.cfg,\n        path_to_archive=path_to_archive,\n        model_name=friendly_model_name,\n        general_description=general_description,\n        data_description=data_description,\n        authors=authors,\n        input_array=input_array,\n        output_array=output,\n        covers=covers,\n        channel_names=channel_names,\n        model_version=model_version,\n    )\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.get_losses","title":"<code>get_losses()</code>","text":"<p>Return data that can be used to plot train and validation loss curves.</p> <p>Returns:</p> Type Description <code>dict of str: list</code> <p>Dictionary containing the losses for each epoch.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def get_losses(self) -&gt; dict[str, list]:\n    \"\"\"Return data that can be used to plot train and validation loss curves.\n\n    Returns\n    -------\n    dict of str: list\n        Dictionary containing the losses for each epoch.\n    \"\"\"\n    return read_csv_logger(self.cfg.experiment_name, self.work_dir / \"csv_logs\")\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.predict","title":"<code>predict(source, *, batch_size=1, tile_size=None, tile_overlap=(48, 48), axes=None, data_type=None, tta_transforms=False, dataloader_params=None, read_source_func=None, extension_filter='', **kwargs)</code>","text":"<pre><code>predict(source: PredictDataModule) -&gt; Union[list[NDArray], NDArray]\n</code></pre><pre><code>predict(source: Union[Path, str], *, batch_size: int = 1, tile_size: tuple[int, ...] | None = None, tile_overlap: tuple[int, ...] | None = (48, 48), axes: str | None = None, data_type: Literal['tiff', 'custom'] | None = None, tta_transforms: bool = False, dataloader_params: dict | None = None, read_source_func: Callable | None = None, extension_filter: str = '') -&gt; Union[list[NDArray], NDArray]\n</code></pre><pre><code>predict(source: NDArray, *, batch_size: int = 1, tile_size: tuple[int, ...] | None = None, tile_overlap: tuple[int, ...] | None = (48, 48), axes: str | None = None, data_type: Literal['array'] | None = None, tta_transforms: bool = False, dataloader_params: dict | None = None) -&gt; Union[list[NDArray], NDArray]\n</code></pre> <p>Make predictions on the provided data.</p> <p>Input can be a CAREamicsPredData instance, a path to a data file, or a numpy array.</p> <p>If <code>data_type</code>, <code>axes</code> and <code>tile_size</code> are not provided, the training configuration parameters will be used, with the <code>patch_size</code> instead of <code>tile_size</code>.</p> <p>Test-time augmentation (TTA) can be switched on using the <code>tta_transforms</code> parameter. The TTA augmentation applies all possible flip and 90 degrees rotations to the prediction input and averages the predictions. TTA augmentation should not be used if you did not train with these augmentations.</p> <p>Note that if you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>(PredictDataModule, Path, str or ndarray)</code> <p>Data to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>1</code> <code>tile_size</code> <code>tuple of int</code> <p>Size of the tiles to use for prediction.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Overlap between tiles, can be None.</p> <code>(48, 48)</code> <code>axes</code> <code>str</code> <p>Axes of the input data, by default None.</p> <code>None</code> <code>data_type</code> <code>(array, tiff, custom)</code> <p>Type of the input data.</p> <code>\"array\"</code> <code>tta_transforms</code> <code>bool</code> <p>Whether to apply test-time augmentation.</p> <code>True</code> <code>dataloader_params</code> <code>dict</code> <p>Parameters to pass to the dataloader.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for the file extension.</p> <code>\"\"</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list of NDArray or NDArray</code> <p>Predictions made by the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the configuration.</p> <code>ValueError</code> <p>If tile size is not divisible by 2**depth for UNet models.</p> <code>ValueError</code> <p>If tile overlap is not specified.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def predict(\n    self,\n    source: Union[PredictDataModule, Path, str, NDArray],\n    *,\n    batch_size: int = 1,\n    tile_size: tuple[int, ...] | None = None,\n    tile_overlap: tuple[int, ...] | None = (48, 48),\n    axes: str | None = None,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"] | None = None,\n    tta_transforms: bool = False,\n    dataloader_params: dict | None = None,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    **kwargs: Any,\n) -&gt; Union[list[NDArray], NDArray]:\n    \"\"\"\n    Make predictions on the provided data.\n\n    Input can be a CAREamicsPredData instance, a path to a data file, or a numpy\n    array.\n\n    If `data_type`, `axes` and `tile_size` are not provided, the training\n    configuration parameters will be used, with the `patch_size` instead of\n    `tile_size`.\n\n    Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n    parameter. The TTA augmentation applies all possible flip and 90 degrees\n    rotations to the prediction input and averages the predictions. TTA augmentation\n    should not be used if you did not train with these augmentations.\n\n    Note that if you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n\n    Parameters\n    ----------\n    source : PredictDataModule, pathlib.Path, str or numpy.ndarray\n        Data to predict on.\n    batch_size : int, default=1\n        Batch size for prediction.\n    tile_size : tuple of int, optional\n        Size of the tiles to use for prediction.\n    tile_overlap : tuple of int, default=(48, 48)\n        Overlap between tiles, can be None.\n    axes : str, optional\n        Axes of the input data, by default None.\n    data_type : {\"array\", \"tiff\", \"custom\"}, optional\n        Type of the input data.\n    tta_transforms : bool, default=True\n        Whether to apply test-time augmentation.\n    dataloader_params : dict, optional\n        Parameters to pass to the dataloader.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, default=\"\"\n        Filter for the file extension.\n    **kwargs : Any\n        Unused.\n\n    Returns\n    -------\n    list of NDArray or NDArray\n        Predictions made by the model.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the configuration.\n    ValueError\n        If tile size is not divisible by 2**depth for UNet models.\n    ValueError\n        If tile overlap is not specified.\n    \"\"\"\n    if (\n        self.cfg.data_config.image_means is None\n        or self.cfg.data_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided in the configuration.\")\n\n    # tile size for UNets\n    if tile_size is not None:\n        model = self.cfg.algorithm_config.model\n\n        if model.architecture == SupportedArchitecture.UNET.value:\n            # tile size must be equal to k*2^n, where n is the number of pooling\n            # layers (equal to the depth) and k is an integer\n            depth = model.depth\n            tile_increment = 2**depth\n\n            for i, t in enumerate(tile_size):\n                if t % tile_increment != 0:\n                    raise ValueError(\n                        f\"Tile size must be divisible by {tile_increment} along \"\n                        f\"all axes (got {t} for axis {i}). If your image size is \"\n                        f\"smaller along one axis (e.g. Z), consider padding the \"\n                        f\"image.\"\n                    )\n\n        # tile overlaps must be specified\n        if tile_overlap is None:\n            raise ValueError(\"Tile overlap must be specified.\")\n\n    # create the prediction\n    self.pred_datamodule = create_predict_datamodule(\n        pred_data=source,\n        data_type=data_type or self.cfg.data_config.data_type,  # type: ignore\n        axes=axes or self.cfg.data_config.axes,\n        image_means=self.cfg.data_config.image_means,\n        image_stds=self.cfg.data_config.image_stds,\n        tile_size=tile_size,\n        tile_overlap=tile_overlap,\n        batch_size=batch_size or self.cfg.data_config.batch_size,\n        tta_transforms=tta_transforms,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        dataloader_params=dataloader_params,\n    )\n\n    # predict\n    predictions = self.trainer.predict(\n        model=self.model, datamodule=self.pred_datamodule\n    )\n    return convert_outputs(predictions, self.pred_datamodule.tiled)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.predict_to_disk","title":"<code>predict_to_disk(source, *, batch_size=1, tile_size=None, tile_overlap=(48, 48), axes=None, data_type=None, tta_transforms=False, dataloader_params=None, read_source_func=None, extension_filter='', write_type='tiff', write_extension=None, write_func=None, write_func_kwargs=None, prediction_dir='predictions', **kwargs)</code>","text":"<p>Make predictions on the provided data and save outputs to files.</p> <p>The predictions will be saved in a new directory 'predictions' within the set working directory. The directory stucture within the 'predictions' directory will match that of the source directory.</p> <p>The <code>source</code> must be from files and not arrays. The file names of the predictions will match those of the source. If there is more than one sample within a file, the samples will be saved to seperate files. The file names of samples will have the name of the corresponding source file but with the sample index appended. E.g. If the the source file name is 'images.tiff' then the first sample's prediction will be saved with the file name \"image_0.tiff\". Input can be a PredictDataModule instance, a path to a data file, or a numpy array.</p> <p>If <code>data_type</code>, <code>axes</code> and <code>tile_size</code> are not provided, the training configuration parameters will be used, with the <code>patch_size</code> instead of <code>tile_size</code>.</p> <p>Test-time augmentation (TTA) can be switched on using the <code>tta_transforms</code> parameter. The TTA augmentation applies all possible flip and 90 degrees rotations to the prediction input and averages the predictions. TTA augmentation should not be used if you did not train with these augmentations.</p> <p>Note that if you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>(PredictDataModule or Path, str)</code> <p>Data to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>1</code> <code>tile_size</code> <code>tuple of int</code> <p>Size of the tiles to use for prediction.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Overlap between tiles.</p> <code>(48, 48)</code> <code>axes</code> <code>str</code> <p>Axes of the input data, by default None.</p> <code>None</code> <code>data_type</code> <code>(array, tiff, custom)</code> <p>Type of the input data.</p> <code>\"array\"</code> <code>tta_transforms</code> <code>bool</code> <p>Whether to apply test-time augmentation.</p> <code>True</code> <code>dataloader_params</code> <code>dict</code> <p>Parameters to pass to the dataloader.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for the file extension.</p> <code>\"\"</code> <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {str: any}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <code>prediction_dir</code> <code>Path | str</code> <p>The path to save the prediction results to. If <code>prediction_dir</code> is not absolute, the directory will be assumed to be relative to the pre-set <code>work_dir</code>. If the directory does not exist it will be created.</p> <code>\"predictions\"</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>write_type</code> is custom and <code>write_extension</code> is None.</p> <code>ValueError</code> <p>If <code>write_type</code> is custom and `write_fun is None.</p> <code>ValueError</code> <p>If <code>source</code> is not <code>str</code>, <code>Path</code> or <code>PredictDataModule</code></p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def predict_to_disk(\n    self,\n    source: Union[PredictDataModule, Path, str],\n    *,\n    batch_size: int = 1,\n    tile_size: tuple[int, ...] | None = None,\n    tile_overlap: tuple[int, ...] | None = (48, 48),\n    axes: str | None = None,\n    data_type: Literal[\"tiff\", \"custom\"] | None = None,\n    tta_transforms: bool = False,\n    dataloader_params: dict | None = None,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    write_type: Literal[\"tiff\", \"custom\"] = \"tiff\",\n    write_extension: str | None = None,\n    write_func: WriteFunc | None = None,\n    write_func_kwargs: dict[str, Any] | None = None,\n    prediction_dir: Union[Path, str] = \"predictions\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Make predictions on the provided data and save outputs to files.\n\n    The predictions will be saved in a new directory 'predictions' within the set\n    working directory. The directory stucture within the 'predictions' directory\n    will match that of the source directory.\n\n    The `source` must be from files and not arrays. The file names of the\n    predictions will match those of the source. If there is more than one sample\n    within a file, the samples will be saved to seperate files. The file names of\n    samples will have the name of the corresponding source file but with the sample\n    index appended. E.g. If the the source file name is 'images.tiff' then the first\n    sample's prediction will be saved with the file name \"image_0.tiff\".\n    Input can be a PredictDataModule instance, a path to a data file, or a numpy\n    array.\n\n    If `data_type`, `axes` and `tile_size` are not provided, the training\n    configuration parameters will be used, with the `patch_size` instead of\n    `tile_size`.\n\n    Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n    parameter. The TTA augmentation applies all possible flip and 90 degrees\n    rotations to the prediction input and averages the predictions. TTA augmentation\n    should not be used if you did not train with these augmentations.\n\n    Note that if you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n\n    Parameters\n    ----------\n    source : PredictDataModule or pathlib.Path, str\n        Data to predict on.\n    batch_size : int, default=1\n        Batch size for prediction.\n    tile_size : tuple of int, optional\n        Size of the tiles to use for prediction.\n    tile_overlap : tuple of int, default=(48, 48)\n        Overlap between tiles.\n    axes : str, optional\n        Axes of the input data, by default None.\n    data_type : {\"array\", \"tiff\", \"custom\"}, optional\n        Type of the input data.\n    tta_transforms : bool, default=True\n        Whether to apply test-time augmentation.\n    dataloader_params : dict, optional\n        Parameters to pass to the dataloader.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, default=\"\"\n        Filter for the file extension.\n    write_type : {\"tiff\", \"custom\"}, default=\"tiff\"\n        The data type to save as, includes custom.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_func_kwargs : dict of {str: any}, optional\n        Additional keyword arguments to be passed to the save function.\n    prediction_dir : Path | str, default=\"predictions\"\n        The path to save the prediction results to. If `prediction_dir` is not\n        absolute, the directory will be assumed to be relative to the pre-set\n        `work_dir`. If the directory does not exist it will be created.\n    **kwargs : Any\n        Unused.\n\n    Raises\n    ------\n    ValueError\n        If `write_type` is custom and `write_extension` is None.\n    ValueError\n        If `write_type` is custom and `write_fun is None.\n    ValueError\n        If `source` is not `str`, `Path` or `PredictDataModule`\n    \"\"\"\n    if write_func_kwargs is None:\n        write_func_kwargs = {}\n\n    if Path(prediction_dir).is_absolute():\n        write_dir = Path(prediction_dir)\n    else:\n        write_dir = self.work_dir / prediction_dir\n    write_dir.mkdir(exist_ok=True, parents=True)\n\n    # guards for custom types\n    if write_type == SupportedData.CUSTOM:\n        if write_extension is None:\n            raise ValueError(\n                \"A `write_extension` must be provided for custom write types.\"\n            )\n        if write_func is None:\n            raise ValueError(\n                \"A `write_func` must be provided for custom write types.\"\n            )\n    else:\n        write_func = get_write_func(write_type)\n        write_extension = SupportedData.get_extension(write_type)\n\n    # extract file names\n    source_path: Union[Path, str, NDArray]\n    source_data_type: Literal[\"array\", \"tiff\", \"custom\"]\n    if isinstance(source, PredictDataModule):\n        source_path = source.pred_data\n        source_data_type = source.data_type  # type: ignore\n        extension_filter = source.extension_filter\n    elif isinstance(source, (str | Path)):\n        source_path = source\n        source_data_type = (\n            data_type or self.cfg.data_config.data_type  # type: ignore\n        )\n        extension_filter = SupportedData.get_extension_pattern(\n            SupportedData(source_data_type)\n        )\n    else:\n        raise ValueError(f\"Unsupported source type: '{type(source)}'.\")\n\n    if source_data_type == \"array\":\n        raise ValueError(\n            \"Predicting to disk is not supported for input type 'array'.\"\n        )\n    assert isinstance(source_path, (Path | str))  # because data_type != \"array\"\n    source_path = Path(source_path)\n\n    file_paths = list_files(source_path, source_data_type, extension_filter)\n\n    # predict and write each file in turn\n    for file_path in file_paths:\n        # source_path is relative to original source path...\n        # should mirror original directory structure\n        prediction = self.predict(\n            source=file_path,\n            batch_size=batch_size,\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            axes=axes,\n            data_type=data_type,\n            tta_transforms=tta_transforms,\n            dataloader_params=dataloader_params,\n            read_source_func=read_source_func,\n            extension_filter=extension_filter,\n            **kwargs,\n        )\n        # TODO: cast to float16?\n        write_data = np.concatenate(prediction)\n\n        # create directory structure and write path\n        if not source_path.is_file():\n            file_write_dir = write_dir / file_path.parent.relative_to(source_path)\n        else:\n            file_write_dir = write_dir\n        file_write_dir.mkdir(parents=True, exist_ok=True)\n        write_path = (file_write_dir / file_path.name).with_suffix(write_extension)\n\n        # write data\n        write_func(file_path=write_path, img=write_data)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.stop_training","title":"<code>stop_training()</code>","text":"<p>Stop the training loop.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def stop_training(self) -&gt; None:\n    \"\"\"Stop the training loop.\"\"\"\n    # raise stop training flag\n    self.trainer.should_stop = True\n    self.trainer.limit_val_batches = 0  # skip  validation\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.train","title":"<code>train(*, datamodule=None, train_source=None, val_source=None, train_target=None, val_target=None, use_in_memory=True, val_percentage=0.1, val_minimum_split=1)</code>","text":"<p>Train the model on the provided data.</p> <p>If a datamodule is provided, then training will be performed using it. Alternatively, the training data can be provided as arrays or paths.</p> <p>If <code>use_in_memory</code> is set to True, the source provided as Path or str will be loaded in memory if it fits. Otherwise, training will be performed by loading patches from the files one by one. Training on arrays is always performed in memory.</p> <p>If no validation source is provided, then the validation is extracted from the training data using <code>val_percentage</code> and <code>val_minimum_split</code>. In the case of data provided as Path or str, the percentage and minimum number are applied to the number of files. For arrays, it is the number of patches.</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <code>TrainDataModule</code> <p>Datamodule to train on, by default None.</p> <code>None</code> <code>train_source</code> <code>Path or str or NDArray</code> <p>Train source, if no datamodule is provided, by default None.</p> <code>None</code> <code>val_source</code> <code>Path or str or NDArray</code> <p>Validation source, if no datamodule is provided, by default None.</p> <code>None</code> <code>train_target</code> <code>Path or str or NDArray</code> <p>Train target source, if no datamodule is provided, by default None.</p> <code>None</code> <code>val_target</code> <code>Path or str or NDArray</code> <p>Validation target source, if no datamodule is provided, by default None.</p> <code>None</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <code>val_percentage</code> <code>float</code> <p>Percentage of validation extracted from training data, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of validation (patch or file) extracted from training data, by default 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>datamodule</code> and <code>train_source</code> are provided.</p> <code>ValueError</code> <p>If sources are not of the same type (e.g. train is an array and val is a Path).</p> <code>ValueError</code> <p>If the training target is provided to N2V.</p> <code>ValueError</code> <p>If neither a datamodule nor a source is provided.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def train(\n    self,\n    *,\n    datamodule: TrainDataModule | None = None,\n    train_source: Union[Path, str, NDArray] | None = None,\n    val_source: Union[Path, str, NDArray] | None = None,\n    train_target: Union[Path, str, NDArray] | None = None,\n    val_target: Union[Path, str, NDArray] | None = None,\n    use_in_memory: bool = True,\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 1,\n) -&gt; None:\n    \"\"\"\n    Train the model on the provided data.\n\n    If a datamodule is provided, then training will be performed using it.\n    Alternatively, the training data can be provided as arrays or paths.\n\n    If `use_in_memory` is set to True, the source provided as Path or str will be\n    loaded in memory if it fits. Otherwise, training will be performed by loading\n    patches from the files one by one. Training on arrays is always performed\n    in memory.\n\n    If no validation source is provided, then the validation is extracted from\n    the training data using `val_percentage` and `val_minimum_split`. In the case\n    of data provided as Path or str, the percentage and minimum number are applied\n    to the number of files. For arrays, it is the number of patches.\n\n    Parameters\n    ----------\n    datamodule : TrainDataModule, optional\n        Datamodule to train on, by default None.\n    train_source : pathlib.Path or str or NDArray, optional\n        Train source, if no datamodule is provided, by default None.\n    val_source : pathlib.Path or str or NDArray, optional\n        Validation source, if no datamodule is provided, by default None.\n    train_target : pathlib.Path or str or NDArray, optional\n        Train target source, if no datamodule is provided, by default None.\n    val_target : pathlib.Path or str or NDArray, optional\n        Validation target source, if no datamodule is provided, by default None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n    val_percentage : float, optional\n        Percentage of validation extracted from training data, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of validation (patch or file) extracted from training data,\n        by default 1.\n\n    Raises\n    ------\n    ValueError\n        If both `datamodule` and `train_source` are provided.\n    ValueError\n        If sources are not of the same type (e.g. train is an array and val is\n        a Path).\n    ValueError\n        If the training target is provided to N2V.\n    ValueError\n        If neither a datamodule nor a source is provided.\n    \"\"\"\n    if datamodule is not None and train_source is not None:\n        raise ValueError(\n            \"Only one of `datamodule` and `train_source` can be provided.\"\n        )\n\n    # check that inputs are the same type\n    source_types = {\n        type(s)\n        for s in (train_source, val_source, train_target, val_target)\n        if s is not None\n    }\n    if len(source_types) &gt; 1:\n        raise ValueError(\"All sources should be of the same type.\")\n\n    # train\n    if datamodule is not None:\n        self._train_on_datamodule(datamodule=datamodule)\n\n    else:\n        # raise error if target is provided to N2V\n        if self.cfg.algorithm_config.algorithm == SupportedAlgorithm.N2V.value:\n            if train_target is not None:\n                raise ValueError(\n                    \"Training target not compatible with N2V training.\"\n                )\n\n        # dispatch the training\n        if isinstance(train_source, np.ndarray):\n            # mypy checks\n            assert isinstance(val_source, np.ndarray) or val_source is None\n            assert isinstance(train_target, np.ndarray) or train_target is None\n            assert isinstance(val_target, np.ndarray) or val_target is None\n\n            self._train_on_array(\n                train_source,\n                val_source,\n                train_target,\n                val_target,\n                val_percentage,\n                val_minimum_split,\n            )\n\n        elif isinstance(train_source, Path) or isinstance(train_source, str):\n            # mypy checks\n            assert (\n                isinstance(val_source, Path)\n                or isinstance(val_source, str)\n                or val_source is None\n            )\n            assert (\n                isinstance(train_target, Path)\n                or isinstance(train_target, str)\n                or train_target is None\n            )\n            assert (\n                isinstance(val_target, Path)\n                or isinstance(val_target, str)\n                or val_target is None\n            )\n\n            self._train_on_path(\n                train_source,\n                val_source,\n                train_target,\n                val_target,\n                use_in_memory,\n                val_percentage,\n                val_minimum_split,\n            )\n\n        else:\n            raise ValueError(\n                f\"Invalid input, expected a str, Path, array or TrainDataModule \"\n                f\"instance (got {type(train_source)}).\"\n            )\n</code></pre>"},{"location":"reference/careamics/conftest/","title":"conftest","text":"<p>File used to discover python modules and run doctest.</p> <p>See https://sybil.readthedocs.io/en/latest/use.html#pytest</p>"},{"location":"reference/careamics/conftest/#careamics.conftest.my_path","title":"<code>my_path(tmpdir_factory)</code>","text":"<p>Fixture used in doctest to create a temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>tmpdir_factory</code> <code>TempPathFactory</code> <p>Temporary path factory from pytest.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Temporary directory path.</p> Source code in <code>src/careamics/conftest.py</code> <pre><code>@pytest.fixture(scope=\"module\")\ndef my_path(tmpdir_factory: TempPathFactory) -&gt; Path:\n    \"\"\"Fixture used in doctest to create a temporary directory.\n\n    Parameters\n    ----------\n    tmpdir_factory : TempPathFactory\n        Temporary path factory from pytest.\n\n    Returns\n    -------\n    Path\n        Temporary directory path.\n    \"\"\"\n    return tmpdir_factory.mktemp(\"my_path\")\n</code></pre>"},{"location":"reference/careamics/cli/conf/","title":"conf","text":"<p>Configuration building convenience functions for the CAREamics CLI.</p>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.ConfOptions","title":"<code>ConfOptions</code>  <code>dataclass</code>","text":"<p>Data class for containing CLI <code>conf</code> command option values.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@dataclass\nclass ConfOptions:\n    \"\"\"Data class for containing CLI `conf` command option values.\"\"\"\n\n    dir: Path\n    name: str\n    force: bool\n    print: bool\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.care","title":"<code>care(ctx, experiment_name, axes, patch_size, batch_size, num_epochs=100, num_steps=None, data_type='tiff', use_augmentations=True, independent_channels=False, loss='mae', n_channels_in=None, n_channels_out=None, logger='none')</code>","text":"<p>Create a configuration for training CARE.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>use_augmentations</code> to False, the only transformation applied will be normalization.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef care(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")] = 100,\n    num_steps: Annotated[\n        int | None,\n        typer.Option(help=\"Number of batches per epoch (limit_train_batches).\"),\n    ] = None,\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = False,\n    loss: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"mae\", \"mse\"]),\n            help=\"Loss function to use.\",\n        ),\n    ] = \"mae\",\n    n_channels_in: Annotated[\n        int | None, typer.Option(help=\"Number of channels in\")\n    ] = None,\n    n_channels_out: Annotated[\n        int | None, typer.Option(help=\"Number of channels out\")\n    ] = None,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training CARE.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `use_augmentations` to False, the only transformation applied will be\n    normalization.\n    \"\"\"\n    config = create_care_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        num_steps=num_steps,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        loss=loss,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        logger=logger,\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.conf_options","title":"<code>conf_options(ctx, dir=WORK_DIR, name='config', force=False, print=False)</code>","text":"<p>Build and save CAREamics configuration files.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.callback()\ndef conf_options(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    dir: Annotated[\n        Path,\n        typer.Option(\n            \"--dir\", \"-d\", exists=True, help=\"Directory to save the config file to.\"\n        ),\n    ] = WORK_DIR,\n    name: Annotated[\n        str, typer.Option(\"--name\", \"-n\", help=\"The config file name.\")\n    ] = \"config\",\n    force: Annotated[\n        bool,\n        typer.Option(\n            \"--force\", \"-f\", help=\"Whether to overwrite existing config files.\"\n        ),\n    ] = False,\n    print: Annotated[\n        bool,\n        typer.Option(\n            \"--print\",\n            \"-p\",\n            help=\"Whether to print the config file to the console.\",\n        ),\n    ] = False,\n):\n    \"\"\"Build and save CAREamics configuration files.\"\"\"\n    # Callback is called still on --help command\n    # If a config exists it will complain that you need to use the -f flag\n    if \"--help\" in sys.argv:\n        return\n    conf_path = (dir / name).with_suffix(\".yaml\")\n    if conf_path.exists() and not force:\n        raise FileExistsError(f\"To overwrite '{conf_path}' use flag --force/-f.\")\n\n    ctx.obj = ConfOptions(dir, name, force, print)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.n2n","title":"<code>n2n(ctx, experiment_name, axes, patch_size, batch_size, num_epochs=100, num_steps=None, data_type='tiff', use_augmentations=True, independent_channels=False, loss='mae', n_channels_in=None, n_channels_out=None, logger='none')</code>","text":"<p>Create a configuration for training Noise2Noise.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>use_augmentations</code> to False, the only transformation applied will be normalization.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef n2n(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")] = 100,\n    num_steps: Annotated[\n        int | None,\n        typer.Option(help=\"Number of batches per epoch (limit_train_batches).\"),\n    ] = None,\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = False,\n    loss: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"mae\", \"mse\"]),\n            help=\"Loss function to use.\",\n        ),\n    ] = \"mae\",\n    n_channels_in: Annotated[\n        int | None, typer.Option(help=\"Number of channels in\")\n    ] = None,\n    n_channels_out: Annotated[\n        int | None, typer.Option(help=\"Number of channels out\")\n    ] = None,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training Noise2Noise.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `use_augmentations` to False, the only transformation applied will be\n    normalization.\n    \"\"\"\n    config = create_n2n_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        num_steps=num_steps,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        loss=loss,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        logger=logger,\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.n2v","title":"<code>n2v(ctx, experiment_name, axes, patch_size, batch_size, num_epochs=100, num_steps=None, data_type='tiff', use_augmentations=True, independent_channels=True, use_n2v2=False, n_channels=None, roi_size=11, masked_pixel_percentage=0.2, struct_n2v_axis='none', struct_n2v_span=5, logger='none')</code>","text":"<p>Create a configuration for training Noise2Void.</p> <p>N2V uses a UNet model to denoise images in a self-supervised manner. To use its variants structN2V and N2V2, set the <code>struct_n2v_axis</code> and <code>struct_n2v_span</code> (structN2V) parameters, or set <code>use_n2v2</code> to True (N2V2).</p> <p>N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip connections, thus removing checkboard artefacts. StructN2V is used when vertical or horizontal correlations are present in the noise; it applies an additional mask to the manipulated pixel neighbors.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels.</p> <p>By default, all channels are trained independently. To train all channels together, set <code>independent_channels</code> to False.</p> <p>By setting <code>use_augmentations</code> to False, the only transformations applied will be normalization and N2V manipulation.</p> <p>The <code>roi_size</code> parameter specifies the size of the area around each pixel that will be manipulated by N2V. The <code>masked_pixel_percentage</code> parameter specifies how many pixels per patch will be manipulated.</p> <p>The parameters of the UNet can be specified in the <code>model_kwargs</code> (passed as a parameter-value dictionary). Note that <code>use_n2v2</code> and 'n_channels' override the corresponding parameters passed in <code>model_kwargs</code>.</p> <p>If you pass \"horizontal\" or \"vertical\" to <code>struct_n2v_axis</code>, then structN2V mask will be applied to each manipulated pixel.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef n2v(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")] = 100,\n    num_steps: Annotated[\n        int | None,\n        typer.Option(help=\"Number of batches per epoch (limit_train_batches).\"),\n    ] = None,\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = True,\n    use_n2v2: Annotated[bool, typer.Option(help=\"Whether to use N2V2\")] = False,\n    n_channels: Annotated[\n        int | None, typer.Option(help=\"Number of channels (in and out)\")\n    ] = None,\n    roi_size: Annotated[int, typer.Option(help=\"N2V pixel manipulation area.\")] = 11,\n    masked_pixel_percentage: Annotated[\n        float, typer.Option(help=\"Percentage of pixels masked in each patch.\")\n    ] = 0.2,\n    struct_n2v_axis: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"horizontal\", \"vertical\", \"none\"])),\n    ] = \"none\",\n    struct_n2v_span: Annotated[\n        int, typer.Option(help=\"Span of the structN2V mask.\")\n    ] = 5,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training Noise2Void.\n\n    N2V uses a UNet model to denoise images in a self-supervised manner. To use its\n    variants structN2V and N2V2, set the `struct_n2v_axis` and `struct_n2v_span`\n    (structN2V) parameters, or set `use_n2v2` to True (N2V2).\n\n    N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip\n    connections, thus removing checkboard artefacts. StructN2V is used when vertical\n    or horizontal correlations are present in the noise; it applies an additional mask\n    to the manipulated pixel neighbors.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels.\n\n    By default, all channels are trained independently. To train all channels together,\n    set `independent_channels` to False.\n\n    By setting `use_augmentations` to False, the only transformations applied will be\n    normalization and N2V manipulation.\n\n    The `roi_size` parameter specifies the size of the area around each pixel that will\n    be manipulated by N2V. The `masked_pixel_percentage` parameter specifies how many\n    pixels per patch will be manipulated.\n\n    The parameters of the UNet can be specified in the `model_kwargs` (passed as a\n    parameter-value dictionary). Note that `use_n2v2` and 'n_channels' override the\n    corresponding parameters passed in `model_kwargs`.\n\n    If you pass \"horizontal\" or \"vertical\" to `struct_n2v_axis`, then structN2V mask\n    will be applied to each manipulated pixel.\n    \"\"\"\n    config = create_n2v_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        num_steps=num_steps,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        use_n2v2=use_n2v2,\n        n_channels=n_channels,\n        roi_size=roi_size,\n        masked_pixel_percentage=masked_pixel_percentage,\n        struct_n2v_axis=struct_n2v_axis,\n        struct_n2v_span=struct_n2v_span,\n        logger=logger,\n        # TODO: Model kwargs\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/main/","title":"main","text":"<p>Module for CLI functionality and entrypoint.</p> <p>Contains the CLI entrypoint, the <code>run</code> function; and first level subcommands <code>train</code> and <code>predict</code>. The <code>conf</code> subcommand is added through the <code>app.add_typer</code> function, and its implementation is contained in the conf.py file.</p>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.predict","title":"<code>predict(model, source, batch_size=1, tile_size=None, tile_overlap=(48, 48, -1), axes=None, data_type='tiff', tta_transforms=False, write_type='tiff', work_dir=None, prediction_dir=Path('predictions'))</code>","text":"<p>Create and save predictions from CAREamics models.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>@app.command()\ndef predict(  # numpydoc ignore=PR01\n    model: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to a configuration file or a trained model.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n        ),\n    ],\n    source: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to the training data. Can be a directory or single file.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")] = 1,\n    tile_size: Annotated[\n        click.Tuple | None,\n        typer.Option(\n            help=(\n                \"Size of the tiles to use for prediction, (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --tile_size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ] = None,\n    tile_overlap: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Overlap between tiles, (if the data is not 3D pass the last value as \"\n                \"-1 e.g. --tile_overlap 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ] = (48, 48, -1),\n    axes: Annotated[\n        str | None,\n        typer.Option(\n            help=\"Axes of the input data. If unused the data is assumed to have the \"\n            \"same axes as the original training data.\"\n        ),\n    ] = None,\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the input data.\"),\n    ] = \"tiff\",\n    tta_transforms: Annotated[\n        bool,\n        typer.Option(\n            \"--tta-transforms/--no-tta-transforms\",\n            \"-t/-T\",\n            help=\"Whether to apply test-time augmentation.\",\n        ),\n    ] = False,\n    write_type: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"tiff\"]), help=\"Type of the output data.\"\n        ),\n    ] = \"tiff\",\n    # TODO: could make dataloader_params as json, necessary?\n    work_dir: Annotated[\n        Path | None,\n        typer.Option(\n            \"--work-dir\",\n            \"-wd\",\n            help=(\"Path to working directory.\"),\n            exists=True,\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = None,\n    prediction_dir: Annotated[\n        Path,\n        typer.Option(\n            \"--prediction-dir\",\n            \"-pd\",\n            help=(\n                \"Directory to save predictions to. If not an abosulte path it will be \"\n                \"relative to the set working directory.\"\n            ),\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = Path(\"predictions\"),\n):\n    \"\"\"Create and save predictions from CAREamics models.\"\"\"\n    engine = CAREamist(source=model, work_dir=work_dir)\n    engine.predict_to_disk(\n        source=source,\n        batch_size=batch_size,\n        tile_size=tile_size,\n        tile_overlap=tile_overlap,\n        axes=axes,\n        data_type=data_type,\n        tta_transforms=tta_transforms,\n        write_type=write_type,\n        prediction_dir=prediction_dir,\n    )\n</code></pre>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.run","title":"<code>run()</code>","text":"<p>CLI Entry point.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>def run():\n    \"\"\"CLI Entry point.\"\"\"\n    app()\n</code></pre>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.train","title":"<code>train(source, train_source, train_target=None, val_source=None, val_target=None, use_in_memory=True, val_percentage=0.1, val_minimum_split=1, work_dir=None)</code>","text":"<p>Train CAREamics models.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>@app.command()\ndef train(  # numpydoc ignore=PR01\n    source: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to a configuration file or a trained model.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n        ),\n    ],\n    train_source: Annotated[\n        Path,\n        typer.Option(\n            \"--train-source\",\n            \"-ts\",\n            help=\"Path to the training data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ],\n    train_target: Annotated[\n        Path | None,\n        typer.Option(\n            \"--train-target\",\n            \"-tt\",\n            help=\"Path to train target data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    val_source: Annotated[\n        Path | None,\n        typer.Option(\n            \"--val-source\",\n            \"-vs\",\n            help=\"Path to validation data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    val_target: Annotated[\n        Path | None,\n        typer.Option(\n            \"--val-target\",\n            \"-vt\",\n            help=\"Path to validation target data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    use_in_memory: Annotated[\n        bool,\n        typer.Option(\n            \"--use-in-memory/--not-in-memory\",\n            \"-m/-M\",\n            help=\"Use in memory dataset if possible.\",\n        ),\n    ] = True,\n    val_percentage: Annotated[\n        float,\n        typer.Option(help=\"Percentage of files to use for validation.\"),\n    ] = 0.1,\n    val_minimum_split: Annotated[\n        int,\n        typer.Option(help=\"Minimum number of files to use for validation,\"),\n    ] = 1,\n    work_dir: Annotated[\n        Path | None,\n        typer.Option(\n            \"--work-dir\",\n            \"-wd\",\n            help=(\"Path to working directory in which to save checkpoints and logs\"),\n            exists=True,\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = None,\n):\n    \"\"\"Train CAREamics models.\"\"\"\n    engine = CAREamist(source=source, work_dir=work_dir)\n    engine.train(\n        train_source=train_source,\n        val_source=val_source,\n        train_target=train_target,\n        val_target=val_target,\n        use_in_memory=use_in_memory,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_split,\n    )\n</code></pre>"},{"location":"reference/careamics/cli/utils/","title":"utils","text":"<p>Utility functions for the CAREamics CLI.</p>"},{"location":"reference/careamics/cli/utils/#careamics.cli.utils.handle_2D_3D_callback","title":"<code>handle_2D_3D_callback(value)</code>","text":"<p>Callback for options that require 2D or 3D inputs.</p> <p>In the case of 2D, the 3rd element should be set to -1.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>(int, int, int)</code> <p>Tile size value.</p> required <p>Returns:</p> Type Description <code>(int, int, int) | (int, int)</code> <p>If the last element in <code>value</code> is -1 the tuple is reduced to the first two values.</p> Source code in <code>src/careamics/cli/utils.py</code> <pre><code>def handle_2D_3D_callback(\n    value: tuple[int, int, int] | None,\n) -&gt; tuple[int, ...] | None:\n    \"\"\"\n    Callback for options that require 2D or 3D inputs.\n\n    In the case of 2D, the 3rd element should be set to -1.\n\n    Parameters\n    ----------\n    value : (int, int, int)\n        Tile size value.\n\n    Returns\n    -------\n    (int, int, int) | (int, int)\n        If the last element in `value` is -1 the tuple is reduced to the first two\n        values.\n    \"\"\"\n    if value is None:\n        return value\n    if value[2] == -1:\n        return value[:2]\n    return value\n</code></pre>"},{"location":"reference/careamics/config/callback_model/","title":"callback_model","text":"<p>Callback Pydantic models.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel","title":"<code>CheckpointModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Checkpoint saving callback Pydantic model.</p> <p>The parameters corresponds to those of <code>pytorch_lightning.callbacks.ModelCheckpoint</code>.</p> <p>See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html#modelcheckpoint</p> Source code in <code>src/careamics/config/callback_model.py</code> <pre><code>class CheckpointModel(BaseModel):\n    \"\"\"Checkpoint saving callback Pydantic model.\n\n    The parameters corresponds to those of\n    `pytorch_lightning.callbacks.ModelCheckpoint`.\n\n    See:\n    https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html#modelcheckpoint\n    \"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, validate_default=True)\n\n    monitor: Literal[\"val_loss\"] | str | None = Field(default=\"val_loss\")\n    \"\"\"Quantity to monitor, currently only `val_loss`.\"\"\"\n\n    verbose: bool = Field(default=False)\n    \"\"\"Verbosity mode.\"\"\"\n\n    save_weights_only: bool = Field(default=False)\n    \"\"\"When `True`, only the model's weights will be saved (model.save_weights).\"\"\"\n\n    save_last: Literal[True, False, \"link\"] | None = Field(default=True)\n    \"\"\"When `True`, saves a last.ckpt copy whenever a checkpoint file gets saved.\"\"\"\n\n    save_top_k: int = Field(\n        default=3,\n        ge=-1,\n        le=100,\n    )\n    \"\"\"If `save_top_k == k, the best k models according to the quantity monitored\n    will be saved. If `save_top_k == 0`, no models are saved. if `save_top_k == -1`,\n    all models are saved.\"\"\"\n\n    mode: Literal[\"min\", \"max\"] = Field(default=\"min\")\n    \"\"\"One of {min, max}. If `save_top_k != 0`, the decision to overwrite the current\n    save file is made based on either the maximization or the minimization of the\n    monitored quantity. For 'val_acc', this should be 'max', for 'val_loss' this should\n    be 'min', etc.\n    \"\"\"\n\n    auto_insert_metric_name: bool = Field(default=False)\n    \"\"\"When `True`, the checkpoints filenames will contain the metric name.\"\"\"\n\n    every_n_train_steps: int | None = Field(default=None, ge=1, le=1000)\n    \"\"\"Number of training steps between checkpoints.\"\"\"\n\n    train_time_interval: timedelta | None = Field(default=None)\n    \"\"\"Checkpoints are monitored at the specified time interval.\"\"\"\n\n    every_n_epochs: int | None = Field(default=None, ge=1, le=100)\n    \"\"\"Number of epochs between checkpoints.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.auto_insert_metric_name","title":"<code>auto_insert_metric_name = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, the checkpoints filenames will contain the metric name.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.every_n_epochs","title":"<code>every_n_epochs = Field(default=None, ge=1, le=100)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs between checkpoints.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.every_n_train_steps","title":"<code>every_n_train_steps = Field(default=None, ge=1, le=1000)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training steps between checkpoints.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.mode","title":"<code>mode = Field(default='min')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>One of {min, max}. If <code>save_top_k != 0</code>, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For 'val_acc', this should be 'max', for 'val_loss' this should be 'min', etc.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.monitor","title":"<code>monitor = Field(default='val_loss')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantity to monitor, currently only <code>val_loss</code>.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_last","title":"<code>save_last = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, saves a last.ckpt copy whenever a checkpoint file gets saved.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_top_k","title":"<code>save_top_k = Field(default=3, ge=(-1), le=100)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>save_top_k == k, the best k models according to the quantity monitored will be saved. If</code>save_top_k == 0<code>, no models are saved. if</code>save_top_k == -1`, all models are saved.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_weights_only","title":"<code>save_weights_only = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, only the model's weights will be saved (model.save_weights).</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.train_time_interval","title":"<code>train_time_interval = Field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoints are monitored at the specified time interval.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.verbose","title":"<code>verbose = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Verbosity mode.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel","title":"<code>EarlyStoppingModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Early stopping callback Pydantic model.</p> <p>The parameters corresponds to those of <code>pytorch_lightning.callbacks.ModelCheckpoint</code>.</p> <p>See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping</p> Source code in <code>src/careamics/config/callback_model.py</code> <pre><code>class EarlyStoppingModel(BaseModel):\n    \"\"\"Early stopping callback Pydantic model.\n\n    The parameters corresponds to those of\n    `pytorch_lightning.callbacks.ModelCheckpoint`.\n\n    See:\n    https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n        validate_default=True,\n    )\n\n    monitor: Literal[\"val_loss\"] = Field(default=\"val_loss\")\n    \"\"\"Quantity to monitor.\"\"\"\n\n    min_delta: float = Field(default=0.0, ge=0.0, le=1.0)\n    \"\"\"Minimum change in the monitored quantity to qualify as an improvement, i.e. an\n    absolute change of less than or equal to min_delta, will count as no improvement.\"\"\"\n\n    patience: int = Field(default=3, ge=1, le=10)\n    \"\"\"Number of checks with no improvement after which training will be stopped.\"\"\"\n\n    verbose: bool = Field(default=False)\n    \"\"\"Verbosity mode.\"\"\"\n\n    mode: Literal[\"min\", \"max\", \"auto\"] = Field(default=\"min\")\n    \"\"\"One of {min, max, auto}.\"\"\"\n\n    check_finite: bool = Field(default=True)\n    \"\"\"When `True`, stops training when the monitored quantity becomes `NaN` or\n    `inf`.\"\"\"\n\n    stopping_threshold: float | None = Field(default=None)\n    \"\"\"Stop training immediately once the monitored quantity reaches this threshold.\"\"\"\n\n    divergence_threshold: float | None = Field(default=None)\n    \"\"\"Stop training as soon as the monitored quantity becomes worse than this\n    threshold.\"\"\"\n\n    check_on_train_epoch_end: bool | None = Field(default=False)\n    \"\"\"Whether to run early stopping at the end of the training epoch. If this is\n    `False`, then the check runs at the end of the validation.\"\"\"\n\n    log_rank_zero_only: bool = Field(default=False)\n    \"\"\"When set `True`, logs the status of the early stopping callback only for rank 0\n    process.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.check_finite","title":"<code>check_finite = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, stops training when the monitored quantity becomes <code>NaN</code> or <code>inf</code>.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.check_on_train_epoch_end","title":"<code>check_on_train_epoch_end = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to run early stopping at the end of the training epoch. If this is <code>False</code>, then the check runs at the end of the validation.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.divergence_threshold","title":"<code>divergence_threshold = Field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop training as soon as the monitored quantity becomes worse than this threshold.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.log_rank_zero_only","title":"<code>log_rank_zero_only = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When set <code>True</code>, logs the status of the early stopping callback only for rank 0 process.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.min_delta","title":"<code>min_delta = Field(default=0.0, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.mode","title":"<code>mode = Field(default='min')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>One of {min, max, auto}.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.monitor","title":"<code>monitor = Field(default='val_loss')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantity to monitor.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.patience","title":"<code>patience = Field(default=3, ge=1, le=10)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of checks with no improvement after which training will be stopped.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.stopping_threshold","title":"<code>stopping_threshold = Field(default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop training immediately once the monitored quantity reaches this threshold.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.verbose","title":"<code>verbose = Field(default=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Verbosity mode.</p>"},{"location":"reference/careamics/config/configuration/","title":"configuration","text":"<p>Pydantic CAREamics configuration.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration","title":"<code>Configuration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>CAREamics configuration.</p> <p>The configuration defines all parameters used to build and train a CAREamics model. These parameters are validated to ensure that they are compatible with each other.</p> <p>It contains three sub-configurations:</p> <ul> <li>AlgorithmModel: configuration for the algorithm training, which includes the     architecture, loss function, optimizer, and other hyperparameters.</li> <li>DataModel: configuration for the dataloader, which includes the type of data,     transformations, mean/std and other parameters.</li> <li>TrainingModel: configuration for the training, which includes the number of     epochs or the callbacks.</li> </ul> <p>Attributes:</p> Name Type Description <code>experiment_name</code> <code>str</code> <p>Name of the experiment, used when saving logs and checkpoints.</p> <code>algorithm</code> <code>AlgorithmModel</code> <p>Algorithm configuration.</p> <code>data</code> <code>DataModel</code> <p>Data configuration.</p> <code>training</code> <code>TrainingModel</code> <p>Training configuration.</p> <p>Methods:</p> Name Description <code>set_3D</code> <p>Switch configuration between 2D and 3D.</p> <code>model_dump</code> <p>exclude_defaults: bool = False, exclude_none: bool = True, **kwargs: Dict ) -&gt; Dict Export configuration to a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Configuration parameter type validation errors.</p> <code>ValueError</code> <p>If the experiment name contains invalid characters or is empty.</p> <code>ValueError</code> <p>If the algorithm is 3D but there is not \"Z\" in the data axes, or 2D algorithm with \"Z\" in data axes.</p> <code>ValueError</code> <p>Algorithm, data or training validation errors.</p> Notes <p>We provide convenience methods to create standards configurations, for instance:</p> <p>from careamics.config import create_n2v_configuration config = create_n2v_configuration( ...     experiment_name=\"n2v_experiment\", ...     data_type=\"array\", ...     axes=\"YX\", ...     patch_size=[64, 64], ...     batch_size=32, ... )</p> <p>The configuration can be exported to a dictionary using the model_dump method:</p> <p>config_dict = config.model_dump()</p> <p>Configurations can also be exported or imported from yaml files:</p> <p>from careamics.config import save_configuration, load_configuration path_to_config = save_configuration(config, my_path / \"config.yml\") other_config = load_configuration(path_to_config)</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; from careamics import Configuration\n&gt;&gt;&gt; config_dict = {\n...         \"experiment_name\": \"N2V_experiment\",\n...         \"algorithm_config\": {\n...             \"algorithm\": \"n2v\",\n...             \"loss\": \"n2v\",\n...             \"model\": {\n...                 \"architecture\": \"UNet\",\n...             },\n...         },\n...         \"training_config\": {},\n...         \"data_config\": {\n...             \"data_type\": \"tiff\",\n...             \"patch_size\": [64, 64],\n...             \"axes\": \"SYX\",\n...         },\n...     }\n&gt;&gt;&gt; config = Configuration(**config_dict)\n</code></pre> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>class Configuration(BaseModel):\n    \"\"\"\n    CAREamics configuration.\n\n    The configuration defines all parameters used to build and train a CAREamics model.\n    These parameters are validated to ensure that they are compatible with each other.\n\n    It contains three sub-configurations:\n\n    - AlgorithmModel: configuration for the algorithm training, which includes the\n        architecture, loss function, optimizer, and other hyperparameters.\n    - DataModel: configuration for the dataloader, which includes the type of data,\n        transformations, mean/std and other parameters.\n    - TrainingModel: configuration for the training, which includes the number of\n        epochs or the callbacks.\n\n    Attributes\n    ----------\n    experiment_name : str\n        Name of the experiment, used when saving logs and checkpoints.\n    algorithm : AlgorithmModel\n        Algorithm configuration.\n    data : DataModel\n        Data configuration.\n    training : TrainingModel\n        Training configuration.\n\n    Methods\n    -------\n    set_3D(is_3D: bool, axes: str, patch_size: List[int]) -&gt; None\n        Switch configuration between 2D and 3D.\n    model_dump(\n        exclude_defaults: bool = False, exclude_none: bool = True, **kwargs: Dict\n        ) -&gt; Dict\n        Export configuration to a dictionary.\n\n    Raises\n    ------\n    ValueError\n        Configuration parameter type validation errors.\n    ValueError\n        If the experiment name contains invalid characters or is empty.\n    ValueError\n        If the algorithm is 3D but there is not \"Z\" in the data axes, or 2D algorithm\n        with \"Z\" in data axes.\n    ValueError\n        Algorithm, data or training validation errors.\n\n    Notes\n    -----\n    We provide convenience methods to create standards configurations, for instance:\n    &gt;&gt;&gt; from careamics.config import create_n2v_configuration\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ... )\n\n    The configuration can be exported to a dictionary using the model_dump method:\n    &gt;&gt;&gt; config_dict = config.model_dump()\n\n    Configurations can also be exported or imported from yaml files:\n    &gt;&gt;&gt; from careamics.config import save_configuration, load_configuration\n    &gt;&gt;&gt; path_to_config = save_configuration(config, my_path / \"config.yml\")\n    &gt;&gt;&gt; other_config = load_configuration(path_to_config)\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; from careamics import Configuration\n    &gt;&gt;&gt; config_dict = {\n    ...         \"experiment_name\": \"N2V_experiment\",\n    ...         \"algorithm_config\": {\n    ...             \"algorithm\": \"n2v\",\n    ...             \"loss\": \"n2v\",\n    ...             \"model\": {\n    ...                 \"architecture\": \"UNet\",\n    ...             },\n    ...         },\n    ...         \"training_config\": {},\n    ...         \"data_config\": {\n    ...             \"data_type\": \"tiff\",\n    ...             \"patch_size\": [64, 64],\n    ...             \"axes\": \"SYX\",\n    ...         },\n    ...     }\n    &gt;&gt;&gt; config = Configuration(**config_dict)\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n        arbitrary_types_allowed=True,\n    )\n\n    # version\n    version: Literal[\"0.1.0\"] = \"0.1.0\"\n    \"\"\"CAREamics configuration version.\"\"\"\n\n    # required parameters\n    experiment_name: str\n    \"\"\"Name of the experiment, used to name logs and checkpoints.\"\"\"\n\n    # Sub-configurations\n    algorithm_config: ALGORITHMS = Field(discriminator=\"algorithm\")\n    \"\"\"Algorithm configuration, holding all parameters required to configure the\n    model.\"\"\"\n\n    data_config: DataConfig | MicroSplitDataConfig\n    \"\"\"Data configuration, holding all parameters required to configure the training\n    data loader.\"\"\"\n\n    training_config: TrainingConfig\n    \"\"\"Training configuration, holding all parameters required to configure the\n    training process.\"\"\"\n\n    @field_validator(\"experiment_name\")\n    @classmethod\n    def no_symbol(cls, name: str) -&gt; str:\n        \"\"\"\n        Validate experiment name.\n\n        A valid experiment name is a non-empty string with only contains letters,\n        numbers, underscores, dashes and spaces.\n\n        Parameters\n        ----------\n        name : str\n            Name to validate.\n\n        Returns\n        -------\n        str\n            Validated name.\n\n        Raises\n        ------\n        ValueError\n            If the name is empty or contains invalid characters.\n        \"\"\"\n        if len(name) == 0 or name.isspace():\n            raise ValueError(\"Experiment name is empty.\")\n\n        # Validate using a regex that it contains only letters, numbers, underscores,\n        # dashes and spaces\n        if not re.match(r\"^[a-zA-Z0-9_\\- ]*$\", name):\n            raise ValueError(\n                f\"Experiment name contains invalid characters (got {name}). \"\n                f\"Only letters, numbers, underscores, dashes and spaces are allowed.\"\n            )\n\n        return name\n\n    @model_validator(mode=\"after\")  # TODO move to n2v configs or remove\n    def validate_n2v_mask_pixel_perc(self: Self) -&gt; Self:\n        \"\"\"\n        Validate that there will always be at least one blind-spot pixel in every patch.\n\n        The probability of creating a blind-spot pixel is a function of the chosen\n        masked pixel percentage and patch size.\n\n        Returns\n        -------\n        Self\n            Validated configuration.\n\n        Raises\n        ------\n        ValueError\n            If the probability of masking a pixel within a patch is less than 1 for the\n            chosen masked pixel percentage and patch size.\n        \"\"\"\n        # No validation needed for non n2v algorithms\n        if not isinstance(self.algorithm_config, N2VAlgorithm):\n            return self\n\n        mask_pixel_perc = self.algorithm_config.n2v_config.masked_pixel_percentage\n        patch_size = self.data_config.patch_size\n        expected_area_per_pixel = 1 / (mask_pixel_perc / 100)\n\n        n_dims = 3 if self.algorithm_config.model.is_3D() else 2\n        patch_size_lower_bound = int(np.ceil(expected_area_per_pixel ** (1 / n_dims)))\n        required_patch_size = tuple(\n            2 ** int(np.ceil(np.log2(patch_size_lower_bound))) for _ in range(n_dims)\n        )\n        required_mask_pixel_perc = (1 / np.prod(patch_size)) * 100\n        if expected_area_per_pixel &gt; np.prod(patch_size):\n            raise ValueError(\n                \"The probability of creating a blind-spot pixel within a patch is \"\n                f\"below 1, for a patch size of {patch_size} with a masked pixel \"\n                f\"percentage of {mask_pixel_perc}%. Either increase the patch size to \"\n                f\"{required_patch_size} or increase the masked pixel percentage to \"\n                f\"at least {required_mask_pixel_perc}%.\"\n            )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_3D(self: Self) -&gt; Self:\n        \"\"\"\n        Change algorithm dimensions to match data.axes.\n\n        Returns\n        -------\n        Self\n            Validated configuration.\n        \"\"\"\n        if \"Z\" in self.data_config.axes and not self.algorithm_config.model.is_3D():\n            # change algorithm to 3D\n            self.algorithm_config.model.set_3D(True)\n        elif \"Z\" not in self.data_config.axes and self.algorithm_config.model.is_3D():\n            # change algorithm to 2D\n            self.algorithm_config.model.set_3D(False)\n\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def set_3D(self, is_3D: bool, axes: str, patch_size: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D flag and axes.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        axes : str\n            Axes of the data.\n        patch_size : list[int]\n            Patch size.\n        \"\"\"\n        # set the flag and axes (this will not trigger validation at the config level)\n        self.algorithm_config.model.set_3D(is_3D)\n        self.data_config.set_3D(axes, patch_size)\n\n        # cheap hack: trigger validation\n        self.algorithm_config = self.algorithm_config\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm name.\n\n        Returns\n        -------\n        str\n            Algorithm name.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_friendly_name()\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Return a description of the algorithm.\n\n        This method is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Description of the algorithm.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_description()\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_citations()\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_references()\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_keywords()\n\n    def model_dump(\n        self,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Override model_dump method in order to set default values.\n\n        As opposed to the parent model_dump method, this method sets exclude none by\n        default.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Additional arguments to pass to the parent model_dump method.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the model parameters.\n        \"\"\"\n        if \"exclude_none\" not in kwargs:\n            kwargs[\"exclude_none\"] = True\n\n        return super().model_dump(**kwargs)\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.algorithm_config","title":"<code>algorithm_config = Field(discriminator='algorithm')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Algorithm configuration, holding all parameters required to configure the model.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.data_config","title":"<code>data_config</code>  <code>instance-attribute</code>","text":"<p>Data configuration, holding all parameters required to configure the training data loader.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.experiment_name","title":"<code>experiment_name</code>  <code>instance-attribute</code>","text":"<p>Name of the experiment, used to name logs and checkpoints.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.training_config","title":"<code>training_config</code>  <code>instance-attribute</code>","text":"<p>Training configuration, holding all parameters required to configure the training process.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.version","title":"<code>version = '0.1.0'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CAREamics configuration version.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_citations()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Return a description of the algorithm.</p> <p>This method is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the algorithm.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Return a description of the algorithm.\n\n    This method is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Description of the algorithm.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_description()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm name.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm name.\n\n    Returns\n    -------\n    str\n        Algorithm name.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_friendly_name()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_keywords()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_references()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Override model_dump method in order to set default values.</p> <p>As opposed to the parent model_dump method, this method sets exclude none by default.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the parent model_dump method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the model parameters.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def model_dump(\n    self,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Override model_dump method in order to set default values.\n\n    As opposed to the parent model_dump method, this method sets exclude none by\n    default.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Additional arguments to pass to the parent model_dump method.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the model parameters.\n    \"\"\"\n    if \"exclude_none\" not in kwargs:\n        kwargs[\"exclude_none\"] = True\n\n    return super().model_dump(**kwargs)\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.no_symbol","title":"<code>no_symbol(name)</code>  <code>classmethod</code>","text":"<p>Validate experiment name.</p> <p>A valid experiment name is a non-empty string with only contains letters, numbers, underscores, dashes and spaces.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is empty or contains invalid characters.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>@field_validator(\"experiment_name\")\n@classmethod\ndef no_symbol(cls, name: str) -&gt; str:\n    \"\"\"\n    Validate experiment name.\n\n    A valid experiment name is a non-empty string with only contains letters,\n    numbers, underscores, dashes and spaces.\n\n    Parameters\n    ----------\n    name : str\n        Name to validate.\n\n    Returns\n    -------\n    str\n        Validated name.\n\n    Raises\n    ------\n    ValueError\n        If the name is empty or contains invalid characters.\n    \"\"\"\n    if len(name) == 0 or name.isspace():\n        raise ValueError(\"Experiment name is empty.\")\n\n    # Validate using a regex that it contains only letters, numbers, underscores,\n    # dashes and spaces\n    if not re.match(r\"^[a-zA-Z0-9_\\- ]*$\", name):\n        raise ValueError(\n            f\"Experiment name contains invalid characters (got {name}). \"\n            f\"Only letters, numbers, underscores, dashes and spaces are allowed.\"\n        )\n\n    return name\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.set_3D","title":"<code>set_3D(is_3D, axes, patch_size)</code>","text":"<p>Set 3D flag and axes.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list[int]</code> <p>Patch size.</p> required Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def set_3D(self, is_3D: bool, axes: str, patch_size: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D flag and axes.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    axes : str\n        Axes of the data.\n    patch_size : list[int]\n        Patch size.\n    \"\"\"\n    # set the flag and axes (this will not trigger validation at the config level)\n    self.algorithm_config.model.set_3D(is_3D)\n    self.data_config.set_3D(axes, patch_size)\n\n    # cheap hack: trigger validation\n    self.algorithm_config = self.algorithm_config\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.validate_3D","title":"<code>validate_3D()</code>","text":"<p>Change algorithm dimensions to match data.axes.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated configuration.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_3D(self: Self) -&gt; Self:\n    \"\"\"\n    Change algorithm dimensions to match data.axes.\n\n    Returns\n    -------\n    Self\n        Validated configuration.\n    \"\"\"\n    if \"Z\" in self.data_config.axes and not self.algorithm_config.model.is_3D():\n        # change algorithm to 3D\n        self.algorithm_config.model.set_3D(True)\n    elif \"Z\" not in self.data_config.axes and self.algorithm_config.model.is_3D():\n        # change algorithm to 2D\n        self.algorithm_config.model.set_3D(False)\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.validate_n2v_mask_pixel_perc","title":"<code>validate_n2v_mask_pixel_perc()</code>","text":"<p>Validate that there will always be at least one blind-spot pixel in every patch.</p> <p>The probability of creating a blind-spot pixel is a function of the chosen masked pixel percentage and patch size.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the probability of masking a pixel within a patch is less than 1 for the chosen masked pixel percentage and patch size.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>@model_validator(mode=\"after\")  # TODO move to n2v configs or remove\ndef validate_n2v_mask_pixel_perc(self: Self) -&gt; Self:\n    \"\"\"\n    Validate that there will always be at least one blind-spot pixel in every patch.\n\n    The probability of creating a blind-spot pixel is a function of the chosen\n    masked pixel percentage and patch size.\n\n    Returns\n    -------\n    Self\n        Validated configuration.\n\n    Raises\n    ------\n    ValueError\n        If the probability of masking a pixel within a patch is less than 1 for the\n        chosen masked pixel percentage and patch size.\n    \"\"\"\n    # No validation needed for non n2v algorithms\n    if not isinstance(self.algorithm_config, N2VAlgorithm):\n        return self\n\n    mask_pixel_perc = self.algorithm_config.n2v_config.masked_pixel_percentage\n    patch_size = self.data_config.patch_size\n    expected_area_per_pixel = 1 / (mask_pixel_perc / 100)\n\n    n_dims = 3 if self.algorithm_config.model.is_3D() else 2\n    patch_size_lower_bound = int(np.ceil(expected_area_per_pixel ** (1 / n_dims)))\n    required_patch_size = tuple(\n        2 ** int(np.ceil(np.log2(patch_size_lower_bound))) for _ in range(n_dims)\n    )\n    required_mask_pixel_perc = (1 / np.prod(patch_size)) * 100\n    if expected_area_per_pixel &gt; np.prod(patch_size):\n        raise ValueError(\n            \"The probability of creating a blind-spot pixel within a patch is \"\n            f\"below 1, for a patch size of {patch_size} with a masked pixel \"\n            f\"percentage of {mask_pixel_perc}%. Either increase the patch size to \"\n            f\"{required_patch_size} or increase the masked pixel percentage to \"\n            f\"at least {required_mask_pixel_perc}%.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/","title":"configuration_factories","text":"<p>Convenience functions to create configurations for training and inference.</p>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.algorithm_factory","title":"<code>algorithm_factory(algorithm)</code>","text":"<p>Create an algorithm model for training CAREamics.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>dict</code> <p>Algorithm dictionary.</p> required <p>Returns:</p> Type Description <code>N2VAlgorithm or N2NAlgorithm or CAREAlgorithm</code> <p>Algorithm model for training CAREamics.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def algorithm_factory(\n    algorithm: dict[str, Any],\n) -&gt; Union[N2VAlgorithm, N2NAlgorithm, CAREAlgorithm]:\n    \"\"\"\n    Create an algorithm model for training CAREamics.\n\n    Parameters\n    ----------\n    algorithm : dict\n        Algorithm dictionary.\n\n    Returns\n    -------\n    N2VAlgorithm or N2NAlgorithm or CAREAlgorithm\n        Algorithm model for training CAREamics.\n    \"\"\"\n    adapter: TypeAdapter = TypeAdapter(\n        Annotated[\n            Union[N2VAlgorithm, N2NAlgorithm, CAREAlgorithm],\n            Field(discriminator=\"algorithm\"),\n        ]\n    )\n    return adapter.validate_python(algorithm)\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_care_configuration","title":"<code>create_care_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs=100, num_steps=None, augmentations=None, independent_channels=True, loss='mae', n_channels_in=None, n_channels_out=None, logger='none', trainer_params=None, model_params=None, optimizer='Adam', optimizer_params=None, lr_scheduler='ReduceLROnPlateau', lr_scheduler_params=None, train_dataloader_params=None, val_dataloader_params=None, checkpoint_params=None)</code>","text":"<p>Create a configuration for training CARE.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'czi', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for. If provided, this will be added to trainer_params.</p> <code>100</code> <code>num_steps</code> <code>int</code> <p>Number of batches in 1 epoch. If provided, this will be added to trainer_params. Translates to <code>limit_train_batches</code> in PyTorch Lightning Trainer. See relevant documentation for more details.</p> <code>None</code> <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently, by default False.</p> <code>True</code> <code>loss</code> <code>Literal['mae', 'mse']</code> <p>Loss function to use.</p> <code>\"mae\"</code> <code>n_channels_in</code> <code>int or None</code> <p>Number of channels in.</p> <code>None</code> <code>n_channels_out</code> <code>int or None</code> <p>Number of channels out.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use.</p> <code>\"none\"</code> <code>trainer_params</code> <code>dict</code> <p>Parameters for the trainer class, see PyTorch Lightning documentation.</p> <code>None</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <code>optimizer</code> <code>Literal['Adam', 'Adamax', 'SGD']</code> <p>Optimizer to use.</p> <code>\"Adam\"</code> <code>optimizer_params</code> <code>dict</code> <p>Parameters for the optimizer, see PyTorch documentation for more details.</p> <code>None</code> <code>lr_scheduler</code> <code>Literal['ReduceLROnPlateau', 'StepLR']</code> <p>Learning rate scheduler to use.</p> <code>\"ReduceLROnPlateau\"</code> <code>lr_scheduler_params</code> <code>dict</code> <p>Parameters for the learning rate scheduler, see PyTorch documentation for more details.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>checkpoint_params</code> <code>dict</code> <p>Parameters for the checkpoint callback, see PyTorch Lightning documentation (<code>ModelCheckpoint</code>) for the list of available parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training CARE.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>You can also limit the number of batches per epoch:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_steps=100  # limit to 100 batches per epoch\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter to replace the default augmentations:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels input (and optionally, the number of channels output):</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=3, # number of input channels\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels_in=3,\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If you would like to train on CZI files, use <code>\"czi\"</code> as <code>data_type</code> and <code>\"SCYX\"</code> as <code>axes</code> for 2-D or <code>\"SCZYX\"</code> for 3-D denoising. Note that <code>\"SCYX\"</code> can also be used for 3-D data but spatial context along the Z dimension will then not be taken into account.</p> <pre><code>&gt;&gt;&gt; config_2d = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCYX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=1,\n... )\n&gt;&gt;&gt; config_3d = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCZYX\",\n...     patch_size=[16, 64, 64],\n...     batch_size=16,\n...     num_epochs=100,\n...     n_channels_in=1,\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_care_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"czi\", \"custom\"],\n    axes: str,\n    patch_size: Sequence[int],\n    batch_size: int,\n    num_epochs: int = 100,\n    num_steps: int | None = None,\n    augmentations: list[Union[XYFlipModel, XYRandomRotate90Model]] | None = None,\n    independent_channels: bool = True,\n    loss: Literal[\"mae\", \"mse\"] = \"mae\",\n    n_channels_in: int | None = None,\n    n_channels_out: int | None = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    trainer_params: dict | None = None,\n    model_params: dict | None = None,\n    optimizer: Literal[\"Adam\", \"Adamax\", \"SGD\"] = \"Adam\",\n    optimizer_params: dict[str, Any] | None = None,\n    lr_scheduler: Literal[\"ReduceLROnPlateau\", \"StepLR\"] = \"ReduceLROnPlateau\",\n    lr_scheduler_params: dict[str, Any] | None = None,\n    train_dataloader_params: dict[str, Any] | None = None,\n    val_dataloader_params: dict[str, Any] | None = None,\n    checkpoint_params: dict[str, Any] | None = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training CARE.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"czi\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int, default=100\n        Number of epochs to train for. If provided, this will be added to\n        trainer_params.\n    num_steps : int, optional\n        Number of batches in 1 epoch. If provided, this will be added to trainer_params.\n        Translates to `limit_train_batches` in PyTorch Lightning Trainer. See relevant\n        documentation for more details.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels independently, by default False.\n    loss : Literal[\"mae\", \"mse\"], default=\"mae\"\n        Loss function to use.\n    n_channels_in : int or None, default=None\n        Number of channels in.\n    n_channels_out : int or None, default=None\n        Number of channels out.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], default=\"none\"\n        Logger to use.\n    trainer_params : dict, optional\n        Parameters for the trainer class, see PyTorch Lightning documentation.\n    model_params : dict, default=None\n        UNetModel parameters.\n    optimizer : Literal[\"Adam\", \"Adamax\", \"SGD\"], default=\"Adam\"\n        Optimizer to use.\n    optimizer_params : dict, default=None\n        Parameters for the optimizer, see PyTorch documentation for more details.\n    lr_scheduler : Literal[\"ReduceLROnPlateau\", \"StepLR\"], default=\"ReduceLROnPlateau\"\n        Learning rate scheduler to use.\n    lr_scheduler_params : dict, default=None\n        Parameters for the learning rate scheduler, see PyTorch documentation for more\n        details.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n    checkpoint_params : dict, default=None\n        Parameters for the checkpoint callback, see PyTorch Lightning documentation\n        (`ModelCheckpoint`) for the list of available parameters.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training CARE.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    You can also limit the number of batches per epoch:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_steps=100  # limit to 100 batches per epoch\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter to replace the\n    default augmentations:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels input (and optionally, the number\n    of channels output):\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=3, # number of input channels\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels_in=3,\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If you would like to train on CZI files, use `\"czi\"` as `data_type` and `\"SCYX\"` as\n    `axes` for 2-D or `\"SCZYX\"` for 3-D denoising. Note that `\"SCYX\"` can also be used\n    for 3-D data but spatial context along the Z dimension will then not be taken into\n    account.\n    &gt;&gt;&gt; config_2d = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCYX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=1,\n    ... )\n    &gt;&gt;&gt; config_3d = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCZYX\",\n    ...     patch_size=[16, 64, 64],\n    ...     batch_size=16,\n    ...     num_epochs=100,\n    ...     n_channels_in=1,\n    ... )\n    \"\"\"\n    return Configuration(\n        **_create_supervised_config_dict(\n            algorithm=\"care\",\n            experiment_name=experiment_name,\n            data_type=data_type,\n            axes=axes,\n            patch_size=patch_size,\n            batch_size=batch_size,\n            augmentations=augmentations,\n            independent_channels=independent_channels,\n            loss=loss,\n            n_channels_in=n_channels_in,\n            n_channels_out=n_channels_out,\n            logger=logger,\n            trainer_params=trainer_params,\n            model_params=model_params,\n            optimizer=optimizer,\n            optimizer_params=optimizer_params,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_params=lr_scheduler_params,\n            train_dataloader_params=train_dataloader_params,\n            val_dataloader_params=val_dataloader_params,\n            checkpoint_params=checkpoint_params,\n            num_epochs=num_epochs,\n            num_steps=num_steps,\n        )\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_hdn_configuration","title":"<code>create_hdn_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs=100, num_steps=None, encoder_conv_strides=(2, 2), decoder_conv_strides=(2, 2), multiscale_count=1, z_dims=(128, 128), output_channels=1, encoder_n_filters=32, decoder_n_filters=32, encoder_dropout=0.0, decoder_dropout=0.0, nonlinearity='ReLU', analytical_kl=False, predict_logvar=None, logvar_lowerbound=None, logger='none', trainer_params=None, augmentations=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training HDN.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained independently. To train all channels together, set <code>independent_channels</code> to False.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_hdn_configuration--todo-revisit-the-necessity-of-model_params","title":"TODO revisit the necessity of model_params","text":"<p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for. If provided, this will be added to trainer_params.</p> <code>100</code> <code>num_steps</code> <code>int</code> <p>Number of batches in 1 epoch. If provided, this will be added to trainer_params. Translates to <code>limit_train_batches</code> in PyTorch Lightning Trainer. See relevant documentation for more details.</p> <code>None</code> <code>encoder_conv_strides</code> <code>tuple[int, ...]</code> <p>Strides for the encoder convolutional layers, by default (2, 2).</p> <code>(2, 2)</code> <code>decoder_conv_strides</code> <code>tuple[int, ...]</code> <p>Strides for the decoder convolutional layers, by default (2, 2).</p> <code>(2, 2)</code> <code>multiscale_count</code> <code>int</code> <p>Number of scales in the multiscale architecture, by default 1.</p> <code>1</code> <code>z_dims</code> <code>tuple[int, ...]</code> <p>Dimensions of the latent space, by default (128, 128).</p> <code>(128, 128)</code> <code>output_channels</code> <code>int</code> <p>Number of output channels, by default 1.</p> <code>1</code> <code>encoder_n_filters</code> <code>int</code> <p>Number of filters in the encoder, by default 32.</p> <code>32</code> <code>decoder_n_filters</code> <code>int</code> <p>Number of filters in the decoder, by default 32.</p> <code>32</code> <code>encoder_dropout</code> <code>float</code> <p>Dropout rate for the encoder, by default 0.0.</p> <code>0.0</code> <code>decoder_dropout</code> <code>float</code> <p>Dropout rate for the decoder, by default 0.0.</p> <code>0.0</code> <code>nonlinearity</code> <code>Literal</code> <p>Nonlinearity function to use, by default \"ReLU\".</p> <code>'ReLU'</code> <code>analytical_kl</code> <code>bool</code> <p>Whether to use analytical KL divergence, by default False.</p> <code>False</code> <code>predict_logvar</code> <code>Literal[None, 'pixelwise']</code> <p>Type of log variance prediction, by default None.</p> <code>None</code> <code>logvar_lowerbound</code> <code>Union[float, None]</code> <p>Lower bound for the log variance, by default None.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use for training, by default \"none\".</p> <code>'none'</code> <code>trainer_params</code> <code>dict</code> <p>Parameters for the trainer class, see PyTorch Lightning documentation.</p> <code>None</code> <code>augmentations</code> <code>Optional[list[Union[XYFlipModel, XYRandomRotate90Model]]]</code> <p>List of augmentations to apply, by default None.</p> <code>None</code> <code>train_dataloader_params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters for the training dataloader, by default None.</p> <code>None</code> <code>val_dataloader_params</code> <code>Optional[dict[str, Any]]</code> <p>Parameters for the validation dataloader, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>The configuration object for training HDN.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_hdn_configuration(\n...     experiment_name=\"hdn_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>You can also limit the number of batches per epoch:</p> <pre><code>&gt;&gt;&gt; config = create_hdn_configuration(\n...     experiment_name=\"hdn_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_steps=100  # limit to 100 batches per epoch\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_hdn_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: Sequence[int],\n    batch_size: int,\n    num_epochs: int = 100,\n    num_steps: int | None = None,\n    encoder_conv_strides: tuple[int, ...] = (2, 2),\n    decoder_conv_strides: tuple[int, ...] = (2, 2),\n    multiscale_count: int = 1,\n    z_dims: tuple[int, ...] = (128, 128),\n    output_channels: int = 1,\n    encoder_n_filters: int = 32,\n    decoder_n_filters: int = 32,\n    encoder_dropout: float = 0.0,\n    decoder_dropout: float = 0.0,\n    nonlinearity: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\", \"ELU\"\n    ] = \"ReLU\",\n    analytical_kl: bool = False,\n    predict_logvar: Literal[\"pixelwise\"] | None = None,\n    logvar_lowerbound: Union[float, None] = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    trainer_params: dict | None = None,\n    augmentations: list[Union[XYFlipModel, XYRandomRotate90Model]] | None = None,\n    train_dataloader_params: dict[str, Any] | None = None,\n    val_dataloader_params: dict[str, Any] | None = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training HDN.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained independently. To train all channels together,\n    set `independent_channels` to False.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    # TODO revisit the necessity of model_params\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int, default=100\n        Number of epochs to train for. If provided, this will be added to\n        trainer_params.\n    num_steps : int, optional\n        Number of batches in 1 epoch. If provided, this will be added to trainer_params.\n        Translates to `limit_train_batches` in PyTorch Lightning Trainer. See relevant\n        documentation for more details.\n    encoder_conv_strides : tuple[int, ...], optional\n        Strides for the encoder convolutional layers, by default (2, 2).\n    decoder_conv_strides : tuple[int, ...], optional\n        Strides for the decoder convolutional layers, by default (2, 2).\n    multiscale_count : int, optional\n        Number of scales in the multiscale architecture, by default 1.\n    z_dims : tuple[int, ...], optional\n        Dimensions of the latent space, by default (128, 128).\n    output_channels : int, optional\n        Number of output channels, by default 1.\n    encoder_n_filters : int, optional\n        Number of filters in the encoder, by default 32.\n    decoder_n_filters : int, optional\n        Number of filters in the decoder, by default 32.\n    encoder_dropout : float, optional\n        Dropout rate for the encoder, by default 0.0.\n    decoder_dropout : float, optional\n        Dropout rate for the decoder, by default 0.0.\n    nonlinearity : Literal, optional\n        Nonlinearity function to use, by default \"ReLU\".\n    analytical_kl : bool, optional\n        Whether to use analytical KL divergence, by default False.\n    predict_logvar : Literal[None, \"pixelwise\"], optional\n        Type of log variance prediction, by default None.\n    logvar_lowerbound : Union[float, None], optional\n        Lower bound for the log variance, by default None.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use for training, by default \"none\".\n    trainer_params : dict, optional\n        Parameters for the trainer class, see PyTorch Lightning documentation.\n    augmentations : Optional[list[Union[XYFlipModel, XYRandomRotate90Model]]], optional\n        List of augmentations to apply, by default None.\n    train_dataloader_params : Optional[dict[str, Any]], optional\n        Parameters for the training dataloader, by default None.\n    val_dataloader_params : Optional[dict[str, Any]], optional\n        Parameters for the validation dataloader, by default None.\n\n    Returns\n    -------\n    Configuration\n        The configuration object for training HDN.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_hdn_configuration(\n    ...     experiment_name=\"hdn_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    You can also limit the number of batches per epoch:\n    &gt;&gt;&gt; config = create_hdn_configuration(\n    ...     experiment_name=\"hdn_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_steps=100  # limit to 100 batches per epoch\n    ... )\n    \"\"\"\n    transform_list = _list_spatial_augmentations(augmentations)\n\n    loss_config = LVAELossConfig(\n        loss_type=\"hdn\", denoisplit_weight=1, musplit_weight=0\n    )  # TODO what are the correct defaults for HDN?\n\n    gaussian_likelihood = GaussianLikelihoodConfig(\n        predict_logvar=predict_logvar, logvar_lowerbound=logvar_lowerbound\n    )\n\n    # algorithm &amp; model\n    algorithm_params = _create_vae_based_algorithm(\n        algorithm=\"hdn\",\n        loss=loss_config,\n        input_shape=patch_size,\n        encoder_conv_strides=encoder_conv_strides,\n        decoder_conv_strides=decoder_conv_strides,\n        multiscale_count=multiscale_count,\n        z_dims=z_dims,\n        output_channels=output_channels,\n        encoder_n_filters=encoder_n_filters,\n        decoder_n_filters=decoder_n_filters,\n        encoder_dropout=encoder_dropout,\n        decoder_dropout=decoder_dropout,\n        nonlinearity=nonlinearity,\n        predict_logvar=predict_logvar,\n        analytical_kl=analytical_kl,\n        gaussian_likelihood=gaussian_likelihood,\n        nm_likelihood=None,\n    )\n\n    # data\n    data_params = _create_data_configuration(\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        augmentations=transform_list,\n        train_dataloader_params=train_dataloader_params,\n        val_dataloader_params=val_dataloader_params,\n    )\n\n    # Handle trainer parameters with num_epochs and num_steps\n    final_trainer_params = {} if trainer_params is None else trainer_params.copy()\n\n    # Add num_epochs and num_steps if provided\n    if num_epochs is not None:\n        final_trainer_params[\"max_epochs\"] = num_epochs\n    if num_steps is not None:\n        final_trainer_params[\"limit_train_batches\"] = num_steps\n\n    # training\n    training_params = _create_training_configuration(\n        trainer_params=final_trainer_params,\n        logger=logger,\n    )\n\n    return Configuration(\n        experiment_name=experiment_name,\n        algorithm_config=algorithm_params,\n        data_config=data_params,\n        training_config=training_params,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration","title":"<code>create_microsplit_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs=100, num_steps=None, encoder_conv_strides=(2, 2), decoder_conv_strides=(2, 2), multiscale_count=3, grid_size=32, z_dims=(128, 128), output_channels=1, encoder_n_filters=32, decoder_n_filters=32, encoder_dropout=0.0, decoder_dropout=0.0, nonlinearity='ReLU', analytical_kl=False, predict_logvar='pixelwise', logvar_lowerbound=None, logger='none', trainer_params=None, augmentations=None, nm_paths=None, data_stats=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training MicroSplit.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>Sequence[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for. If provided, this will be added to trainer_params.</p> <code>100</code> <code>num_steps</code> <code>int</code> <p>Number of batches in 1 epoch. If provided, this will be added to trainer_params. Translates to <code>limit_train_batches</code> in PyTorch Lightning Trainer. See relevant documentation for more details.</p> <code>None</code> <code>encoder_conv_strides</code> <code>tuple[int, ...]</code> <p>Strides for the encoder convolutional layers, by default (2, 2).</p> <code>(2, 2)</code> <code>decoder_conv_strides</code> <code>tuple[int, ...]</code> <p>Strides for the decoder convolutional layers, by default (2, 2).</p> <code>(2, 2)</code> <code>multiscale_count</code> <code>int</code> <p>Number of multiscale levels, by default 1.</p> <code>3</code> <code>grid_size</code> <code>int</code> <p>Size of the grid for the lateral context, by default 32.</p> <code>32</code> <code>z_dims</code> <code>tuple[int, ...]</code> <p>List of latent dimensions for each hierarchy level in the LVAE, by default (128, 128).</p> <code>(128, 128)</code> <code>output_channels</code> <code>int</code> <p>Number of output channels for the model, by default 1.</p> <code>1</code> <code>encoder_n_filters</code> <code>int</code> <p>Number of filters in the encoder, by default 32.</p> <code>32</code> <code>decoder_n_filters</code> <code>int</code> <p>Number of filters in the decoder, by default 32.</p> <code>32</code> <code>encoder_dropout</code> <code>float</code> <p>Dropout rate for the encoder, by default 0.0.</p> <code>0.0</code> <code>decoder_dropout</code> <code>float</code> <p>Dropout rate for the decoder, by default 0.0.</p> <code>0.0</code> <code>nonlinearity</code> <code>Literal</code> <p>Nonlinearity to use in the model, by default \"ReLU\".</p> <code>'ReLU'</code> <code>analytical_kl</code> <code>bool</code> <p>Whether to use analytical KL divergence, by default False.</p> <code>False</code> <code>predict_logvar</code> <code>Literal['pixelwise'] | None</code> <p>Type of log-variance prediction, by default None.</p> <code>'pixelwise'</code> <code>logvar_lowerbound</code> <code>Union[float, None]</code> <p>Lower bound for the log variance, by default None.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use for training, by default \"none\".</p> <code>'none'</code> <code>trainer_params</code> <code>dict</code> <p>Parameters for the trainer class, see PyTorch Lightning documentation.</p> <code>None</code> <code>augmentations</code> <code>list[Union[XYFlipModel, XYRandomRotate90Model]] | None</code> <p>List of augmentations to apply, by default None.</p> <code>None</code> <code>nm_paths</code> <code>list[str] | None</code> <p>Paths to the noise model files, by default None.</p> <code>None</code> <code>data_stats</code> <code>tuple[float, float] | None</code> <p>Data statistics (mean, std), by default None.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict[str, Any] | None</code> <p>Parameters for the training dataloader, by default None.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict[str, Any] | None</code> <p>Parameters for the validation dataloader, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>A configuration object for the microsplit algorithm.</p> <p>Examples:</p> <p>Minimum example:</p>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--config-create_microsplit_configuration","title":"&gt;&gt;&gt; config = create_microsplit_configuration(","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--experiment_namemicrosplit_experiment","title":"...     experiment_name=\"microsplit_experiment\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--data_typearray","title":"...     data_type=\"array\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--axesyx","title":"...     axes=\"YX\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--patch_size64-64","title":"...     patch_size=[64, 64],","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--batch_size32","title":"...     batch_size=32,","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--num_epochs100","title":"...     num_epochs=100","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--_1","title":"... )","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--you-can-also-limit-the-number-of-batches-per-epoch","title":"You can also limit the number of batches per epoch:","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--config-create_microsplit_configuration_1","title":"&gt;&gt;&gt; config = create_microsplit_configuration(","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--experiment_namemicrosplit_experiment_1","title":"...     experiment_name=\"microsplit_experiment\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--data_typearray_1","title":"...     data_type=\"array\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--axesyx_1","title":"...     axes=\"YX\",","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--patch_size64-64_1","title":"...     patch_size=[64, 64],","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--batch_size32_1","title":"...     batch_size=32,","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--num_steps100-limit-to-100-batches-per-epoch","title":"...     num_steps=100  # limit to 100 batches per epoch","text":""},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_microsplit_configuration--_2","title":"... )","text":"Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_microsplit_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: Sequence[int],\n    batch_size: int,\n    num_epochs: int = 100,\n    num_steps: int | None = None,\n    encoder_conv_strides: tuple[int, ...] = (2, 2),\n    decoder_conv_strides: tuple[int, ...] = (2, 2),\n    multiscale_count: int = 3,\n    grid_size: int = 32,  # TODO most likely can be derived from patch size\n    z_dims: tuple[int, ...] = (128, 128),\n    output_channels: int = 1,\n    encoder_n_filters: int = 32,\n    decoder_n_filters: int = 32,\n    encoder_dropout: float = 0.0,\n    decoder_dropout: float = 0.0,\n    nonlinearity: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\", \"ELU\"\n    ] = \"ReLU\",  # TODO do we need all these?\n    analytical_kl: bool = False,\n    predict_logvar: Literal[\"pixelwise\"] = \"pixelwise\",\n    logvar_lowerbound: Union[float, None] = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    trainer_params: dict | None = None,\n    augmentations: list[Union[XYFlipModel, XYRandomRotate90Model]] | None = None,\n    nm_paths: list[str] | None = None,\n    data_stats: tuple[float, float] | None = None,\n    train_dataloader_params: dict[str, Any] | None = None,\n    val_dataloader_params: dict[str, Any] | None = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training MicroSplit.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : Sequence[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int, default=100\n        Number of epochs to train for. If provided, this will be added to\n        trainer_params.\n    num_steps : int, optional\n        Number of batches in 1 epoch. If provided, this will be added to trainer_params.\n        Translates to `limit_train_batches` in PyTorch Lightning Trainer. See relevant\n        documentation for more details.\n    encoder_conv_strides : tuple[int, ...], optional\n        Strides for the encoder convolutional layers, by default (2, 2).\n    decoder_conv_strides : tuple[int, ...], optional\n        Strides for the decoder convolutional layers, by default (2, 2).\n    multiscale_count : int, optional\n        Number of multiscale levels, by default 1.\n    grid_size : int, optional\n        Size of the grid for the lateral context, by default 32.\n    z_dims : tuple[int, ...], optional\n        List of latent dimensions for each hierarchy level in the LVAE, by default\n        (128, 128).\n    output_channels : int, optional\n        Number of output channels for the model, by default 1.\n    encoder_n_filters : int, optional\n        Number of filters in the encoder, by default 32.\n    decoder_n_filters : int, optional\n        Number of filters in the decoder, by default 32.\n    encoder_dropout : float, optional\n        Dropout rate for the encoder, by default 0.0.\n    decoder_dropout : float, optional\n        Dropout rate for the decoder, by default 0.0.\n    nonlinearity : Literal, optional\n        Nonlinearity to use in the model, by default \"ReLU\".\n    analytical_kl : bool, optional\n        Whether to use analytical KL divergence, by default False.\n    predict_logvar : Literal[\"pixelwise\"] | None, optional\n        Type of log-variance prediction, by default None.\n    logvar_lowerbound : Union[float, None], optional\n        Lower bound for the log variance, by default None.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use for training, by default \"none\".\n    trainer_params : dict, optional\n        Parameters for the trainer class, see PyTorch Lightning documentation.\n    augmentations : list[Union[XYFlipModel, XYRandomRotate90Model]] | None, optional\n        List of augmentations to apply, by default None.\n    nm_paths : list[str] | None, optional\n        Paths to the noise model files, by default None.\n    data_stats : tuple[float, float] | None, optional\n        Data statistics (mean, std), by default None.\n    train_dataloader_params : dict[str, Any] | None, optional\n        Parameters for the training dataloader, by default None.\n    val_dataloader_params : dict[str, Any] | None, optional\n        Parameters for the validation dataloader, by default None.\n\n    Returns\n    -------\n    Configuration\n        A configuration object for the microsplit algorithm.\n\n    Examples\n    --------\n    Minimum example:\n    # &gt;&gt;&gt; config = create_microsplit_configuration(\n    # ...     experiment_name=\"microsplit_experiment\",\n    # ...     data_type=\"array\",\n    # ...     axes=\"YX\",\n    # ...     patch_size=[64, 64],\n    # ...     batch_size=32,\n    # ...     num_epochs=100\n\n    # ... )\n\n    # You can also limit the number of batches per epoch:\n    # &gt;&gt;&gt; config = create_microsplit_configuration(\n    # ...     experiment_name=\"microsplit_experiment\",\n    # ...     data_type=\"array\",\n    # ...     axes=\"YX\",\n    # ...     patch_size=[64, 64],\n    # ...     batch_size=32,\n    # ...     num_steps=100  # limit to 100 batches per epoch\n    # ... )\n    \"\"\"\n    transform_list = _list_spatial_augmentations(augmentations)\n\n    loss_config = LVAELossConfig(\n        loss_type=\"denoisplit_musplit\", denoisplit_weight=0.9, musplit_weight=0.1\n    )  # TODO losses need to be refactored! just for example. Add validator if sum to 1\n\n    # Create likelihood configurations\n    gaussian_likelihood_config, noise_model_config, nm_likelihood_config = (\n        get_likelihood_config(\n            loss_type=\"denoisplit_musplit\",\n            predict_logvar=predict_logvar,\n            logvar_lowerbound=logvar_lowerbound,\n            nm_paths=nm_paths,\n            data_stats=data_stats,\n        )\n    )\n\n    # Create the LVAE model\n    network_model = _create_vae_configuration(\n        input_shape=patch_size,\n        encoder_conv_strides=encoder_conv_strides,\n        decoder_conv_strides=decoder_conv_strides,\n        multiscale_count=multiscale_count,\n        z_dims=z_dims,\n        output_channels=output_channels,\n        encoder_n_filters=encoder_n_filters,\n        decoder_n_filters=decoder_n_filters,\n        encoder_dropout=encoder_dropout,\n        decoder_dropout=decoder_dropout,\n        nonlinearity=nonlinearity,\n        predict_logvar=predict_logvar,\n        analytical_kl=analytical_kl,\n    )\n\n    # Create the MicroSplit algorithm configuration\n    algorithm_params = {\n        \"algorithm\": \"microsplit\",\n        \"loss\": loss_config,\n        \"model\": network_model,\n        \"gaussian_likelihood\": gaussian_likelihood_config,\n        \"noise_model\": noise_model_config,\n        \"noise_model_likelihood\": nm_likelihood_config,\n    }\n\n    # Convert to MicroSplitAlgorithm instance\n    algorithm_config = MicroSplitAlgorithm(**algorithm_params)\n\n    # data\n    data_params = _create_microsplit_data_configuration(\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        grid_size=grid_size,\n        multiscale_count=multiscale_count,\n        batch_size=batch_size,\n        augmentations=transform_list,\n        train_dataloader_params=train_dataloader_params,\n        val_dataloader_params=val_dataloader_params,\n    )\n\n    # Handle trainer parameters with num_epochs and num_steps\n    final_trainer_params = {} if trainer_params is None else trainer_params.copy()\n\n    # Add num_epochs and num_steps if provided\n    if num_epochs is not None:\n        final_trainer_params[\"max_epochs\"] = num_epochs\n    if num_steps is not None:\n        final_trainer_params[\"limit_train_batches\"] = num_steps\n\n    # training\n    training_params = _create_training_configuration(\n        trainer_params=final_trainer_params,\n        logger=logger,\n    )\n\n    return Configuration(\n        experiment_name=experiment_name,\n        algorithm_config=algorithm_config,\n        data_config=data_params,\n        training_config=training_params,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_n2n_configuration","title":"<code>create_n2n_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs=100, num_steps=None, augmentations=None, independent_channels=True, loss='mae', n_channels_in=None, n_channels_out=None, logger='none', trainer_params=None, model_params=None, optimizer='Adam', optimizer_params=None, lr_scheduler='ReduceLROnPlateau', lr_scheduler_params=None, train_dataloader_params=None, val_dataloader_params=None, checkpoint_params=None)</code>","text":"<p>Create a configuration for training Noise2Noise.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'czi', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for. If provided, this will be added to trainer_params.</p> <code>100</code> <code>num_steps</code> <code>int</code> <p>Number of batches in 1 epoch. If provided, this will be added to trainer_params. Translates to <code>limit_train_batches</code> in PyTorch Lightning Trainer. See relevant documentation for more details.</p> <code>None</code> <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently, by default False.</p> <code>True</code> <code>loss</code> <code>Literal['mae', 'mse']</code> <p>Loss function to use, by default \"mae\".</p> <code>'mae'</code> <code>n_channels_in</code> <code>int or None</code> <p>Number of channels in.</p> <code>None</code> <code>n_channels_out</code> <code>int or None</code> <p>Number of channels out.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use, by default \"none\".</p> <code>'none'</code> <code>trainer_params</code> <code>dict</code> <p>Parameters for the trainer class, see PyTorch Lightning documentation.</p> <code>None</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <code>optimizer</code> <code>Literal['Adam', 'Adamax', 'SGD']</code> <p>Optimizer to use.</p> <code>\"Adam\"</code> <code>optimizer_params</code> <code>dict</code> <p>Parameters for the optimizer, see PyTorch documentation for more details.</p> <code>None</code> <code>lr_scheduler</code> <code>Literal['ReduceLROnPlateau', 'StepLR']</code> <p>Learning rate scheduler to use.</p> <code>\"ReduceLROnPlateau\"</code> <code>lr_scheduler_params</code> <code>dict</code> <p>Parameters for the learning rate scheduler, see PyTorch documentation for more details.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>checkpoint_params</code> <code>dict</code> <p>Parameters for the checkpoint callback, see PyTorch Lightning documentation (<code>ModelCheckpoint</code>) for the list of available parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training Noise2Noise.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>You can also limit the number of batches per epoch:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_steps=100  # limit to 100 batches per epoch\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels input (and optionally, the number of channels output):</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=3, # number of input channels\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels_in=3,\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If you would like to train on CZI files, use <code>\"czi\"</code> as <code>data_type</code> and <code>\"SCYX\"</code> as <code>axes</code> for 2-D or <code>\"SCZYX\"</code> for 3-D denoising. Note that <code>\"SCYX\"</code> can also be used for 3-D data but spatial context along the Z dimension will then not be taken into account.</p> <pre><code>&gt;&gt;&gt; config_2d = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCYX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=1,\n... )\n&gt;&gt;&gt; config_3d = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCZYX\",\n...     patch_size=[16, 64, 64],\n...     batch_size=16,\n...     num_epochs=100,\n...     n_channels_in=1,\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_n2n_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"czi\", \"custom\"],\n    axes: str,\n    patch_size: Sequence[int],\n    batch_size: int,\n    num_epochs: int = 100,\n    num_steps: int | None = None,\n    augmentations: list[Union[XYFlipModel, XYRandomRotate90Model]] | None = None,\n    independent_channels: bool = True,\n    loss: Literal[\"mae\", \"mse\"] = \"mae\",\n    n_channels_in: int | None = None,\n    n_channels_out: int | None = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    trainer_params: dict | None = None,\n    model_params: dict | None = None,\n    optimizer: Literal[\"Adam\", \"Adamax\", \"SGD\"] = \"Adam\",\n    optimizer_params: dict[str, Any] | None = None,\n    lr_scheduler: Literal[\"ReduceLROnPlateau\", \"StepLR\"] = \"ReduceLROnPlateau\",\n    lr_scheduler_params: dict[str, Any] | None = None,\n    train_dataloader_params: dict[str, Any] | None = None,\n    val_dataloader_params: dict[str, Any] | None = None,\n    checkpoint_params: dict[str, Any] | None = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training Noise2Noise.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"czi\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int, default=100\n        Number of epochs to train for. If provided, this will be added to\n        trainer_params.\n    num_steps : int, optional\n        Number of batches in 1 epoch. If provided, this will be added to trainer_params.\n        Translates to `limit_train_batches` in PyTorch Lightning Trainer. See relevant\n        documentation for more details.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels independently, by default False.\n    loss : Literal[\"mae\", \"mse\"], optional\n        Loss function to use, by default \"mae\".\n    n_channels_in : int or None, default=None\n        Number of channels in.\n    n_channels_out : int or None, default=None\n        Number of channels out.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use, by default \"none\".\n    trainer_params : dict, optional\n        Parameters for the trainer class, see PyTorch Lightning documentation.\n    model_params : dict, default=None\n        UNetModel parameters.\n    optimizer : Literal[\"Adam\", \"Adamax\", \"SGD\"], default=\"Adam\"\n        Optimizer to use.\n    optimizer_params : dict, default=None\n        Parameters for the optimizer, see PyTorch documentation for more details.\n    lr_scheduler : Literal[\"ReduceLROnPlateau\", \"StepLR\"], default=\"ReduceLROnPlateau\"\n        Learning rate scheduler to use.\n    lr_scheduler_params : dict, default=None\n        Parameters for the learning rate scheduler, see PyTorch documentation for more\n        details.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n    checkpoint_params : dict, default=None\n        Parameters for the checkpoint callback, see PyTorch Lightning documentation\n        (`ModelCheckpoint`) for the list of available parameters.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training Noise2Noise.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    You can also limit the number of batches per epoch:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_steps=100  # limit to 100 batches per epoch\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels input (and optionally, the number\n    of channels output):\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=3, # number of input channels\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels_in=3,\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If you would like to train on CZI files, use `\"czi\"` as `data_type` and `\"SCYX\"` as\n    `axes` for 2-D or `\"SCZYX\"` for 3-D denoising. Note that `\"SCYX\"` can also be used\n    for 3-D data but spatial context along the Z dimension will then not be taken into\n    account.\n    &gt;&gt;&gt; config_2d = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCYX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=1,\n    ... )\n    &gt;&gt;&gt; config_3d = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCZYX\",\n    ...     patch_size=[16, 64, 64],\n    ...     batch_size=16,\n    ...     num_epochs=100,\n    ...     n_channels_in=1,\n    ... )\n    \"\"\"\n    return Configuration(\n        **_create_supervised_config_dict(\n            algorithm=\"n2n\",\n            experiment_name=experiment_name,\n            data_type=data_type,\n            axes=axes,\n            patch_size=patch_size,\n            batch_size=batch_size,\n            trainer_params=trainer_params,\n            augmentations=augmentations,\n            independent_channels=independent_channels,\n            loss=loss,\n            n_channels_in=n_channels_in,\n            n_channels_out=n_channels_out,\n            logger=logger,\n            model_params=model_params,\n            optimizer=optimizer,\n            optimizer_params=optimizer_params,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_params=lr_scheduler_params,\n            train_dataloader_params=train_dataloader_params,\n            val_dataloader_params=val_dataloader_params,\n            checkpoint_params=checkpoint_params,\n            num_epochs=num_epochs,\n            num_steps=num_steps,\n        )\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_n2v_configuration","title":"<code>create_n2v_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs=100, num_steps=None, augmentations=None, independent_channels=True, use_n2v2=False, n_channels=None, roi_size=11, masked_pixel_percentage=0.2, struct_n2v_axis='none', struct_n2v_span=5, trainer_params=None, logger='none', model_params=None, optimizer='Adam', optimizer_params=None, lr_scheduler='ReduceLROnPlateau', lr_scheduler_params=None, train_dataloader_params=None, val_dataloader_params=None, checkpoint_params=None)</code>","text":"<p>Create a configuration for training Noise2Void.</p> <p>N2V uses a UNet model to denoise images in a self-supervised manner. To use its variants structN2V and N2V2, set the <code>struct_n2v_axis</code> and <code>struct_n2v_span</code> (structN2V) parameters, or set <code>use_n2v2</code> to True (N2V2).</p> <p>N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip connections, thus removing checkboard artefacts. StructN2V is used when vertical or horizontal correlations are present in the noise; it applies an additional mask to the manipulated pixel neighbors.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels.</p> <p>By default, all channels are trained independently. To train all channels together, set <code>independent_channels</code> to False.</p> <p>By default, the transformations applied are a random flip along X or Y, and a random 90 degrees rotation in the XY plane. Normalization is always applied, as well as the N2V manipulation.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>The <code>roi_size</code> parameter specifies the size of the area around each pixel that will be manipulated by N2V. The <code>masked_pixel_percentage</code> parameter specifies how many pixels per patch will be manipulated.</p> <p>The parameters of the UNet can be specified in the <code>model_params</code> (passed as a parameter-value dictionary). Note that <code>use_n2v2</code> and 'n_channels' override the corresponding parameters passed in <code>model_params</code>.</p> <p>If you pass \"horizontal\" or \"vertical\" to <code>struct_n2v_axis</code>, then structN2V mask will be applied to each manipulated pixel.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'czi', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train for. If provided, this will be added to trainer_params.</p> <code>100</code> <code>num_steps</code> <code>int</code> <p>Number of batches in 1 epoch. If provided, this will be added to trainer_params. Translates to <code>limit_train_batches</code> in PyTorch Lightning Trainer. See relevant documentation for more details.</p> <code>None</code> <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels together, by default True.</p> <code>True</code> <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2, by default False.</p> <code>False</code> <code>n_channels</code> <code>int or None</code> <p>Number of channels (in and out).</p> <code>None</code> <code>roi_size</code> <code>int</code> <p>N2V pixel manipulation area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels masked in each patch, by default 0.2.</p> <code>0.2</code> <code>struct_n2v_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>Axis along which to apply structN2V mask, by default \"none\".</p> <code>'none'</code> <code>struct_n2v_span</code> <code>int</code> <p>Span of the structN2V mask, by default 5.</p> <code>5</code> <code>trainer_params</code> <code>dict</code> <p>Parameters for the trainer, see the relevant documentation.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use, by default \"none\".</p> <code>'none'</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <code>optimizer</code> <code>Literal['Adam', 'Adamax', 'SGD']</code> <p>Optimizer to use.</p> <code>\"Adam\"</code> <code>optimizer_params</code> <code>dict</code> <p>Parameters for the optimizer, see PyTorch documentation for more details.</p> <code>None</code> <code>lr_scheduler</code> <code>Literal['ReduceLROnPlateau', 'StepLR']</code> <p>Learning rate scheduler to use.</p> <code>\"ReduceLROnPlateau\"</code> <code>lr_scheduler_params</code> <code>dict</code> <p>Parameters for the learning rate scheduler, see PyTorch documentation for more details.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>checkpoint_params</code> <code>dict</code> <p>Parameters for the checkpoint callback, see PyTorch Lightning documentation (<code>ModelCheckpoint</code>) for the list of available parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training N2V.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>You can also limit the number of batches per epoch:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_steps=100  # limit to 100 batches per epoch\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>To use N2V2, simply pass the <code>use_n2v2</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v2_experiment\",\n...     data_type=\"tiff\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     use_n2v2=True\n... )\n</code></pre> <p>For structN2V, there are two parameters to set, <code>struct_n2v_axis</code> and <code>struct_n2v_span</code>:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"structn2v_experiment\",\n...     data_type=\"tiff\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     struct_n2v_axis=\"horizontal\",\n...     struct_n2v_span=7\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels=3\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels=3\n... )\n</code></pre> <p>If you would like to train on CZI files, use <code>\"czi\"</code> as <code>data_type</code> and <code>\"SCYX\"</code> as <code>axes</code> for 2-D or <code>\"SCZYX\"</code> for 3-D denoising. Note that <code>\"SCYX\"</code> can also be used for 3-D data but spatial context along the Z dimension will then not be taken into account.</p> <pre><code>&gt;&gt;&gt; config_2d = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCYX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels=1,\n... )\n&gt;&gt;&gt; config_3d = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"czi\",\n...     axes=\"SCZYX\",\n...     patch_size=[16, 64, 64],\n...     batch_size=16,\n...     num_epochs=100,\n...     n_channels=1,\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_n2v_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"czi\", \"custom\"],\n    axes: str,\n    patch_size: Sequence[int],\n    batch_size: int,\n    num_epochs: int = 100,\n    num_steps: int | None = None,\n    augmentations: list[Union[XYFlipModel, XYRandomRotate90Model]] | None = None,\n    independent_channels: bool = True,\n    use_n2v2: bool = False,\n    n_channels: int | None = None,\n    roi_size: int = 11,\n    masked_pixel_percentage: float = 0.2,\n    struct_n2v_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_n2v_span: int = 5,\n    trainer_params: dict | None = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    model_params: dict | None = None,\n    optimizer: Literal[\"Adam\", \"Adamax\", \"SGD\"] = \"Adam\",\n    optimizer_params: dict[str, Any] | None = None,\n    lr_scheduler: Literal[\"ReduceLROnPlateau\", \"StepLR\"] = \"ReduceLROnPlateau\",\n    lr_scheduler_params: dict[str, Any] | None = None,\n    train_dataloader_params: dict[str, Any] | None = None,\n    val_dataloader_params: dict[str, Any] | None = None,\n    checkpoint_params: dict[str, Any] | None = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training Noise2Void.\n\n    N2V uses a UNet model to denoise images in a self-supervised manner. To use its\n    variants structN2V and N2V2, set the `struct_n2v_axis` and `struct_n2v_span`\n    (structN2V) parameters, or set `use_n2v2` to True (N2V2).\n\n    N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip\n    connections, thus removing checkboard artefacts. StructN2V is used when vertical\n    or horizontal correlations are present in the noise; it applies an additional mask\n    to the manipulated pixel neighbors.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels.\n\n    By default, all channels are trained independently. To train all channels together,\n    set `independent_channels` to False.\n\n    By default, the transformations applied are a random flip along X or Y, and a random\n    90 degrees rotation in the XY plane. Normalization is always applied, as well as the\n    N2V manipulation.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    The `roi_size` parameter specifies the size of the area around each pixel that will\n    be manipulated by N2V. The `masked_pixel_percentage` parameter specifies how many\n    pixels per patch will be manipulated.\n\n    The parameters of the UNet can be specified in the `model_params` (passed as a\n    parameter-value dictionary). Note that `use_n2v2` and 'n_channels' override the\n    corresponding parameters passed in `model_params`.\n\n    If you pass \"horizontal\" or \"vertical\" to `struct_n2v_axis`, then structN2V mask\n    will be applied to each manipulated pixel.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"czi\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int, default=100\n        Number of epochs to train for. If provided, this will be added to\n        trainer_params.\n    num_steps : int, optional\n        Number of batches in 1 epoch. If provided, this will be added to trainer_params.\n        Translates to `limit_train_batches` in PyTorch Lightning Trainer. See relevant\n        documentation for more details.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels together, by default True.\n    use_n2v2 : bool, optional\n        Whether to use N2V2, by default False.\n    n_channels : int or None, default=None\n        Number of channels (in and out).\n    roi_size : int, optional\n        N2V pixel manipulation area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels masked in each patch, by default 0.2.\n    struct_n2v_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        Axis along which to apply structN2V mask, by default \"none\".\n    struct_n2v_span : int, optional\n        Span of the structN2V mask, by default 5.\n    trainer_params : dict, optional\n        Parameters for the trainer, see the relevant documentation.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use, by default \"none\".\n    model_params : dict, default=None\n        UNetModel parameters.\n    optimizer : Literal[\"Adam\", \"Adamax\", \"SGD\"], default=\"Adam\"\n        Optimizer to use.\n    optimizer_params : dict, default=None\n        Parameters for the optimizer, see PyTorch documentation for more details.\n    lr_scheduler : Literal[\"ReduceLROnPlateau\", \"StepLR\"], default=\"ReduceLROnPlateau\"\n        Learning rate scheduler to use.\n    lr_scheduler_params : dict, default=None\n        Parameters for the learning rate scheduler, see PyTorch documentation for more\n        details.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n    checkpoint_params : dict, default=None\n        Parameters for the checkpoint callback, see PyTorch Lightning documentation\n        (`ModelCheckpoint`) for the list of available parameters.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training N2V.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    You can also limit the number of batches per epoch:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_steps=100  # limit to 100 batches per epoch\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    To use N2V2, simply pass the `use_n2v2` parameter:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v2_experiment\",\n    ...     data_type=\"tiff\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     use_n2v2=True\n    ... )\n\n    For structN2V, there are two parameters to set, `struct_n2v_axis` and\n    `struct_n2v_span`:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"structn2v_experiment\",\n    ...     data_type=\"tiff\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     struct_n2v_axis=\"horizontal\",\n    ...     struct_n2v_span=7\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels=3\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels=3\n    ... )\n\n    If you would like to train on CZI files, use `\"czi\"` as `data_type` and `\"SCYX\"` as\n    `axes` for 2-D or `\"SCZYX\"` for 3-D denoising. Note that `\"SCYX\"` can also be used\n    for 3-D data but spatial context along the Z dimension will then not be taken into\n    account.\n    &gt;&gt;&gt; config_2d = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCYX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels=1,\n    ... )\n    &gt;&gt;&gt; config_3d = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"czi\",\n    ...     axes=\"SCZYX\",\n    ...     patch_size=[16, 64, 64],\n    ...     batch_size=16,\n    ...     num_epochs=100,\n    ...     n_channels=1,\n    ... )\n    \"\"\"\n    # if there are channels, we need to specify their number\n    if \"C\" in axes and n_channels is None:\n        raise ValueError(\"Number of channels must be specified when using channels.\")\n    elif \"C\" not in axes and (n_channels is not None and n_channels &gt; 1):\n        raise ValueError(\n            f\"C is not present in the axes, but number of channels is specified \"\n            f\"(got {n_channels} channel).\"\n        )\n\n    if n_channels is None:\n        n_channels = 1\n\n    # augmentations\n    spatial_transforms = _list_spatial_augmentations(augmentations)\n\n    # create the N2VManipulate transform using the supplied parameters\n    n2v_transform = N2VManipulateModel(\n        name=SupportedTransform.N2V_MANIPULATE.value,\n        strategy=(\n            SupportedPixelManipulation.MEDIAN.value\n            if use_n2v2\n            else SupportedPixelManipulation.UNIFORM.value\n        ),\n        roi_size=roi_size,\n        masked_pixel_percentage=masked_pixel_percentage,\n        struct_mask_axis=struct_n2v_axis,\n        struct_mask_span=struct_n2v_span,\n    )\n\n    # algorithm\n    algorithm_params = _create_algorithm_configuration(\n        axes=axes,\n        algorithm=\"n2v\",\n        loss=\"n2v\",\n        independent_channels=independent_channels,\n        n_channels_in=n_channels,\n        n_channels_out=n_channels,\n        use_n2v2=use_n2v2,\n        model_params=model_params,\n        optimizer=optimizer,\n        optimizer_params=optimizer_params,\n        lr_scheduler=lr_scheduler,\n        lr_scheduler_params=lr_scheduler_params,\n    )\n    algorithm_params[\"n2v_config\"] = n2v_transform\n\n    # data\n    data_params = _create_data_configuration(\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        augmentations=spatial_transforms,\n        train_dataloader_params=train_dataloader_params,\n        val_dataloader_params=val_dataloader_params,\n    )\n\n    # training\n    # Handle trainer parameters with num_epochs and nun_steps\n    final_trainer_params = {} if trainer_params is None else trainer_params.copy()\n\n    # Add num_epochs and nun_steps if provided\n    if num_epochs is not None:\n        final_trainer_params[\"max_epochs\"] = num_epochs\n    if num_steps is not None:\n        final_trainer_params[\"limit_train_batches\"] = num_steps\n\n    training_params = _create_training_configuration(\n        trainer_params=final_trainer_params,\n        logger=logger,\n        checkpoint_params=checkpoint_params,\n    )\n\n    return Configuration(\n        experiment_name=experiment_name,\n        algorithm_config=algorithm_params,\n        data_config=data_params,\n        training_config=training_params,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.get_likelihood_config","title":"<code>get_likelihood_config(loss_type, predict_logvar=None, logvar_lowerbound=-5.0, nm_paths=None, data_stats=None)</code>","text":"<p>Get the likelihood configuration for split models.</p> <p>Returns a tuple containing the following optional entries:     - GaussianLikelihoodConfig: Gaussian likelihood configuration for musplit losses     - MultiChannelNMConfig: Multi-channel noise model configuration for denoisplit     losses     - NMLikelihoodConfig: Noise model likelihood configuration for denoisplit losses</p> <p>Parameters:</p> Name Type Description Default <code>loss_type</code> <code>Literal['musplit', 'denoisplit', 'denoisplit_musplit']</code> <p>The type of loss function to use.</p> required <code>predict_logvar</code> <code>Literal['pixelwise'] | None</code> <p>Type of log variance prediction, by default None. Required when loss_type is \"musplit\" or \"denoisplit_musplit\".</p> <code>None</code> <code>logvar_lowerbound</code> <code>float | None</code> <p>Lower bound for the log variance, by default -5.0. Used when loss_type is \"musplit\" or \"denoisplit_musplit\".</p> <code>-5.0</code> <code>nm_paths</code> <code>list[str] | None</code> <p>Paths to the noise model files, by default None. Required when loss_type is \"denoisplit\" or \"denoisplit_musplit\".</p> <code>None</code> <code>data_stats</code> <code>tuple[float, float] | None</code> <p>Data statistics (mean, std), by default None. Required when loss_type is \"denoisplit\" or \"denoisplit_musplit\".</p> <code>None</code> <p>Returns:</p> Type Description <code>GaussianLikelihoodConfig or None</code> <p>Configuration for the Gaussian likelihood model.</p> <code>MultiChannelNMConfig or None</code> <p>Configuration for the multi-channel noise model.</p> <code>NMLikelihoodConfig or None</code> <p>Configuration for the noise model likelihood.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing for the specified loss_type.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def get_likelihood_config(\n    loss_type: Literal[\"musplit\", \"denoisplit\", \"denoisplit_musplit\"],\n    # TODO remove different microsplit loss types, refac\n    predict_logvar: Literal[\"pixelwise\"] | None = None,\n    logvar_lowerbound: float | None = -5.0,\n    nm_paths: list[str] | None = None,\n    data_stats: tuple[float, float] | None = None,\n) -&gt; tuple[\n    GaussianLikelihoodConfig | None,\n    MultiChannelNMConfig | None,\n    NMLikelihoodConfig | None,\n]:\n    \"\"\"Get the likelihood configuration for split models.\n\n    Returns a tuple containing the following optional entries:\n        - GaussianLikelihoodConfig: Gaussian likelihood configuration for musplit losses\n        - MultiChannelNMConfig: Multi-channel noise model configuration for denoisplit\n        losses\n        - NMLikelihoodConfig: Noise model likelihood configuration for denoisplit losses\n\n    Parameters\n    ----------\n    loss_type : Literal[\"musplit\", \"denoisplit\", \"denoisplit_musplit\"]\n        The type of loss function to use.\n    predict_logvar : Literal[\"pixelwise\"] | None, optional\n        Type of log variance prediction, by default None.\n        Required when loss_type is \"musplit\" or \"denoisplit_musplit\".\n    logvar_lowerbound : float | None, optional\n        Lower bound for the log variance, by default -5.0.\n        Used when loss_type is \"musplit\" or \"denoisplit_musplit\".\n    nm_paths : list[str] | None, optional\n        Paths to the noise model files, by default None.\n        Required when loss_type is \"denoisplit\" or \"denoisplit_musplit\".\n    data_stats : tuple[float, float] | None, optional\n        Data statistics (mean, std), by default None.\n        Required when loss_type is \"denoisplit\" or \"denoisplit_musplit\".\n\n    Returns\n    -------\n    GaussianLikelihoodConfig or None\n        Configuration for the Gaussian likelihood model.\n    MultiChannelNMConfig or None\n        Configuration for the multi-channel noise model.\n    NMLikelihoodConfig or None\n        Configuration for the noise model likelihood.\n\n    Raises\n    ------\n    ValueError\n        If required parameters are missing for the specified loss_type.\n    \"\"\"\n    # gaussian likelihood\n    if loss_type in [\"musplit\", \"denoisplit_musplit\"]:\n        # if predict_logvar is None:\n        #    raise ValueError(f\"predict_logvar is required for loss_type '{loss_type}'\")\n        # TODO validators should be in pydantic models\n        gaussian_lik_config = GaussianLikelihoodConfig(\n            predict_logvar=predict_logvar,\n            logvar_lowerbound=logvar_lowerbound,\n        )\n    else:\n        gaussian_lik_config = None\n\n    # noise model likelihood\n    if loss_type in [\"denoisplit\", \"denoisplit_musplit\"]:\n        # if nm_paths is None:\n        #     raise ValueError(f\"nm_paths is required for loss_type '{loss_type}'\")\n        # if data_stats is None:\n        #     raise ValueError(f\"data_stats is required for loss_type '{loss_type}'\")\n        # TODO validators should be in pydantic models\n        gmm_list = []\n        if nm_paths is not None:\n            for NM_path in nm_paths:\n                gmm_list.append(\n                    GaussianMixtureNMConfig(\n                        model_type=\"GaussianMixtureNoiseModel\",\n                        path=NM_path,\n                    )\n                )\n        noise_model_config = MultiChannelNMConfig(noise_models=gmm_list)\n        nm_lik_config = NMLikelihoodConfig()  # TODO this config isn't needed probably\n    else:\n        noise_model_config = None\n        nm_lik_config = None\n\n    return gaussian_lik_config, noise_model_config, nm_lik_config\n</code></pre>"},{"location":"reference/careamics/config/configuration_io/","title":"configuration_io","text":"<p>I/O functions for Configuration objects.</p>"},{"location":"reference/careamics/config/configuration_io/#careamics.config.configuration_io.load_configuration","title":"<code>load_configuration(path)</code>","text":"<p>Load configuration from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to the configuration.</p> required <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> Source code in <code>src/careamics/config/configuration_io.py</code> <pre><code>def load_configuration(path: Union[str, Path]) -&gt; Configuration:\n    \"\"\"\n    Load configuration from a yaml file.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to the configuration.\n\n    Returns\n    -------\n    Configuration\n        Configuration.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the configuration file does not exist.\n    \"\"\"\n    # load dictionary from yaml\n    if not Path(path).exists():\n        raise FileNotFoundError(\n            f\"Configuration file {path} does not exist in \" f\" {Path.cwd()!s}\"\n        )\n\n    dictionary = yaml.load(Path(path).open(\"r\"), Loader=yaml.SafeLoader)\n\n    return Configuration(**dictionary)\n</code></pre>"},{"location":"reference/careamics/config/configuration_io/#careamics.config.configuration_io.save_configuration","title":"<code>save_configuration(config, path)</code>","text":"<p>Save configuration to path.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>Configuration to save.</p> required <code>path</code> <code>str or Path</code> <p>Path to a existing folder in which to save the configuration, or to a valid configuration file path (uses a .yml or .yaml extension).</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path object representing the configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path does not point to an existing directory or .yml file.</p> Source code in <code>src/careamics/config/configuration_io.py</code> <pre><code>def save_configuration(config: Configuration, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save configuration to path.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to save.\n    path : str or Path\n        Path to a existing folder in which to save the configuration, or to a valid\n        configuration file path (uses a .yml or .yaml extension).\n\n    Returns\n    -------\n    Path\n        Path object representing the configuration.\n\n    Raises\n    ------\n    ValueError\n        If the path does not point to an existing directory or .yml file.\n    \"\"\"\n    # make sure path is a Path object\n    config_path = Path(path)\n\n    # check if path is pointing to an existing directory or .yml file\n    if config_path.exists():\n        if config_path.is_dir():\n            config_path = Path(config_path, \"config.yml\")\n        elif config_path.suffix != \".yml\" and config_path.suffix != \".yaml\":\n            raise ValueError(\n                f\"Path must be a directory or .yml or .yaml file (got {config_path}).\"\n            )\n    else:\n        if config_path.suffix != \".yml\" and config_path.suffix != \".yaml\":\n            raise ValueError(\n                f\"Path must be a directory or .yml or .yaml file (got {config_path}).\"\n            )\n\n    # save configuration as dictionary to yaml\n    with open(config_path, \"w\") as f:\n        # dump configuration\n        yaml.dump(config.model_dump(), f, default_flow_style=False, sort_keys=False)\n\n    return config_path\n</code></pre>"},{"location":"reference/careamics/config/inference_model/","title":"inference_model","text":"<p>Pydantic model representing CAREamics prediction configuration.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig","title":"<code>InferenceConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for the prediction model.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>class InferenceConfig(BaseModel):\n    \"\"\"Configuration class for the prediction model.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, arbitrary_types_allowed=True)\n\n    data_type: Literal[\"array\", \"tiff\", \"czi\", \"custom\"]  # As defined in SupportedData\n    \"\"\"Type of input data: numpy.ndarray (array) or path (tiff, czi, or custom).\"\"\"\n\n    tile_size: Union[list[int]] | None = Field(default=None, min_length=2, max_length=3)\n    \"\"\"Tile size of prediction, only effective if `tile_overlap` is specified.\"\"\"\n\n    tile_overlap: Union[list[int]] | None = Field(\n        default=None, min_length=2, max_length=3\n    )\n    \"\"\"Overlap between tiles, only effective if `tile_size` is specified.\"\"\"\n\n    axes: str\n    \"\"\"Data axes (TSCZYX) in the order of the input data.\"\"\"\n\n    image_means: list = Field(..., min_length=0, max_length=32)\n    \"\"\"Mean values for each input channel.\"\"\"\n\n    image_stds: list = Field(..., min_length=0, max_length=32)\n    \"\"\"Standard deviation values for each input channel.\"\"\"\n\n    # TODO only default TTAs are supported for now\n    tta_transforms: bool = Field(default=True)\n    \"\"\"Whether to apply test-time augmentation (all 90 degrees rotations and flips).\"\"\"\n\n    # Dataloader parameters\n    batch_size: int = Field(default=1, ge=1)\n    \"\"\"Batch size for prediction.\"\"\"\n\n    @field_validator(\"tile_overlap\")\n    @classmethod\n    def all_elements_non_zero_even(\n        cls, tile_overlap: list[int] | None\n    ) -&gt; list[int] | None:\n        \"\"\"\n        Validate tile overlap.\n\n        Overlaps must be non-zero, positive and even.\n\n        Parameters\n        ----------\n        tile_overlap : list[int] or None\n            Patch size.\n\n        Returns\n        -------\n        list[int] or None\n            Validated tile overlap.\n\n        Raises\n        ------\n        ValueError\n            If the patch size is 0.\n        ValueError\n            If the patch size is not even.\n        \"\"\"\n        if tile_overlap is not None:\n            for dim in tile_overlap:\n                if dim &lt; 1:\n                    raise ValueError(\n                        f\"Patch size must be non-zero positive (got {dim}).\"\n                    )\n\n                if dim % 2 != 0:\n                    raise ValueError(f\"Patch size must be even (got {dim}).\")\n\n        return tile_overlap\n\n    @field_validator(\"tile_size\")\n    @classmethod\n    def tile_min_8_power_of_2(cls, tile_list: list[int] | None) -&gt; list[int] | None:\n        \"\"\"\n        Validate that each entry is greater or equal than 8 and a power of 2.\n\n        Parameters\n        ----------\n        tile_list : list of int\n            Patch size.\n\n        Returns\n        -------\n        list of int\n            Validated patch size.\n\n        Raises\n        ------\n        ValueError\n            If the patch size if smaller than 8.\n        ValueError\n            If the patch size is not a power of 2.\n        \"\"\"\n        patch_size_ge_than_8_power_of_2(tile_list)\n\n        return tile_list\n\n    @field_validator(\"axes\")\n    @classmethod\n    def axes_valid(cls, axes: str) -&gt; str:\n        \"\"\"\n        Validate axes.\n\n        Axes must:\n        - be a combination of 'STCZYX'\n        - not contain duplicates\n        - contain at least 2 contiguous axes: X and Y\n        - contain at most 4 axes\n        - not contain both S and T axes\n\n        Parameters\n        ----------\n        axes : str\n            Axes to validate.\n\n        Returns\n        -------\n        str\n            Validated axes.\n\n        Raises\n        ------\n        ValueError\n            If axes are not valid.\n        \"\"\"\n        # Validate axes\n        check_axes_validity(axes)\n\n        return axes\n\n    @model_validator(mode=\"after\")\n    def validate_dimensions(self: Self) -&gt; Self:\n        \"\"\"\n        Validate 2D/3D dimensions between axes and tile size.\n\n        Returns\n        -------\n        Self\n            Validated prediction model.\n        \"\"\"\n        expected_len = 3 if \"Z\" in self.axes else 2\n\n        if self.tile_size is not None and self.tile_overlap is not None:\n            if len(self.tile_size) != expected_len:\n                raise ValueError(\n                    f\"Tile size must have {expected_len} dimensions given axes \"\n                    f\"{self.axes} (got {self.tile_size}).\"\n                )\n\n            if len(self.tile_overlap) != expected_len:\n                raise ValueError(\n                    f\"Tile overlap must have {expected_len} dimensions given axes \"\n                    f\"{self.axes} (got {self.tile_overlap}).\"\n                )\n\n            if any(\n                (i &gt;= j)\n                for i, j in zip(self.tile_overlap, self.tile_size, strict=False)\n            ):\n                raise ValueError(\"Tile overlap must be smaller than tile size.\")\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def std_only_with_mean(self: Self) -&gt; Self:\n        \"\"\"\n        Check that mean and std are either both None, or both specified.\n\n        Returns\n        -------\n        Self\n            Validated prediction model.\n\n        Raises\n        ------\n        ValueError\n            If std is not None and mean is None.\n        \"\"\"\n        # check that mean and std are either both None, or both specified\n        if not self.image_means and not self.image_stds:\n            raise ValueError(\"Mean and std must be specified during inference.\")\n\n        if (self.image_means and not self.image_stds) or (\n            self.image_stds and not self.image_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified.\"\n            )\n\n        elif (self.image_means is not None and self.image_stds is not None) and (\n            len(self.image_means) != len(self.image_stds)\n        ):\n            raise ValueError(\n                \"Mean and std must be specified for each \" \"input channel.\"\n            )\n\n        return self\n\n    def _update(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Update multiple arguments at once.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Key-value pairs of arguments to update.\n        \"\"\"\n        self.__dict__.update(kwargs)\n        self.__class__.model_validate(self.__dict__)\n\n    def set_3D(self, axes: str, tile_size: list[int], tile_overlap: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D parameters.\n\n        Parameters\n        ----------\n        axes : str\n            Axes.\n        tile_size : list of int\n            Tile size.\n        tile_overlap : list of int\n            Tile overlap.\n        \"\"\"\n        self._update(axes=axes, tile_size=tile_size, tile_overlap=tile_overlap)\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.axes","title":"<code>axes</code>  <code>instance-attribute</code>","text":"<p>Data axes (TSCZYX) in the order of the input data.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.batch_size","title":"<code>batch_size = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for prediction.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of input data: numpy.ndarray (array) or path (tiff, czi, or custom).</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.image_means","title":"<code>image_means = Field(..., min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean values for each input channel.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.image_stds","title":"<code>image_stds = Field(..., min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviation values for each input channel.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_overlap","title":"<code>tile_overlap = Field(default=None, min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between tiles, only effective if <code>tile_size</code> is specified.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_size","title":"<code>tile_size = Field(default=None, min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tile size of prediction, only effective if <code>tile_overlap</code> is specified.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tta_transforms","title":"<code>tta_transforms = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply test-time augmentation (all 90 degrees rotations and flips).</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.all_elements_non_zero_even","title":"<code>all_elements_non_zero_even(tile_overlap)</code>  <code>classmethod</code>","text":"<p>Validate tile overlap.</p> <p>Overlaps must be non-zero, positive and even.</p> <p>Parameters:</p> Name Type Description Default <code>tile_overlap</code> <code>list[int] or None</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list[int] or None</code> <p>Validated tile overlap.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is 0.</p> <code>ValueError</code> <p>If the patch size is not even.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"tile_overlap\")\n@classmethod\ndef all_elements_non_zero_even(\n    cls, tile_overlap: list[int] | None\n) -&gt; list[int] | None:\n    \"\"\"\n    Validate tile overlap.\n\n    Overlaps must be non-zero, positive and even.\n\n    Parameters\n    ----------\n    tile_overlap : list[int] or None\n        Patch size.\n\n    Returns\n    -------\n    list[int] or None\n        Validated tile overlap.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is 0.\n    ValueError\n        If the patch size is not even.\n    \"\"\"\n    if tile_overlap is not None:\n        for dim in tile_overlap:\n            if dim &lt; 1:\n                raise ValueError(\n                    f\"Patch size must be non-zero positive (got {dim}).\"\n                )\n\n            if dim % 2 != 0:\n                raise ValueError(f\"Patch size must be even (got {dim}).\")\n\n    return tile_overlap\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.axes_valid","title":"<code>axes_valid(axes)</code>  <code>classmethod</code>","text":"<p>Validate axes.</p> <p>Axes must: - be a combination of 'STCZYX' - not contain duplicates - contain at least 2 contiguous axes: X and Y - contain at most 4 axes - not contain both S and T axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axes are not valid.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"axes\")\n@classmethod\ndef axes_valid(cls, axes: str) -&gt; str:\n    \"\"\"\n    Validate axes.\n\n    Axes must:\n    - be a combination of 'STCZYX'\n    - not contain duplicates\n    - contain at least 2 contiguous axes: X and Y\n    - contain at most 4 axes\n    - not contain both S and T axes\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n\n    Returns\n    -------\n    str\n        Validated axes.\n\n    Raises\n    ------\n    ValueError\n        If axes are not valid.\n    \"\"\"\n    # Validate axes\n    check_axes_validity(axes)\n\n    return axes\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.set_3D","title":"<code>set_3D(axes, tile_size, tile_overlap)</code>","text":"<p>Set 3D parameters.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes.</p> required <code>tile_size</code> <code>list of int</code> <p>Tile size.</p> required <code>tile_overlap</code> <code>list of int</code> <p>Tile overlap.</p> required Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>def set_3D(self, axes: str, tile_size: list[int], tile_overlap: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D parameters.\n\n    Parameters\n    ----------\n    axes : str\n        Axes.\n    tile_size : list of int\n        Tile size.\n    tile_overlap : list of int\n        Tile overlap.\n    \"\"\"\n    self._update(axes=axes, tile_size=tile_size, tile_overlap=tile_overlap)\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.std_only_with_mean","title":"<code>std_only_with_mean()</code>","text":"<p>Check that mean and std are either both None, or both specified.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated prediction model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If std is not None and mean is None.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef std_only_with_mean(self: Self) -&gt; Self:\n    \"\"\"\n    Check that mean and std are either both None, or both specified.\n\n    Returns\n    -------\n    Self\n        Validated prediction model.\n\n    Raises\n    ------\n    ValueError\n        If std is not None and mean is None.\n    \"\"\"\n    # check that mean and std are either both None, or both specified\n    if not self.image_means and not self.image_stds:\n        raise ValueError(\"Mean and std must be specified during inference.\")\n\n    if (self.image_means and not self.image_stds) or (\n        self.image_stds and not self.image_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified.\"\n        )\n\n    elif (self.image_means is not None and self.image_stds is not None) and (\n        len(self.image_means) != len(self.image_stds)\n    ):\n        raise ValueError(\n            \"Mean and std must be specified for each \" \"input channel.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_min_8_power_of_2","title":"<code>tile_min_8_power_of_2(tile_list)</code>  <code>classmethod</code>","text":"<p>Validate that each entry is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>tile_list</code> <code>list of int</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list of int</code> <p>Validated patch size.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size if smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"tile_size\")\n@classmethod\ndef tile_min_8_power_of_2(cls, tile_list: list[int] | None) -&gt; list[int] | None:\n    \"\"\"\n    Validate that each entry is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    tile_list : list of int\n        Patch size.\n\n    Returns\n    -------\n    list of int\n        Validated patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size if smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    patch_size_ge_than_8_power_of_2(tile_list)\n\n    return tile_list\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.validate_dimensions","title":"<code>validate_dimensions()</code>","text":"<p>Validate 2D/3D dimensions between axes and tile size.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated prediction model.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dimensions(self: Self) -&gt; Self:\n    \"\"\"\n    Validate 2D/3D dimensions between axes and tile size.\n\n    Returns\n    -------\n    Self\n        Validated prediction model.\n    \"\"\"\n    expected_len = 3 if \"Z\" in self.axes else 2\n\n    if self.tile_size is not None and self.tile_overlap is not None:\n        if len(self.tile_size) != expected_len:\n            raise ValueError(\n                f\"Tile size must have {expected_len} dimensions given axes \"\n                f\"{self.axes} (got {self.tile_size}).\"\n            )\n\n        if len(self.tile_overlap) != expected_len:\n            raise ValueError(\n                f\"Tile overlap must have {expected_len} dimensions given axes \"\n                f\"{self.axes} (got {self.tile_overlap}).\"\n            )\n\n        if any(\n            (i &gt;= j)\n            for i, j in zip(self.tile_overlap, self.tile_size, strict=False)\n        ):\n            raise ValueError(\"Tile overlap must be smaller than tile size.\")\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/","title":"likelihood_model","text":"<p>Likelihood model.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.Tensor","title":"<code>Tensor = Annotated[Union[np.ndarray, torch.Tensor], PlainSerializer(_array_to_json, return_type=str), PlainValidator(_to_torch)]</code>  <code>module-attribute</code>","text":"<p>Annotated tensor type, used to serialize arrays or tensors to JSON strings and deserialize them back to tensors.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig","title":"<code>GaussianLikelihoodConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaussian likelihood configuration.</p> Source code in <code>src/careamics/config/likelihood_model.py</code> <pre><code>class GaussianLikelihoodConfig(BaseModel):\n    \"\"\"Gaussian likelihood configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    predict_logvar: Literal[\"pixelwise\"] | None = None\n    \"\"\"If `pixelwise`, log-variance is computed for each pixel, else log-variance\n    is not computed.\"\"\"\n\n    logvar_lowerbound: Union[float, None] = None\n    \"\"\"The lowerbound value for log-variance.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig.logvar_lowerbound","title":"<code>logvar_lowerbound = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The lowerbound value for log-variance.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig.predict_logvar","title":"<code>predict_logvar = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>pixelwise</code>, log-variance is computed for each pixel, else log-variance is not computed.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig","title":"<code>NMLikelihoodConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Noise model likelihood configuration.</p> <p>NOTE: we need to define the data mean and std here because the noise model is trained on not-normalized data. Hence, we need to unnormalize the model output to compute the noise model likelihood.</p> Source code in <code>src/careamics/config/likelihood_model.py</code> <pre><code>class NMLikelihoodConfig(BaseModel):\n    \"\"\"Noise model likelihood configuration.\n\n    NOTE: we need to define the data mean and std here because the noise model\n    is trained on not-normalized data. Hence, we need to unnormalize the model\n    output to compute the noise model likelihood.\n    \"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, arbitrary_types_allowed=True)\n\n    # TODO remove and use as parameters to the likelihood functions?\n    data_mean: Tensor | None = None\n    \"\"\"The mean of the data, used to unnormalize data for noise model evaluation.\n    Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).\"\"\"\n\n    # TODO remove and use as parameters to the likelihood functions?\n    data_std: Tensor | None = None\n    \"\"\"The standard deviation of the data, used to unnormalize data for noise\n    model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig.data_mean","title":"<code>data_mean = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mean of the data, used to unnormalize data for noise model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig.data_std","title":"<code>data_std = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The standard deviation of the data, used to unnormalize data for noise model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).</p>"},{"location":"reference/careamics/config/loss_model/","title":"loss_model","text":"<p>Configuration classes for LVAE losses.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig","title":"<code>KLLossConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>KL loss configuration.</p> Source code in <code>src/careamics/config/loss_model.py</code> <pre><code>class KLLossConfig(BaseModel):\n    \"\"\"KL loss configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, validate_default=True)\n\n    loss_type: Literal[\"kl\", \"kl_restricted\"] = \"kl\"\n    \"\"\"Type of KL divergence used as KL loss.\"\"\"\n    rescaling: Literal[\"latent_dim\", \"image_dim\"] = \"latent_dim\"\n    \"\"\"Rescaling of the KL loss.\"\"\"\n    aggregation: Literal[\"sum\", \"mean\"] = \"mean\"\n    \"\"\"Aggregation of the KL loss across different layers.\"\"\"\n    free_bits_coeff: float = 0.0\n    \"\"\"Free bits coefficient for the KL loss.\"\"\"\n    annealing: bool = False\n    \"\"\"Whether to apply KL loss annealing.\"\"\"\n    start: int = -1\n    \"\"\"Epoch at which KL loss annealing starts.\"\"\"\n    annealtime: int = 10\n    \"\"\"Number of epochs for which KL loss annealing is applied.\"\"\"\n    current_epoch: int = 0\n    \"\"\"Current epoch in the training loop.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.aggregation","title":"<code>aggregation = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Aggregation of the KL loss across different layers.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.annealing","title":"<code>annealing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply KL loss annealing.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.annealtime","title":"<code>annealtime = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs for which KL loss annealing is applied.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.current_epoch","title":"<code>current_epoch = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current epoch in the training loop.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.free_bits_coeff","title":"<code>free_bits_coeff = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Free bits coefficient for the KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.loss_type","title":"<code>loss_type = 'kl'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of KL divergence used as KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.rescaling","title":"<code>rescaling = 'latent_dim'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rescaling of the KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.start","title":"<code>start = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Epoch at which KL loss annealing starts.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig","title":"<code>LVAELossConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LVAE loss configuration.</p> Source code in <code>src/careamics/config/loss_model.py</code> <pre><code>class LVAELossConfig(BaseModel):\n    \"\"\"LVAE loss configuration.\"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True, validate_default=True, arbitrary_types_allowed=True\n    )\n\n    loss_type: Literal[\n        \"hdn\", \"microsplit\", \"musplit\", \"denoisplit\", \"denoisplit_musplit\"\n    ]\n    \"\"\"Type of loss to use for LVAE.\"\"\"\n\n    reconstruction_weight: float = 1.0\n    \"\"\"Weight for the reconstruction loss in the total net loss\n    (i.e., `net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss`).\"\"\"\n    kl_weight: float = 1.0\n    \"\"\"Weight for the KL loss in the total net loss.\n    (i.e., `net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss`).\"\"\"\n    musplit_weight: float = 0.1\n    \"\"\"Weight for the muSplit loss (used in the muSplit-denoiSplit loss).\"\"\"\n    denoisplit_weight: float = 0.9\n    \"\"\"Weight for the denoiSplit loss (used in the muSplit-deonoiSplit loss).\"\"\"\n    kl_params: KLLossConfig = KLLossConfig()\n    \"\"\"KL loss configuration.\"\"\"\n    # TODO revisit weights for the losses\n    # TODO: remove?\n    non_stochastic: bool = False\n    \"\"\"Whether to sample latents and compute KL.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.denoisplit_weight","title":"<code>denoisplit_weight = 0.9</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the denoiSplit loss (used in the muSplit-deonoiSplit loss).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.kl_params","title":"<code>kl_params = KLLossConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>KL loss configuration.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.kl_weight","title":"<code>kl_weight = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the KL loss in the total net loss. (i.e., <code>net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss</code>).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.loss_type","title":"<code>loss_type</code>  <code>instance-attribute</code>","text":"<p>Type of loss to use for LVAE.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.musplit_weight","title":"<code>musplit_weight = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the muSplit loss (used in the muSplit-denoiSplit loss).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.non_stochastic","title":"<code>non_stochastic = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to sample latents and compute KL.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.reconstruction_weight","title":"<code>reconstruction_weight = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the reconstruction loss in the total net loss (i.e., <code>net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss</code>).</p>"},{"location":"reference/careamics/config/nm_model/","title":"nm_model","text":"<p>Noise models config.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.Array","title":"<code>Array = Annotated[Union[np.ndarray, torch.Tensor], PlainSerializer(_array_to_json, return_type=str), PlainValidator(_to_numpy)]</code>  <code>module-attribute</code>","text":"<p>Annotated array type, used to serialize arrays or tensors to JSON strings and deserialize them back to arrays.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig","title":"<code>GaussianMixtureNMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaussian mixture noise model.</p> Source code in <code>src/careamics/config/nm_model.py</code> <pre><code>class GaussianMixtureNMConfig(BaseModel):\n    \"\"\"Gaussian mixture noise model.\"\"\"\n\n    model_config = ConfigDict(\n        protected_namespaces=(),\n        validate_assignment=True,\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n    )\n    # model type\n    model_type: Literal[\"GaussianMixtureNoiseModel\"]\n\n    path: Union[Path, str] | None = None\n    \"\"\"Path to the directory where the trained noise model (*.npz) is saved in the\n    `train` method.\"\"\"\n\n    # TODO remove and use as parameters to the NM functions?\n    signal: Union[str, Path, np.ndarray] | None = Field(default=None, exclude=True)\n    \"\"\"Path to the file containing signal or respective numpy array.\"\"\"\n\n    # TODO remove and use as parameters to the NM functions?\n    observation: Union[str, Path, np.ndarray] | None = Field(default=None, exclude=True)\n    \"\"\"Path to the file containing observation or respective numpy array.\"\"\"\n\n    weight: Array | None = None\n    \"\"\"A [3*n_gaussian, n_coeff] sized array containing the values of the weights\n    describing the GMM noise model, with each row corresponding to one\n    parameter of each gaussian, namely [mean, standard deviation and weight].\n    Specifically, rows are organized as follows:\n    - first n_gaussian rows correspond to the means\n    - next n_gaussian rows correspond to the weights\n    - last n_gaussian rows correspond to the standard deviations\n    If `weight=None`, the weight array is initialized using the `min_signal`\n    and `max_signal` parameters.\"\"\"\n\n    n_gaussian: int = Field(default=1, ge=1)\n    \"\"\"Number of gaussians used for the GMM.\"\"\"\n\n    n_coeff: int = Field(default=2, ge=2)\n    \"\"\"Number of coefficients to describe the functional relationship between gaussian\n    parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic\n    relationship and so on.\"\"\"\n\n    min_signal: float = Field(default=0.0, ge=0.0)\n    \"\"\"Minimum signal intensity expected in the image.\"\"\"\n\n    max_signal: float = Field(default=1.0, ge=0.0)\n    \"\"\"Maximum signal intensity expected in the image.\"\"\"\n\n    min_sigma: float = Field(default=200.0, ge=0.0)  # TODO took from nb in pn2v\n    \"\"\"Minimum value of `standard deviation` allowed in the GMM.\n    All values of `standard deviation` below this are clamped to this value.\"\"\"\n\n    tol: float = Field(default=1e-10)\n    \"\"\"Tolerance used in the computation of the noise model likelihood.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def validate_path(self: Self) -&gt; Self:\n        \"\"\"Validate that the path points to a valid .npz file if provided.\n\n        Returns\n        -------\n        Self\n            Returns itself.\n\n        Raises\n        ------\n        ValueError\n            If the path is provided but does not point to a valid .npz file.\n        \"\"\"\n        if self.path is not None:\n            path = Path(self.path)\n            if not path.exists():\n                raise ValueError(f\"Path {path} does not exist.\")\n            if path.suffix != \".npz\":\n                raise ValueError(f\"Path {path} must point to a .npz file.\")\n            if not path.is_file():\n                raise ValueError(f\"Path {path} must point to a file.\")\n        return self\n</code></pre>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.max_signal","title":"<code>max_signal = Field(default=1.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum signal intensity expected in the image.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.min_sigma","title":"<code>min_sigma = Field(default=200.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value of <code>standard deviation</code> allowed in the GMM. All values of <code>standard deviation</code> below this are clamped to this value.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.min_signal","title":"<code>min_signal = Field(default=0.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum signal intensity expected in the image.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.n_coeff","title":"<code>n_coeff = Field(default=2, ge=2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of coefficients to describe the functional relationship between gaussian parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic relationship and so on.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.n_gaussian","title":"<code>n_gaussian = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of gaussians used for the GMM.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.observation","title":"<code>observation = Field(default=None, exclude=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the file containing observation or respective numpy array.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.path","title":"<code>path = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the directory where the trained noise model (*.npz) is saved in the <code>train</code> method.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.signal","title":"<code>signal = Field(default=None, exclude=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the file containing signal or respective numpy array.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.tol","title":"<code>tol = Field(default=1e-10)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tolerance used in the computation of the noise model likelihood.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.weight","title":"<code>weight = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A [3*n_gaussian, n_coeff] sized array containing the values of the weights describing the GMM noise model, with each row corresponding to one parameter of each gaussian, namely [mean, standard deviation and weight]. Specifically, rows are organized as follows: - first n_gaussian rows correspond to the means - next n_gaussian rows correspond to the weights - last n_gaussian rows correspond to the standard deviations If <code>weight=None</code>, the weight array is initialized using the <code>min_signal</code> and <code>max_signal</code> parameters.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.validate_path","title":"<code>validate_path()</code>","text":"<p>Validate that the path points to a valid .npz file if provided.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Returns itself.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path is provided but does not point to a valid .npz file.</p> Source code in <code>src/careamics/config/nm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_path(self: Self) -&gt; Self:\n    \"\"\"Validate that the path points to a valid .npz file if provided.\n\n    Returns\n    -------\n    Self\n        Returns itself.\n\n    Raises\n    ------\n    ValueError\n        If the path is provided but does not point to a valid .npz file.\n    \"\"\"\n    if self.path is not None:\n        path = Path(self.path)\n        if not path.exists():\n            raise ValueError(f\"Path {path} does not exist.\")\n        if path.suffix != \".npz\":\n            raise ValueError(f\"Path {path} must point to a .npz file.\")\n        if not path.is_file():\n            raise ValueError(f\"Path {path} must point to a file.\")\n    return self\n</code></pre>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.MultiChannelNMConfig","title":"<code>MultiChannelNMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Noise Model config aggregating noise models for single output channels.</p> Source code in <code>src/careamics/config/nm_model.py</code> <pre><code>class MultiChannelNMConfig(BaseModel):\n    \"\"\"Noise Model config aggregating noise models for single output channels.\"\"\"\n\n    # TODO: check that this model config is OK\n    model_config = ConfigDict(\n        validate_assignment=True, arbitrary_types_allowed=True, extra=\"allow\"\n    )\n    noise_models: list[GaussianMixtureNMConfig]\n    \"\"\"List of noise models, one for each target channel.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.MultiChannelNMConfig.noise_models","title":"<code>noise_models</code>  <code>instance-attribute</code>","text":"<p>List of noise models, one for each target channel.</p>"},{"location":"reference/careamics/config/optimizer_models/","title":"optimizer_models","text":"<p>Optimizers and schedulers Pydantic models.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel","title":"<code>LrSchedulerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Torch learning rate scheduler Pydantic model.</p> <p>Only parameters supported by the corresponding torch lr scheduler will be taken into account. For more details, check: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate</p> <p>Note that mandatory parameters (see the specific LrScheduler signature in the link above) must be provided. For example, StepLR requires <code>step_size</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>{'ReduceLROnPlateau', 'StepLR'}</code> <p>Name of the learning rate scheduler.</p> <code>parameters</code> <code>dict</code> <p>Parameters of the learning rate scheduler (see torch documentation).</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>class LrSchedulerModel(BaseModel):\n    \"\"\"Torch learning rate scheduler Pydantic model.\n\n    Only parameters supported by the corresponding torch lr scheduler will be taken\n    into account. For more details, check:\n    https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n\n    Note that mandatory parameters (see the specific LrScheduler signature in the\n    link above) must be provided. For example, StepLR requires `step_size`.\n\n    Attributes\n    ----------\n    name : {\"ReduceLROnPlateau\", \"StepLR\"}\n        Name of the learning rate scheduler.\n    parameters : dict\n        Parameters of the learning rate scheduler (see torch documentation).\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Mandatory field\n    name: Literal[\"ReduceLROnPlateau\", \"StepLR\"] = Field(default=\"ReduceLROnPlateau\")\n    \"\"\"Name of the learning rate scheduler, supported schedulers are defined in\n    SupportedScheduler.\"\"\"\n\n    # Optional parameters\n    parameters: dict = Field(default={}, validate_default=True)\n    \"\"\"Parameters of the learning rate scheduler, see PyTorch documentation for more\n    details.\"\"\"\n\n    @field_validator(\"parameters\")\n    @classmethod\n    def filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n        \"\"\"Filter parameters based on the learning rate scheduler's signature.\n\n        Parameters\n        ----------\n        user_params : dict\n            User parameters.\n        values : ValidationInfo\n            Pydantic field validation info, used to get the scheduler name.\n\n        Returns\n        -------\n        dict\n            Filtered scheduler parameters.\n\n        Raises\n        ------\n        ValueError\n            If the scheduler is StepLR and the step_size parameter is not specified.\n        \"\"\"\n        # retrieve the corresponding scheduler class\n        scheduler_class = getattr(optim.lr_scheduler, values.data[\"name\"])\n\n        # filter the user parameters according to the scheduler's signature\n        parameters = filter_parameters(scheduler_class, user_params)\n\n        if values.data[\"name\"] == \"StepLR\" and \"step_size\" not in parameters:\n            raise ValueError(\n                \"StepLR scheduler requires `step_size` parameter, check that it has \"\n                \"correctly been specified in `parameters`.\"\n            )\n\n        return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.name","title":"<code>name = Field(default='ReduceLROnPlateau')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the learning rate scheduler, supported schedulers are defined in SupportedScheduler.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.parameters","title":"<code>parameters = Field(default={}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters of the learning rate scheduler, see PyTorch documentation for more details.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.filter_parameters","title":"<code>filter_parameters(user_params, values)</code>  <code>classmethod</code>","text":"<p>Filter parameters based on the learning rate scheduler's signature.</p> <p>Parameters:</p> Name Type Description Default <code>user_params</code> <code>dict</code> <p>User parameters.</p> required <code>values</code> <code>ValidationInfo</code> <p>Pydantic field validation info, used to get the scheduler name.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Filtered scheduler parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scheduler is StepLR and the step_size parameter is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@field_validator(\"parameters\")\n@classmethod\ndef filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n    \"\"\"Filter parameters based on the learning rate scheduler's signature.\n\n    Parameters\n    ----------\n    user_params : dict\n        User parameters.\n    values : ValidationInfo\n        Pydantic field validation info, used to get the scheduler name.\n\n    Returns\n    -------\n    dict\n        Filtered scheduler parameters.\n\n    Raises\n    ------\n    ValueError\n        If the scheduler is StepLR and the step_size parameter is not specified.\n    \"\"\"\n    # retrieve the corresponding scheduler class\n    scheduler_class = getattr(optim.lr_scheduler, values.data[\"name\"])\n\n    # filter the user parameters according to the scheduler's signature\n    parameters = filter_parameters(scheduler_class, user_params)\n\n    if values.data[\"name\"] == \"StepLR\" and \"step_size\" not in parameters:\n        raise ValueError(\n            \"StepLR scheduler requires `step_size` parameter, check that it has \"\n            \"correctly been specified in `parameters`.\"\n        )\n\n    return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel","title":"<code>OptimizerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Torch optimizer Pydantic model.</p> <p>Only parameters supported by the corresponding torch optimizer will be taken into account. For more details, check: https://pytorch.org/docs/stable/optim.html#algorithms</p> <p>Note that mandatory parameters (see the specific Optimizer signature in the link above) must be provided. For example, SGD requires <code>lr</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>{'Adam', 'SGD'}</code> <p>Name of the optimizer.</p> <code>parameters</code> <code>dict</code> <p>Parameters of the optimizer (see torch documentation).</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>class OptimizerModel(BaseModel):\n    \"\"\"Torch optimizer Pydantic model.\n\n    Only parameters supported by the corresponding torch optimizer will be taken\n    into account. For more details, check:\n    https://pytorch.org/docs/stable/optim.html#algorithms\n\n    Note that mandatory parameters (see the specific Optimizer signature in the\n    link above) must be provided. For example, SGD requires `lr`.\n\n    Attributes\n    ----------\n    name : {\"Adam\", \"SGD\"}\n        Name of the optimizer.\n    parameters : dict\n        Parameters of the optimizer (see torch documentation).\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Mandatory field\n    name: Literal[\"Adam\", \"SGD\", \"Adamax\"] = Field(\n        default=\"Adam\", validate_default=True\n    )\n    \"\"\"Name of the optimizer, supported optimizers are defined in SupportedOptimizer.\"\"\"\n\n    # Optional parameters, empty dict default value to allow filtering dictionary\n    parameters: dict = Field(\n        default={},\n        validate_default=True,\n    )\n    \"\"\"Parameters of the optimizer, see PyTorch documentation for more details.\"\"\"\n\n    @field_validator(\"parameters\")\n    @classmethod\n    def filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n        \"\"\"\n        Validate optimizer parameters.\n\n        This method filters out unknown parameters, given the optimizer name.\n\n        Parameters\n        ----------\n        user_params : dict\n            Parameters passed on to the torch optimizer.\n        values : ValidationInfo\n            Pydantic field validation info, used to get the optimizer name.\n\n        Returns\n        -------\n        dict\n            Filtered optimizer parameters.\n\n        Raises\n        ------\n        ValueError\n            If the optimizer name is not specified.\n        \"\"\"\n        optimizer_name = values.data[\"name\"]\n\n        # retrieve the corresponding optimizer class\n        optimizer_class = getattr(optim, optimizer_name)\n\n        # filter the user parameters according to the optimizer's signature\n        parameters = filter_parameters(optimizer_class, user_params)\n\n        return parameters\n\n    @model_validator(mode=\"after\")\n    def sgd_lr_parameter(self) -&gt; Self:\n        \"\"\"\n        Check that SGD optimizer has the mandatory `lr` parameter specified.\n\n        This is specific for PyTorch &lt; 2.2.\n\n        Returns\n        -------\n        Self\n            Validated optimizer.\n\n        Raises\n        ------\n        ValueError\n            If the optimizer is SGD and the lr parameter is not specified.\n        \"\"\"\n        if self.name == SupportedOptimizer.SGD and \"lr\" not in self.parameters:\n            raise ValueError(\n                \"SGD optimizer requires `lr` parameter, check that it has correctly \"\n                \"been specified in `parameters`.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.name","title":"<code>name = Field(default='Adam', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the optimizer, supported optimizers are defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.parameters","title":"<code>parameters = Field(default={}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters of the optimizer, see PyTorch documentation for more details.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.filter_parameters","title":"<code>filter_parameters(user_params, values)</code>  <code>classmethod</code>","text":"<p>Validate optimizer parameters.</p> <p>This method filters out unknown parameters, given the optimizer name.</p> <p>Parameters:</p> Name Type Description Default <code>user_params</code> <code>dict</code> <p>Parameters passed on to the torch optimizer.</p> required <code>values</code> <code>ValidationInfo</code> <p>Pydantic field validation info, used to get the optimizer name.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Filtered optimizer parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the optimizer name is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@field_validator(\"parameters\")\n@classmethod\ndef filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n    \"\"\"\n    Validate optimizer parameters.\n\n    This method filters out unknown parameters, given the optimizer name.\n\n    Parameters\n    ----------\n    user_params : dict\n        Parameters passed on to the torch optimizer.\n    values : ValidationInfo\n        Pydantic field validation info, used to get the optimizer name.\n\n    Returns\n    -------\n    dict\n        Filtered optimizer parameters.\n\n    Raises\n    ------\n    ValueError\n        If the optimizer name is not specified.\n    \"\"\"\n    optimizer_name = values.data[\"name\"]\n\n    # retrieve the corresponding optimizer class\n    optimizer_class = getattr(optim, optimizer_name)\n\n    # filter the user parameters according to the optimizer's signature\n    parameters = filter_parameters(optimizer_class, user_params)\n\n    return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.sgd_lr_parameter","title":"<code>sgd_lr_parameter()</code>","text":"<p>Check that SGD optimizer has the mandatory <code>lr</code> parameter specified.</p> <p>This is specific for PyTorch &lt; 2.2.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated optimizer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the optimizer is SGD and the lr parameter is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef sgd_lr_parameter(self) -&gt; Self:\n    \"\"\"\n    Check that SGD optimizer has the mandatory `lr` parameter specified.\n\n    This is specific for PyTorch &lt; 2.2.\n\n    Returns\n    -------\n    Self\n        Validated optimizer.\n\n    Raises\n    ------\n    ValueError\n        If the optimizer is SGD and the lr parameter is not specified.\n    \"\"\"\n    if self.name == SupportedOptimizer.SGD and \"lr\" not in self.parameters:\n        raise ValueError(\n            \"SGD optimizer requires `lr` parameter, check that it has correctly \"\n            \"been specified in `parameters`.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/tile_information/","title":"tile_information","text":"<p>Pydantic model representing the metadata of a prediction tile.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation","title":"<code>TileInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model containing tile information.</p> <p>This model is used to represent the information required to stitch back a tile into a larger image. It is used throughout the prediction pipeline of CAREamics.</p> <p>Array shape should be C(Z)YX, where Z is an optional dimensions.</p> Source code in <code>src/careamics/config/tile_information.py</code> <pre><code>class TileInformation(BaseModel):\n    \"\"\"\n    Pydantic model containing tile information.\n\n    This model is used to represent the information required to stitch back a tile into\n    a larger image. It is used throughout the prediction pipeline of CAREamics.\n\n    Array shape should be C(Z)YX, where Z is an optional dimensions.\n    \"\"\"\n\n    model_config = ConfigDict(validate_default=True)\n\n    array_shape: DimTuple  # TODO: find a way to add custom error message?\n    \"\"\"Shape of the original (untiled) array.\"\"\"\n\n    last_tile: bool = False\n    \"\"\"Whether this tile is the last one of the array.\"\"\"\n\n    overlap_crop_coords: tuple[tuple[int, ...], ...]\n    \"\"\"Inner coordinates of the tile where to crop the prediction in order to stitch\n    it back into the original image.\"\"\"\n\n    stitch_coords: tuple[tuple[int, ...], ...]\n    \"\"\"Coordinates in the original image where to stitch the cropped tile back.\"\"\"\n\n    sample_id: int\n    \"\"\"Sample ID of the tile.\"\"\"\n\n    # TODO: Test that ZYX axes are not singleton ?\n\n    def __eq__(self, other_tile: object):\n        \"\"\"Check if two tile information objects are equal.\n\n        Parameters\n        ----------\n        other_tile : object\n            Tile information object to compare with.\n\n        Returns\n        -------\n        bool\n            Whether the two tile information objects are equal.\n        \"\"\"\n        if not isinstance(other_tile, TileInformation):\n            return NotImplemented\n\n        return (\n            self.array_shape == other_tile.array_shape\n            and self.last_tile == other_tile.last_tile\n            and self.overlap_crop_coords == other_tile.overlap_crop_coords\n            and self.stitch_coords == other_tile.stitch_coords\n            and self.sample_id == other_tile.sample_id\n        )\n</code></pre>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.array_shape","title":"<code>array_shape</code>  <code>instance-attribute</code>","text":"<p>Shape of the original (untiled) array.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.last_tile","title":"<code>last_tile = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether this tile is the last one of the array.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.overlap_crop_coords","title":"<code>overlap_crop_coords</code>  <code>instance-attribute</code>","text":"<p>Inner coordinates of the tile where to crop the prediction in order to stitch it back into the original image.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.sample_id","title":"<code>sample_id</code>  <code>instance-attribute</code>","text":"<p>Sample ID of the tile.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.stitch_coords","title":"<code>stitch_coords</code>  <code>instance-attribute</code>","text":"<p>Coordinates in the original image where to stitch the cropped tile back.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.__eq__","title":"<code>__eq__(other_tile)</code>","text":"<p>Check if two tile information objects are equal.</p> <p>Parameters:</p> Name Type Description Default <code>other_tile</code> <code>object</code> <p>Tile information object to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the two tile information objects are equal.</p> Source code in <code>src/careamics/config/tile_information.py</code> <pre><code>def __eq__(self, other_tile: object):\n    \"\"\"Check if two tile information objects are equal.\n\n    Parameters\n    ----------\n    other_tile : object\n        Tile information object to compare with.\n\n    Returns\n    -------\n    bool\n        Whether the two tile information objects are equal.\n    \"\"\"\n    if not isinstance(other_tile, TileInformation):\n        return NotImplemented\n\n    return (\n        self.array_shape == other_tile.array_shape\n        and self.last_tile == other_tile.last_tile\n        and self.overlap_crop_coords == other_tile.overlap_crop_coords\n        and self.stitch_coords == other_tile.stitch_coords\n        and self.sample_id == other_tile.sample_id\n    )\n</code></pre>"},{"location":"reference/careamics/config/training_model/","title":"training_model","text":"<p>Training configuration.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters related to the training.</p> <p>Mandatory parameters are:     - num_epochs: number of epochs, greater than 0.     - batch_size: batch size, greater than 0.     - augmentation: whether to use data augmentation or not (True or False).</p> <p>Attributes:</p> Name Type Description <code>num_epochs</code> <code>int</code> <p>Number of epochs, greater than 0.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"\n    Parameters related to the training.\n\n    Mandatory parameters are:\n        - num_epochs: number of epochs, greater than 0.\n        - batch_size: batch size, greater than 0.\n        - augmentation: whether to use data augmentation or not (True or False).\n\n    Attributes\n    ----------\n    num_epochs : int\n        Number of epochs, greater than 0.\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n    lightning_trainer_config: dict | None = None\n    \"\"\"Configuration for the PyTorch Lightning Trainer, following PyTorch Lightning\n    Trainer class\"\"\"\n\n    logger: Literal[\"wandb\", \"tensorboard\"] | None = None\n    \"\"\"Logger to use during training. If None, no logger will be used. Available\n    loggers are defined in SupportedLogger.\"\"\"\n\n    # Only basic callbacks\n    checkpoint_callback: CheckpointModel = CheckpointModel()\n    \"\"\"Checkpoint callback configuration, following PyTorch Lightning Checkpoint\n    callback.\"\"\"\n\n    early_stopping_callback: EarlyStoppingModel | None = Field(\n        default=None, validate_default=True\n    )\n    \"\"\"Early stopping callback configuration, following PyTorch Lightning Checkpoint\n    callback.\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def has_logger(self) -&gt; bool:\n        \"\"\"Check if the logger is defined.\n\n        Returns\n        -------\n        bool\n            Whether the logger is defined or not.\n        \"\"\"\n        return self.logger is not None\n</code></pre>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.checkpoint_callback","title":"<code>checkpoint_callback = CheckpointModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint callback configuration, following PyTorch Lightning Checkpoint callback.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.early_stopping_callback","title":"<code>early_stopping_callback = Field(default=None, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Early stopping callback configuration, following PyTorch Lightning Checkpoint callback.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.lightning_trainer_config","title":"<code>lightning_trainer_config = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration for the PyTorch Lightning Trainer, following PyTorch Lightning Trainer class</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.logger","title":"<code>logger = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logger to use during training. If None, no logger will be used. Available loggers are defined in SupportedLogger.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.has_logger","title":"<code>has_logger()</code>","text":"<p>Check if the logger is defined.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the logger is defined or not.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>def has_logger(self) -&gt; bool:\n    \"\"\"Check if the logger is defined.\n\n    Returns\n    -------\n    bool\n        Whether the logger is defined or not.\n    \"\"\"\n    return self.logger is not None\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/","title":"care_algorithm_model","text":"<p>CARE algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm","title":"<code>CAREAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>CARE algorithm configuration.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>care</code> <p>CARE Algorithm name.</p> <code>loss</code> <code>{mae, mse}</code> <p>CARE-compatible loss function.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>class CAREAlgorithm(UNetBasedAlgorithm):\n    \"\"\"CARE algorithm configuration.\n\n    Attributes\n    ----------\n    algorithm : \"care\"\n        CARE Algorithm name.\n    loss : {\"mae\", \"mse\"}\n        CARE-compatible loss function.\n    \"\"\"\n\n    algorithm: Literal[\"care\"] = \"care\"\n    \"\"\"CARE Algorithm name.\"\"\"\n\n    loss: Literal[\"mae\", \"mse\"] = \"mae\"\n    \"\"\"CARE-compatible loss function.\"\"\"\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_without_n2v2),\n        AfterValidator(model_without_final_activation),\n    ]\n    \"\"\"UNet without a final activation function and without the `n2v2` modifications.\"\"\"\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return CARE\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            CARE,\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return CARE_REF.text + \" doi: \" + CARE_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [CARE_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return CARE_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.algorithm","title":"<code>algorithm = 'care'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CARE Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.loss","title":"<code>loss = 'mae'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CARE-compatible loss function.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet without a final activation function and without the <code>n2v2</code> modifications.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [CARE_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return CARE_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return CARE\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        CARE,\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return CARE_REF.text + \" doi: \" + CARE_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/","title":"hdn_algorithm_model","text":"<p>HDN algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm","title":"<code>HDNAlgorithm</code>","text":"<p>               Bases: <code>VAEBasedAlgorithm</code></p> <p>HDN algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>class HDNAlgorithm(VAEBasedAlgorithm):\n    \"\"\"HDN algorithm configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    algorithm: Literal[\"hdn\"] = \"hdn\"\n\n    loss: LVAELossConfig\n\n    model: LVAEModel  # TODO add validators\n\n    is_supervised: bool = False\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return HDN\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"VAE\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return HDN_REF.text + \" doi: \" + HDN_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [HDN_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return HDN_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [HDN_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return HDN_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return HDN\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"VAE\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/hdn_algorithm_model/#careamics.config.algorithms.hdn_algorithm_model.HDNAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/hdn_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return HDN_REF.text + \" doi: \" + HDN_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/","title":"microsplit_algorithm_model","text":"<p>MicroSplit algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm","title":"<code>MicroSplitAlgorithm</code>","text":"<p>               Bases: <code>VAEBasedAlgorithm</code></p> <p>MicroSplit algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>class MicroSplitAlgorithm(VAEBasedAlgorithm):\n    \"\"\"MicroSplit algorithm configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    algorithm: Literal[\"microsplit\"] = \"microsplit\"\n\n    loss: LVAELossConfig\n\n    model: LVAEModel  # TODO add validators\n\n    is_supervised: bool = True\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return MICROSPLIT\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"VAE\",\n            \"self-supervised\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return MICROSPLIT_REF.text + \" doi: \" + MICROSPLIT_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [MICROSPLIT_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return MICROSPLIT_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [MICROSPLIT_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return MICROSPLIT_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return MICROSPLIT\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"VAE\",\n        \"self-supervised\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/microsplit_algorithm_model/#careamics.config.algorithms.microsplit_algorithm_model.MicroSplitAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/microsplit_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return MICROSPLIT_REF.text + \" doi: \" + MICROSPLIT_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/","title":"n2n_algorithm_model","text":"<p>N2N Algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm","title":"<code>N2NAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>Noise2Noise Algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>class N2NAlgorithm(UNetBasedAlgorithm):\n    \"\"\"Noise2Noise Algorithm configuration.\"\"\"\n\n    algorithm: Literal[\"n2n\"] = \"n2n\"\n    \"\"\"N2N Algorithm name.\"\"\"\n\n    loss: Literal[\"mae\", \"mse\"] = \"mae\"\n    \"\"\"N2N-compatible loss function.\"\"\"\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_without_n2v2),\n        AfterValidator(model_without_final_activation),\n    ]\n    \"\"\"UNet without a final activation function and without the `n2v2` modifications.\"\"\"\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return N2N\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            N2N,\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return N2N_REF.text + \" doi: \" + N2N_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [N2N_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return N2N_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.algorithm","title":"<code>algorithm = 'n2n'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2N Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.loss","title":"<code>loss = 'mae'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2N-compatible loss function.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet without a final activation function and without the <code>n2v2</code> modifications.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [N2N_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return N2N_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return N2N\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        N2N,\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return N2N_REF.text + \" doi: \" + N2N_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/","title":"n2v_algorithm_model","text":"<p>N2V Algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm","title":"<code>N2VAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>N2V Algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>class N2VAlgorithm(UNetBasedAlgorithm):\n    \"\"\"N2V Algorithm configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    algorithm: Literal[\"n2v\"] = \"n2v\"\n    \"\"\"N2V Algorithm name.\"\"\"\n\n    loss: Literal[\"n2v\"] = \"n2v\"\n    \"\"\"N2V loss function.\"\"\"\n\n    n2v_config: N2VManipulateModel = N2VManipulateModel()\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_matching_in_out_channels),\n        AfterValidator(model_without_final_activation),\n    ]\n\n    @model_validator(mode=\"after\")\n    def validate_n2v2(self) -&gt; Self:\n        \"\"\"Validate that the N2V2 strategy and models are set correctly.\n\n        Returns\n        -------\n        Self\n            The validateed configuration.\n\n        Raises\n        ------\n        ValueError\n            If N2V2 is used with the wrong pixel manipulation strategy.\n        \"\"\"\n        if self.model.n2v2:\n            if self.n2v_config.strategy != SupportedPixelManipulation.MEDIAN.value:\n                raise ValueError(\n                    f\"N2V2 can only be used with the \"\n                    f\"{SupportedPixelManipulation.MEDIAN} pixel manipulation strategy. \"\n                    f\"Change the `strategy` parameters in `n2v_config` to \"\n                    f\"{SupportedPixelManipulation.MEDIAN}.\"\n                )\n        else:\n            if self.n2v_config.strategy != SupportedPixelManipulation.UNIFORM.value:\n                raise ValueError(\n                    f\"N2V can only be used with the \"\n                    f\"{SupportedPixelManipulation.UNIFORM} pixel manipulation strategy.\"\n                    f\" Change the `strategy` parameters in `n2v_config` to \"\n                    f\"{SupportedPixelManipulation.UNIFORM}.\"\n                )\n        return self\n\n    def set_n2v2(self, use_n2v2: bool) -&gt; None:\n        \"\"\"\n        Set the configuration to use N2V2 or the vanilla Noise2Void.\n\n        This method ensures that N2V2 is set correctly and remain coherent, as opposed\n        to setting the different parameters individually.\n\n        Parameters\n        ----------\n        use_n2v2 : bool\n            Whether to use N2V2.\n        \"\"\"\n        if use_n2v2:\n            self.n2v_config.strategy = SupportedPixelManipulation.MEDIAN.value\n            self.model.n2v2 = True\n        else:\n            self.n2v_config.strategy = SupportedPixelManipulation.UNIFORM.value\n            self.model.n2v2 = False\n\n    def is_struct_n2v(self) -&gt; bool:\n        \"\"\"Check if the configuration is using structN2V.\n\n        Returns\n        -------\n        bool\n            Whether the configuration is using structN2V.\n        \"\"\"\n        return self.n2v_config.struct_mask_axis != SupportedStructAxis.NONE.value\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the friendly name of the algorithm.\n\n        Returns\n        -------\n        str\n            Friendly name.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        if use_n2v2 and use_structN2V:\n            return STRUCT_N2V2\n        elif use_n2v2:\n            return N2V2\n        elif use_structN2V:\n            return STRUCT_N2V\n        else:\n            return N2V\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        keywords = [\n            \"denoising\",\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            N2V,\n        ]\n\n        if use_n2v2:\n            keywords.append(N2V2)\n        if use_structN2V:\n            keywords.append(STRUCT_N2V)\n\n        return keywords\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        references = [\n            N2V_REF.text + \" doi: \" + N2V_REF.doi,\n            N2V2_REF.text + \" doi: \" + N2V2_REF.doi,\n            STRUCTN2V_REF.text + \" doi: \" + STRUCTN2V_REF.doi,\n        ]\n\n        # return the (struct)N2V(2) references\n        if use_n2v2 and use_structN2V:\n            return \"\\n\".join(references)\n        elif use_n2v2:\n            references.pop(-1)\n            return \"\\n\".join(references)\n        elif use_structN2V:\n            references.pop(-2)\n            return \"\\n\".join(references)\n        else:\n            return references[0]\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        references = [N2V_REF]\n\n        if use_n2v2:\n            references.append(N2V2_REF)\n\n        if use_structN2V:\n            references.append(STRUCTN2V_REF)\n\n        return references\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Return a description of the algorithm.\n\n        This method is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Description of the algorithm.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        if use_n2v2 and use_structN2V:\n            return STR_N2V2_DESCRIPTION\n        elif use_n2v2:\n            return N2V2_DESCRIPTION\n        elif use_structN2V:\n            return STR_N2V_DESCRIPTION\n        else:\n            return N2V_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.algorithm","title":"<code>algorithm = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2V Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.loss","title":"<code>loss = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2V loss function.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    references = [N2V_REF]\n\n    if use_n2v2:\n        references.append(N2V2_REF)\n\n    if use_structN2V:\n        references.append(STRUCTN2V_REF)\n\n    return references\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Return a description of the algorithm.</p> <p>This method is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Return a description of the algorithm.\n\n    This method is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Description of the algorithm.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    if use_n2v2 and use_structN2V:\n        return STR_N2V2_DESCRIPTION\n    elif use_n2v2:\n        return N2V2_DESCRIPTION\n    elif use_structN2V:\n        return STR_N2V_DESCRIPTION\n    else:\n        return N2V_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the friendly name of the algorithm.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the friendly name of the algorithm.\n\n    Returns\n    -------\n    str\n        Friendly name.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    if use_n2v2 and use_structN2V:\n        return STRUCT_N2V2\n    elif use_n2v2:\n        return N2V2\n    elif use_structN2V:\n        return STRUCT_N2V\n    else:\n        return N2V\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    keywords = [\n        \"denoising\",\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        N2V,\n    ]\n\n    if use_n2v2:\n        keywords.append(N2V2)\n    if use_structN2V:\n        keywords.append(STRUCT_N2V)\n\n    return keywords\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    references = [\n        N2V_REF.text + \" doi: \" + N2V_REF.doi,\n        N2V2_REF.text + \" doi: \" + N2V2_REF.doi,\n        STRUCTN2V_REF.text + \" doi: \" + STRUCTN2V_REF.doi,\n    ]\n\n    # return the (struct)N2V(2) references\n    if use_n2v2 and use_structN2V:\n        return \"\\n\".join(references)\n    elif use_n2v2:\n        references.pop(-1)\n        return \"\\n\".join(references)\n    elif use_structN2V:\n        references.pop(-2)\n        return \"\\n\".join(references)\n    else:\n        return references[0]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.is_struct_n2v","title":"<code>is_struct_n2v()</code>","text":"<p>Check if the configuration is using structN2V.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the configuration is using structN2V.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def is_struct_n2v(self) -&gt; bool:\n    \"\"\"Check if the configuration is using structN2V.\n\n    Returns\n    -------\n    bool\n        Whether the configuration is using structN2V.\n    \"\"\"\n    return self.n2v_config.struct_mask_axis != SupportedStructAxis.NONE.value\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.set_n2v2","title":"<code>set_n2v2(use_n2v2)</code>","text":"<p>Set the configuration to use N2V2 or the vanilla Noise2Void.</p> <p>This method ensures that N2V2 is set correctly and remain coherent, as opposed to setting the different parameters individually.</p> <p>Parameters:</p> Name Type Description Default <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2.</p> required Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def set_n2v2(self, use_n2v2: bool) -&gt; None:\n    \"\"\"\n    Set the configuration to use N2V2 or the vanilla Noise2Void.\n\n    This method ensures that N2V2 is set correctly and remain coherent, as opposed\n    to setting the different parameters individually.\n\n    Parameters\n    ----------\n    use_n2v2 : bool\n        Whether to use N2V2.\n    \"\"\"\n    if use_n2v2:\n        self.n2v_config.strategy = SupportedPixelManipulation.MEDIAN.value\n        self.model.n2v2 = True\n    else:\n        self.n2v_config.strategy = SupportedPixelManipulation.UNIFORM.value\n        self.model.n2v2 = False\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.validate_n2v2","title":"<code>validate_n2v2()</code>","text":"<p>Validate that the N2V2 strategy and models are set correctly.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validateed configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If N2V2 is used with the wrong pixel manipulation strategy.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_n2v2(self) -&gt; Self:\n    \"\"\"Validate that the N2V2 strategy and models are set correctly.\n\n    Returns\n    -------\n    Self\n        The validateed configuration.\n\n    Raises\n    ------\n    ValueError\n        If N2V2 is used with the wrong pixel manipulation strategy.\n    \"\"\"\n    if self.model.n2v2:\n        if self.n2v_config.strategy != SupportedPixelManipulation.MEDIAN.value:\n            raise ValueError(\n                f\"N2V2 can only be used with the \"\n                f\"{SupportedPixelManipulation.MEDIAN} pixel manipulation strategy. \"\n                f\"Change the `strategy` parameters in `n2v_config` to \"\n                f\"{SupportedPixelManipulation.MEDIAN}.\"\n            )\n    else:\n        if self.n2v_config.strategy != SupportedPixelManipulation.UNIFORM.value:\n            raise ValueError(\n                f\"N2V can only be used with the \"\n                f\"{SupportedPixelManipulation.UNIFORM} pixel manipulation strategy.\"\n                f\" Change the `strategy` parameters in `n2v_config` to \"\n                f\"{SupportedPixelManipulation.UNIFORM}.\"\n            )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/","title":"unet_algorithm_model","text":"<p>UNet-based algorithm Pydantic model.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm","title":"<code>UNetBasedAlgorithm</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>General UNet-based algorithm configuration.</p> <p>This Pydantic model validates the parameters governing the components of the training algorithm: which algorithm, loss function, model architecture, optimizer, and learning rate scheduler to use.</p> <p>Currently, we only support N2V, CARE, and N2N algorithms. In order to train these algorithms, use the corresponding configuration child classes (e.g. <code>N2VAlgorithm</code>) to ensure coherent parameters (e.g. specific losses).</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>{n2v, care, n2n}</code> <p>Algorithm to use.</p> <code>loss</code> <code>{n2v, mae, mse}</code> <p>Loss function to use.</p> <code>model</code> <code>UNetModel</code> <p>Model architecture to use.</p> <code>optimizer</code> <code>(OptimizerModel, optional)</code> <p>Optimizer to use.</p> <code>lr_scheduler</code> <code>(LrSchedulerModel, optional)</code> <p>Learning rate scheduler to use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Algorithm parameter type validation errors.</p> <code>ValueError</code> <p>If the algorithm, loss and model are not compatible.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>class UNetBasedAlgorithm(BaseModel):\n    \"\"\"General UNet-based algorithm configuration.\n\n    This Pydantic model validates the parameters governing the components of the\n    training algorithm: which algorithm, loss function, model architecture, optimizer,\n    and learning rate scheduler to use.\n\n    Currently, we only support N2V, CARE, and N2N algorithms. In order to train these\n    algorithms, use the corresponding configuration child classes (e.g.\n    `N2VAlgorithm`) to ensure coherent parameters (e.g. specific losses).\n\n\n    Attributes\n    ----------\n    algorithm : {\"n2v\", \"care\", \"n2n\"}\n        Algorithm to use.\n    loss : {\"n2v\", \"mae\", \"mse\"}\n        Loss function to use.\n    model : UNetModel\n        Model architecture to use.\n    optimizer : OptimizerModel, optional\n        Optimizer to use.\n    lr_scheduler : LrSchedulerModel, optional\n        Learning rate scheduler to use.\n\n    Raises\n    ------\n    ValueError\n        Algorithm parameter type validation errors.\n    ValueError\n        If the algorithm, loss and model are not compatible.\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        protected_namespaces=(),  # allows to use model_* as a field name\n        validate_assignment=True,\n        extra=\"allow\",\n    )\n\n    # Mandatory fields\n    algorithm: Literal[\"n2v\", \"care\", \"n2n\"]\n    \"\"\"Algorithm name, as defined in SupportedAlgorithm.\"\"\"\n\n    loss: Literal[\"n2v\", \"mae\", \"mse\"]\n    \"\"\"Loss function to use, as defined in SupportedLoss.\"\"\"\n\n    model: UNetModel\n    \"\"\"UNet model configuration.\"\"\"\n\n    # Optional fields\n    optimizer: OptimizerModel = OptimizerModel()\n    \"\"\"Optimizer to use, defined in SupportedOptimizer.\"\"\"\n\n    lr_scheduler: LrSchedulerModel = LrSchedulerModel()\n    \"\"\"Learning rate scheduler to use, defined in SupportedLrScheduler.\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string representing the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    @classmethod\n    def get_compatible_algorithms(cls) -&gt; list[str]:\n        \"\"\"Get the list of compatible algorithms.\n\n        Returns\n        -------\n        list of str\n            List of compatible algorithms.\n        \"\"\"\n        return [\"n2v\", \"care\", \"n2n\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.algorithm","title":"<code>algorithm</code>  <code>instance-attribute</code>","text":"<p>Algorithm name, as defined in SupportedAlgorithm.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.loss","title":"<code>loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use, as defined in SupportedLoss.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.lr_scheduler","title":"<code>lr_scheduler = LrSchedulerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate scheduler to use, defined in SupportedLrScheduler.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet model configuration.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.optimizer","title":"<code>optimizer = OptimizerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optimizer to use, defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string representing the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string representing the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.get_compatible_algorithms","title":"<code>get_compatible_algorithms()</code>  <code>classmethod</code>","text":"<p>Get the list of compatible algorithms.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of compatible algorithms.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>@classmethod\ndef get_compatible_algorithms(cls) -&gt; list[str]:\n    \"\"\"Get the list of compatible algorithms.\n\n    Returns\n    -------\n    list of str\n        List of compatible algorithms.\n    \"\"\"\n    return [\"n2v\", \"care\", \"n2n\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/","title":"vae_algorithm_model","text":"<p>VAE-based algorithm Pydantic model.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm","title":"<code>VAEBasedAlgorithm</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>VAE-based algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm--todo","title":"TODO","text":"<p>Examples:</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm--todo-add-once-finalized","title":"TODO add once finalized","text":"Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>class VAEBasedAlgorithm(BaseModel):\n    \"\"\"VAE-based algorithm configuration.\n\n    # TODO\n\n    Examples\n    --------\n    # TODO add once finalized\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        protected_namespaces=(),  # allows to use model_* as a field name\n        validate_assignment=True,\n        extra=\"allow\",\n    )\n\n    # Mandatory fields\n    # defined in SupportedAlgorithm\n    # TODO: Use supported Enum classes for typing?\n    #   - values can still be passed as strings and they will be cast to Enum\n    algorithm: Literal[\"hdn\", \"microsplit\"]\n\n    # NOTE: these are all configs (pydantic models)\n    loss: LVAELossConfig\n    model: LVAEModel\n    noise_model: MultiChannelNMConfig | None = None\n    noise_model_likelihood: NMLikelihoodConfig | None = None\n    gaussian_likelihood: GaussianLikelihoodConfig | None = None  # TODO change to str\n\n    mmse_count: int = 1\n    is_supervised: bool = False\n\n    # Optional fields\n    optimizer: OptimizerModel = OptimizerModel()\n    \"\"\"Optimizer to use, defined in SupportedOptimizer.\"\"\"\n\n    lr_scheduler: LrSchedulerModel = LrSchedulerModel()\n\n    @model_validator(mode=\"after\")\n    def algorithm_cross_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the algorithm model based on `algorithm`.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        # hdn\n        # TODO move to designated configurations\n        if self.algorithm == SupportedAlgorithm.HDN:\n            if self.loss.loss_type != SupportedLoss.HDN:\n                raise ValueError(\n                    f\"Algorithm {self.algorithm} only supports loss `hdn`.\"\n                )\n            if self.model.multiscale_count &gt; 1:\n                raise ValueError(\"Algorithm `hdn` does not support multiscale models.\")\n        # musplit\n        if self.algorithm == SupportedAlgorithm.MICROSPLIT:\n            if self.loss.loss_type not in [\n                SupportedLoss.MUSPLIT,\n                SupportedLoss.DENOISPLIT,\n                SupportedLoss.DENOISPLIT_MUSPLIT,\n            ]:  # TODO Update losses configs, make loss just microsplit\n                raise ValueError(\n                    f\"Algorithm {self.algorithm} only supports loss `microsplit`.\"\n                )  # TODO Update losses configs\n\n            if (\n                self.loss.loss_type == SupportedLoss.DENOISPLIT\n                and self.model.predict_logvar is not None\n            ):\n                raise ValueError(\n                    \"Algorithm `denoisplit` with loss `denoisplit` only supports \"\n                    \"`predict_logvar` as `None`.\"\n                )\n            if (\n                self.loss.loss_type == SupportedLoss.DENOISPLIT\n                and self.noise_model is None\n            ):\n                raise ValueError(\"Algorithm `denoisplit` requires a noise model.\")\n        # TODO: what if algorithm is not musplit or denoisplit\n        return self\n\n    @model_validator(mode=\"after\")\n    def output_channels_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the consistency between number of out channels and noise models.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.noise_model is not None:\n            assert self.model.output_channels == len(self.noise_model.noise_models), (\n                f\"Number of output channels ({self.model.output_channels}) must match \"\n                f\"the number of noise models ({len(self.noise_model.noise_models)}).\"\n            )\n\n        if self.algorithm == SupportedAlgorithm.HDN:\n            assert self.model.output_channels == 1, (\n                f\"Number of output channels ({self.model.output_channels}) must be 1 \"\n                \"for algorithm `hdn`.\"\n            )\n        return self\n\n    @model_validator(mode=\"after\")\n    def predict_logvar_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the consistency of `predict_logvar` throughout the model.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.gaussian_likelihood is not None:\n            assert (\n                self.model.predict_logvar == self.gaussian_likelihood.predict_logvar\n            ), (\n                f\"Model `predict_logvar` ({self.model.predict_logvar}) must match \"\n                \"Gaussian likelihood model `predict_logvar` \"\n                f\"({self.gaussian_likelihood.predict_logvar}).\",\n            )\n        # if self.algorithm == SupportedAlgorithm.HDN:\n        #     assert (\n        #         self.model.predict_logvar is None\n        #     ), \"Model `predict_logvar` must be `None` for algorithm `hdn`.\"\n        #     if self.gaussian_likelihood is not None:\n        #         assert self.gaussian_likelihood.predict_logvar is None, (\n        #             \"Gaussian likelihood model `predict_logvar` must be `None` \"\n        #             \"for algorithm `hdn`.\"\n        #         )\n        # TODO check this\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string representing the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    @classmethod\n    def get_compatible_algorithms(cls) -&gt; list[str]:\n        \"\"\"Get the list of compatible algorithms.\n\n        Returns\n        -------\n        list of str\n            List of compatible algorithms.\n        \"\"\"\n        return [\"hdn\", \"microsplit\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.optimizer","title":"<code>optimizer = OptimizerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optimizer to use, defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string representing the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string representing the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.algorithm_cross_validation","title":"<code>algorithm_cross_validation()</code>","text":"<p>Validate the algorithm model based on <code>algorithm</code>.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef algorithm_cross_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the algorithm model based on `algorithm`.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    # hdn\n    # TODO move to designated configurations\n    if self.algorithm == SupportedAlgorithm.HDN:\n        if self.loss.loss_type != SupportedLoss.HDN:\n            raise ValueError(\n                f\"Algorithm {self.algorithm} only supports loss `hdn`.\"\n            )\n        if self.model.multiscale_count &gt; 1:\n            raise ValueError(\"Algorithm `hdn` does not support multiscale models.\")\n    # musplit\n    if self.algorithm == SupportedAlgorithm.MICROSPLIT:\n        if self.loss.loss_type not in [\n            SupportedLoss.MUSPLIT,\n            SupportedLoss.DENOISPLIT,\n            SupportedLoss.DENOISPLIT_MUSPLIT,\n        ]:  # TODO Update losses configs, make loss just microsplit\n            raise ValueError(\n                f\"Algorithm {self.algorithm} only supports loss `microsplit`.\"\n            )  # TODO Update losses configs\n\n        if (\n            self.loss.loss_type == SupportedLoss.DENOISPLIT\n            and self.model.predict_logvar is not None\n        ):\n            raise ValueError(\n                \"Algorithm `denoisplit` with loss `denoisplit` only supports \"\n                \"`predict_logvar` as `None`.\"\n            )\n        if (\n            self.loss.loss_type == SupportedLoss.DENOISPLIT\n            and self.noise_model is None\n        ):\n            raise ValueError(\"Algorithm `denoisplit` requires a noise model.\")\n    # TODO: what if algorithm is not musplit or denoisplit\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.get_compatible_algorithms","title":"<code>get_compatible_algorithms()</code>  <code>classmethod</code>","text":"<p>Get the list of compatible algorithms.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of compatible algorithms.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@classmethod\ndef get_compatible_algorithms(cls) -&gt; list[str]:\n    \"\"\"Get the list of compatible algorithms.\n\n    Returns\n    -------\n    list of str\n        List of compatible algorithms.\n    \"\"\"\n    return [\"hdn\", \"microsplit\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.output_channels_validation","title":"<code>output_channels_validation()</code>","text":"<p>Validate the consistency between number of out channels and noise models.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef output_channels_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the consistency between number of out channels and noise models.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.noise_model is not None:\n        assert self.model.output_channels == len(self.noise_model.noise_models), (\n            f\"Number of output channels ({self.model.output_channels}) must match \"\n            f\"the number of noise models ({len(self.noise_model.noise_models)}).\"\n        )\n\n    if self.algorithm == SupportedAlgorithm.HDN:\n        assert self.model.output_channels == 1, (\n            f\"Number of output channels ({self.model.output_channels}) must be 1 \"\n            \"for algorithm `hdn`.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.predict_logvar_validation","title":"<code>predict_logvar_validation()</code>","text":"<p>Validate the consistency of <code>predict_logvar</code> throughout the model.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef predict_logvar_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the consistency of `predict_logvar` throughout the model.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.gaussian_likelihood is not None:\n        assert (\n            self.model.predict_logvar == self.gaussian_likelihood.predict_logvar\n        ), (\n            f\"Model `predict_logvar` ({self.model.predict_logvar}) must match \"\n            \"Gaussian likelihood model `predict_logvar` \"\n            f\"({self.gaussian_likelihood.predict_logvar}).\",\n        )\n    # if self.algorithm == SupportedAlgorithm.HDN:\n    #     assert (\n    #         self.model.predict_logvar is None\n    #     ), \"Model `predict_logvar` must be `None` for algorithm `hdn`.\"\n    #     if self.gaussian_likelihood is not None:\n    #         assert self.gaussian_likelihood.predict_logvar is None, (\n    #             \"Gaussian likelihood model `predict_logvar` must be `None` \"\n    #             \"for algorithm `hdn`.\"\n    #         )\n    # TODO check this\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/architecture_model/","title":"architecture_model","text":"<p>Base model for the various CAREamics architectures.</p>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel","title":"<code>ArchitectureModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base Pydantic model for all model architectures.</p> <p>The <code>model_dump</code> method allows removing the <code>architecture</code> key from the model.</p> Source code in <code>src/careamics/config/architectures/architecture_model.py</code> <pre><code>class ArchitectureModel(BaseModel):\n    \"\"\"\n    Base Pydantic model for all model architectures.\n\n    The `model_dump` method allows removing the `architecture` key from the model.\n    \"\"\"\n\n    architecture: str\n    \"\"\"Name of the architecture.\"\"\"\n\n    def model_dump(self, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Dump the model as a dictionary, ignoring the architecture keyword.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Additional keyword arguments from Pydantic BaseModel model_dump method.\n\n        Returns\n        -------\n        {str: Any}\n            Model as a dictionary.\n        \"\"\"\n        model_dict = super().model_dump(**kwargs)\n\n        # remove the architecture key\n        model_dict.pop(\"architecture\")\n\n        return model_dict\n</code></pre>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":"<p>Name of the architecture.</p>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Dump the model as a dictionary, ignoring the architecture keyword.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from Pydantic BaseModel model_dump method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>{str: Any}</code> <p>Model as a dictionary.</p> Source code in <code>src/careamics/config/architectures/architecture_model.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Dump the model as a dictionary, ignoring the architecture keyword.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Additional keyword arguments from Pydantic BaseModel model_dump method.\n\n    Returns\n    -------\n    {str: Any}\n        Model as a dictionary.\n    \"\"\"\n    model_dict = super().model_dump(**kwargs)\n\n    # remove the architecture key\n    model_dict.pop(\"architecture\")\n\n    return model_dict\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/","title":"lvae_model","text":"<p>LVAE Pydantic model.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel","title":"<code>LVAEModel</code>","text":"<p>               Bases: <code>ArchitectureModel</code></p> <p>LVAE model.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>class LVAEModel(ArchitectureModel):\n    \"\"\"LVAE model.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, validate_default=True)\n\n    architecture: Literal[\"LVAE\"]\n\n    input_shape: tuple[int, ...] = Field(default=(64, 64), validate_default=True)\n    \"\"\"Shape of the input patch (Z, Y, X) or (Y, X) if the data is 2D.\"\"\"\n    encoder_conv_strides: list = Field(default=[2, 2], validate_default=True)\n\n    # TODO make this per hierarchy step ?\n    decoder_conv_strides: list = Field(default=[2, 2], validate_default=True)\n    \"\"\"Dimensions (2D or 3D) of the convolutional layers.\"\"\"\n\n    multiscale_count: int = Field(default=1)\n    # TODO there should be a check for multiscale_count in dataset !!\n\n    # 1 - off, len(z_dims) + 1 # TODO Consider starting from 0\n    z_dims: list = Field(default=[128, 128, 128, 128])\n    output_channels: int = Field(default=1, ge=1)\n    encoder_n_filters: int = Field(default=64, ge=8, le=1024)\n    decoder_n_filters: int = Field(default=64, ge=8, le=1024)\n    encoder_dropout: float = Field(default=0.1, ge=0.0, le=0.9)\n    decoder_dropout: float = Field(default=0.1, ge=0.0, le=0.9)\n    nonlinearity: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\", \"ELU\"\n    ] = Field(\n        default=\"ELU\",\n    )\n\n    predict_logvar: Literal[None, \"pixelwise\"] = \"pixelwise\"\n    analytical_kl: bool = Field(default=False)\n\n    @model_validator(mode=\"after\")\n    def validate_conv_strides(self: Self) -&gt; Self:\n        \"\"\"\n        Validate the convolutional strides.\n\n        Returns\n        -------\n        list\n            Validated strides.\n\n        Raises\n        ------\n        ValueError\n            If the number of strides is not 2.\n        \"\"\"\n        if len(self.encoder_conv_strides) &lt; 2 or len(self.encoder_conv_strides) &gt; 3:\n            raise ValueError(\n                f\"Strides must be 2 or 3 (got {len(self.encoder_conv_strides)}).\"\n            )\n\n        if len(self.decoder_conv_strides) &lt; 2 or len(self.decoder_conv_strides) &gt; 3:\n            raise ValueError(\n                f\"Strides must be 2 or 3 (got {len(self.decoder_conv_strides)}).\"\n            )\n\n        # adding 1 to encoder strides for the number of input channels\n        if len(self.input_shape) != len(self.encoder_conv_strides):\n            raise ValueError(\n                f\"Input dimensions must be equal to the number of encoder conv strides\"\n                f\" (got {len(self.input_shape)} and {len(self.encoder_conv_strides)}).\"\n            )\n\n        if len(self.encoder_conv_strides) &lt; len(self.decoder_conv_strides):\n            raise ValueError(\n                f\"Decoder can't be 3D when encoder is 2D (got\"\n                f\" {len(self.encoder_conv_strides)} and\"\n                f\"{len(self.decoder_conv_strides)}).\"\n            )\n\n        if any(s &lt; 1 for s in self.encoder_conv_strides) or any(\n            s &lt; 1 for s in self.decoder_conv_strides\n        ):\n            raise ValueError(\n                f\"All strides must be greater or equal to 1\"\n                f\"(got {self.encoder_conv_strides} and {self.decoder_conv_strides}).\"\n            )\n        # TODO: validate max stride size ?\n        return self\n\n    @field_validator(\"input_shape\")\n    @classmethod\n    def validate_input_shape(cls, input_shape: list) -&gt; list:\n        \"\"\"\n        Validate the input shape.\n\n        Parameters\n        ----------\n        input_shape : list\n            Shape of the input patch.\n\n        Returns\n        -------\n        list\n            Validated input shape.\n\n        Raises\n        ------\n        ValueError\n            If the number of dimensions is not 3 or 4.\n        \"\"\"\n        if len(input_shape) &lt; 2 or len(input_shape) &gt; 3:\n            raise ValueError(\n                f\"Number of input dimensions must be 2 for 2D data 3 for 3D\"\n                f\"(got {len(input_shape)}).\"\n            )\n\n        if any(s &lt; 1 for s in input_shape):\n            raise ValueError(\n                f\"Input shape must be greater than 1 in all dimensions\"\n                f\"(got {input_shape}).\"\n            )\n\n        if any(s &lt; 64 for s in input_shape[-2:]):\n            raise ValueError(\n                f\"Input shape must be greater or equal to 64 in XY dimensions\"\n                f\"(got {input_shape}).\"\n            )\n\n        return input_shape\n\n    @field_validator(\"encoder_n_filters\")\n    @classmethod\n    def validate_encoder_even(cls, encoder_n_filters: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        encoder_n_filters : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if encoder_n_filters % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {encoder_n_filters}).\"\n            )\n\n        return encoder_n_filters\n\n    @field_validator(\"decoder_n_filters\")\n    @classmethod\n    def validate_decoder_even(cls, decoder_n_filters: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        decoder_n_filters : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if decoder_n_filters % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {decoder_n_filters}).\"\n            )\n\n        return decoder_n_filters\n\n    @field_validator(\"z_dims\")\n    def validate_z_dims(cls, z_dims: tuple) -&gt; tuple:\n        \"\"\"\n        Validate the z_dims.\n\n        Parameters\n        ----------\n        z_dims : tuple\n            Tuple of z dimensions.\n\n        Returns\n        -------\n        tuple\n            Validated z dimensions.\n\n        Raises\n        ------\n        ValueError\n            If the number of z dimensions is not 4.\n        \"\"\"\n        if len(z_dims) &lt; 2:\n            raise ValueError(\n                f\"Number of z dimensions must be at least 2 (got {len(z_dims)}).\"\n            )\n\n        return z_dims\n\n    @model_validator(mode=\"after\")\n    def validate_multiscale_count(self: Self) -&gt; Self:\n        \"\"\"\n        Validate the multiscale count.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.multiscale_count &lt; 1 or self.multiscale_count &gt; len(self.z_dims) + 1:\n            raise ValueError(\n                f\"Multiscale count must be 1 for LC off or less or equal to the number\"\n                f\" of Z dims + 1 (got {self.multiscale_count} and {len(self.z_dims)}).\"\n            )\n        return self\n\n    def set_3D(self, is_3D: bool) -&gt; None:\n        \"\"\"\n        Set 3D model by setting the `conv_dims` parameters.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        \"\"\"\n        if is_3D:\n            self.conv_dims = 3\n        else:\n            self.conv_dims = 2\n\n    def is_3D(self) -&gt; bool:\n        \"\"\"\n        Return whether the model is 3D or not.\n\n        Returns\n        -------\n        bool\n            Whether the model is 3D or not.\n        \"\"\"\n        return len(self.input_shape) == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.decoder_conv_strides","title":"<code>decoder_conv_strides = Field(default=[2, 2], validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dimensions (2D or 3D) of the convolutional layers.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.input_shape","title":"<code>input_shape = Field(default=(64, 64), validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Shape of the input patch (Z, Y, X) or (Y, X) if the data is 2D.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.is_3D","title":"<code>is_3D()</code>","text":"<p>Return whether the model is 3D or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the model is 3D or not.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>def is_3D(self) -&gt; bool:\n    \"\"\"\n    Return whether the model is 3D or not.\n\n    Returns\n    -------\n    bool\n        Whether the model is 3D or not.\n    \"\"\"\n    return len(self.input_shape) == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.set_3D","title":"<code>set_3D(is_3D)</code>","text":"<p>Set 3D model by setting the <code>conv_dims</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>def set_3D(self, is_3D: bool) -&gt; None:\n    \"\"\"\n    Set 3D model by setting the `conv_dims` parameters.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    \"\"\"\n    if is_3D:\n        self.conv_dims = 3\n    else:\n        self.conv_dims = 2\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_conv_strides","title":"<code>validate_conv_strides()</code>","text":"<p>Validate the convolutional strides.</p> <p>Returns:</p> Type Description <code>list</code> <p>Validated strides.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of strides is not 2.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_conv_strides(self: Self) -&gt; Self:\n    \"\"\"\n    Validate the convolutional strides.\n\n    Returns\n    -------\n    list\n        Validated strides.\n\n    Raises\n    ------\n    ValueError\n        If the number of strides is not 2.\n    \"\"\"\n    if len(self.encoder_conv_strides) &lt; 2 or len(self.encoder_conv_strides) &gt; 3:\n        raise ValueError(\n            f\"Strides must be 2 or 3 (got {len(self.encoder_conv_strides)}).\"\n        )\n\n    if len(self.decoder_conv_strides) &lt; 2 or len(self.decoder_conv_strides) &gt; 3:\n        raise ValueError(\n            f\"Strides must be 2 or 3 (got {len(self.decoder_conv_strides)}).\"\n        )\n\n    # adding 1 to encoder strides for the number of input channels\n    if len(self.input_shape) != len(self.encoder_conv_strides):\n        raise ValueError(\n            f\"Input dimensions must be equal to the number of encoder conv strides\"\n            f\" (got {len(self.input_shape)} and {len(self.encoder_conv_strides)}).\"\n        )\n\n    if len(self.encoder_conv_strides) &lt; len(self.decoder_conv_strides):\n        raise ValueError(\n            f\"Decoder can't be 3D when encoder is 2D (got\"\n            f\" {len(self.encoder_conv_strides)} and\"\n            f\"{len(self.decoder_conv_strides)}).\"\n        )\n\n    if any(s &lt; 1 for s in self.encoder_conv_strides) or any(\n        s &lt; 1 for s in self.decoder_conv_strides\n    ):\n        raise ValueError(\n            f\"All strides must be greater or equal to 1\"\n            f\"(got {self.encoder_conv_strides} and {self.decoder_conv_strides}).\"\n        )\n    # TODO: validate max stride size ?\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_decoder_even","title":"<code>validate_decoder_even(decoder_n_filters)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_n_filters</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"decoder_n_filters\")\n@classmethod\ndef validate_decoder_even(cls, decoder_n_filters: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    decoder_n_filters : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if decoder_n_filters % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {decoder_n_filters}).\"\n        )\n\n    return decoder_n_filters\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_encoder_even","title":"<code>validate_encoder_even(encoder_n_filters)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_n_filters</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"encoder_n_filters\")\n@classmethod\ndef validate_encoder_even(cls, encoder_n_filters: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    encoder_n_filters : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if encoder_n_filters % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {encoder_n_filters}).\"\n        )\n\n    return encoder_n_filters\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_input_shape","title":"<code>validate_input_shape(input_shape)</code>  <code>classmethod</code>","text":"<p>Validate the input shape.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>list</code> <p>Shape of the input patch.</p> required <p>Returns:</p> Type Description <code>list</code> <p>Validated input shape.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of dimensions is not 3 or 4.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"input_shape\")\n@classmethod\ndef validate_input_shape(cls, input_shape: list) -&gt; list:\n    \"\"\"\n    Validate the input shape.\n\n    Parameters\n    ----------\n    input_shape : list\n        Shape of the input patch.\n\n    Returns\n    -------\n    list\n        Validated input shape.\n\n    Raises\n    ------\n    ValueError\n        If the number of dimensions is not 3 or 4.\n    \"\"\"\n    if len(input_shape) &lt; 2 or len(input_shape) &gt; 3:\n        raise ValueError(\n            f\"Number of input dimensions must be 2 for 2D data 3 for 3D\"\n            f\"(got {len(input_shape)}).\"\n        )\n\n    if any(s &lt; 1 for s in input_shape):\n        raise ValueError(\n            f\"Input shape must be greater than 1 in all dimensions\"\n            f\"(got {input_shape}).\"\n        )\n\n    if any(s &lt; 64 for s in input_shape[-2:]):\n        raise ValueError(\n            f\"Input shape must be greater or equal to 64 in XY dimensions\"\n            f\"(got {input_shape}).\"\n        )\n\n    return input_shape\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_multiscale_count","title":"<code>validate_multiscale_count()</code>","text":"<p>Validate the multiscale count.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_multiscale_count(self: Self) -&gt; Self:\n    \"\"\"\n    Validate the multiscale count.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.multiscale_count &lt; 1 or self.multiscale_count &gt; len(self.z_dims) + 1:\n        raise ValueError(\n            f\"Multiscale count must be 1 for LC off or less or equal to the number\"\n            f\" of Z dims + 1 (got {self.multiscale_count} and {len(self.z_dims)}).\"\n        )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_z_dims","title":"<code>validate_z_dims(z_dims)</code>","text":"<p>Validate the z_dims.</p> <p>Parameters:</p> Name Type Description Default <code>z_dims</code> <code>tuple</code> <p>Tuple of z dimensions.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Validated z dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of z dimensions is not 4.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"z_dims\")\ndef validate_z_dims(cls, z_dims: tuple) -&gt; tuple:\n    \"\"\"\n    Validate the z_dims.\n\n    Parameters\n    ----------\n    z_dims : tuple\n        Tuple of z dimensions.\n\n    Returns\n    -------\n    tuple\n        Validated z dimensions.\n\n    Raises\n    ------\n    ValueError\n        If the number of z dimensions is not 4.\n    \"\"\"\n    if len(z_dims) &lt; 2:\n        raise ValueError(\n            f\"Number of z dimensions must be at least 2 (got {len(z_dims)}).\"\n        )\n\n    return z_dims\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/","title":"unet_model","text":"<p>UNet Pydantic model.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel","title":"<code>UNetModel</code>","text":"<p>               Bases: <code>ArchitectureModel</code></p> <p>Pydantic model for a N2V(2)-compatible UNet.</p> <p>Attributes:</p> Name Type Description <code>depth</code> <code>int</code> <p>Depth of the model, between 1 and 10 (default 2).</p> <code>num_channels_init</code> <code>int</code> <p>Number of filters of the first level of the network, should be even and minimum 8 (default 96).</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>class UNetModel(ArchitectureModel):\n    \"\"\"\n    Pydantic model for a N2V(2)-compatible UNet.\n\n    Attributes\n    ----------\n    depth : int\n        Depth of the model, between 1 and 10 (default 2).\n    num_channels_init : int\n        Number of filters of the first level of the network, should be even\n        and minimum 8 (default 96).\n    \"\"\"\n\n    # pydantic model config\n    model_config = ConfigDict(validate_assignment=True)\n\n    # discriminator used for choosing the pydantic model in Model\n    architecture: Literal[\"UNet\"]\n    \"\"\"Name of the architecture.\"\"\"\n\n    # parameters\n    # validate_defaults allow ignoring default values in the dump if they were not set\n    conv_dims: Literal[2, 3] = Field(default=2, validate_default=True)\n    \"\"\"Dimensions (2D or 3D) of the convolutional layers.\"\"\"\n\n    num_classes: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Number of classes or channels in the model output.\"\"\"\n\n    in_channels: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Number of channels in the input to the model.\"\"\"\n\n    depth: int = Field(default=2, ge=1, le=10, validate_default=True)\n    \"\"\"Number of levels in the UNet.\"\"\"\n\n    num_channels_init: int = Field(default=32, ge=8, le=1024, validate_default=True)\n    \"\"\"Number of convolutional filters in the first layer of the UNet.\"\"\"\n\n    # TODO we are not using this, so why make it a choice?\n    final_activation: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\"\n    ] = Field(default=\"None\", validate_default=True)\n    \"\"\"Final activation function.\"\"\"\n\n    n2v2: bool = Field(default=False, validate_default=True)\n    \"\"\"Whether to use N2V2 architecture modifications, with blur pool layers and fewer\n    skip connections.\n    \"\"\"\n\n    independent_channels: bool = Field(default=True, validate_default=True)\n    \"\"\"Whether information is processed independently in each channel, used to train\n    channels independently.\"\"\"\n\n    use_batch_norm: bool = Field(default=True, validate_default=True)\n    \"\"\"Whether to use batch normalization in the model.\"\"\"\n\n    @field_validator(\"num_channels_init\")\n    @classmethod\n    def validate_num_channels_init(cls, num_channels_init: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        num_channels_init : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if num_channels_init % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {num_channels_init}).\"\n            )\n\n        return num_channels_init\n\n    def set_3D(self, is_3D: bool) -&gt; None:\n        \"\"\"\n        Set 3D model by setting the `conv_dims` parameters.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        \"\"\"\n        if is_3D:\n            self.conv_dims = 3\n        else:\n            self.conv_dims = 2\n\n    def is_3D(self) -&gt; bool:\n        \"\"\"\n        Return whether the model is 3D or not.\n\n        Returns\n        -------\n        bool\n            Whether the model is 3D or not.\n        \"\"\"\n        return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":"<p>Name of the architecture.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.conv_dims","title":"<code>conv_dims = Field(default=2, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dimensions (2D or 3D) of the convolutional layers.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.depth","title":"<code>depth = Field(default=2, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of levels in the UNet.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.final_activation","title":"<code>final_activation = Field(default='None', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Final activation function.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.in_channels","title":"<code>in_channels = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels in the input to the model.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.independent_channels","title":"<code>independent_channels = Field(default=True, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether information is processed independently in each channel, used to train channels independently.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.n2v2","title":"<code>n2v2 = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use N2V2 architecture modifications, with blur pool layers and fewer skip connections.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.num_channels_init","title":"<code>num_channels_init = Field(default=32, ge=8, le=1024, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of convolutional filters in the first layer of the UNet.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.num_classes","title":"<code>num_classes = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of classes or channels in the model output.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.use_batch_norm","title":"<code>use_batch_norm = Field(default=True, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use batch normalization in the model.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.is_3D","title":"<code>is_3D()</code>","text":"<p>Return whether the model is 3D or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the model is 3D or not.</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>def is_3D(self) -&gt; bool:\n    \"\"\"\n    Return whether the model is 3D or not.\n\n    Returns\n    -------\n    bool\n        Whether the model is 3D or not.\n    \"\"\"\n    return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.set_3D","title":"<code>set_3D(is_3D)</code>","text":"<p>Set 3D model by setting the <code>conv_dims</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>def set_3D(self, is_3D: bool) -&gt; None:\n    \"\"\"\n    Set 3D model by setting the `conv_dims` parameters.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    \"\"\"\n    if is_3D:\n        self.conv_dims = 3\n    else:\n        self.conv_dims = 2\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.validate_num_channels_init","title":"<code>validate_num_channels_init(num_channels_init)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>num_channels_init</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>@field_validator(\"num_channels_init\")\n@classmethod\ndef validate_num_channels_init(cls, num_channels_init: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    num_channels_init : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if num_channels_init % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {num_channels_init}).\"\n        )\n\n    return num_channels_init\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/","title":"data_model","text":"<p>Data configuration.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.Float","title":"<code>Float = Annotated[float, PlainSerializer(np_float_to_scientific_str, return_type=str)]</code>  <code>module-attribute</code>","text":"<p>Annotated float type, used to serialize floats to strings.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig","title":"<code>DataConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data configuration.</p> <p>If std is specified, mean must be specified as well. Note that setting the std first and then the mean (if they were both <code>None</code> before) will raise a validation error. Prefer instead <code>set_mean_and_std</code> to set both at once. Means and stds are expected to be lists of floats, one for each channel. For supervised tasks, the mean and std of the target could be different from the input data.</p> <p>All supported transforms are defined in the SupportedTransform enum.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; data = DataConfig(\n...     data_type=\"array\", # defined in SupportedData\n...     patch_size=[128, 128],\n...     batch_size=4,\n...     axes=\"YX\"\n... )\n</code></pre> <p>To change the image_means and image_stds of the data:</p> <pre><code>&gt;&gt;&gt; data.set_means_and_stds(image_means=[214.3], image_stds=[84.5])\n</code></pre> <p>One can pass also a list of transformations, by keyword, using the SupportedTransform value:</p> <pre><code>&gt;&gt;&gt; from careamics.config.support import SupportedTransform\n&gt;&gt;&gt; data = DataConfig(\n...     data_type=\"tiff\",\n...     patch_size=[128, 128],\n...     batch_size=4,\n...     axes=\"YX\",\n...     transforms=[\n...         {\n...             \"name\": \"XYFlip\",\n...         }\n...     ]\n... )\n</code></pre> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>class DataConfig(BaseModel):\n    \"\"\"Data configuration.\n\n    If std is specified, mean must be specified as well. Note that setting the std first\n    and then the mean (if they were both `None` before) will raise a validation error.\n    Prefer instead `set_mean_and_std` to set both at once. Means and stds are expected\n    to be lists of floats, one for each channel. For supervised tasks, the mean and std\n    of the target could be different from the input data.\n\n    All supported transforms are defined in the SupportedTransform enum.\n\n    Examples\n    --------\n    Minimum example:\n\n    &gt;&gt;&gt; data = DataConfig(\n    ...     data_type=\"array\", # defined in SupportedData\n    ...     patch_size=[128, 128],\n    ...     batch_size=4,\n    ...     axes=\"YX\"\n    ... )\n\n    To change the image_means and image_stds of the data:\n    &gt;&gt;&gt; data.set_means_and_stds(image_means=[214.3], image_stds=[84.5])\n\n    One can pass also a list of transformations, by keyword, using the\n    SupportedTransform value:\n    &gt;&gt;&gt; from careamics.config.support import SupportedTransform\n    &gt;&gt;&gt; data = DataConfig(\n    ...     data_type=\"tiff\",\n    ...     patch_size=[128, 128],\n    ...     batch_size=4,\n    ...     axes=\"YX\",\n    ...     transforms=[\n    ...         {\n    ...             \"name\": \"XYFlip\",\n    ...         }\n    ...     ]\n    ... )\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Dataset configuration\n    data_type: Literal[\"array\", \"tiff\", \"czi\", \"custom\"]\n    \"\"\"Type of input data, numpy.ndarray (array) or paths (tiff, czi, and custom), as\n    defined in SupportedData.\"\"\"\n\n    axes: str\n    \"\"\"Axes of the data, as defined in SupportedAxes.\"\"\"\n\n    patch_size: Union[list[int]] = Field(..., min_length=2, max_length=3)\n    \"\"\"Patch size, as used during training.\"\"\"\n\n    batch_size: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Batch size for training.\"\"\"\n\n    # Optional fields\n    image_means: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Means of the data across channels, used for normalization.\"\"\"\n\n    image_stds: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Standard deviations of the data across channels, used for normalization.\"\"\"\n\n    target_means: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Means of the target data across channels, used for normalization.\"\"\"\n\n    target_stds: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Standard deviations of the target data across channels, used for\n    normalization.\"\"\"\n\n    transforms: Sequence[Union[XYFlipModel, XYRandomRotate90Model]] = Field(\n        default=[\n            XYFlipModel(),\n            XYRandomRotate90Model(),\n        ],\n        validate_default=True,\n    )\n    \"\"\"List of transformations to apply to the data, available transforms are defined\n    in SupportedTransform.\"\"\"\n\n    train_dataloader_params: dict[str, Any] = Field(\n        default={\"shuffle\": True}, validate_default=True\n    )\n    \"\"\"Dictionary of PyTorch training dataloader parameters. The dataloader parameters,\n    should include the `shuffle` key, which is set to `True` by default. We strongly\n    recommend to keep it as `True` to ensure the best training results.\"\"\"\n\n    val_dataloader_params: dict[str, Any] = Field(default={}, validate_default=True)\n    \"\"\"Dictionary of PyTorch validation dataloader parameters.\"\"\"\n\n    @field_validator(\"patch_size\")\n    @classmethod\n    def all_elements_power_of_2_minimum_8(\n        cls, patch_list: Union[list[int]]\n    ) -&gt; Union[list[int]]:\n        \"\"\"\n        Validate patch size.\n\n        Patch size must be powers of 2 and minimum 8.\n\n        Parameters\n        ----------\n        patch_list : list of int\n            Patch size.\n\n        Returns\n        -------\n        list of int\n            Validated patch size.\n\n        Raises\n        ------\n        ValueError\n            If the patch size is smaller than 8.\n        ValueError\n            If the patch size is not a power of 2.\n        \"\"\"\n        patch_size_ge_than_8_power_of_2(patch_list)\n\n        return patch_list\n\n    @field_validator(\"axes\")\n    @classmethod\n    def axes_valid(cls, axes: str) -&gt; str:\n        \"\"\"\n        Validate axes.\n\n        Axes must:\n        - be a combination of 'STCZYX'\n        - not contain duplicates\n        - contain at least 2 contiguous axes: X and Y\n        - contain at most 4 axes\n        - not contain both S and T axes\n\n        Parameters\n        ----------\n        axes : str\n            Axes to validate.\n\n        Returns\n        -------\n        str\n            Validated axes.\n\n        Raises\n        ------\n        ValueError\n            If axes are not valid.\n        \"\"\"\n        # Validate axes\n        check_axes_validity(axes)\n\n        return axes\n\n    @field_validator(\"train_dataloader_params\", \"val_dataloader_params\", mode=\"before\")\n    @classmethod\n    def set_default_pin_memory(\n        cls, dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Set default pin_memory for dataloader parameters if not provided.\n\n        - If 'pin_memory' is not set, it defaults to True if CUDA is available.\n\n        Parameters\n        ----------\n        dataloader_params : dict of {str: Any}\n            The dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The dataloader parameters with pin_memory default applied.\n        \"\"\"\n        if \"pin_memory\" not in dataloader_params:\n            import torch\n\n            dataloader_params[\"pin_memory\"] = torch.cuda.is_available()\n\n        return dataloader_params\n\n    @field_validator(\"train_dataloader_params\", mode=\"before\")\n    @classmethod\n    def set_default_train_workers(\n        cls, dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Set default num_workers for training dataloader if not provided.\n\n        - If 'num_workers' is not set, it defaults to the number of available CPU cores.\n\n        Parameters\n        ----------\n        dataloader_params : dict of {str: Any}\n            The training dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The dataloader parameters with num_workers default applied.\n        \"\"\"\n        if \"num_workers\" not in dataloader_params:\n            # Use 0 workers during tests, otherwise use all available CPU cores\n            if \"pytest\" in sys.modules:\n                dataloader_params[\"num_workers\"] = 0\n            else:\n                dataloader_params[\"num_workers\"] = os.cpu_count()\n\n        return dataloader_params\n\n    @model_validator(mode=\"after\")\n    def set_val_workers_to_match_train(self: Self) -&gt; Self:\n        \"\"\"\n        Set validation dataloader num_workers to match training dataloader.\n\n        If num_workers is not specified in val_dataloader_params, it will be set to the\n        same value as train_dataloader_params[\"num_workers\"].\n\n        Returns\n        -------\n        Self\n            Validated data model with synchronized num_workers.\n        \"\"\"\n        if \"num_workers\" not in self.val_dataloader_params:\n            self.val_dataloader_params[\"num_workers\"] = self.train_dataloader_params[\n                \"num_workers\"\n            ]\n        return self\n\n    @field_validator(\"train_dataloader_params\")\n    @classmethod\n    def shuffle_train_dataloader(\n        cls, train_dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Validate that \"shuffle\" is included in the training dataloader params.\n\n        A warning will be raised if `shuffle=False`.\n\n        Parameters\n        ----------\n        train_dataloader_params : dict of {str: Any}\n            The training dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The validated training dataloader parameters.\n\n        Raises\n        ------\n        ValueError\n            If \"shuffle\" is not included in the training dataloader params.\n        \"\"\"\n        if \"shuffle\" not in train_dataloader_params:\n            raise ValueError(\n                \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n            )\n        elif (\"shuffle\" in train_dataloader_params) and (\n            not train_dataloader_params[\"shuffle\"]\n        ):\n            warn(\n                \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n                \"the training dataloader and may lead to lower quality results.\",\n                stacklevel=1,\n            )\n        return train_dataloader_params\n\n    @model_validator(mode=\"after\")\n    def std_only_with_mean(self: Self) -&gt; Self:\n        \"\"\"\n        Check that mean and std are either both None, or both specified.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If std is not None and mean is None.\n        \"\"\"\n        # check that mean and std are either both None, or both specified\n        if (self.image_means and not self.image_stds) or (\n            self.image_stds and not self.image_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified.\"\n            )\n\n        elif (self.image_means is not None and self.image_stds is not None) and (\n            len(self.image_means) != len(self.image_stds)\n        ):\n            raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n        if (self.target_means and not self.target_stds) or (\n            self.target_stds and not self.target_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified \"\n            )\n\n        elif self.target_means is not None and self.target_stds is not None:\n            if len(self.target_means) != len(self.target_stds):\n                raise ValueError(\n                    \"Mean and std must be either both None, or both specified for each \"\n                    \"target channel.\"\n                )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_dimensions(self: Self) -&gt; Self:\n        \"\"\"\n        Validate 2D/3D dimensions between axes, patch size and transforms.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If the transforms are not valid.\n        \"\"\"\n        if \"Z\" in self.axes:\n            if len(self.patch_size) != 3:\n                raise ValueError(\n                    f\"Patch size must have 3 dimensions if the data is 3D \"\n                    f\"({self.axes}).\"\n                )\n\n        else:\n            if len(self.patch_size) != 2:\n                raise ValueError(\n                    f\"Patch size must have 3 dimensions if the data is 3D \"\n                    f\"({self.axes}).\"\n                )\n\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def _update(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Update multiple arguments at once.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Keyword arguments to update.\n        \"\"\"\n        self.__dict__.update(kwargs)\n        self.__class__.model_validate(self.__dict__)\n\n    def set_means_and_stds(\n        self,\n        image_means: Union[NDArray, tuple, list, None],\n        image_stds: Union[NDArray, tuple, list, None],\n        target_means: Union[NDArray, tuple, list, None] | None = None,\n        target_stds: Union[NDArray, tuple, list, None] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Set mean and standard deviation of the data across channels.\n\n        This method should be used instead setting the fields directly, as it would\n        otherwise trigger a validation error.\n\n        Parameters\n        ----------\n        image_means : numpy.ndarray, tuple or list\n            Mean values for normalization.\n        image_stds : numpy.ndarray, tuple or list\n            Standard deviation values for normalization.\n        target_means : numpy.ndarray, tuple or list, optional\n            Target mean values for normalization, by default ().\n        target_stds : numpy.ndarray, tuple or list, optional\n            Target standard deviation values for normalization, by default ().\n        \"\"\"\n        # make sure we pass a list\n        if image_means is not None:\n            image_means = list(image_means)\n        if image_stds is not None:\n            image_stds = list(image_stds)\n        if target_means is not None:\n            target_means = list(target_means)\n        if target_stds is not None:\n            target_stds = list(target_stds)\n\n        self._update(\n            image_means=image_means,\n            image_stds=image_stds,\n            target_means=target_means,\n            target_stds=target_stds,\n        )\n\n    def set_3D(self, axes: str, patch_size: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D parameters.\n\n        Parameters\n        ----------\n        axes : str\n            Axes.\n        patch_size : list of int\n            Patch size.\n        \"\"\"\n        self._update(axes=axes, patch_size=patch_size)\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.axes","title":"<code>axes</code>  <code>instance-attribute</code>","text":"<p>Axes of the data, as defined in SupportedAxes.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.batch_size","title":"<code>batch_size = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for training.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of input data, numpy.ndarray (array) or paths (tiff, czi, and custom), as defined in SupportedData.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.image_means","title":"<code>image_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.image_stds","title":"<code>image_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.patch_size","title":"<code>patch_size = Field(..., min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Patch size, as used during training.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.target_means","title":"<code>target_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.target_stds","title":"<code>target_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.train_dataloader_params","title":"<code>train_dataloader_params = Field(default={'shuffle': True}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch training dataloader parameters. The dataloader parameters, should include the <code>shuffle</code> key, which is set to <code>True</code> by default. We strongly recommend to keep it as <code>True</code> to ensure the best training results.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.transforms","title":"<code>transforms = Field(default=[XYFlipModel(), XYRandomRotate90Model()], validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations to apply to the data, available transforms are defined in SupportedTransform.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.val_dataloader_params","title":"<code>val_dataloader_params = Field(default={}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch validation dataloader parameters.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.all_elements_power_of_2_minimum_8","title":"<code>all_elements_power_of_2_minimum_8(patch_list)</code>  <code>classmethod</code>","text":"<p>Validate patch size.</p> <p>Patch size must be powers of 2 and minimum 8.</p> <p>Parameters:</p> Name Type Description Default <code>patch_list</code> <code>list of int</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list of int</code> <p>Validated patch size.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"patch_size\")\n@classmethod\ndef all_elements_power_of_2_minimum_8(\n    cls, patch_list: Union[list[int]]\n) -&gt; Union[list[int]]:\n    \"\"\"\n    Validate patch size.\n\n    Patch size must be powers of 2 and minimum 8.\n\n    Parameters\n    ----------\n    patch_list : list of int\n        Patch size.\n\n    Returns\n    -------\n    list of int\n        Validated patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    patch_size_ge_than_8_power_of_2(patch_list)\n\n    return patch_list\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.axes_valid","title":"<code>axes_valid(axes)</code>  <code>classmethod</code>","text":"<p>Validate axes.</p> <p>Axes must: - be a combination of 'STCZYX' - not contain duplicates - contain at least 2 contiguous axes: X and Y - contain at most 4 axes - not contain both S and T axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axes are not valid.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"axes\")\n@classmethod\ndef axes_valid(cls, axes: str) -&gt; str:\n    \"\"\"\n    Validate axes.\n\n    Axes must:\n    - be a combination of 'STCZYX'\n    - not contain duplicates\n    - contain at least 2 contiguous axes: X and Y\n    - contain at most 4 axes\n    - not contain both S and T axes\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n\n    Returns\n    -------\n    str\n        Validated axes.\n\n    Raises\n    ------\n    ValueError\n        If axes are not valid.\n    \"\"\"\n    # Validate axes\n    check_axes_validity(axes)\n\n    return axes\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_3D","title":"<code>set_3D(axes, patch_size)</code>","text":"<p>Set 3D parameters.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes.</p> required <code>patch_size</code> <code>list of int</code> <p>Patch size.</p> required Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def set_3D(self, axes: str, patch_size: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D parameters.\n\n    Parameters\n    ----------\n    axes : str\n        Axes.\n    patch_size : list of int\n        Patch size.\n    \"\"\"\n    self._update(axes=axes, patch_size=patch_size)\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_default_pin_memory","title":"<code>set_default_pin_memory(dataloader_params)</code>  <code>classmethod</code>","text":"<p>Set default pin_memory for dataloader parameters if not provided.</p> <ul> <li>If 'pin_memory' is not set, it defaults to True if CUDA is available.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataloader_params</code> <code>dict of {str: Any}</code> <p>The dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The dataloader parameters with pin_memory default applied.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\", \"val_dataloader_params\", mode=\"before\")\n@classmethod\ndef set_default_pin_memory(\n    cls, dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Set default pin_memory for dataloader parameters if not provided.\n\n    - If 'pin_memory' is not set, it defaults to True if CUDA is available.\n\n    Parameters\n    ----------\n    dataloader_params : dict of {str: Any}\n        The dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The dataloader parameters with pin_memory default applied.\n    \"\"\"\n    if \"pin_memory\" not in dataloader_params:\n        import torch\n\n        dataloader_params[\"pin_memory\"] = torch.cuda.is_available()\n\n    return dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_default_train_workers","title":"<code>set_default_train_workers(dataloader_params)</code>  <code>classmethod</code>","text":"<p>Set default num_workers for training dataloader if not provided.</p> <ul> <li>If 'num_workers' is not set, it defaults to the number of available CPU cores.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataloader_params</code> <code>dict of {str: Any}</code> <p>The training dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The dataloader parameters with num_workers default applied.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\", mode=\"before\")\n@classmethod\ndef set_default_train_workers(\n    cls, dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Set default num_workers for training dataloader if not provided.\n\n    - If 'num_workers' is not set, it defaults to the number of available CPU cores.\n\n    Parameters\n    ----------\n    dataloader_params : dict of {str: Any}\n        The training dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The dataloader parameters with num_workers default applied.\n    \"\"\"\n    if \"num_workers\" not in dataloader_params:\n        # Use 0 workers during tests, otherwise use all available CPU cores\n        if \"pytest\" in sys.modules:\n            dataloader_params[\"num_workers\"] = 0\n        else:\n            dataloader_params[\"num_workers\"] = os.cpu_count()\n\n    return dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_means_and_stds","title":"<code>set_means_and_stds(image_means, image_stds, target_means=None, target_stds=None)</code>","text":"<p>Set mean and standard deviation of the data across channels.</p> <p>This method should be used instead setting the fields directly, as it would otherwise trigger a validation error.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>(ndarray, tuple or list)</code> <p>Mean values for normalization.</p> required <code>image_stds</code> <code>(ndarray, tuple or list)</code> <p>Standard deviation values for normalization.</p> required <code>target_means</code> <code>(ndarray, tuple or list)</code> <p>Target mean values for normalization, by default ().</p> <code>None</code> <code>target_stds</code> <code>(ndarray, tuple or list)</code> <p>Target standard deviation values for normalization, by default ().</p> <code>None</code> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def set_means_and_stds(\n    self,\n    image_means: Union[NDArray, tuple, list, None],\n    image_stds: Union[NDArray, tuple, list, None],\n    target_means: Union[NDArray, tuple, list, None] | None = None,\n    target_stds: Union[NDArray, tuple, list, None] | None = None,\n) -&gt; None:\n    \"\"\"\n    Set mean and standard deviation of the data across channels.\n\n    This method should be used instead setting the fields directly, as it would\n    otherwise trigger a validation error.\n\n    Parameters\n    ----------\n    image_means : numpy.ndarray, tuple or list\n        Mean values for normalization.\n    image_stds : numpy.ndarray, tuple or list\n        Standard deviation values for normalization.\n    target_means : numpy.ndarray, tuple or list, optional\n        Target mean values for normalization, by default ().\n    target_stds : numpy.ndarray, tuple or list, optional\n        Target standard deviation values for normalization, by default ().\n    \"\"\"\n    # make sure we pass a list\n    if image_means is not None:\n        image_means = list(image_means)\n    if image_stds is not None:\n        image_stds = list(image_stds)\n    if target_means is not None:\n        target_means = list(target_means)\n    if target_stds is not None:\n        target_stds = list(target_stds)\n\n    self._update(\n        image_means=image_means,\n        image_stds=image_stds,\n        target_means=target_means,\n        target_stds=target_stds,\n    )\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_val_workers_to_match_train","title":"<code>set_val_workers_to_match_train()</code>","text":"<p>Set validation dataloader num_workers to match training dataloader.</p> <p>If num_workers is not specified in val_dataloader_params, it will be set to the same value as train_dataloader_params[\"num_workers\"].</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model with synchronized num_workers.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_val_workers_to_match_train(self: Self) -&gt; Self:\n    \"\"\"\n    Set validation dataloader num_workers to match training dataloader.\n\n    If num_workers is not specified in val_dataloader_params, it will be set to the\n    same value as train_dataloader_params[\"num_workers\"].\n\n    Returns\n    -------\n    Self\n        Validated data model with synchronized num_workers.\n    \"\"\"\n    if \"num_workers\" not in self.val_dataloader_params:\n        self.val_dataloader_params[\"num_workers\"] = self.train_dataloader_params[\n            \"num_workers\"\n        ]\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.shuffle_train_dataloader","title":"<code>shuffle_train_dataloader(train_dataloader_params)</code>  <code>classmethod</code>","text":"<p>Validate that \"shuffle\" is included in the training dataloader params.</p> <p>A warning will be raised if <code>shuffle=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader_params</code> <code>dict of {str: Any}</code> <p>The training dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The validated training dataloader parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \"shuffle\" is not included in the training dataloader params.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\")\n@classmethod\ndef shuffle_train_dataloader(\n    cls, train_dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Validate that \"shuffle\" is included in the training dataloader params.\n\n    A warning will be raised if `shuffle=False`.\n\n    Parameters\n    ----------\n    train_dataloader_params : dict of {str: Any}\n        The training dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The validated training dataloader parameters.\n\n    Raises\n    ------\n    ValueError\n        If \"shuffle\" is not included in the training dataloader params.\n    \"\"\"\n    if \"shuffle\" not in train_dataloader_params:\n        raise ValueError(\n            \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n        )\n    elif (\"shuffle\" in train_dataloader_params) and (\n        not train_dataloader_params[\"shuffle\"]\n    ):\n        warn(\n            \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n            \"the training dataloader and may lead to lower quality results.\",\n            stacklevel=1,\n        )\n    return train_dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.std_only_with_mean","title":"<code>std_only_with_mean()</code>","text":"<p>Check that mean and std are either both None, or both specified.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If std is not None and mean is None.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef std_only_with_mean(self: Self) -&gt; Self:\n    \"\"\"\n    Check that mean and std are either both None, or both specified.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If std is not None and mean is None.\n    \"\"\"\n    # check that mean and std are either both None, or both specified\n    if (self.image_means and not self.image_stds) or (\n        self.image_stds and not self.image_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified.\"\n        )\n\n    elif (self.image_means is not None and self.image_stds is not None) and (\n        len(self.image_means) != len(self.image_stds)\n    ):\n        raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n    if (self.target_means and not self.target_stds) or (\n        self.target_stds and not self.target_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified \"\n        )\n\n    elif self.target_means is not None and self.target_stds is not None:\n        if len(self.target_means) != len(self.target_stds):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified for each \"\n                \"target channel.\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.validate_dimensions","title":"<code>validate_dimensions()</code>","text":"<p>Validate 2D/3D dimensions between axes, patch size and transforms.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transforms are not valid.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dimensions(self: Self) -&gt; Self:\n    \"\"\"\n    Validate 2D/3D dimensions between axes, patch size and transforms.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If the transforms are not valid.\n    \"\"\"\n    if \"Z\" in self.axes:\n        if len(self.patch_size) != 3:\n            raise ValueError(\n                f\"Patch size must have 3 dimensions if the data is 3D \"\n                f\"({self.axes}).\"\n            )\n\n    else:\n        if len(self.patch_size) != 2:\n            raise ValueError(\n                f\"Patch size must have 3 dimensions if the data is 3D \"\n                f\"({self.axes}).\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.np_float_to_scientific_str","title":"<code>np_float_to_scientific_str(x)</code>","text":"<p>Return a string scientific representation of a float.</p> <p>In particular, this method is used to serialize floats to strings, allowing numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Input value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Scientific string representation of the input value.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def np_float_to_scientific_str(x: float) -&gt; str:\n    \"\"\"Return a string scientific representation of a float.\n\n    In particular, this method is used to serialize floats to strings, allowing\n    numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.\n\n    Parameters\n    ----------\n    x : float\n        Input value.\n\n    Returns\n    -------\n    str\n        Scientific string representation of the input value.\n    \"\"\"\n    return np.format_float_scientific(x, precision=7)\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/","title":"ng_data_model","text":"<p>Data configuration.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.CoordFilters","title":"<code>CoordFilters = Union[MaskFilterModel]</code>  <code>module-attribute</code>","text":"<p>Coordinate filters.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.Float","title":"<code>Float = Annotated[float, PlainSerializer(np_float_to_scientific_str, return_type=str)]</code>  <code>module-attribute</code>","text":"<p>Annotated float type, used to serialize floats to strings.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.PatchFilters","title":"<code>PatchFilters = Union[MaxFilterModel, MeanSTDFilterModel, ShannonFilterModel]</code>  <code>module-attribute</code>","text":"<p>Patch filters.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.PatchingStrategies","title":"<code>PatchingStrategies = Union[RandomPatchingModel, TiledPatchingModel, WholePatchingModel]</code>  <code>module-attribute</code>","text":"<p>Patching strategies.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig","title":"<code>NGDataConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Next-Generation Dataset configuration.</p> <p>NGDataConfig are used for both training and prediction, with the patching strategy determining how the data is processed. Note that <code>random</code> is the only patching strategy compatible with training, while <code>tiled</code> and <code>whole</code> are only used for prediction.</p> <p>If std is specified, mean must be specified as well. Note that setting the std first and then the mean (if they were both <code>None</code> before) will raise a validation error. Prefer instead <code>set_means_and_stds</code> to set both at once. Means and stds are expected to be lists of floats, one for each channel. For supervised tasks, the mean and std of the target could be different from the input data.</p> <p>All supported transforms are defined in the SupportedTransform enum.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>class NGDataConfig(BaseModel):\n    \"\"\"Next-Generation Dataset configuration.\n\n    NGDataConfig are used for both training and prediction, with the patching strategy\n    determining how the data is processed. Note that `random` is the only patching\n    strategy compatible with training, while `tiled` and `whole` are only used for\n    prediction.\n\n    If std is specified, mean must be specified as well. Note that setting the std first\n    and then the mean (if they were both `None` before) will raise a validation error.\n    Prefer instead `set_means_and_stds` to set both at once. Means and stds are expected\n    to be lists of floats, one for each channel. For supervised tasks, the mean and std\n    of the target could be different from the input data.\n\n    All supported transforms are defined in the SupportedTransform enum.\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Dataset configuration\n    data_type: Literal[\"array\", \"tiff\", \"zarr\", \"custom\"]\n    \"\"\"Type of input data.\"\"\"\n\n    axes: str\n    \"\"\"Axes of the data, as defined in SupportedAxes.\"\"\"\n\n    patching: PatchingStrategies = Field(..., discriminator=\"name\")\n    \"\"\"Patching strategy to use. Note that `random` is the only supported strategy for\n    training, while `tiled` and `whole` are only used for prediction.\"\"\"\n\n    # Optional fields\n    batch_size: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Batch size for training.\"\"\"\n\n    patch_filter: PatchFilters | None = Field(default=None, discriminator=\"name\")\n    \"\"\"Patch filter to apply when using random patching. Only available during\n    training.\"\"\"\n\n    coord_filter: CoordFilters | None = Field(default=None, discriminator=\"name\")\n    \"\"\"Coordinate filter to apply when using random patching. Only available during\n    training.\"\"\"\n\n    patch_filter_patience: int = Field(default=5, ge=1)\n    \"\"\"Number of consecutive patches not passing the filter before accepting the next\n    patch.\"\"\"\n\n    image_means: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Means of the data across channels, used for normalization.\"\"\"\n\n    image_stds: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Standard deviations of the data across channels, used for normalization.\"\"\"\n\n    target_means: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Means of the target data across channels, used for normalization.\"\"\"\n\n    target_stds: list[Float] | None = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Standard deviations of the target data across channels, used for\n    normalization.\"\"\"\n\n    transforms: Sequence[Union[XYFlipModel, XYRandomRotate90Model]] = Field(\n        default=(\n            XYFlipModel(),\n            XYRandomRotate90Model(),\n        ),\n        validate_default=True,\n    )\n    \"\"\"List of transformations to apply to the data, available transforms are defined\n    in SupportedTransform.\"\"\"\n\n    train_dataloader_params: dict[str, Any] = Field(\n        default={\"shuffle\": True}, validate_default=True\n    )\n    \"\"\"Dictionary of PyTorch training dataloader parameters. The dataloader parameters,\n    should include the `shuffle` key, which is set to `True` by default. We strongly\n    recommend to keep it as `True` to ensure the best training results.\"\"\"\n\n    val_dataloader_params: dict[str, Any] = Field(default={})\n    \"\"\"Dictionary of PyTorch validation dataloader parameters.\"\"\"\n\n    test_dataloader_params: dict[str, Any] = Field(default={})\n    \"\"\"Dictionary of PyTorch test dataloader parameters.\"\"\"\n\n    seed: int | None = Field(default_factory=generate_random_seed, gt=0)\n    \"\"\"Random seed for reproducibility. If not specified, a random seed is generated.\"\"\"\n\n    @field_validator(\"axes\")\n    @classmethod\n    def axes_valid(cls, axes: str) -&gt; str:\n        \"\"\"\n        Validate axes.\n\n        Axes must:\n        - be a combination of 'STCZYX'\n        - not contain duplicates\n        - contain at least 2 contiguous axes: X and Y\n        - contain at most 4 axes\n        - not contain both S and T axes\n\n        Parameters\n        ----------\n        axes : str\n            Axes to validate.\n\n        Returns\n        -------\n        str\n            Validated axes.\n\n        Raises\n        ------\n        ValueError\n            If axes are not valid.\n        \"\"\"\n        # Validate axes\n        check_axes_validity(axes)\n\n        return axes\n\n    @field_validator(\"train_dataloader_params\")\n    @classmethod\n    def shuffle_train_dataloader(\n        cls, train_dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Validate that \"shuffle\" is included in the training dataloader params.\n\n        A warning will be raised if `shuffle=False`.\n\n        Parameters\n        ----------\n        train_dataloader_params : dict of {str: Any}\n            The training dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The validated training dataloader parameters.\n\n        Raises\n        ------\n        ValueError\n            If \"shuffle\" is not included in the training dataloader params.\n        \"\"\"\n        if \"shuffle\" not in train_dataloader_params:\n            raise ValueError(\n                \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n            )\n        elif (\"shuffle\" in train_dataloader_params) and (\n            not train_dataloader_params[\"shuffle\"]\n        ):\n            warn(\n                \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n                \"the training dataloader and may lead to lower quality results.\",\n                stacklevel=1,\n            )\n        return train_dataloader_params\n\n    @model_validator(mode=\"after\")\n    def std_only_with_mean(self: Self) -&gt; Self:\n        \"\"\"\n        Check that mean and std are either both None, or both specified.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If std is not None and mean is None.\n        \"\"\"\n        # check that mean and std are either both None, or both specified\n        if (self.image_means and not self.image_stds) or (\n            self.image_stds and not self.image_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified.\"\n            )\n\n        elif (self.image_means is not None and self.image_stds is not None) and (\n            len(self.image_means) != len(self.image_stds)\n        ):\n            raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n        if (self.target_means and not self.target_stds) or (\n            self.target_stds and not self.target_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified \"\n            )\n\n        elif self.target_means is not None and self.target_stds is not None:\n            if len(self.target_means) != len(self.target_stds):\n                raise ValueError(\n                    \"Mean and std must be either both None, or both specified for each \"\n                    \"target channel.\"\n                )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_dimensions(self: Self) -&gt; Self:\n        \"\"\"\n        Validate 2D/3D dimensions between axes and patch size.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If the patch size dimension is not compatible with the axes.\n        \"\"\"\n        if \"Z\" in self.axes:\n            if (\n                hasattr(self.patching, \"patch_size\")\n                and len(self.patching.patch_size) != 3\n            ):\n                raise ValueError(\n                    f\"`patch_size` in `patching` must have 3 dimensions if the data is\"\n                    f\" 3D, got axes {self.axes}).\"\n                )\n        else:\n            if (\n                hasattr(self.patching, \"patch_size\")\n                and len(self.patching.patch_size) != 2\n            ):\n                raise ValueError(\n                    f\"`patch_size` in `patching` must have 2 dimensions if the data is\"\n                    f\" 3D, got axes {self.axes}).\"\n                )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def propagate_seed_to_filters(self: Self) -&gt; Self:\n        \"\"\"\n        Propagate the main seed to patch and coordinate filters that support seeds.\n\n        This ensures that all filters use the same seed for reproducibility,\n        unless they already have a seed explicitly set.\n\n        Returns\n        -------\n        Self\n            Data model with propagated seeds.\n        \"\"\"\n        if self.seed is not None:\n            if self.patch_filter is not None:\n                if (\n                    hasattr(self.patch_filter, \"seed\")\n                    and self.patch_filter.seed is None\n                ):\n                    self.patch_filter.seed = self.seed\n\n            if self.coord_filter is not None:\n                if (\n                    hasattr(self.coord_filter, \"seed\")\n                    and self.coord_filter.seed is None\n                ):\n                    self.coord_filter.seed = self.seed\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def propagate_seed_to_transforms(self: Self) -&gt; Self:\n        \"\"\"\n        Propagate the main seed to all transforms that support seeds.\n\n        This ensures that all transforms use the same seed for reproducibility,\n        unless they already have a seed explicitly set.\n\n        Returns\n        -------\n        Self\n            Data model with propagated seeds.\n        \"\"\"\n        if self.seed is not None:\n            for transform in self.transforms:\n                if hasattr(transform, \"seed\") and transform.seed is None:\n                    transform.seed = self.seed\n        return self\n\n    @field_validator(\"train_dataloader_params\", \"val_dataloader_params\", mode=\"before\")\n    @classmethod\n    def set_default_pin_memory(\n        cls, dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Set default pin_memory for dataloader parameters if not provided.\n\n        - If 'pin_memory' is not set, it defaults to True if CUDA is available.\n\n        Parameters\n        ----------\n        dataloader_params : dict of {str: Any}\n            The dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The dataloader parameters with pin_memory default applied.\n        \"\"\"\n        if \"pin_memory\" not in dataloader_params:\n            import torch\n\n            dataloader_params[\"pin_memory\"] = torch.cuda.is_available()\n        return dataloader_params\n\n    @field_validator(\"train_dataloader_params\", mode=\"before\")\n    @classmethod\n    def set_default_train_workers(\n        cls, dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Set default num_workers for training dataloader if not provided.\n\n        - If 'num_workers' is not set, it defaults to the number of available CPU cores.\n\n        Parameters\n        ----------\n        dataloader_params : dict of {str: Any}\n            The training dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The dataloader parameters with num_workers default applied.\n        \"\"\"\n        if \"num_workers\" not in dataloader_params:\n            # Use 0 workers during tests, otherwise use all available CPU cores\n            if \"pytest\" in sys.modules:\n                dataloader_params[\"num_workers\"] = 0\n            else:\n                dataloader_params[\"num_workers\"] = os.cpu_count()\n\n        return dataloader_params\n\n    @model_validator(mode=\"after\")\n    def set_val_workers_to_match_train(self: Self) -&gt; Self:\n        \"\"\"\n        Set validation dataloader num_workers to match training dataloader.\n\n        If num_workers is not specified in val_dataloader_params, it will be set to the\n        same value as train_dataloader_params[\"num_workers\"].\n\n        Returns\n        -------\n        Self\n            Validated data model with synchronized num_workers.\n        \"\"\"\n        if \"num_workers\" not in self.val_dataloader_params:\n            self.val_dataloader_params[\"num_workers\"] = self.train_dataloader_params[\n                \"num_workers\"\n            ]\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def _update(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Update multiple arguments at once.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Keyword arguments to update.\n        \"\"\"\n        self.__dict__.update(kwargs)\n        self.__class__.model_validate(self.__dict__)\n\n    def set_means_and_stds(\n        self,\n        image_means: Union[NDArray, tuple, list, None],\n        image_stds: Union[NDArray, tuple, list, None],\n        target_means: Union[NDArray, tuple, list, None] | None = None,\n        target_stds: Union[NDArray, tuple, list, None] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Set mean and standard deviation of the data across channels.\n\n        This method should be used instead setting the fields directly, as it would\n        otherwise trigger a validation error.\n\n        Parameters\n        ----------\n        image_means : numpy.ndarray, tuple or list\n            Mean values for normalization.\n        image_stds : numpy.ndarray, tuple or list\n            Standard deviation values for normalization.\n        target_means : numpy.ndarray, tuple or list, optional\n            Target mean values for normalization, by default ().\n        target_stds : numpy.ndarray, tuple or list, optional\n            Target standard deviation values for normalization, by default ().\n        \"\"\"\n        # make sure we pass a list\n        if image_means is not None:\n            image_means = list(image_means)\n        if image_stds is not None:\n            image_stds = list(image_stds)\n        if target_means is not None:\n            target_means = list(target_means)\n        if target_stds is not None:\n            target_stds = list(target_stds)\n\n        self._update(\n            image_means=image_means,\n            image_stds=image_stds,\n            target_means=target_means,\n            target_stds=target_stds,\n        )\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.axes","title":"<code>axes</code>  <code>instance-attribute</code>","text":"<p>Axes of the data, as defined in SupportedAxes.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.batch_size","title":"<code>batch_size = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for training.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.coord_filter","title":"<code>coord_filter = Field(default=None, discriminator='name')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Coordinate filter to apply when using random patching. Only available during training.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of input data.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.image_means","title":"<code>image_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.image_stds","title":"<code>image_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.patch_filter","title":"<code>patch_filter = Field(default=None, discriminator='name')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Patch filter to apply when using random patching. Only available during training.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.patch_filter_patience","title":"<code>patch_filter_patience = Field(default=5, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of consecutive patches not passing the filter before accepting the next patch.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.patching","title":"<code>patching = Field(..., discriminator='name')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Patching strategy to use. Note that <code>random</code> is the only supported strategy for training, while <code>tiled</code> and <code>whole</code> are only used for prediction.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.seed","title":"<code>seed = Field(default_factory=generate_random_seed, gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed for reproducibility. If not specified, a random seed is generated.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.target_means","title":"<code>target_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.target_stds","title":"<code>target_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.test_dataloader_params","title":"<code>test_dataloader_params = Field(default={})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch test dataloader parameters.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.train_dataloader_params","title":"<code>train_dataloader_params = Field(default={'shuffle': True}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch training dataloader parameters. The dataloader parameters, should include the <code>shuffle</code> key, which is set to <code>True</code> by default. We strongly recommend to keep it as <code>True</code> to ensure the best training results.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.transforms","title":"<code>transforms = Field(default=(XYFlipModel(), XYRandomRotate90Model()), validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations to apply to the data, available transforms are defined in SupportedTransform.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.val_dataloader_params","title":"<code>val_dataloader_params = Field(default={})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch validation dataloader parameters.</p>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.axes_valid","title":"<code>axes_valid(axes)</code>  <code>classmethod</code>","text":"<p>Validate axes.</p> <p>Axes must: - be a combination of 'STCZYX' - not contain duplicates - contain at least 2 contiguous axes: X and Y - contain at most 4 axes - not contain both S and T axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axes are not valid.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@field_validator(\"axes\")\n@classmethod\ndef axes_valid(cls, axes: str) -&gt; str:\n    \"\"\"\n    Validate axes.\n\n    Axes must:\n    - be a combination of 'STCZYX'\n    - not contain duplicates\n    - contain at least 2 contiguous axes: X and Y\n    - contain at most 4 axes\n    - not contain both S and T axes\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n\n    Returns\n    -------\n    str\n        Validated axes.\n\n    Raises\n    ------\n    ValueError\n        If axes are not valid.\n    \"\"\"\n    # Validate axes\n    check_axes_validity(axes)\n\n    return axes\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.propagate_seed_to_filters","title":"<code>propagate_seed_to_filters()</code>","text":"<p>Propagate the main seed to patch and coordinate filters that support seeds.</p> <p>This ensures that all filters use the same seed for reproducibility, unless they already have a seed explicitly set.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Data model with propagated seeds.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef propagate_seed_to_filters(self: Self) -&gt; Self:\n    \"\"\"\n    Propagate the main seed to patch and coordinate filters that support seeds.\n\n    This ensures that all filters use the same seed for reproducibility,\n    unless they already have a seed explicitly set.\n\n    Returns\n    -------\n    Self\n        Data model with propagated seeds.\n    \"\"\"\n    if self.seed is not None:\n        if self.patch_filter is not None:\n            if (\n                hasattr(self.patch_filter, \"seed\")\n                and self.patch_filter.seed is None\n            ):\n                self.patch_filter.seed = self.seed\n\n        if self.coord_filter is not None:\n            if (\n                hasattr(self.coord_filter, \"seed\")\n                and self.coord_filter.seed is None\n            ):\n                self.coord_filter.seed = self.seed\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.propagate_seed_to_transforms","title":"<code>propagate_seed_to_transforms()</code>","text":"<p>Propagate the main seed to all transforms that support seeds.</p> <p>This ensures that all transforms use the same seed for reproducibility, unless they already have a seed explicitly set.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Data model with propagated seeds.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef propagate_seed_to_transforms(self: Self) -&gt; Self:\n    \"\"\"\n    Propagate the main seed to all transforms that support seeds.\n\n    This ensures that all transforms use the same seed for reproducibility,\n    unless they already have a seed explicitly set.\n\n    Returns\n    -------\n    Self\n        Data model with propagated seeds.\n    \"\"\"\n    if self.seed is not None:\n        for transform in self.transforms:\n            if hasattr(transform, \"seed\") and transform.seed is None:\n                transform.seed = self.seed\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.set_default_pin_memory","title":"<code>set_default_pin_memory(dataloader_params)</code>  <code>classmethod</code>","text":"<p>Set default pin_memory for dataloader parameters if not provided.</p> <ul> <li>If 'pin_memory' is not set, it defaults to True if CUDA is available.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataloader_params</code> <code>dict of {str: Any}</code> <p>The dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The dataloader parameters with pin_memory default applied.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\", \"val_dataloader_params\", mode=\"before\")\n@classmethod\ndef set_default_pin_memory(\n    cls, dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Set default pin_memory for dataloader parameters if not provided.\n\n    - If 'pin_memory' is not set, it defaults to True if CUDA is available.\n\n    Parameters\n    ----------\n    dataloader_params : dict of {str: Any}\n        The dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The dataloader parameters with pin_memory default applied.\n    \"\"\"\n    if \"pin_memory\" not in dataloader_params:\n        import torch\n\n        dataloader_params[\"pin_memory\"] = torch.cuda.is_available()\n    return dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.set_default_train_workers","title":"<code>set_default_train_workers(dataloader_params)</code>  <code>classmethod</code>","text":"<p>Set default num_workers for training dataloader if not provided.</p> <ul> <li>If 'num_workers' is not set, it defaults to the number of available CPU cores.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataloader_params</code> <code>dict of {str: Any}</code> <p>The training dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The dataloader parameters with num_workers default applied.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\", mode=\"before\")\n@classmethod\ndef set_default_train_workers(\n    cls, dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Set default num_workers for training dataloader if not provided.\n\n    - If 'num_workers' is not set, it defaults to the number of available CPU cores.\n\n    Parameters\n    ----------\n    dataloader_params : dict of {str: Any}\n        The training dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The dataloader parameters with num_workers default applied.\n    \"\"\"\n    if \"num_workers\" not in dataloader_params:\n        # Use 0 workers during tests, otherwise use all available CPU cores\n        if \"pytest\" in sys.modules:\n            dataloader_params[\"num_workers\"] = 0\n        else:\n            dataloader_params[\"num_workers\"] = os.cpu_count()\n\n    return dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.set_means_and_stds","title":"<code>set_means_and_stds(image_means, image_stds, target_means=None, target_stds=None)</code>","text":"<p>Set mean and standard deviation of the data across channels.</p> <p>This method should be used instead setting the fields directly, as it would otherwise trigger a validation error.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>(ndarray, tuple or list)</code> <p>Mean values for normalization.</p> required <code>image_stds</code> <code>(ndarray, tuple or list)</code> <p>Standard deviation values for normalization.</p> required <code>target_means</code> <code>(ndarray, tuple or list)</code> <p>Target mean values for normalization, by default ().</p> <code>None</code> <code>target_stds</code> <code>(ndarray, tuple or list)</code> <p>Target standard deviation values for normalization, by default ().</p> <code>None</code> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>def set_means_and_stds(\n    self,\n    image_means: Union[NDArray, tuple, list, None],\n    image_stds: Union[NDArray, tuple, list, None],\n    target_means: Union[NDArray, tuple, list, None] | None = None,\n    target_stds: Union[NDArray, tuple, list, None] | None = None,\n) -&gt; None:\n    \"\"\"\n    Set mean and standard deviation of the data across channels.\n\n    This method should be used instead setting the fields directly, as it would\n    otherwise trigger a validation error.\n\n    Parameters\n    ----------\n    image_means : numpy.ndarray, tuple or list\n        Mean values for normalization.\n    image_stds : numpy.ndarray, tuple or list\n        Standard deviation values for normalization.\n    target_means : numpy.ndarray, tuple or list, optional\n        Target mean values for normalization, by default ().\n    target_stds : numpy.ndarray, tuple or list, optional\n        Target standard deviation values for normalization, by default ().\n    \"\"\"\n    # make sure we pass a list\n    if image_means is not None:\n        image_means = list(image_means)\n    if image_stds is not None:\n        image_stds = list(image_stds)\n    if target_means is not None:\n        target_means = list(target_means)\n    if target_stds is not None:\n        target_stds = list(target_stds)\n\n    self._update(\n        image_means=image_means,\n        image_stds=image_stds,\n        target_means=target_means,\n        target_stds=target_stds,\n    )\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.set_val_workers_to_match_train","title":"<code>set_val_workers_to_match_train()</code>","text":"<p>Set validation dataloader num_workers to match training dataloader.</p> <p>If num_workers is not specified in val_dataloader_params, it will be set to the same value as train_dataloader_params[\"num_workers\"].</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model with synchronized num_workers.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef set_val_workers_to_match_train(self: Self) -&gt; Self:\n    \"\"\"\n    Set validation dataloader num_workers to match training dataloader.\n\n    If num_workers is not specified in val_dataloader_params, it will be set to the\n    same value as train_dataloader_params[\"num_workers\"].\n\n    Returns\n    -------\n    Self\n        Validated data model with synchronized num_workers.\n    \"\"\"\n    if \"num_workers\" not in self.val_dataloader_params:\n        self.val_dataloader_params[\"num_workers\"] = self.train_dataloader_params[\n            \"num_workers\"\n        ]\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.shuffle_train_dataloader","title":"<code>shuffle_train_dataloader(train_dataloader_params)</code>  <code>classmethod</code>","text":"<p>Validate that \"shuffle\" is included in the training dataloader params.</p> <p>A warning will be raised if <code>shuffle=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader_params</code> <code>dict of {str: Any}</code> <p>The training dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The validated training dataloader parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \"shuffle\" is not included in the training dataloader params.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\")\n@classmethod\ndef shuffle_train_dataloader(\n    cls, train_dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Validate that \"shuffle\" is included in the training dataloader params.\n\n    A warning will be raised if `shuffle=False`.\n\n    Parameters\n    ----------\n    train_dataloader_params : dict of {str: Any}\n        The training dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The validated training dataloader parameters.\n\n    Raises\n    ------\n    ValueError\n        If \"shuffle\" is not included in the training dataloader params.\n    \"\"\"\n    if \"shuffle\" not in train_dataloader_params:\n        raise ValueError(\n            \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n        )\n    elif (\"shuffle\" in train_dataloader_params) and (\n        not train_dataloader_params[\"shuffle\"]\n    ):\n        warn(\n            \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n            \"the training dataloader and may lead to lower quality results.\",\n            stacklevel=1,\n        )\n    return train_dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.std_only_with_mean","title":"<code>std_only_with_mean()</code>","text":"<p>Check that mean and std are either both None, or both specified.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If std is not None and mean is None.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef std_only_with_mean(self: Self) -&gt; Self:\n    \"\"\"\n    Check that mean and std are either both None, or both specified.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If std is not None and mean is None.\n    \"\"\"\n    # check that mean and std are either both None, or both specified\n    if (self.image_means and not self.image_stds) or (\n        self.image_stds and not self.image_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified.\"\n        )\n\n    elif (self.image_means is not None and self.image_stds is not None) and (\n        len(self.image_means) != len(self.image_stds)\n    ):\n        raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n    if (self.target_means and not self.target_stds) or (\n        self.target_stds and not self.target_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified \"\n        )\n\n    elif self.target_means is not None and self.target_stds is not None:\n        if len(self.target_means) != len(self.target_stds):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified for each \"\n                \"target channel.\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.NGDataConfig.validate_dimensions","title":"<code>validate_dimensions()</code>","text":"<p>Validate 2D/3D dimensions between axes and patch size.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size dimension is not compatible with the axes.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dimensions(self: Self) -&gt; Self:\n    \"\"\"\n    Validate 2D/3D dimensions between axes and patch size.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If the patch size dimension is not compatible with the axes.\n    \"\"\"\n    if \"Z\" in self.axes:\n        if (\n            hasattr(self.patching, \"patch_size\")\n            and len(self.patching.patch_size) != 3\n        ):\n            raise ValueError(\n                f\"`patch_size` in `patching` must have 3 dimensions if the data is\"\n                f\" 3D, got axes {self.axes}).\"\n            )\n    else:\n        if (\n            hasattr(self.patching, \"patch_size\")\n            and len(self.patching.patch_size) != 2\n        ):\n            raise ValueError(\n                f\"`patch_size` in `patching` must have 2 dimensions if the data is\"\n                f\" 3D, got axes {self.axes}).\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.generate_random_seed","title":"<code>generate_random_seed()</code>","text":"<p>Generate a random seed for reproducibility.</p> <p>Returns:</p> Type Description <code>int</code> <p>A random integer between 1 and 2^31 - 1.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>def generate_random_seed() -&gt; int:\n    \"\"\"Generate a random seed for reproducibility.\n\n    Returns\n    -------\n    int\n        A random integer between 1 and 2^31 - 1.\n    \"\"\"\n    return random.randint(1, 2**31 - 1)\n</code></pre>"},{"location":"reference/careamics/config/data/ng_data_model/#careamics.config.data.ng_data_model.np_float_to_scientific_str","title":"<code>np_float_to_scientific_str(x)</code>","text":"<p>Return a string scientific representation of a float.</p> <p>In particular, this method is used to serialize floats to strings, allowing numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Input value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Scientific string representation of the input value.</p> Source code in <code>src/careamics/config/data/ng_data_model.py</code> <pre><code>def np_float_to_scientific_str(x: float) -&gt; str:\n    \"\"\"Return a string scientific representation of a float.\n\n    In particular, this method is used to serialize floats to strings, allowing\n    numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.\n\n    Parameters\n    ----------\n    x : float\n        Input value.\n\n    Returns\n    -------\n    str\n        Scientific string representation of the input value.\n    \"\"\"\n    return np.format_float_scientific(x, precision=7)\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/filter_model/","title":"filter_model","text":"<p>Base class for patch and coordinate filtering models.</p>"},{"location":"reference/careamics/config/data/patch_filter/filter_model/#careamics.config.data.patch_filter.filter_model.FilterModel","title":"<code>FilterModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for patch and coordinate filtering models.</p> Source code in <code>src/careamics/config/data/patch_filter/filter_model.py</code> <pre><code>class FilterModel(BaseModel):\n    \"\"\"Base class for patch and coordinate filtering models.\"\"\"\n\n    name: str\n    \"\"\"Name of the filter.\"\"\"\n\n    p: float = Field(1.0, ge=0.0, le=1.0)\n    \"\"\"Probability of applying the filter to a patch or coordinate.\"\"\"\n\n    seed: int | None = Field(default=None, gt=0)\n    \"\"\"Seed for the random number generator for reproducibility.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/filter_model/#careamics.config.data.patch_filter.filter_model.FilterModel.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>Name of the filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/filter_model/#careamics.config.data.patch_filter.filter_model.FilterModel.p","title":"<code>p = Field(1.0, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Probability of applying the filter to a patch or coordinate.</p>"},{"location":"reference/careamics/config/data/patch_filter/filter_model/#careamics.config.data.patch_filter.filter_model.FilterModel.seed","title":"<code>seed = Field(default=None, gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Seed for the random number generator for reproducibility.</p>"},{"location":"reference/careamics/config/data/patch_filter/mask_filter_model/","title":"mask_filter_model","text":"<p>Pydantic model for the mask coordinate filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/mask_filter_model/#careamics.config.data.patch_filter.mask_filter_model.MaskFilterModel","title":"<code>MaskFilterModel</code>","text":"<p>               Bases: <code>FilterModel</code></p> <p>Pydantic model for the mask coordinate filter.</p> Source code in <code>src/careamics/config/data/patch_filter/mask_filter_model.py</code> <pre><code>class MaskFilterModel(FilterModel):\n    \"\"\"Pydantic model for the mask coordinate filter.\"\"\"\n\n    name: Literal[\"mask\"] = \"mask\"\n    \"\"\"Name of the filter.\"\"\"\n\n    coverage: float = Field(0.5, ge=0.0, le=1.0)\n    \"\"\"Percentage of masked pixels required to keep a patch.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/mask_filter_model/#careamics.config.data.patch_filter.mask_filter_model.MaskFilterModel.coverage","title":"<code>coverage = Field(0.5, ge=0.0, le=1.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Percentage of masked pixels required to keep a patch.</p>"},{"location":"reference/careamics/config/data/patch_filter/mask_filter_model/#careamics.config.data.patch_filter.mask_filter_model.MaskFilterModel.name","title":"<code>name = 'mask'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/max_filter_model/","title":"max_filter_model","text":"<p>Pydantic model for the max patch filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/max_filter_model/#careamics.config.data.patch_filter.max_filter_model.MaxFilterModel","title":"<code>MaxFilterModel</code>","text":"<p>               Bases: <code>FilterModel</code></p> <p>Pydantic model for the max patch filter.</p> Source code in <code>src/careamics/config/data/patch_filter/max_filter_model.py</code> <pre><code>class MaxFilterModel(FilterModel):\n    \"\"\"Pydantic model for the max patch filter.\"\"\"\n\n    name: Literal[\"max\"] = \"max\"\n    \"\"\"Name of the filter.\"\"\"\n\n    threshold: float\n    \"\"\"Threshold for the minimum of the max-filtered patch.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/max_filter_model/#careamics.config.data.patch_filter.max_filter_model.MaxFilterModel.name","title":"<code>name = 'max'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/max_filter_model/#careamics.config.data.patch_filter.max_filter_model.MaxFilterModel.threshold","title":"<code>threshold</code>  <code>instance-attribute</code>","text":"<p>Threshold for the minimum of the max-filtered patch.</p>"},{"location":"reference/careamics/config/data/patch_filter/meanstd_filter_model/","title":"meanstd_filter_model","text":"<p>Pydantic model for the mean std patch filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/meanstd_filter_model/#careamics.config.data.patch_filter.meanstd_filter_model.MeanSTDFilterModel","title":"<code>MeanSTDFilterModel</code>","text":"<p>               Bases: <code>FilterModel</code></p> <p>Pydantic model for the mean std patch filter.</p> Source code in <code>src/careamics/config/data/patch_filter/meanstd_filter_model.py</code> <pre><code>class MeanSTDFilterModel(FilterModel):\n    \"\"\"Pydantic model for the mean std patch filter.\"\"\"\n\n    name: Literal[\"mean_std\"] = \"mean_std\"\n    \"\"\"Name of the filter.\"\"\"\n\n    mean_threshold: float\n    \"\"\"Minimum mean intensity required to keep a patch.\"\"\"\n\n    std_threshold: float | None = None\n    \"\"\"Minimum standard deviation required to keep a patch.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/meanstd_filter_model/#careamics.config.data.patch_filter.meanstd_filter_model.MeanSTDFilterModel.mean_threshold","title":"<code>mean_threshold</code>  <code>instance-attribute</code>","text":"<p>Minimum mean intensity required to keep a patch.</p>"},{"location":"reference/careamics/config/data/patch_filter/meanstd_filter_model/#careamics.config.data.patch_filter.meanstd_filter_model.MeanSTDFilterModel.name","title":"<code>name = 'mean_std'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/meanstd_filter_model/#careamics.config.data.patch_filter.meanstd_filter_model.MeanSTDFilterModel.std_threshold","title":"<code>std_threshold = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum standard deviation required to keep a patch.</p>"},{"location":"reference/careamics/config/data/patch_filter/shannon_filter_model/","title":"shannon_filter_model","text":"<p>Pydantic model for the Shannon entropy patch filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/shannon_filter_model/#careamics.config.data.patch_filter.shannon_filter_model.ShannonFilterModel","title":"<code>ShannonFilterModel</code>","text":"<p>               Bases: <code>FilterModel</code></p> <p>Pydantic model for the Shannon entropy patch filter.</p> Source code in <code>src/careamics/config/data/patch_filter/shannon_filter_model.py</code> <pre><code>class ShannonFilterModel(FilterModel):\n    \"\"\"Pydantic model for the Shannon entropy patch filter.\"\"\"\n\n    name: Literal[\"shannon\"] = \"shannon\"\n    \"\"\"Name of the filter.\"\"\"\n\n    threshold: float\n    \"\"\"Minimum Shannon entropy required to keep a patch.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patch_filter/shannon_filter_model/#careamics.config.data.patch_filter.shannon_filter_model.ShannonFilterModel.name","title":"<code>name = 'shannon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the filter.</p>"},{"location":"reference/careamics/config/data/patch_filter/shannon_filter_model/#careamics.config.data.patch_filter.shannon_filter_model.ShannonFilterModel.threshold","title":"<code>threshold</code>  <code>instance-attribute</code>","text":"<p>Minimum Shannon entropy required to keep a patch.</p>"},{"location":"reference/careamics/config/data/patching_strategies/_overlapping_patched_model/","title":"_overlapping_patched_model","text":"<p>Sequential patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/_patched_model/","title":"_patched_model","text":"<p>Generic patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/random_patching_model/","title":"random_patching_model","text":"<p>Random patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/random_patching_model/#careamics.config.data.patching_strategies.random_patching_model.RandomPatchingModel","title":"<code>RandomPatchingModel</code>","text":"<p>               Bases: <code>_PatchedModel</code></p> <p>Random patching Pydantic model.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>random</code> <p>The name of the patching strategy.</p> <code>patch_size</code> <code>sequence of int</code> <p>The size of the patch in each spatial dimension, each patch size must be a power of 2 and larger than 8.</p> Source code in <code>src/careamics/config/data/patching_strategies/random_patching_model.py</code> <pre><code>class RandomPatchingModel(_PatchedModel):\n    \"\"\"Random patching Pydantic model.\n\n    Attributes\n    ----------\n    name : \"random\"\n        The name of the patching strategy.\n    patch_size : sequence of int\n        The size of the patch in each spatial dimension, each patch size must be a power\n        of 2 and larger than 8.\n    \"\"\"\n\n    name: Literal[\"random\"] = \"random\"\n    \"\"\"The name of the patching strategy.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patching_strategies/random_patching_model/#careamics.config.data.patching_strategies.random_patching_model.RandomPatchingModel.name","title":"<code>name = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the patching strategy.</p>"},{"location":"reference/careamics/config/data/patching_strategies/sequential_patching_model/","title":"sequential_patching_model","text":"<p>Sequential patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/sequential_patching_model/#careamics.config.data.patching_strategies.sequential_patching_model.SequentialPatchingModel","title":"<code>SequentialPatchingModel</code>","text":"<p>               Bases: <code>_OverlappingPatchedModel</code></p> <p>Sequential patching Pydantic model.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>sequential</code> <p>The name of the patching strategy.</p> <code>patch_size</code> <code>sequence of int</code> <p>The size of the patch in each spatial dimension, each patch size must be a power of 2 and larger than 8.</p> <code>overlaps</code> <code>list of int, optional</code> <p>The overlaps between patches in each spatial dimension. If <code>None</code>, no overlap is applied. The overlaps must be smaller than the patch size in each spatial dimension, and the number of dimensions be either 2 or 3.</p> Source code in <code>src/careamics/config/data/patching_strategies/sequential_patching_model.py</code> <pre><code>class SequentialPatchingModel(_OverlappingPatchedModel):\n    \"\"\"Sequential patching Pydantic model.\n\n    Attributes\n    ----------\n    name : \"sequential\"\n        The name of the patching strategy.\n    patch_size : sequence of int\n        The size of the patch in each spatial dimension, each patch size must be a power\n        of 2 and larger than 8.\n    overlaps : list of int, optional\n        The overlaps between patches in each spatial dimension. If `None`, no overlap is\n        applied. The overlaps must be smaller than the patch size in each spatial\n        dimension, and the number of dimensions be either 2 or 3.\n    \"\"\"\n\n    name: Literal[\"sequential\"] = \"sequential\"\n    \"\"\"The name of the patching strategy.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patching_strategies/sequential_patching_model/#careamics.config.data.patching_strategies.sequential_patching_model.SequentialPatchingModel.name","title":"<code>name = 'sequential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the patching strategy.</p>"},{"location":"reference/careamics/config/data/patching_strategies/tiled_patching_model/","title":"tiled_patching_model","text":"<p>Tiled patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/tiled_patching_model/#careamics.config.data.patching_strategies.tiled_patching_model.TiledPatchingModel","title":"<code>TiledPatchingModel</code>","text":"<p>               Bases: <code>_OverlappingPatchedModel</code></p> <p>Tiled patching Pydantic model.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>tiled</code> <p>The name of the patching strategy.</p> <code>patch_size</code> <code>sequence of int</code> <p>The size of the patch in each spatial dimension, each patch size must be a power of 2 and larger than 8.</p> <code>overlaps</code> <code>sequence of int</code> <p>The overlaps between patches in each spatial dimension. The overlaps must be smaller than the patch size in each spatial dimension, and the number of dimensions be either 2 or 3.</p> Source code in <code>src/careamics/config/data/patching_strategies/tiled_patching_model.py</code> <pre><code>class TiledPatchingModel(_OverlappingPatchedModel):\n    \"\"\"Tiled patching Pydantic model.\n\n    Attributes\n    ----------\n    name : \"tiled\"\n        The name of the patching strategy.\n    patch_size : sequence of int\n        The size of the patch in each spatial dimension, each patch size must be a power\n        of 2 and larger than 8.\n    overlaps : sequence of int\n        The overlaps between patches in each spatial dimension. The overlaps must be\n        smaller than the patch size in each spatial dimension, and the number of\n        dimensions be either 2 or 3.\n    \"\"\"\n\n    name: Literal[\"tiled\"] = \"tiled\"\n    \"\"\"The name of the patching strategy.\"\"\"\n\n    overlaps: Sequence[int] = Field(\n        ...,\n        min_length=2,\n        max_length=3,\n    )\n    \"\"\"The overlaps between patches in each spatial dimension. The overlaps must be\n    smaller than the patch size in each spatial dimension, and the number of dimensions\n    be either 2 or 3.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patching_strategies/tiled_patching_model/#careamics.config.data.patching_strategies.tiled_patching_model.TiledPatchingModel.name","title":"<code>name = 'tiled'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the patching strategy.</p>"},{"location":"reference/careamics/config/data/patching_strategies/tiled_patching_model/#careamics.config.data.patching_strategies.tiled_patching_model.TiledPatchingModel.overlaps","title":"<code>overlaps = Field(..., min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The overlaps between patches in each spatial dimension. The overlaps must be smaller than the patch size in each spatial dimension, and the number of dimensions be either 2 or 3.</p>"},{"location":"reference/careamics/config/data/patching_strategies/whole_patching_model/","title":"whole_patching_model","text":"<p>Whole image patching Pydantic model.</p>"},{"location":"reference/careamics/config/data/patching_strategies/whole_patching_model/#careamics.config.data.patching_strategies.whole_patching_model.WholePatchingModel","title":"<code>WholePatchingModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Whole image patching Pydantic model.</p> Source code in <code>src/careamics/config/data/patching_strategies/whole_patching_model.py</code> <pre><code>class WholePatchingModel(BaseModel):\n    \"\"\"Whole image patching Pydantic model.\"\"\"\n\n    name: Literal[\"whole\"] = \"whole\"\n    \"\"\"The name of the patching strategy.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/data/patching_strategies/whole_patching_model/#careamics.config.data.patching_strategies.whole_patching_model.WholePatchingModel.name","title":"<code>name = 'whole'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the patching strategy.</p>"},{"location":"reference/careamics/config/support/supported_activations/","title":"supported_activations","text":"<p>Activations supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_activations/#careamics.config.support.supported_activations.SupportedActivation","title":"<code>SupportedActivation</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported activation functions.</p> <ul> <li>None, no activation will be used.</li> <li>Sigmoid</li> <li>Softmax</li> <li>Tanh</li> <li>ReLU</li> <li>LeakyReLU</li> </ul> <p>All activations are defined in PyTorch.</p> <p>See: https://pytorch.org/docs/stable/nn.html#loss-functions</p> Source code in <code>src/careamics/config/support/supported_activations.py</code> <pre><code>class SupportedActivation(str, BaseEnum):\n    \"\"\"Supported activation functions.\n\n    - None, no activation will be used.\n    - Sigmoid\n    - Softmax\n    - Tanh\n    - ReLU\n    - LeakyReLU\n\n    All activations are defined in PyTorch.\n\n    See: https://pytorch.org/docs/stable/nn.html#loss-functions\n    \"\"\"\n\n    NONE = \"None\"\n    SIGMOID = \"Sigmoid\"\n    SOFTMAX = \"Softmax\"\n    TANH = \"Tanh\"\n    RELU = \"ReLU\"\n    LEAKYRELU = \"LeakyReLU\"\n    ELU = \"ELU\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_algorithms/","title":"supported_algorithms","text":"<p>Algorithms supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm","title":"<code>SupportedAlgorithm</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Algorithms available in CAREamics.</p> <p>These definitions are the same as the keyword <code>name</code> of the algorithm configurations.</p> Source code in <code>src/careamics/config/support/supported_algorithms.py</code> <pre><code>class SupportedAlgorithm(str, BaseEnum):\n    \"\"\"Algorithms available in CAREamics.\n\n    These definitions are the same as the keyword `name` of the algorithm\n    configurations.\n    \"\"\"\n\n    N2V = \"n2v\"\n    \"\"\"Noise2Void algorithm, a self-supervised approach based on blind denoising.\"\"\"\n\n    CARE = \"care\"\n    \"\"\"Content-aware image restoration, a supervised algorithm used for a variety\n    of tasks.\"\"\"\n\n    N2N = \"n2n\"\n    \"\"\"Noise2Noise algorithm, a self-supervised denoising scheme based on comparing\n    noisy images of the same sample.\"\"\"\n\n    MUSPLIT = \"musplit\"\n    \"\"\"An image splitting approach based on ladder VAE architectures.\"\"\"\n\n    MICROSPLIT = \"microsplit\"\n    \"\"\"A micro-level image splitting approach based on ladder VAE architectures.\"\"\"\n\n    DENOISPLIT = \"denoisplit\"\n    \"\"\"An image splitting and denoising approach based on ladder VAE architectures.\"\"\"\n\n    HDN = \"hdn\"\n    \"\"\"Hierarchical Denoising Network, an unsupervised denoising algorithm\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.CARE","title":"<code>CARE = 'care'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content-aware image restoration, a supervised algorithm used for a variety of tasks.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.DENOISPLIT","title":"<code>DENOISPLIT = 'denoisplit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An image splitting and denoising approach based on ladder VAE architectures.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.HDN","title":"<code>HDN = 'hdn'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hierarchical Denoising Network, an unsupervised denoising algorithm</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.MICROSPLIT","title":"<code>MICROSPLIT = 'microsplit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A micro-level image splitting approach based on ladder VAE architectures.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.MUSPLIT","title":"<code>MUSPLIT = 'musplit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An image splitting approach based on ladder VAE architectures.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.N2N","title":"<code>N2N = 'n2n'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise2Noise algorithm, a self-supervised denoising scheme based on comparing noisy images of the same sample.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.N2V","title":"<code>N2V = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise2Void algorithm, a self-supervised approach based on blind denoising.</p>"},{"location":"reference/careamics/config/support/supported_architectures/","title":"supported_architectures","text":"<p>Architectures supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture","title":"<code>SupportedArchitecture</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported architectures.</p> Source code in <code>src/careamics/config/support/supported_architectures.py</code> <pre><code>class SupportedArchitecture(str, BaseEnum):\n    \"\"\"Supported architectures.\"\"\"\n\n    UNET = \"UNet\"\n    \"\"\"UNet architecture used with N2V, CARE and Noise2Noise.\"\"\"\n\n    LVAE = \"LVAE\"\n    \"\"\"Ladder Variational Autoencoder used for muSplit and denoiSplit.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture.LVAE","title":"<code>LVAE = 'LVAE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ladder Variational Autoencoder used for muSplit and denoiSplit.</p>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture.UNET","title":"<code>UNET = 'UNet'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>UNet architecture used with N2V, CARE and Noise2Noise.</p>"},{"location":"reference/careamics/config/support/supported_data/","title":"supported_data","text":"<p>Data supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData","title":"<code>SupportedData</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported data types.</p> <p>Attributes:</p> Name Type Description <code>ARRAY</code> <code>str</code> <p>Array data.</p> <code>TIFF</code> <code>str</code> <p>TIFF image data.</p> <code>CZI</code> <code>str</code> <p>CZI image data.</p> <code>CUSTOM</code> <code>str</code> <p>Custom data.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>class SupportedData(str, BaseEnum):\n    \"\"\"Supported data types.\n\n    Attributes\n    ----------\n    ARRAY : str\n        Array data.\n    TIFF : str\n        TIFF image data.\n    CZI : str\n        CZI image data.\n    CUSTOM : str\n        Custom data.\n    \"\"\"\n\n    ARRAY = \"array\"\n    TIFF = \"tiff\"\n    CZI = \"czi\"\n    CUSTOM = \"custom\"\n    # ZARR = \"zarr\"\n\n    # TODO remove?\n    @classmethod\n    def _missing_(cls, value: object) -&gt; str:\n        \"\"\"\n        Override default behaviour for missing values.\n\n        This method is called when `value` is not found in the enum values. It converts\n        `value` to lowercase, removes \".\" if it is the first character and tries to\n        match it with enum values.\n\n        Parameters\n        ----------\n        value : object\n            Value to be matched with enum values.\n\n        Returns\n        -------\n        str\n            Matched enum value.\n        \"\"\"\n        if isinstance(value, str):\n            lower_value = value.lower()\n\n            if lower_value.startswith(\".\"):\n                lower_value = lower_value[1:]\n\n            # attempt to match lowercase value with enum values\n            for member in cls:\n                if member.value == lower_value:\n                    return member\n\n        # still missing\n        return super()._missing_(value)\n\n    @classmethod\n    def get_extension_pattern(cls, data_type: Union[str, SupportedData]) -&gt; str:\n        \"\"\"\n        Get Path.rglob and fnmatch compatible extension.\n\n        Parameters\n        ----------\n        data_type : SupportedData\n            Data type.\n\n        Returns\n        -------\n        str\n            Corresponding extension pattern.\n        \"\"\"\n        if data_type == cls.ARRAY:\n            raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n        elif data_type == cls.TIFF:\n            return \"*.tif*\"\n        elif data_type == cls.CZI:\n            return \"*.czi\"\n        elif data_type == cls.CUSTOM:\n            return \"*.*\"\n        else:\n            raise ValueError(f\"Data type {data_type} is not supported.\")\n\n    @classmethod\n    def get_extension(cls, data_type: Union[str, SupportedData]) -&gt; str:\n        \"\"\"\n        Get file extension of corresponding data type.\n\n        Parameters\n        ----------\n        data_type : str or SupportedData\n            Data type.\n\n        Returns\n        -------\n        str\n            Corresponding extension.\n        \"\"\"\n        if data_type == cls.ARRAY:\n            raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n        elif data_type == cls.TIFF:\n            return \".tiff\"\n        elif data_type == cls.CZI:\n            return \".czi\"\n        elif data_type == cls.CUSTOM:\n            # TODO: improve this message\n            raise NotImplementedError(\"Custom extensions have to be passed elsewhere.\")\n        else:\n            raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData.get_extension","title":"<code>get_extension(data_type)</code>  <code>classmethod</code>","text":"<p>Get file extension of corresponding data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str or SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Corresponding extension.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>@classmethod\ndef get_extension(cls, data_type: Union[str, SupportedData]) -&gt; str:\n    \"\"\"\n    Get file extension of corresponding data type.\n\n    Parameters\n    ----------\n    data_type : str or SupportedData\n        Data type.\n\n    Returns\n    -------\n    str\n        Corresponding extension.\n    \"\"\"\n    if data_type == cls.ARRAY:\n        raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n    elif data_type == cls.TIFF:\n        return \".tiff\"\n    elif data_type == cls.CZI:\n        return \".czi\"\n    elif data_type == cls.CUSTOM:\n        # TODO: improve this message\n        raise NotImplementedError(\"Custom extensions have to be passed elsewhere.\")\n    else:\n        raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData.get_extension_pattern","title":"<code>get_extension_pattern(data_type)</code>  <code>classmethod</code>","text":"<p>Get Path.rglob and fnmatch compatible extension.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Corresponding extension pattern.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>@classmethod\ndef get_extension_pattern(cls, data_type: Union[str, SupportedData]) -&gt; str:\n    \"\"\"\n    Get Path.rglob and fnmatch compatible extension.\n\n    Parameters\n    ----------\n    data_type : SupportedData\n        Data type.\n\n    Returns\n    -------\n    str\n        Corresponding extension pattern.\n    \"\"\"\n    if data_type == cls.ARRAY:\n        raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n    elif data_type == cls.TIFF:\n        return \"*.tif*\"\n    elif data_type == cls.CZI:\n        return \"*.czi\"\n    elif data_type == cls.CUSTOM:\n        return \"*.*\"\n    else:\n        raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_filters/","title":"supported_filters","text":"<p>Coordinate and patch filters supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_filters/#careamics.config.support.supported_filters.SupportedCoordinateFilters","title":"<code>SupportedCoordinateFilters</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported coordinate filters.</p> Source code in <code>src/careamics/config/support/supported_filters.py</code> <pre><code>class SupportedCoordinateFilters(str, BaseEnum):\n    \"\"\"Supported coordinate filters.\"\"\"\n\n    MASK = \"mask\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_filters/#careamics.config.support.supported_filters.SupportedPatchFilters","title":"<code>SupportedPatchFilters</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported patch filters.</p> Source code in <code>src/careamics/config/support/supported_filters.py</code> <pre><code>class SupportedPatchFilters(str, BaseEnum):\n    \"\"\"Supported patch filters.\"\"\"\n\n    MAX = \"max\"\n    MEANSTD = \"mean_std\"\n    SHANNON = \"shannon\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_loggers/","title":"supported_loggers","text":"<p>Logger supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_loggers/#careamics.config.support.supported_loggers.SupportedLogger","title":"<code>SupportedLogger</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Available loggers.</p> Source code in <code>src/careamics/config/support/supported_loggers.py</code> <pre><code>class SupportedLogger(str, BaseEnum):\n    \"\"\"Available loggers.\"\"\"\n\n    WANDB = \"wandb\"\n    TENSORBOARD = \"tensorboard\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_losses/","title":"supported_losses","text":"<p>Losses supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_losses/#careamics.config.support.supported_losses.SupportedLoss","title":"<code>SupportedLoss</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported losses.</p> <p>Attributes:</p> Name Type Description <code>MSE</code> <code>str</code> <p>Mean Squared Error loss.</p> <code>MAE</code> <code>str</code> <p>Mean Absolute Error loss.</p> <code>N2V</code> <code>str</code> <p>Noise2Void loss.</p> Source code in <code>src/careamics/config/support/supported_losses.py</code> <pre><code>class SupportedLoss(str, BaseEnum):\n    \"\"\"Supported losses.\n\n    Attributes\n    ----------\n    MSE : str\n        Mean Squared Error loss.\n    MAE : str\n        Mean Absolute Error loss.\n    N2V : str\n        Noise2Void loss.\n    \"\"\"\n\n    MSE = \"mse\"\n    MAE = \"mae\"\n    N2V = \"n2v\"\n    # PN2V = \"pn2v\"\n    HDN = \"hdn\"\n    MUSPLIT = \"musplit\"\n    MICROSPLIT = \"microsplit\"\n    DENOISPLIT = \"denoisplit\"\n    DENOISPLIT_MUSPLIT = (\n        \"denoisplit_musplit\"  # TODO refac losses, leave only microsplit\n    )\n</code></pre>"},{"location":"reference/careamics/config/support/supported_optimizers/","title":"supported_optimizers","text":"<p>Optimizers and schedulers supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_optimizers/#careamics.config.support.supported_optimizers.SupportedOptimizer","title":"<code>SupportedOptimizer</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported optimizers.</p> <p>Attributes:</p> Name Type Description <code>Adam</code> <code>str</code> <p>Adam optimizer.</p> <code>SGD</code> <code>str</code> <p>Stochastic Gradient Descent optimizer.</p> Source code in <code>src/careamics/config/support/supported_optimizers.py</code> <pre><code>class SupportedOptimizer(str, BaseEnum):\n    \"\"\"Supported optimizers.\n\n    Attributes\n    ----------\n    Adam : str\n        Adam optimizer.\n    SGD : str\n        Stochastic Gradient Descent optimizer.\n    \"\"\"\n\n    # ASGD = \"ASGD\"\n    # Adadelta = \"Adadelta\"\n    # Adagrad = \"Adagrad\"\n    ADAM = \"Adam\"\n    # AdamW = \"AdamW\"\n    ADAMAX = \"Adamax\"\n    # LBFGS = \"LBFGS\"\n    # NAdam = \"NAdam\"\n    # RAdam = \"RAdam\"\n    # RMSprop = \"RMSprop\"\n    # Rprop = \"Rprop\"\n    SGD = \"SGD\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_optimizers/#careamics.config.support.supported_optimizers.SupportedScheduler","title":"<code>SupportedScheduler</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported schedulers.</p> <p>Attributes:</p> Name Type Description <code>ReduceLROnPlateau</code> <code>str</code> <p>Reduce learning rate on plateau.</p> <code>StepLR</code> <code>str</code> <p>Step learning rate.</p> Source code in <code>src/careamics/config/support/supported_optimizers.py</code> <pre><code>class SupportedScheduler(str, BaseEnum):\n    \"\"\"Supported schedulers.\n\n    Attributes\n    ----------\n    ReduceLROnPlateau : str\n        Reduce learning rate on plateau.\n    StepLR : str\n        Step learning rate.\n    \"\"\"\n\n    # ChainedScheduler = \"ChainedScheduler\"\n    # ConstantLR = \"ConstantLR\"\n    # CosineAnnealingLR = \"CosineAnnealingLR\"\n    # CosineAnnealingWarmRestarts = \"CosineAnnealingWarmRestarts\"\n    # CyclicLR = \"CyclicLR\"\n    # ExponentialLR = \"ExponentialLR\"\n    # LambdaLR = \"LambdaLR\"\n    # LinearLR = \"LinearLR\"\n    # MultiStepLR = \"MultiStepLR\"\n    # MultiplicativeLR = \"MultiplicativeLR\"\n    # OneCycleLR = \"OneCycleLR\"\n    # PolynomialLR = \"PolynomialLR\"\n    REDUCE_LR_ON_PLATEAU = \"ReduceLROnPlateau\"\n    # SequentialLR = \"SequentialLR\"\n    STEP_LR = \"StepLR\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_patching_strategies/","title":"supported_patching_strategies","text":"<p>Patching strategies supported by Careamics.</p>"},{"location":"reference/careamics/config/support/supported_patching_strategies/#careamics.config.support.supported_patching_strategies.SupportedPatchingStrategy","title":"<code>SupportedPatchingStrategy</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Patching strategies supported by Careamics.</p> Source code in <code>src/careamics/config/support/supported_patching_strategies.py</code> <pre><code>class SupportedPatchingStrategy(str, BaseEnum):\n    \"\"\"Patching strategies supported by Careamics.\"\"\"\n\n    FIXED_RANDOM = \"fixed_random\"\n    \"\"\"Fixed random patching strategy, used during training.\"\"\"\n\n    RANDOM = \"random\"\n    \"\"\"Random patching strategy, used during training.\"\"\"\n\n    # SEQUENTIAL = \"sequential\"\n    # \"\"\"Sequential patching strategy, used during training.\"\"\"\n\n    TILED = \"tiled\"\n    \"\"\"Tiled patching strategy, used during prediction.\"\"\"\n\n    WHOLE = \"whole\"\n    \"\"\"Whole image patching strategy, used during prediction.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_patching_strategies/#careamics.config.support.supported_patching_strategies.SupportedPatchingStrategy.FIXED_RANDOM","title":"<code>FIXED_RANDOM = 'fixed_random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fixed random patching strategy, used during training.</p>"},{"location":"reference/careamics/config/support/supported_patching_strategies/#careamics.config.support.supported_patching_strategies.SupportedPatchingStrategy.RANDOM","title":"<code>RANDOM = 'random'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random patching strategy, used during training.</p>"},{"location":"reference/careamics/config/support/supported_patching_strategies/#careamics.config.support.supported_patching_strategies.SupportedPatchingStrategy.TILED","title":"<code>TILED = 'tiled'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tiled patching strategy, used during prediction.</p>"},{"location":"reference/careamics/config/support/supported_patching_strategies/#careamics.config.support.supported_patching_strategies.SupportedPatchingStrategy.WHOLE","title":"<code>WHOLE = 'whole'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whole image patching strategy, used during prediction.</p>"},{"location":"reference/careamics/config/support/supported_pixel_manipulations/","title":"supported_pixel_manipulations","text":"<p>Pixel manipulation methods supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_pixel_manipulations/#careamics.config.support.supported_pixel_manipulations.SupportedPixelManipulation","title":"<code>SupportedPixelManipulation</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported Noise2Void pixel manipulations.</p> <ul> <li>Uniform: Replace masked pixel value by a (uniformly) randomly selected neighbor     pixel value.</li> <li>Median: Replace masked pixel value by the mean of the neighborhood.</li> </ul> Source code in <code>src/careamics/config/support/supported_pixel_manipulations.py</code> <pre><code>class SupportedPixelManipulation(str, BaseEnum):\n    \"\"\"Supported Noise2Void pixel manipulations.\n\n    - Uniform: Replace masked pixel value by a (uniformly) randomly selected neighbor\n        pixel value.\n    - Median: Replace masked pixel value by the mean of the neighborhood.\n    \"\"\"\n\n    UNIFORM = \"uniform\"\n    MEDIAN = \"median\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_struct_axis/","title":"supported_struct_axis","text":"<p>StructN2V axes supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_struct_axis/#careamics.config.support.supported_struct_axis.SupportedStructAxis","title":"<code>SupportedStructAxis</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported structN2V mask axes.</p> <p>Attributes:</p> Name Type Description <code>HORIZONTAL</code> <code>str</code> <p>Horizontal axis.</p> <code>VERTICAL</code> <code>str</code> <p>Vertical axis.</p> <code>NONE</code> <code>str</code> <p>No axis, the mask is not applied.</p> Source code in <code>src/careamics/config/support/supported_struct_axis.py</code> <pre><code>class SupportedStructAxis(str, BaseEnum):\n    \"\"\"Supported structN2V mask axes.\n\n    Attributes\n    ----------\n    HORIZONTAL : str\n        Horizontal axis.\n    VERTICAL : str\n        Vertical axis.\n    NONE : str\n        No axis, the mask is not applied.\n    \"\"\"\n\n    HORIZONTAL = \"horizontal\"\n    VERTICAL = \"vertical\"\n    NONE = \"none\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_transforms/","title":"supported_transforms","text":"<p>Transforms supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_transforms/#careamics.config.support.supported_transforms.SupportedTransform","title":"<code>SupportedTransform</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Transforms officially supported by CAREamics.</p> Source code in <code>src/careamics/config/support/supported_transforms.py</code> <pre><code>class SupportedTransform(str, BaseEnum):\n    \"\"\"Transforms officially supported by CAREamics.\"\"\"\n\n    XY_FLIP = \"XYFlip\"\n    XY_RANDOM_ROTATE90 = \"XYRandomRotate90\"\n    NORMALIZE = \"Normalize\"\n    N2V_MANIPULATE = \"N2VManipulate\"\n</code></pre>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/","title":"n2v_manipulate_model","text":"<p>Pydantic model for the N2VManipulate transform.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel","title":"<code>N2VManipulateModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent N2V manipulation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['N2VManipulate']</code> <p>Name of the transformation.</p> <code>roi_size</code> <code>int</code> <p>Size of the masking region, by default 11.</p> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of masked pixels, by default 0.2.</p> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Strategy pixel value replacement, by default \"uniform\".</p> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>Axis of the structN2V mask, by default \"none\".</p> <code>struct_mask_span</code> <code>int</code> <p>Span of the structN2V mask, by default 5.</p> Source code in <code>src/careamics/config/transformations/n2v_manipulate_model.py</code> <pre><code>class N2VManipulateModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent N2V manipulation.\n\n    Attributes\n    ----------\n    name : Literal[\"N2VManipulate\"]\n        Name of the transformation.\n    roi_size : int\n        Size of the masking region, by default 11.\n    masked_pixel_percentage : float\n        Percentage of masked pixels, by default 0.2.\n    strategy : Literal[\"uniform\", \"median\"]\n        Strategy pixel value replacement, by default \"uniform\".\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"]\n        Axis of the structN2V mask, by default \"none\".\n    struct_mask_span : int\n        Span of the structN2V mask, by default 5.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"N2VManipulate\"] = \"N2VManipulate\"\n\n    roi_size: int = Field(default=11, ge=3, le=21)\n    \"\"\"Size of the region where the pixel manipulation is applied.\"\"\"\n\n    masked_pixel_percentage: float = Field(default=0.2, ge=0.05, le=10.0)\n    \"\"\"Percentage of masked pixels per image.\"\"\"\n\n    remove_center: bool = Field(default=True)  # TODO remove it\n    \"\"\"Exclude center pixel from average calculation.\"\"\"  # TODO rephrase this\n\n    strategy: Literal[\"uniform\", \"median\"] = Field(default=\"uniform\")\n    \"\"\"Strategy for pixel value replacement.\"\"\"\n\n    struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = Field(default=\"none\")\n    \"\"\"Orientation of the structN2V mask. Set to `\\\"non\\\"` to not apply StructN2V.\"\"\"\n\n    struct_mask_span: int = Field(default=5, ge=3, le=15)\n    \"\"\"Size of the structN2V mask.\"\"\"\n\n    @field_validator(\"roi_size\", \"struct_mask_span\")\n    @classmethod\n    def odd_value(cls, v: int) -&gt; int:\n        \"\"\"\n        Validate that the value is odd.\n\n        Parameters\n        ----------\n        v : int\n            Value to validate.\n\n        Returns\n        -------\n        int\n            The validated value.\n\n        Raises\n        ------\n        ValueError\n            If the value is even.\n        \"\"\"\n        if v % 2 == 0:\n            raise ValueError(\"Size must be an odd number.\")\n        return v\n</code></pre>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.masked_pixel_percentage","title":"<code>masked_pixel_percentage = Field(default=0.2, ge=0.05, le=10.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Percentage of masked pixels per image.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.remove_center","title":"<code>remove_center = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exclude center pixel from average calculation.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.roi_size","title":"<code>roi_size = Field(default=11, ge=3, le=21)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the region where the pixel manipulation is applied.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.strategy","title":"<code>strategy = Field(default='uniform')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for pixel value replacement.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.struct_mask_axis","title":"<code>struct_mask_axis = Field(default='none')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Orientation of the structN2V mask. Set to <code>\"non\"</code> to not apply StructN2V.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.struct_mask_span","title":"<code>struct_mask_span = Field(default=5, ge=3, le=15)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the structN2V mask.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.odd_value","title":"<code>odd_value(v)</code>  <code>classmethod</code>","text":"<p>Validate that the value is odd.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>Value to validate.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The validated value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is even.</p> Source code in <code>src/careamics/config/transformations/n2v_manipulate_model.py</code> <pre><code>@field_validator(\"roi_size\", \"struct_mask_span\")\n@classmethod\ndef odd_value(cls, v: int) -&gt; int:\n    \"\"\"\n    Validate that the value is odd.\n\n    Parameters\n    ----------\n    v : int\n        Value to validate.\n\n    Returns\n    -------\n    int\n        The validated value.\n\n    Raises\n    ------\n    ValueError\n        If the value is even.\n    \"\"\"\n    if v % 2 == 0:\n        raise ValueError(\"Size must be an odd number.\")\n    return v\n</code></pre>"},{"location":"reference/careamics/config/transformations/normalize_model/","title":"normalize_model","text":"<p>Pydantic model for the Normalize transform.</p>"},{"location":"reference/careamics/config/transformations/normalize_model/#careamics.config.transformations.normalize_model.NormalizeModel","title":"<code>NormalizeModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent Normalize transformation.</p> <p>The Normalize transform is a zero mean and unit variance transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['Normalize']</code> <p>Name of the transformation.</p> <code>mean</code> <code>float</code> <p>Mean value for normalization.</p> <code>std</code> <code>float</code> <p>Standard deviation value for normalization.</p> Source code in <code>src/careamics/config/transformations/normalize_model.py</code> <pre><code>class NormalizeModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent Normalize transformation.\n\n    The Normalize transform is a zero mean and unit variance transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"Normalize\"]\n        Name of the transformation.\n    mean : float\n        Mean value for normalization.\n    std : float\n        Standard deviation value for normalization.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"Normalize\"] = \"Normalize\"\n    image_means: list = Field(..., min_length=0, max_length=32)\n    image_stds: list = Field(..., min_length=0, max_length=32)\n    target_means: list | None = Field(default=None, min_length=0, max_length=32)\n    target_stds: list | None = Field(default=None, min_length=0, max_length=32)\n\n    @model_validator(mode=\"after\")\n    def validate_means_stds(self: Self) -&gt; Self:\n        \"\"\"Validate that the means and stds have the same length.\n\n        Returns\n        -------\n        Self\n            The instance of the model.\n        \"\"\"\n        if len(self.image_means) != len(self.image_stds):\n            raise ValueError(\"The number of image means and stds must be the same.\")\n\n        if (self.target_means is None) != (self.target_stds is None):\n            raise ValueError(\n                \"Both target means and stds must be provided together, or bot None.\"\n            )\n\n        if self.target_means is not None and self.target_stds is not None:\n            if len(self.target_means) != len(self.target_stds):\n                raise ValueError(\n                    \"The number of target means and stds must be the same.\"\n                )\n\n        return self\n</code></pre>"},{"location":"reference/careamics/config/transformations/normalize_model/#careamics.config.transformations.normalize_model.NormalizeModel.validate_means_stds","title":"<code>validate_means_stds()</code>","text":"<p>Validate that the means and stds have the same length.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The instance of the model.</p> Source code in <code>src/careamics/config/transformations/normalize_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_means_stds(self: Self) -&gt; Self:\n    \"\"\"Validate that the means and stds have the same length.\n\n    Returns\n    -------\n    Self\n        The instance of the model.\n    \"\"\"\n    if len(self.image_means) != len(self.image_stds):\n        raise ValueError(\"The number of image means and stds must be the same.\")\n\n    if (self.target_means is None) != (self.target_stds is None):\n        raise ValueError(\n            \"Both target means and stds must be provided together, or bot None.\"\n        )\n\n    if self.target_means is not None and self.target_stds is not None:\n        if len(self.target_means) != len(self.target_stds):\n            raise ValueError(\n                \"The number of target means and stds must be the same.\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_model/","title":"transform_model","text":"<p>Parent model for the transforms.</p>"},{"location":"reference/careamics/config/transformations/transform_model/#careamics.config.transformations.transform_model.TransformModel","title":"<code>TransformModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model used to represent a transformation.</p> <p>The <code>model_dump</code> method is overwritten to exclude the name field.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the transformation.</p> Source code in <code>src/careamics/config/transformations/transform_model.py</code> <pre><code>class TransformModel(BaseModel):\n    \"\"\"\n    Pydantic model used to represent a transformation.\n\n    The `model_dump` method is overwritten to exclude the name field.\n\n    Attributes\n    ----------\n    name : str\n        Name of the transformation.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",  # throw errors if the parameters are not properly passed\n    )\n\n    name: str\n\n    def model_dump(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the model as a dictionary.\n\n        Parameters\n        ----------\n        **kwargs\n            Pydantic BaseMode model_dump method keyword arguments.\n\n        Returns\n        -------\n        {str: Any}\n            Dictionary representation of the model.\n        \"\"\"\n        model_dict = super().model_dump(**kwargs)\n\n        # remove the name field\n        model_dict.pop(\"name\")\n\n        return model_dict\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_model/#careamics.config.transformations.transform_model.TransformModel.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Return the model as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Pydantic BaseMode model_dump method keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>{str: Any}</code> <p>Dictionary representation of the model.</p> Source code in <code>src/careamics/config/transformations/transform_model.py</code> <pre><code>def model_dump(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the model as a dictionary.\n\n    Parameters\n    ----------\n    **kwargs\n        Pydantic BaseMode model_dump method keyword arguments.\n\n    Returns\n    -------\n    {str: Any}\n        Dictionary representation of the model.\n    \"\"\"\n    model_dict = super().model_dump(**kwargs)\n\n    # remove the name field\n    model_dict.pop(\"name\")\n\n    return model_dict\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_unions/","title":"transform_unions","text":"<p>Type used to represent all transformations users can create.</p>"},{"location":"reference/careamics/config/transformations/transform_unions/#careamics.config.transformations.transform_unions.NORM_AND_SPATIAL_UNION","title":"<code>NORM_AND_SPATIAL_UNION = Annotated[Union[NormalizeModel, XYFlipModel, XYRandomRotate90Model], Discriminator('name')]</code>  <code>module-attribute</code>","text":"<p>All transforms including normalization.</p>"},{"location":"reference/careamics/config/transformations/transform_unions/#careamics.config.transformations.transform_unions.SPATIAL_TRANSFORMS_UNION","title":"<code>SPATIAL_TRANSFORMS_UNION = Annotated[Union[XYFlipModel, XYRandomRotate90Model], Discriminator('name')]</code>  <code>module-attribute</code>","text":"<p>Available spatial transforms in CAREamics.</p>"},{"location":"reference/careamics/config/transformations/xy_flip_model/","title":"xy_flip_model","text":"<p>Pydantic model for the XYFlip transform.</p>"},{"location":"reference/careamics/config/transformations/xy_flip_model/#careamics.config.transformations.xy_flip_model.XYFlipModel","title":"<code>XYFlipModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent XYFlip transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['XYFlip']</code> <p>Name of the transformation.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed for the random number generator,  by default None.</p> Source code in <code>src/careamics/config/transformations/xy_flip_model.py</code> <pre><code>class XYFlipModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent XYFlip transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"XYFlip\"]\n        Name of the transformation.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Seed for the random number generator,  by default None.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"XYFlip\"] = \"XYFlip\"\n    flip_x: bool = Field(\n        True,\n        description=\"Whether to flip along the X axis.\",\n    )\n    flip_y: bool = Field(\n        True,\n        description=\"Whether to flip along the Y axis.\",\n    )\n    p: float = Field(\n        0.5,\n        description=\"Probability of applying the transform.\",\n        ge=0,\n        le=1,\n    )\n    seed: int | None = None\n</code></pre>"},{"location":"reference/careamics/config/transformations/xy_random_rotate90_model/","title":"xy_random_rotate90_model","text":"<p>Pydantic model for the XYRandomRotate90 transform.</p>"},{"location":"reference/careamics/config/transformations/xy_random_rotate90_model/#careamics.config.transformations.xy_random_rotate90_model.XYRandomRotate90Model","title":"<code>XYRandomRotate90Model</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent the XY random 90 degree rotation transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['XYRandomRotate90']</code> <p>Name of the transformation.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed for the random number generator, by default None.</p> Source code in <code>src/careamics/config/transformations/xy_random_rotate90_model.py</code> <pre><code>class XYRandomRotate90Model(TransformModel):\n    \"\"\"\n    Pydantic model used to represent the XY random 90 degree rotation transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"XYRandomRotate90\"]\n        Name of the transformation.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Seed for the random number generator, by default None.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"XYRandomRotate90\"] = \"XYRandomRotate90\"\n    p: float = Field(\n        0.5,\n        description=\"Probability of applying the transform.\",\n        ge=0,\n        le=1,\n    )\n    seed: int | None = None\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/","title":"model_validators","text":"<p>Architecture model validators.</p>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_matching_in_out_channels","title":"<code>model_matching_in_out_channels(model)</code>","text":"<p>Validate that the UNet model has the same number of channel inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>Validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has different number of input and output channels.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_matching_in_out_channels(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the UNet model has the same number of channel inputs and outputs.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        Validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has different number of input and output channels.\n    \"\"\"\n    if model.num_classes != model.in_channels:\n        raise ValueError(\n            \"The algorithm requires the same number of input and output channels. \"\n            \"Make sure that `in_channels` and `num_classes` are equal.\"\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_without_final_activation","title":"<code>model_without_final_activation(model)</code>","text":"<p>Validate that the UNet model does not have the final_activation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>The validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has the final_activation attribute set.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_without_final_activation(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the UNet model does not have the final_activation.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        The validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has the final_activation attribute set.\n    \"\"\"\n    if model.final_activation != \"None\":\n        raise ValueError(\n            \"The algorithm does not support a `final_activation` in the model. \"\n            'Set it to `\"None\"`.'\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_without_n2v2","title":"<code>model_without_n2v2(model)</code>","text":"<p>Validate that the Unet model does not have the n2v2 attribute.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>The validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has the <code>n2v2</code> attribute set to <code>True</code>.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_without_n2v2(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the Unet model does not have the n2v2 attribute.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        The validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has the `n2v2` attribute set to `True`.\n    \"\"\"\n    if model.n2v2:\n        raise ValueError(\n            \"The algorithm does not support the `n2v2` attribute in the model. \"\n            \"Set it to `False`.\"\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/","title":"validator_utils","text":"<p>Validator functions.</p> <p>These functions are used to validate dimensions and axes of inputs.</p>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.check_axes_validity","title":"<code>check_axes_validity(axes)</code>","text":"<p>Sanity check on axes.</p> <p>The constraints on the axes are the following: - must be a combination of 'STCZYX' - must not contain duplicates - must contain at least 2 contiguous axes: X and Y - must contain at most 4 axes - cannot contain both S and T axes</p> <p>Axes do not need to be in the order 'STCZYX', as this depends on the user data.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def check_axes_validity(axes: str) -&gt; None:\n    \"\"\"\n    Sanity check on axes.\n\n    The constraints on the axes are the following:\n    - must be a combination of 'STCZYX'\n    - must not contain duplicates\n    - must contain at least 2 contiguous axes: X and Y\n    - must contain at most 4 axes\n    - cannot contain both S and T axes\n\n    Axes do not need to be in the order 'STCZYX', as this depends on the user data.\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n    \"\"\"\n    _axes = axes.upper()\n\n    # Minimum is 2 (XY) and maximum is 4 (TZYX)\n    if len(_axes) &lt; 2 or len(_axes) &gt; 6:\n        raise ValueError(\n            f\"Invalid axes {axes}. Must contain at least 2 and at most 6 axes.\"\n        )\n\n    if \"YX\" not in _axes and \"XY\" not in _axes:\n        raise ValueError(\n            f\"Invalid axes {axes}. Must contain at least X and Y axes consecutively.\"\n        )\n\n    # all characters must be in REF_AXES = 'STCZYX'\n    if not all(s in _AXES for s in _axes):\n        raise ValueError(f\"Invalid axes {axes}. Must be a combination of {_AXES}.\")\n\n    # check for repeating characters\n    for i, s in enumerate(_axes):\n        if i != _axes.rfind(s):\n            raise ValueError(\n                f\"Invalid axes {axes}. Cannot contain duplicate axes\"\n                f\" (got multiple {axes[i]}).\"\n            )\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.patch_size_ge_than_8_power_of_2","title":"<code>patch_size_ge_than_8_power_of_2(patch_list)</code>","text":"<p>Validate that each entry is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>patch_list</code> <code>Sequence of int, or None</code> <p>Patch size.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size if smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def patch_size_ge_than_8_power_of_2(\n    patch_list: Sequence[int] | None,\n) -&gt; None:\n    \"\"\"\n    Validate that each entry is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    patch_list : Sequence of int, or None\n        Patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size if smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    if patch_list is not None:\n        for dim in patch_list:\n            value_ge_than_8_power_of_2(dim)\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.value_ge_than_8_power_of_2","title":"<code>value_ge_than_8_power_of_2(value)</code>","text":"<p>Validate that the value is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is smaller than 8.</p> <code>ValueError</code> <p>If the value is not a power of 2.</p> Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def value_ge_than_8_power_of_2(\n    value: int,\n) -&gt; None:\n    \"\"\"\n    Validate that the value is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    value : int\n        Value to validate.\n\n    Raises\n    ------\n    ValueError\n        If the value is smaller than 8.\n    ValueError\n        If the value is not a power of 2.\n    \"\"\"\n    if value &lt; 8:\n        raise ValueError(f\"Value must be greater than 8 (got {value}).\")\n\n    if (value &amp; (value - 1)) != 0:\n        raise ValueError(f\"Value must be a power of 2 (got {value}).\")\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/","title":"in_memory_dataset","text":"<p>In-memory dataset module.</p>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset","title":"<code>InMemoryDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset storing data in memory and allowing generating patches from it.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>CAREamics DataConfig</code> <p>(see careamics.config.data_model.DataConfig) Data configuration.</p> required <code>inputs</code> <code>ndarray or list[Path]</code> <p>Input data.</p> required <code>input_target</code> <code>ndarray or list[Path]</code> <p>Target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>class InMemoryDataset(Dataset):\n    \"\"\"Dataset storing data in memory and allowing generating patches from it.\n\n    Parameters\n    ----------\n    data_config : CAREamics DataConfig\n        (see careamics.config.data_model.DataConfig)\n        Data configuration.\n    inputs : numpy.ndarray or list[pathlib.Path]\n        Input data.\n    input_target : numpy.ndarray or list[pathlib.Path], optional\n        Target data, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        inputs: Union[np.ndarray, list[Path]],\n        input_target: Union[np.ndarray, list[Path]] | None = None,\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        data_config : GeneralDataConfig\n            Data configuration.\n        inputs : numpy.ndarray or list[pathlib.Path]\n            Input data.\n        input_target : numpy.ndarray or list[pathlib.Path], optional\n            Target data, by default None.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n        \"\"\"\n        self.data_config = data_config\n        self.inputs = inputs\n        self.input_targets = input_target\n        self.axes = self.data_config.axes\n        self.patch_size = self.data_config.patch_size\n\n        # read function\n        self.read_source_func = read_source_func\n\n        # generate patches\n        supervised = self.input_targets is not None\n        patches_data = self._prepare_patches(supervised)\n\n        # unpack the dataclass\n        self.data = patches_data.patches\n        self.data_targets = patches_data.targets\n\n        # set image statistics\n        if self.data_config.image_means is None:\n            self.image_stats = patches_data.image_stats\n            logger.info(\n                f\"Computed dataset mean: {self.image_stats.means}, \"\n                f\"std: {self.image_stats.stds}\"\n            )\n        else:\n            self.image_stats = Stats(\n                self.data_config.image_means, self.data_config.image_stds\n            )\n\n        # set target statistics\n        if self.data_config.target_means is None:\n            self.target_stats = patches_data.target_stats\n        else:\n            self.target_stats = Stats(\n                self.data_config.target_means, self.data_config.target_stds\n            )\n\n        # update mean and std in configuration\n        # the object is mutable and should then be recorded in the CAREamist obj\n        self.data_config.set_means_and_stds(\n            image_means=self.image_stats.means,\n            image_stds=self.image_stats.stds,\n            target_means=self.target_stats.means,\n            target_stds=self.target_stats.stds,\n        )\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_stats.means,\n                    image_stds=self.image_stats.stds,\n                    target_means=self.target_stats.means,\n                    target_stds=self.target_stats.stds,\n                )\n            ]\n            + list(self.data_config.transforms),\n        )\n\n    def _prepare_patches(self, supervised: bool) -&gt; PatchedOutput:\n        \"\"\"\n        Iterate over data source and create an array of patches.\n\n        Parameters\n        ----------\n        supervised : bool\n            Whether the dataset is supervised or not.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of patches.\n        \"\"\"\n        if supervised:\n            if isinstance(self.inputs, np.ndarray) and isinstance(\n                self.input_targets, np.ndarray\n            ):\n                return prepare_patches_supervised_array(\n                    self.inputs,\n                    self.axes,\n                    self.input_targets,\n                    self.patch_size,\n                )\n            elif isinstance(self.inputs, list) and isinstance(self.input_targets, list):\n                return prepare_patches_supervised(\n                    self.inputs,\n                    self.input_targets,\n                    self.axes,\n                    self.patch_size,\n                    self.read_source_func,\n                )\n            else:\n                raise ValueError(\n                    f\"Data and target must be of the same type, either both numpy \"\n                    f\"arrays or both lists of paths, got {type(self.inputs)} (data) \"\n                    f\"and {type(self.input_targets)} (target).\"\n                )\n        else:\n            if isinstance(self.inputs, np.ndarray):\n                return prepare_patches_unsupervised_array(\n                    self.inputs,\n                    self.axes,\n                    self.patch_size,\n                )\n            else:\n                return prepare_patches_unsupervised(\n                    self.inputs,\n                    self.axes,\n                    self.patch_size,\n                    self.read_source_func,\n                )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return self.data.shape[0]\n\n    def __getitem__(self, index: int) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple of numpy.ndarray\n            Patch.\n\n        Raises\n        ------\n        ValueError\n            If dataset mean and std are not set.\n        \"\"\"\n        patch = self.data[index]\n\n        # if there is a target\n        if self.data_targets is not None:\n            # get target\n            target = self.data_targets[index]\n            return self.patch_transform(patch=patch, target=target)\n\n        return self.patch_transform(patch=patch)\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        This does not return the target data statistics, only those of the input.\n\n        Returns\n        -------\n        tuple of list of floats\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.image_stats.get_statistics()\n\n    def split_dataset(\n        self,\n        percentage: float = 0.1,\n        minimum_patches: int = 1,\n    ) -&gt; InMemoryDataset:\n        \"\"\"Split a new dataset away from the current one.\n\n        This method is used to extract random validation patches from the dataset.\n\n        Parameters\n        ----------\n        percentage : float, optional\n            Percentage of patches to extract, by default 0.1.\n        minimum_patches : int, optional\n            Minimum number of patches to extract, by default 5.\n\n        Returns\n        -------\n        CAREamics InMemoryDataset\n            New dataset with the extracted patches.\n\n        Raises\n        ------\n        ValueError\n            If `percentage` is not between 0 and 1.\n        ValueError\n            If `minimum_number` is not between 1 and the number of patches.\n        \"\"\"\n        if percentage &lt; 0 or percentage &gt; 1:\n            raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n        if minimum_patches &lt; 1 or minimum_patches &gt; len(self):\n            raise ValueError(\n                f\"Minimum number of patches must be between 1 and \"\n                f\"{len(self)} (number of patches), got \"\n                f\"{minimum_patches}. Adjust the patch size or the minimum number of \"\n                f\"patches.\"\n            )\n\n        total_patches = len(self)\n\n        # number of patches to extract (either percentage rounded or minimum number)\n        n_patches = max(round(total_patches * percentage), minimum_patches)\n\n        # get random indices\n        indices = np.random.choice(total_patches, n_patches, replace=False)\n\n        # extract patches\n        val_patches = self.data[indices]\n\n        # remove patches from self.patch\n        self.data = np.delete(self.data, indices, axis=0)\n\n        # same for targets\n        if self.data_targets is not None:\n            val_targets = self.data_targets[indices]\n            self.data_targets = np.delete(self.data_targets, indices, axis=0)\n\n        # clone the dataset\n        dataset = copy.deepcopy(self)\n\n        # reassign patches\n        dataset.data = val_patches\n\n        # reassign targets\n        if self.data_targets is not None:\n            dataset.data_targets = val_targets\n\n        return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Patch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset mean and std are not set.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple of numpy.ndarray\n        Patch.\n\n    Raises\n    ------\n    ValueError\n        If dataset mean and std are not set.\n    \"\"\"\n    patch = self.data[index]\n\n    # if there is a target\n    if self.data_targets is not None:\n        # get target\n        target = self.data_targets[index]\n        return self.patch_transform(patch=patch, target=target)\n\n    return self.patch_transform(patch=patch)\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__init__","title":"<code>__init__(data_config, inputs, input_target=None, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>GeneralDataConfig</code> <p>Data configuration.</p> required <code>inputs</code> <code>ndarray or list[Path]</code> <p>Input data.</p> required <code>input_target</code> <code>ndarray or list[Path]</code> <p>Target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    inputs: Union[np.ndarray, list[Path]],\n    input_target: Union[np.ndarray, list[Path]] | None = None,\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    data_config : GeneralDataConfig\n        Data configuration.\n    inputs : numpy.ndarray or list[pathlib.Path]\n        Input data.\n    input_target : numpy.ndarray or list[pathlib.Path], optional\n        Target data, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n    self.data_config = data_config\n    self.inputs = inputs\n    self.input_targets = input_target\n    self.axes = self.data_config.axes\n    self.patch_size = self.data_config.patch_size\n\n    # read function\n    self.read_source_func = read_source_func\n\n    # generate patches\n    supervised = self.input_targets is not None\n    patches_data = self._prepare_patches(supervised)\n\n    # unpack the dataclass\n    self.data = patches_data.patches\n    self.data_targets = patches_data.targets\n\n    # set image statistics\n    if self.data_config.image_means is None:\n        self.image_stats = patches_data.image_stats\n        logger.info(\n            f\"Computed dataset mean: {self.image_stats.means}, \"\n            f\"std: {self.image_stats.stds}\"\n        )\n    else:\n        self.image_stats = Stats(\n            self.data_config.image_means, self.data_config.image_stds\n        )\n\n    # set target statistics\n    if self.data_config.target_means is None:\n        self.target_stats = patches_data.target_stats\n    else:\n        self.target_stats = Stats(\n            self.data_config.target_means, self.data_config.target_stds\n        )\n\n    # update mean and std in configuration\n    # the object is mutable and should then be recorded in the CAREamist obj\n    self.data_config.set_means_and_stds(\n        image_means=self.image_stats.means,\n        image_stds=self.image_stats.stds,\n        target_means=self.target_stats.means,\n        target_stds=self.target_stats.stds,\n    )\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=self.target_stats.means,\n                target_stds=self.target_stats.stds,\n            )\n        ]\n        + list(self.data_config.transforms),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return self.data.shape[0]\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>This does not return the target data statistics, only those of the input.</p> <p>Returns:</p> Type Description <code>tuple of list of floats</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    This does not return the target data statistics, only those of the input.\n\n    Returns\n    -------\n    tuple of list of floats\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.image_stats.get_statistics()\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.split_dataset","title":"<code>split_dataset(percentage=0.1, minimum_patches=1)</code>","text":"<p>Split a new dataset away from the current one.</p> <p>This method is used to extract random validation patches from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>percentage</code> <code>float</code> <p>Percentage of patches to extract, by default 0.1.</p> <code>0.1</code> <code>minimum_patches</code> <code>int</code> <p>Minimum number of patches to extract, by default 5.</p> <code>1</code> <p>Returns:</p> Type Description <code>CAREamics InMemoryDataset</code> <p>New dataset with the extracted patches.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>percentage</code> is not between 0 and 1.</p> <code>ValueError</code> <p>If <code>minimum_number</code> is not between 1 and the number of patches.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def split_dataset(\n    self,\n    percentage: float = 0.1,\n    minimum_patches: int = 1,\n) -&gt; InMemoryDataset:\n    \"\"\"Split a new dataset away from the current one.\n\n    This method is used to extract random validation patches from the dataset.\n\n    Parameters\n    ----------\n    percentage : float, optional\n        Percentage of patches to extract, by default 0.1.\n    minimum_patches : int, optional\n        Minimum number of patches to extract, by default 5.\n\n    Returns\n    -------\n    CAREamics InMemoryDataset\n        New dataset with the extracted patches.\n\n    Raises\n    ------\n    ValueError\n        If `percentage` is not between 0 and 1.\n    ValueError\n        If `minimum_number` is not between 1 and the number of patches.\n    \"\"\"\n    if percentage &lt; 0 or percentage &gt; 1:\n        raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n    if minimum_patches &lt; 1 or minimum_patches &gt; len(self):\n        raise ValueError(\n            f\"Minimum number of patches must be between 1 and \"\n            f\"{len(self)} (number of patches), got \"\n            f\"{minimum_patches}. Adjust the patch size or the minimum number of \"\n            f\"patches.\"\n        )\n\n    total_patches = len(self)\n\n    # number of patches to extract (either percentage rounded or minimum number)\n    n_patches = max(round(total_patches * percentage), minimum_patches)\n\n    # get random indices\n    indices = np.random.choice(total_patches, n_patches, replace=False)\n\n    # extract patches\n    val_patches = self.data[indices]\n\n    # remove patches from self.patch\n    self.data = np.delete(self.data, indices, axis=0)\n\n    # same for targets\n    if self.data_targets is not None:\n        val_targets = self.data_targets[indices]\n        self.data_targets = np.delete(self.data_targets, indices, axis=0)\n\n    # clone the dataset\n    dataset = copy.deepcopy(self)\n\n    # reassign patches\n    dataset.data = val_patches\n\n    # reassign targets\n    if self.data_targets is not None:\n        dataset.data_targets = val_targets\n\n    return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/","title":"in_memory_pred_dataset","text":"<p>In-memory prediction dataset.</p>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset","title":"<code>InMemoryPredDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Simple prediction dataset returning images along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>class InMemoryPredDataset(Dataset):\n    \"\"\"Simple prediction dataset returning images along the sample axis.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        inputs: NDArray,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Prediction configuration.\n        inputs : NDArray\n            Input data.\n\n        Raises\n        ------\n        ValueError\n            If data_path is not a directory.\n        \"\"\"\n        self.pred_config = prediction_config\n        self.input_array = inputs\n        self.axes = self.pred_config.axes\n        self.image_means = self.pred_config.image_means\n        self.image_stds = self.pred_config.image_stds\n\n        # Reshape data\n        self.data = reshape_array(self.input_array, self.axes)\n\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n            ],\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int) -&gt; tuple[NDArray, ...]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple(numpy.ndarray, ...)\n            Transformed patch.\n        \"\"\"\n        return self.patch_transform(patch=self.data[index])\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple(ndarray, ...)</code> <p>Transformed patch.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[NDArray, ...]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple(numpy.ndarray, ...)\n        Transformed patch.\n    \"\"\"\n    return self.patch_transform(patch=self.data[index])\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__init__","title":"<code>__init__(prediction_config, inputs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_path is not a directory.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    inputs: NDArray,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n\n    Raises\n    ------\n    ValueError\n        If data_path is not a directory.\n    \"\"\"\n    self.pred_config = prediction_config\n    self.input_array = inputs\n    self.axes = self.pred_config.axes\n    self.image_means = self.pred_config.image_means\n    self.image_stds = self.pred_config.image_stds\n\n    # Reshape data\n    self.data = reshape_array(self.input_array, self.axes)\n\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/","title":"in_memory_tiled_pred_dataset","text":"<p>In-memory tiled prediction dataset.</p>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset","title":"<code>InMemoryTiledPredDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Prediction dataset storing data in memory and returning tiles of each image.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>class InMemoryTiledPredDataset(Dataset):\n    \"\"\"Prediction dataset storing data in memory and returning tiles of each image.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        inputs: NDArray,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Prediction configuration.\n        inputs : NDArray\n            Input data.\n\n        Raises\n        ------\n        ValueError\n            If data_path is not a directory.\n        \"\"\"\n        if (\n            prediction_config.tile_size is None\n            or prediction_config.tile_overlap is None\n        ):\n            raise ValueError(\n                \"Tile size and overlap must be provided to use the tiled prediction \"\n                \"dataset.\"\n            )\n\n        self.pred_config = prediction_config\n        self.input_array = inputs\n        self.axes = self.pred_config.axes\n        self.tile_size = prediction_config.tile_size\n        self.tile_overlap = prediction_config.tile_overlap\n        self.image_means = self.pred_config.image_means\n        self.image_stds = self.pred_config.image_stds\n\n        # Generate patches\n        self.data = self._prepare_tiles()\n\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n            ],\n        )\n\n    def _prepare_tiles(self) -&gt; list[tuple[NDArray, TileInformation]]:\n        \"\"\"\n        Iterate over data source and create an array of patches.\n\n        Returns\n        -------\n        list of tuples of NDArray and TileInformation\n            List of tiles and tile information.\n        \"\"\"\n        # reshape array\n        reshaped_sample = reshape_array(self.input_array, self.axes)\n\n        # generate patches, which returns a generator\n        patch_generator = extract_tiles(\n            arr=reshaped_sample,\n            tile_size=self.tile_size,\n            overlaps=self.tile_overlap,\n        )\n        patches_list = list(patch_generator)\n\n        if len(patches_list) == 0:\n            raise ValueError(\"No tiles generated, \")\n\n        return patches_list\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int) -&gt; tuple[tuple[NDArray, ...], TileInformation]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple of NDArray and TileInformation\n            Transformed patch.\n        \"\"\"\n        tile_array, tile_info = self.data[index]\n\n        # Apply transforms\n        transformed_tile = self.patch_transform(patch=tile_array)\n\n        return transformed_tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple of NDArray and TileInformation</code> <p>Transformed patch.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[tuple[NDArray, ...], TileInformation]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple of NDArray and TileInformation\n        Transformed patch.\n    \"\"\"\n    tile_array, tile_info = self.data[index]\n\n    # Apply transforms\n    transformed_tile = self.patch_transform(patch=tile_array)\n\n    return transformed_tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__init__","title":"<code>__init__(prediction_config, inputs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_path is not a directory.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    inputs: NDArray,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n\n    Raises\n    ------\n    ValueError\n        If data_path is not a directory.\n    \"\"\"\n    if (\n        prediction_config.tile_size is None\n        or prediction_config.tile_overlap is None\n    ):\n        raise ValueError(\n            \"Tile size and overlap must be provided to use the tiled prediction \"\n            \"dataset.\"\n        )\n\n    self.pred_config = prediction_config\n    self.input_array = inputs\n    self.axes = self.pred_config.axes\n    self.tile_size = prediction_config.tile_size\n    self.tile_overlap = prediction_config.tile_overlap\n    self.image_means = self.pred_config.image_means\n    self.image_stds = self.pred_config.image_stds\n\n    # Generate patches\n    self.data = self._prepare_tiles()\n\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/","title":"iterable_dataset","text":"<p>Iterable dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset","title":"<code>PathIterableDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Dataset allowing extracting patches w/o loading whole data into memory.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataConfig</code> <p>Data configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>Optional list of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>list of pathlib.Path</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>class PathIterableDataset(IterableDataset):\n    \"\"\"\n    Dataset allowing extracting patches w/o loading whole data into memory.\n\n    Parameters\n    ----------\n    data_config : DataConfig\n        Data configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    target_files : list of pathlib.Path, optional\n        Optional list of target files, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n\n    Attributes\n    ----------\n    data_path : list of pathlib.Path\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        src_files: list[Path],\n        target_files: list[Path] | None = None,\n        read_source_func: Callable = read_tiff,\n    ) -&gt; None:\n        \"\"\"Constructors.\n\n        Parameters\n        ----------\n        data_config : GeneralDataConfig\n            Data configuration.\n        src_files : list[Path]\n            List of data files.\n        target_files : list[Path] or None, optional\n            Optional list of target files, by default None.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        \"\"\"\n        self.data_config = data_config\n        self.data_files = src_files\n        self.target_files = target_files\n        self.read_source_func = read_source_func\n\n        # compute mean and std over the dataset\n        # only checking the image_mean because the DataConfig class ensures that\n        # if image_mean is provided, image_std is also provided\n        if not self.data_config.image_means:\n            self.image_stats, self.target_stats = self._calculate_mean_and_std()\n            logger.info(\n                f\"Computed dataset mean: {self.image_stats.means},\"\n                f\"std: {self.image_stats.stds}\"\n            )\n\n            # update the mean in the config\n            self.data_config.set_means_and_stds(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=(\n                    list(self.target_stats.means)\n                    if self.target_stats.means is not None\n                    else None\n                ),\n                target_stds=(\n                    list(self.target_stats.stds)\n                    if self.target_stats.stds is not None\n                    else None\n                ),\n            )\n\n        else:\n            # if mean and std are provided in the config, use them\n            self.image_stats, self.target_stats = (\n                Stats(self.data_config.image_means, self.data_config.image_stds),\n                Stats(self.data_config.target_means, self.data_config.target_stds),\n            )\n\n        # create transform composed of normalization and other transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_stats.means,\n                    image_stds=self.image_stats.stds,\n                    target_means=self.target_stats.means,\n                    target_stds=self.target_stats.stds,\n                )\n            ]\n            + list(data_config.transforms)\n        )\n\n    def _calculate_mean_and_std(self) -&gt; tuple[Stats, Stats]:\n        \"\"\"\n        Calculate mean and std of the dataset.\n\n        Returns\n        -------\n        tuple of Stats and optional Stats\n            Data classes containing the image and target statistics.\n        \"\"\"\n        num_samples = 0\n        image_stats = WelfordStatistics()\n        if self.target_files is not None:\n            target_stats = WelfordStatistics()\n\n        for sample, target in iterate_over_files(\n            self.data_config, self.data_files, self.target_files, self.read_source_func\n        ):\n            # update the image statistics\n            image_stats.update(sample, num_samples)\n\n            # update the target statistics if target is available\n            if target is not None:\n                target_stats.update(target, num_samples)\n\n            num_samples += 1\n\n        if num_samples == 0:\n            raise ValueError(\"No samples found in the dataset.\")\n\n        # Average the means and stds per sample\n        image_means, image_stds = image_stats.finalize()\n\n        if target is not None:\n            target_means, target_stds = target_stats.finalize()\n\n            return (\n                Stats(image_means, image_stds),\n                Stats(np.array(target_means), np.array(target_stds)),\n            )\n        else:\n            return Stats(image_means, image_stds), Stats(None, None)\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[np.ndarray, ...], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        np.ndarray\n            Single patch.\n        \"\"\"\n        assert (\n            self.image_stats.means is not None and self.image_stats.stds is not None\n        ), \"Mean and std must be provided\"\n\n        # iterate over files\n        for sample_input, sample_target in iterate_over_files(\n            self.data_config, self.data_files, self.target_files, self.read_source_func\n        ):\n            patches = extract_patches_random(\n                arr=sample_input,\n                patch_size=self.data_config.patch_size,\n                target=sample_target,\n            )\n\n            # iterate over patches\n            # patches are tuples of (patch, target) if target is available\n            # or (patch, None) only if no target is available\n            # patch is of dimensions (C)ZYX\n            for patch_data in patches:\n                yield self.patch_transform(\n                    patch=patch_data[0],\n                    target=patch_data[1],\n                )\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        Returns\n        -------\n        tuple of list of floats\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.image_stats.get_statistics()\n\n    def get_number_of_files(self) -&gt; int:\n        \"\"\"\n        Return the number of files in the dataset.\n\n        Returns\n        -------\n        int\n            Number of files in the dataset.\n        \"\"\"\n        return len(self.data_files)\n\n    def split_dataset(\n        self,\n        percentage: float = 0.1,\n        minimum_number: int = 5,\n    ) -&gt; PathIterableDataset:\n        \"\"\"Split up dataset in two.\n\n        Splits the datest sing a percentage of the data (files) to extract, or the\n        minimum number of the percentage is less than the minimum number.\n\n        Parameters\n        ----------\n        percentage : float, optional\n            Percentage of files to split up, by default 0.1.\n        minimum_number : int, optional\n            Minimum number of files to split up, by default 5.\n\n        Returns\n        -------\n        IterableDataset\n            Dataset containing the split data.\n\n        Raises\n        ------\n        ValueError\n            If the percentage is smaller than 0 or larger than 1.\n        ValueError\n            If the minimum number is smaller than 1 or larger than the number of files.\n        \"\"\"\n        if percentage &lt; 0 or percentage &gt; 1:\n            raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n        if minimum_number &lt; 1 or minimum_number &gt; self.get_number_of_files():\n            raise ValueError(\n                f\"Minimum number of files must be between 1 and \"\n                f\"{self.get_number_of_files()} (number of files), got \"\n                f\"{minimum_number}.\"\n            )\n\n        # compute number of files\n        total_files = self.get_number_of_files()\n        n_files = max(round(percentage * total_files), minimum_number)\n\n        # get random indices\n        indices = np.random.choice(total_files, n_files, replace=False)\n\n        # extract files\n        val_files = [self.data_files[i] for i in indices]\n\n        # remove patches from self.patch\n        data_files = []\n        for i, file in enumerate(self.data_files):\n            if i not in indices:\n                data_files.append(file)\n        self.data_files = data_files\n\n        # same for targets\n        if self.target_files is not None:\n            val_target_files = [self.target_files[i] for i in indices]\n\n            data_target_files = []\n            for i, file in enumerate(self.target_files):\n                if i not in indices:\n                    data_target_files.append(file)\n            self.target_files = data_target_files\n\n        # clone the dataset\n        dataset = copy.deepcopy(self)\n\n        # reassign patches\n        dataset.data_files = val_files\n\n        # reassign targets\n        if self.target_files is not None:\n            dataset.target_files = val_target_files\n\n        return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.__init__","title":"<code>__init__(data_config, src_files, target_files=None, read_source_func=read_tiff)</code>","text":"<p>Constructors.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>GeneralDataConfig</code> <p>Data configuration.</p> required <code>src_files</code> <code>list[Path]</code> <p>List of data files.</p> required <code>target_files</code> <code>list[Path] or None</code> <p>Optional list of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    src_files: list[Path],\n    target_files: list[Path] | None = None,\n    read_source_func: Callable = read_tiff,\n) -&gt; None:\n    \"\"\"Constructors.\n\n    Parameters\n    ----------\n    data_config : GeneralDataConfig\n        Data configuration.\n    src_files : list[Path]\n        List of data files.\n    target_files : list[Path] or None, optional\n        Optional list of target files, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    \"\"\"\n    self.data_config = data_config\n    self.data_files = src_files\n    self.target_files = target_files\n    self.read_source_func = read_source_func\n\n    # compute mean and std over the dataset\n    # only checking the image_mean because the DataConfig class ensures that\n    # if image_mean is provided, image_std is also provided\n    if not self.data_config.image_means:\n        self.image_stats, self.target_stats = self._calculate_mean_and_std()\n        logger.info(\n            f\"Computed dataset mean: {self.image_stats.means},\"\n            f\"std: {self.image_stats.stds}\"\n        )\n\n        # update the mean in the config\n        self.data_config.set_means_and_stds(\n            image_means=self.image_stats.means,\n            image_stds=self.image_stats.stds,\n            target_means=(\n                list(self.target_stats.means)\n                if self.target_stats.means is not None\n                else None\n            ),\n            target_stds=(\n                list(self.target_stats.stds)\n                if self.target_stats.stds is not None\n                else None\n            ),\n        )\n\n    else:\n        # if mean and std are provided in the config, use them\n        self.image_stats, self.target_stats = (\n            Stats(self.data_config.image_means, self.data_config.image_stds),\n            Stats(self.data_config.target_means, self.data_config.target_stds),\n        )\n\n    # create transform composed of normalization and other transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=self.target_stats.means,\n                target_stds=self.target_stats.stds,\n            )\n        ]\n        + list(data_config.transforms)\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Single patch.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[np.ndarray, ...], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    np.ndarray\n        Single patch.\n    \"\"\"\n    assert (\n        self.image_stats.means is not None and self.image_stats.stds is not None\n    ), \"Mean and std must be provided\"\n\n    # iterate over files\n    for sample_input, sample_target in iterate_over_files(\n        self.data_config, self.data_files, self.target_files, self.read_source_func\n    ):\n        patches = extract_patches_random(\n            arr=sample_input,\n            patch_size=self.data_config.patch_size,\n            target=sample_target,\n        )\n\n        # iterate over patches\n        # patches are tuples of (patch, target) if target is available\n        # or (patch, None) only if no target is available\n        # patch is of dimensions (C)ZYX\n        for patch_data in patches:\n            yield self.patch_transform(\n                patch=patch_data[0],\n                target=patch_data[1],\n            )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>Returns:</p> Type Description <code>tuple of list of floats</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    Returns\n    -------\n    tuple of list of floats\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.image_stats.get_statistics()\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.get_number_of_files","title":"<code>get_number_of_files()</code>","text":"<p>Return the number of files in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of files in the dataset.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def get_number_of_files(self) -&gt; int:\n    \"\"\"\n    Return the number of files in the dataset.\n\n    Returns\n    -------\n    int\n        Number of files in the dataset.\n    \"\"\"\n    return len(self.data_files)\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.split_dataset","title":"<code>split_dataset(percentage=0.1, minimum_number=5)</code>","text":"<p>Split up dataset in two.</p> <p>Splits the datest sing a percentage of the data (files) to extract, or the minimum number of the percentage is less than the minimum number.</p> <p>Parameters:</p> Name Type Description Default <code>percentage</code> <code>float</code> <p>Percentage of files to split up, by default 0.1.</p> <code>0.1</code> <code>minimum_number</code> <code>int</code> <p>Minimum number of files to split up, by default 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>IterableDataset</code> <p>Dataset containing the split data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the percentage is smaller than 0 or larger than 1.</p> <code>ValueError</code> <p>If the minimum number is smaller than 1 or larger than the number of files.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def split_dataset(\n    self,\n    percentage: float = 0.1,\n    minimum_number: int = 5,\n) -&gt; PathIterableDataset:\n    \"\"\"Split up dataset in two.\n\n    Splits the datest sing a percentage of the data (files) to extract, or the\n    minimum number of the percentage is less than the minimum number.\n\n    Parameters\n    ----------\n    percentage : float, optional\n        Percentage of files to split up, by default 0.1.\n    minimum_number : int, optional\n        Minimum number of files to split up, by default 5.\n\n    Returns\n    -------\n    IterableDataset\n        Dataset containing the split data.\n\n    Raises\n    ------\n    ValueError\n        If the percentage is smaller than 0 or larger than 1.\n    ValueError\n        If the minimum number is smaller than 1 or larger than the number of files.\n    \"\"\"\n    if percentage &lt; 0 or percentage &gt; 1:\n        raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n    if minimum_number &lt; 1 or minimum_number &gt; self.get_number_of_files():\n        raise ValueError(\n            f\"Minimum number of files must be between 1 and \"\n            f\"{self.get_number_of_files()} (number of files), got \"\n            f\"{minimum_number}.\"\n        )\n\n    # compute number of files\n    total_files = self.get_number_of_files()\n    n_files = max(round(percentage * total_files), minimum_number)\n\n    # get random indices\n    indices = np.random.choice(total_files, n_files, replace=False)\n\n    # extract files\n    val_files = [self.data_files[i] for i in indices]\n\n    # remove patches from self.patch\n    data_files = []\n    for i, file in enumerate(self.data_files):\n        if i not in indices:\n            data_files.append(file)\n    self.data_files = data_files\n\n    # same for targets\n    if self.target_files is not None:\n        val_target_files = [self.target_files[i] for i in indices]\n\n        data_target_files = []\n        for i, file in enumerate(self.target_files):\n            if i not in indices:\n                data_target_files.append(file)\n        self.target_files = data_target_files\n\n    # clone the dataset\n    dataset = copy.deepcopy(self)\n\n    # reassign patches\n    dataset.data_files = val_files\n\n    # reassign targets\n    if self.target_files is not None:\n        dataset.target_files = val_target_files\n\n    return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/","title":"iterable_pred_dataset","text":"<p>Iterable prediction dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset","title":"<code>IterablePredDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Simple iterable prediction dataset.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>List[Path]</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>Union[str, Path]</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> <code>mean</code> <code>(Optional[float], optional)</code> <p>Expected mean of the dataset, by default None.</p> <code>std</code> <code>(Optional[float], optional)</code> <p>Expected standard deviation of the dataset, by default None.</p> <code>patch_transform</code> <code>(Optional[Callable], optional)</code> <p>Patch transform callable, by default None.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>class IterablePredDataset(IterableDataset):\n    \"\"\"Simple iterable prediction dataset.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : List[Path]\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Attributes\n    ----------\n    data_path : Union[str, Path]\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    mean : Optional[float], optional\n        Expected mean of the dataset, by default None.\n    std : Optional[float], optional\n        Expected standard deviation of the dataset, by default None.\n    patch_transform : Optional[Callable], optional\n        Patch transform callable, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        src_files: list[Path],\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Inference configuration.\n        src_files : list of pathlib.Path\n            List of data files.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the inference configuration.\n        \"\"\"\n        self.prediction_config = prediction_config\n        self.data_files = src_files\n        self.axes = prediction_config.axes\n        self.read_source_func = read_source_func\n\n        # check mean and std and create normalize transform\n        if (\n            self.prediction_config.image_means is None\n            or self.prediction_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided for prediction.\")\n        else:\n            self.image_means = self.prediction_config.image_means\n            self.image_stds = self.prediction_config.image_stds\n\n        # instantiate normalize transform\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_means,\n                    image_stds=self.image_stds,\n                )\n            ],\n        )\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[NDArray, ...], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        (numpy.ndarray, numpy.ndarray or None)\n            Single patch.\n        \"\"\"\n        assert (\n            self.image_means is not None and self.image_stds is not None\n        ), \"Mean and std must be provided\"\n\n        for sample, _ in iterate_over_files(\n            self.prediction_config,\n            self.data_files,\n            read_source_func=self.read_source_func,\n        ):\n            # sample has S dimension\n            for i in range(sample.shape[0]):\n\n                yield self.patch_transform(patch=sample[i])\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset.__init__","title":"<code>__init__(prediction_config, src_files, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the inference configuration.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    src_files: list[Path],\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the inference configuration.\n    \"\"\"\n    self.prediction_config = prediction_config\n    self.data_files = src_files\n    self.axes = prediction_config.axes\n    self.read_source_func = read_source_func\n\n    # check mean and std and create normalize transform\n    if (\n        self.prediction_config.image_means is None\n        or self.prediction_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided for prediction.\")\n    else:\n        self.image_means = self.prediction_config.image_means\n        self.image_stds = self.prediction_config.image_stds\n\n    # instantiate normalize transform\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_means,\n                image_stds=self.image_stds,\n            )\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>(ndarray, ndarray or None)</code> <p>Single patch.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[NDArray, ...], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    (numpy.ndarray, numpy.ndarray or None)\n        Single patch.\n    \"\"\"\n    assert (\n        self.image_means is not None and self.image_stds is not None\n    ), \"Mean and std must be provided\"\n\n    for sample, _ in iterate_over_files(\n        self.prediction_config,\n        self.data_files,\n        read_source_func=self.read_source_func,\n    ):\n        # sample has S dimension\n        for i in range(sample.shape[0]):\n\n            yield self.patch_transform(patch=sample[i])\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/","title":"iterable_tiled_pred_dataset","text":"<p>Iterable tiled prediction dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset","title":"<code>IterableTiledPredDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Tiled prediction dataset.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str or Path</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> <code>mean</code> <code>(float, optional)</code> <p>Expected mean of the dataset, by default None.</p> <code>std</code> <code>(float, optional)</code> <p>Expected standard deviation of the dataset, by default None.</p> <code>patch_transform</code> <code>(Callable, optional)</code> <p>Patch transform callable, by default None.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>class IterableTiledPredDataset(IterableDataset):\n    \"\"\"Tiled prediction dataset.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Attributes\n    ----------\n    data_path : str or pathlib.Path\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    mean : float, optional\n        Expected mean of the dataset, by default None.\n    std : float, optional\n        Expected standard deviation of the dataset, by default None.\n    patch_transform : Callable, optional\n        Patch transform callable, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        src_files: list[Path],\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Inference configuration.\n        src_files : List[Path]\n            List of data files.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the inference configuration.\n        \"\"\"\n        if (\n            prediction_config.tile_size is None\n            or prediction_config.tile_overlap is None\n        ):\n            raise ValueError(\n                \"Tile size and overlap must be provided for tiled prediction.\"\n            )\n\n        self.prediction_config = prediction_config\n        self.data_files = src_files\n        self.axes = prediction_config.axes\n        self.tile_size = prediction_config.tile_size\n        self.tile_overlap = prediction_config.tile_overlap\n        self.read_source_func = read_source_func\n\n        # check mean and std and create normalize transform\n        if (\n            self.prediction_config.image_means is None\n            or self.prediction_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided for prediction.\")\n        else:\n            self.image_means = self.prediction_config.image_means\n            self.image_stds = self.prediction_config.image_stds\n\n            # instantiate normalize transform\n            self.patch_transform = Compose(\n                transform_list=[\n                    NormalizeModel(\n                        image_means=self.image_means,\n                        image_stds=self.image_stds,\n                    )\n                ],\n            )\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[tuple[NDArray, ...], TileInformation], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple\n            Generator of single tiles.\n        \"\"\"\n        assert (\n            self.image_means is not None and self.image_stds is not None\n        ), \"Mean and std must be provided\"\n\n        for sample, _ in iterate_over_files(\n            self.prediction_config,\n            self.data_files,\n            read_source_func=self.read_source_func,\n        ):\n            # generate patches, return a generator of single tiles\n            patch_gen = extract_tiles(\n                arr=sample,\n                tile_size=self.tile_size,\n                overlaps=self.tile_overlap,\n            )\n\n            # apply transform to patches\n            for patch_array, tile_info in patch_gen:\n                transformed_patch = self.patch_transform(patch=patch_array)\n\n                yield transformed_patch, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset.__init__","title":"<code>__init__(prediction_config, src_files, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>List[Path]</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the inference configuration.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    src_files: list[Path],\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : List[Path]\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the inference configuration.\n    \"\"\"\n    if (\n        prediction_config.tile_size is None\n        or prediction_config.tile_overlap is None\n    ):\n        raise ValueError(\n            \"Tile size and overlap must be provided for tiled prediction.\"\n        )\n\n    self.prediction_config = prediction_config\n    self.data_files = src_files\n    self.axes = prediction_config.axes\n    self.tile_size = prediction_config.tile_size\n    self.tile_overlap = prediction_config.tile_overlap\n    self.read_source_func = read_source_func\n\n    # check mean and std and create normalize transform\n    if (\n        self.prediction_config.image_means is None\n        or self.prediction_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided for prediction.\")\n    else:\n        self.image_means = self.prediction_config.image_means\n        self.image_stds = self.prediction_config.image_stds\n\n        # instantiate normalize transform\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_means,\n                    image_stds=self.image_stds,\n                )\n            ],\n        )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple</code> <p>Generator of single tiles.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[tuple[NDArray, ...], TileInformation], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple\n        Generator of single tiles.\n    \"\"\"\n    assert (\n        self.image_means is not None and self.image_stds is not None\n    ), \"Mean and std must be provided\"\n\n    for sample, _ in iterate_over_files(\n        self.prediction_config,\n        self.data_files,\n        read_source_func=self.read_source_func,\n    ):\n        # generate patches, return a generator of single tiles\n        patch_gen = extract_tiles(\n            arr=sample,\n            tile_size=self.tile_size,\n            overlaps=self.tile_overlap,\n        )\n\n        # apply transform to patches\n        for patch_array, tile_info in patch_gen:\n            transformed_patch = self.patch_transform(patch=patch_array)\n\n            yield transformed_patch, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/dataset_utils/","title":"dataset_utils","text":"<p>Dataset utilities.</p>"},{"location":"reference/careamics/dataset/dataset_utils/dataset_utils/#careamics.dataset.dataset_utils.dataset_utils.reshape_array","title":"<code>reshape_array(x, axes)</code>","text":"<p>Reshape the data to (S, C, (Z), Y, X) by moving axes.</p> <p>If the data has both S and T axes, the two axes will be merged. A singleton dimension is added if there are no C axis.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <code>axes</code> <code>str</code> <p>Description of axes in format <code>STCZYX</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Reshaped array with shape (S, C, (Z), Y, X).</p> Source code in <code>src/careamics/dataset/dataset_utils/dataset_utils.py</code> <pre><code>def reshape_array(x: np.ndarray, axes: str) -&gt; np.ndarray:\n    \"\"\"Reshape the data to (S, C, (Z), Y, X) by moving axes.\n\n    If the data has both S and T axes, the two axes will be merged. A singleton\n    dimension is added if there are no C axis.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array.\n    axes : str\n        Description of axes in format `STCZYX`.\n\n    Returns\n    -------\n    np.ndarray\n        Reshaped array with shape (S, C, (Z), Y, X).\n    \"\"\"\n    _x = x\n    _axes = axes\n\n    # sanity checks\n    if len(_axes) != len(_x.shape):\n        raise ValueError(\n            f\"Incompatible data shape ({_x.shape}) and axes ({_axes}). Are the axes \"\n            f\"correct?\"\n        )\n\n    # get new x shape\n    new_x_shape, new_axes, indices = _get_shape_order(_x.shape, _axes)\n\n    # if S is not in the list of axes, then add a singleton S\n    if \"S\" not in new_axes:\n        new_axes = \"S\" + new_axes\n        _x = _x[np.newaxis, ...]\n        new_x_shape = (1,) + new_x_shape\n\n        # need to change the array of indices\n        indices = [0] + [1 + i for i in indices]\n\n    # reshape by moving axes\n    destination = list(range(len(indices)))\n    _x = np.moveaxis(_x, indices, destination)\n\n    # remove T if necessary\n    if \"T\" in new_axes:\n        new_x_shape = (-1,) + new_x_shape[2:]  # remove T and S\n        new_axes = new_axes.replace(\"T\", \"\")\n\n        # reshape S and T together\n        _x = _x.reshape(new_x_shape)\n\n    # add channel\n    if \"C\" not in new_axes:\n        # Add channel axis after S\n        _x = np.expand_dims(_x, new_axes.index(\"S\") + 1)\n\n    return _x\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/","title":"file_utils","text":"<p>File utilities.</p>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.get_files_size","title":"<code>get_files_size(files)</code>","text":"<p>Get files size in MB.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list of pathlib.Path</code> <p>List of files.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Total size of the files in MB.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def get_files_size(files: list[Path]) -&gt; float:\n    \"\"\"Get files size in MB.\n\n    Parameters\n    ----------\n    files : list of pathlib.Path\n        List of files.\n\n    Returns\n    -------\n    float\n        Total size of the files in MB.\n    \"\"\"\n    return np.sum([f.stat().st_size / 1024**2 for f in files])\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.list_files","title":"<code>list_files(data_path, data_type, extension_filter='')</code>","text":"<p>List recursively files in <code>data_path</code> and return a sorted list.</p> <p>If <code>data_path</code> is a file, its name is validated against the <code>data_type</code> using <code>fnmatch</code>, and the method returns <code>data_path</code> itself.</p> <p>By default, if <code>data_type</code> is equal to <code>custom</code>, all files will be listed. To further filter the files, use <code>extension_filter</code>.</p> <p><code>extension_filter</code> must be compatible with <code>fnmatch</code> and <code>Path.rglob</code>, e.g. \".npy\" or \".czi\".</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, Path]</code> <p>Path to the folder containing the data.</p> required <code>data_type</code> <code>Union[str, SupportedData]</code> <p>One of the supported data type (e.g. tif, custom).</p> required <code>extension_filter</code> <code>str</code> <p>Extension filter, by default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>list of pathlib.Path objects.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data path does not exist.</p> <code>ValueError</code> <p>If the data path is empty or no files with the extension were found.</p> <code>ValueError</code> <p>If the file does not match the requested extension.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def list_files(\n    data_path: Union[str, Path],\n    data_type: Union[str, SupportedData],\n    extension_filter: str = \"\",\n) -&gt; list[Path]:\n    \"\"\"List recursively files in `data_path` and return a sorted list.\n\n    If `data_path` is a file, its name is validated against the `data_type` using\n    `fnmatch`, and the method returns `data_path` itself.\n\n    By default, if `data_type` is equal to `custom`, all files will be listed. To\n    further filter the files, use `extension_filter`.\n\n    `extension_filter` must be compatible with `fnmatch` and `Path.rglob`, e.g. \"*.npy\"\n    or \"*.czi\".\n\n    Parameters\n    ----------\n    data_path : Union[str, Path]\n        Path to the folder containing the data.\n    data_type : Union[str, SupportedData]\n        One of the supported data type (e.g. tif, custom).\n    extension_filter : str, optional\n        Extension filter, by default \"\".\n\n    Returns\n    -------\n    list[Path]\n        list of pathlib.Path objects.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the data path does not exist.\n    ValueError\n        If the data path is empty or no files with the extension were found.\n    ValueError\n        If the file does not match the requested extension.\n    \"\"\"\n    # convert to Path\n    data_path = Path(data_path)\n\n    # raise error if does not exists\n    if not data_path.exists():\n        raise FileNotFoundError(f\"Data path {data_path} does not exist.\")\n\n    # get extension compatible with fnmatch and rglob search\n    extension = SupportedData.get_extension_pattern(data_type)\n\n    if data_type == SupportedData.CUSTOM and extension_filter != \"\":\n        extension = extension_filter\n\n    # search recurively\n    if data_path.is_dir():\n        # search recursively the path for files with the extension\n        files = sorted(data_path.rglob(extension))\n    else:\n        # raise error if it has the wrong extension\n        if not fnmatch(str(data_path.absolute()), extension):\n            raise ValueError(\n                f\"File {data_path} does not match the requested extension \"\n                f'\"{extension}\".'\n            )\n\n        # save in list\n        files = [data_path]\n\n    # raise error if no files were found\n    if len(files) == 0:\n        raise ValueError(\n            f'Data path {data_path} is empty or files with extension \"{extension}\" '\n            f\"were not found.\"\n        )\n\n    return files\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.validate_source_target_files","title":"<code>validate_source_target_files(src_files, tar_files)</code>","text":"<p>Validate source and target path lists.</p> <p>The two lists should have the same number of files, and the filenames should match.</p> <p>Parameters:</p> Name Type Description Default <code>src_files</code> <code>list of pathlib.Path</code> <p>List of source files.</p> required <code>tar_files</code> <code>list of pathlib.Path</code> <p>List of target files.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of files in source and target folders is not the same.</p> <code>ValueError</code> <p>If some filenames in Train and target folders are not the same.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def validate_source_target_files(src_files: list[Path], tar_files: list[Path]) -&gt; None:\n    \"\"\"\n    Validate source and target path lists.\n\n    The two lists should have the same number of files, and the filenames should match.\n\n    Parameters\n    ----------\n    src_files : list of pathlib.Path\n        List of source files.\n    tar_files : list of pathlib.Path\n        List of target files.\n\n    Raises\n    ------\n    ValueError\n        If the number of files in source and target folders is not the same.\n    ValueError\n        If some filenames in Train and target folders are not the same.\n    \"\"\"\n    # check equal length\n    if len(src_files) != len(tar_files):\n        raise ValueError(\n            f\"The number of source files ({len(src_files)}) is not equal to the number \"\n            f\"of target files ({len(tar_files)}).\"\n        )\n\n    # check identical names\n    src_names = {f.name for f in src_files}\n    tar_names = {f.name for f in tar_files}\n    difference = src_names.symmetric_difference(tar_names)\n\n    if len(difference) &gt; 0:\n        raise ValueError(f\"Source and target files have different names: {difference}.\")\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/iterate_over_files/","title":"iterate_over_files","text":"<p>Function to iterate over files.</p>"},{"location":"reference/careamics/dataset/dataset_utils/iterate_over_files/#careamics.dataset.dataset_utils.iterate_over_files.iterate_over_files","title":"<code>iterate_over_files(data_config, data_files, target_files=None, read_source_func=read_tiff)</code>","text":"<p>Iterate over data source and yield whole reshaped images.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>CAREamics DataConfig or InferenceConfig</code> <p>Configuration.</p> required <code>data_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>List of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source, by default read_tiff.</p> <code>read_tiff</code> <p>Yields:</p> Type Description <code>NDArray</code> <p>Image.</p> Source code in <code>src/careamics/dataset/dataset_utils/iterate_over_files.py</code> <pre><code>def iterate_over_files(\n    data_config: Union[DataConfig, InferenceConfig],\n    data_files: list[Path],\n    target_files: list[Path] | None = None,\n    read_source_func: Callable = read_tiff,\n) -&gt; Generator[tuple[NDArray, NDArray | None], None, None]:\n    \"\"\"Iterate over data source and yield whole reshaped images.\n\n    Parameters\n    ----------\n    data_config : CAREamics DataConfig or InferenceConfig\n        Configuration.\n    data_files : list of pathlib.Path\n        List of data files.\n    target_files : list of pathlib.Path, optional\n        List of target files, by default None.\n    read_source_func : Callable, optional\n        Function to read the source, by default read_tiff.\n\n    Yields\n    ------\n    NDArray\n        Image.\n    \"\"\"\n    # When num_workers &gt; 0, each worker process will have a different copy of the\n    # dataset object\n    # Configuring each copy independently to avoid having duplicate data returned\n    # from the workers\n    worker_info = get_worker_info()\n    worker_id = worker_info.id if worker_info is not None else 0\n    num_workers = worker_info.num_workers if worker_info is not None else 1\n\n    # iterate over the files\n    for i, filename in enumerate(data_files):\n        # retrieve file corresponding to the worker id\n        if i % num_workers == worker_id:\n            try:\n                # read data\n                sample = read_source_func(filename, data_config.axes)\n\n                # reshape array\n                reshaped_sample = reshape_array(sample, data_config.axes)\n\n                # read target, if available\n                if target_files is not None:\n                    if filename.name != target_files[i].name:\n                        raise ValueError(\n                            f\"File {filename} does not match target file \"\n                            f\"{target_files[i]}. Have you passed sorted \"\n                            f\"arrays?\"\n                        )\n\n                    # read target\n                    target = read_source_func(target_files[i], data_config.axes)\n\n                    # reshape target\n                    reshaped_target = reshape_array(target, data_config.axes)\n\n                    yield reshaped_sample, reshaped_target\n                else:\n                    yield reshaped_sample, None\n\n            except Exception as e:\n                logger.error(f\"Error reading file {filename}: {e}\")\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/","title":"running_stats","text":"<p>Computing data statistics.</p>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics","title":"<code>WelfordStatistics</code>","text":"<p>Compute Welford statistics iteratively.</p> <p>The Welford algorithm is used to compute the mean and variance of an array iteratively. Based on the implementation from: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>class WelfordStatistics:\n    \"\"\"Compute Welford statistics iteratively.\n\n    The Welford algorithm is used to compute the mean and variance of an array\n    iteratively. Based on the implementation from:\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n    \"\"\"\n\n    def update(self, array: NDArray, sample_idx: int) -&gt; None:\n        \"\"\"Update the Welford statistics.\n\n        Parameters\n        ----------\n        array : NDArray\n            Input array.\n        sample_idx : int\n            Current sample number.\n        \"\"\"\n        self.sample_idx = sample_idx\n        sample_channels = np.array(np.split(array, array.shape[1], axis=1))\n\n        # Initialize the statistics\n        if self.sample_idx == 0:\n            # Compute the mean and standard deviation\n            self.mean, _ = compute_normalization_stats(array)\n            # Initialize the count and m2 with zero-valued arrays of shape (C,)\n            self.count, self.mean, self.m2 = update_iterative_stats(\n                count=np.zeros(array.shape[1]),\n                mean=self.mean,\n                m2=np.zeros(array.shape[1]),\n                new_values=sample_channels,\n            )\n        else:\n            # Update the statistics\n            self.count, self.mean, self.m2 = update_iterative_stats(\n                count=self.count, mean=self.mean, m2=self.m2, new_values=sample_channels\n            )\n\n        self.sample_idx += 1\n\n    def finalize(self) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"Finalize the Welford statistics.\n\n        Returns\n        -------\n        tuple or numpy arrays\n            Final mean and standard deviation.\n        \"\"\"\n        return finalize_iterative_stats(self.count, self.mean, self.m2)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the Welford statistics.</p> <p>Returns:</p> Type Description <code>tuple or numpy arrays</code> <p>Final mean and standard deviation.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def finalize(self) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"Finalize the Welford statistics.\n\n    Returns\n    -------\n    tuple or numpy arrays\n        Final mean and standard deviation.\n    \"\"\"\n    return finalize_iterative_stats(self.count, self.mean, self.m2)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics.update","title":"<code>update(array, sample_idx)</code>","text":"<p>Update the Welford statistics.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>Input array.</p> required <code>sample_idx</code> <code>int</code> <p>Current sample number.</p> required Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def update(self, array: NDArray, sample_idx: int) -&gt; None:\n    \"\"\"Update the Welford statistics.\n\n    Parameters\n    ----------\n    array : NDArray\n        Input array.\n    sample_idx : int\n        Current sample number.\n    \"\"\"\n    self.sample_idx = sample_idx\n    sample_channels = np.array(np.split(array, array.shape[1], axis=1))\n\n    # Initialize the statistics\n    if self.sample_idx == 0:\n        # Compute the mean and standard deviation\n        self.mean, _ = compute_normalization_stats(array)\n        # Initialize the count and m2 with zero-valued arrays of shape (C,)\n        self.count, self.mean, self.m2 = update_iterative_stats(\n            count=np.zeros(array.shape[1]),\n            mean=self.mean,\n            m2=np.zeros(array.shape[1]),\n            new_values=sample_channels,\n        )\n    else:\n        # Update the statistics\n        self.count, self.mean, self.m2 = update_iterative_stats(\n            count=self.count, mean=self.mean, m2=self.m2, new_values=sample_channels\n        )\n\n    self.sample_idx += 1\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.compute_normalization_stats","title":"<code>compute_normalization_stats(image)</code>","text":"<p>Compute mean and standard deviation of an array.</p> <p>Expected input shape is (S, C, (Z), Y, X). The mean and standard deviation are computed per channel.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>tuple of (list of floats, list of floats)</code> <p>Lists of mean and standard deviation values per channel.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def compute_normalization_stats(image: NDArray) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Compute mean and standard deviation of an array.\n\n    Expected input shape is (S, C, (Z), Y, X). The mean and standard deviation are\n    computed per channel.\n\n    Parameters\n    ----------\n    image : NDArray\n        Input array.\n\n    Returns\n    -------\n    tuple of (list of floats, list of floats)\n        Lists of mean and standard deviation values per channel.\n    \"\"\"\n    # Define the lists for storing mean and std values\n    means, stds = [], []\n    # Iterate over the channels dimension and compute mean and std\n    for ax in range(image.shape[1]):\n        means.append(image[:, ax, ...].mean())\n        stds.append(image[:, ax, ...].std())\n    return np.stack(means), np.stack(stds)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.finalize_iterative_stats","title":"<code>finalize_iterative_stats(count, mean, m2)</code>","text":"<p>Finalize the mean and variance computation.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>NDArray</code> <p>Number of elements in the array. Shape: (C,).</p> required <code>mean</code> <code>NDArray</code> <p>Mean of the array. Shape: (C,).</p> required <code>m2</code> <code>NDArray</code> <p>Variance of the array. Shape: (C,).</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>Final channel-wise mean and standard deviation.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def finalize_iterative_stats(\n    count: NDArray, mean: NDArray, m2: NDArray\n) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"Finalize the mean and variance computation.\n\n    Parameters\n    ----------\n    count : NDArray\n        Number of elements in the array. Shape: (C,).\n    mean : NDArray\n        Mean of the array. Shape: (C,).\n    m2 : NDArray\n        Variance of the array. Shape: (C,).\n\n    Returns\n    -------\n    tuple[NDArray, NDArray]\n        Final channel-wise mean and standard deviation.\n    \"\"\"\n    std = np.sqrt(m2 / count)\n    if any(c &lt; 2 for c in count):\n        return np.full(mean.shape, np.nan), np.full(std.shape, np.nan)\n    else:\n        return mean, std\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.update_iterative_stats","title":"<code>update_iterative_stats(count, mean, m2, new_values)</code>","text":"<p>Update the mean and variance of an array iteratively.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>NDArray</code> <p>Number of elements in the array. Shape: (C,).</p> required <code>mean</code> <code>NDArray</code> <p>Mean of the array. Shape: (C,).</p> required <code>m2</code> <code>NDArray</code> <p>Variance of the array. Shape: (C,).</p> required <code>new_values</code> <code>NDArray</code> <p>New values to add to the mean and variance. Shape: (C, 1, 1, Z, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray, NDArray]</code> <p>Updated count, mean, and variance.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def update_iterative_stats(\n    count: NDArray, mean: NDArray, m2: NDArray, new_values: NDArray\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"Update the mean and variance of an array iteratively.\n\n    Parameters\n    ----------\n    count : NDArray\n        Number of elements in the array. Shape: (C,).\n    mean : NDArray\n        Mean of the array. Shape: (C,).\n    m2 : NDArray\n        Variance of the array. Shape: (C,).\n    new_values : NDArray\n        New values to add to the mean and variance. Shape: (C, 1, 1, Z, Y, X).\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        Updated count, mean, and variance.\n    \"\"\"\n    num_channels = len(new_values)\n\n    # --- update channel-wise counts ---\n    count += np.ones_like(count) * np.prod(new_values.shape[1:])\n\n    # --- update channel-wise mean ---\n    # compute (new_values - old_mean) -&gt; shape: (C, Z*Y*X)\n    delta = new_values.reshape(num_channels, -1) - mean.reshape(num_channels, 1)\n    mean += np.sum(delta / count.reshape(num_channels, 1), axis=1)\n\n    # --- update channel-wise SoS ---\n    # compute (new_values - new_mean) -&gt; shape: (C, Z*Y*X)\n    delta2 = new_values.reshape(num_channels, -1) - mean.reshape(num_channels, 1)\n    m2 += np.sum(delta * delta2, axis=1)\n\n    return count, mean, m2\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/","title":"patching","text":"<p>Patching functions.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput","title":"<code>PatchedOutput</code>  <code>dataclass</code>","text":"<p>Dataclass to store patches and statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>@dataclass\nclass PatchedOutput:\n    \"\"\"Dataclass to store patches and statistics.\"\"\"\n\n    patches: Union[NDArray]\n    \"\"\"Image patches.\"\"\"\n\n    targets: Union[NDArray, None]\n    \"\"\"Target patches.\"\"\"\n\n    image_stats: Stats\n    \"\"\"Statistics of the image patches.\"\"\"\n\n    target_stats: Stats\n    \"\"\"Statistics of the target patches.\"\"\"\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.image_stats","title":"<code>image_stats</code>  <code>instance-attribute</code>","text":"<p>Statistics of the image patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.patches","title":"<code>patches</code>  <code>instance-attribute</code>","text":"<p>Image patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.target_stats","title":"<code>target_stats</code>  <code>instance-attribute</code>","text":"<p>Statistics of the target patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.targets","title":"<code>targets</code>  <code>instance-attribute</code>","text":"<p>Target patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats","title":"<code>Stats</code>  <code>dataclass</code>","text":"<p>Dataclass to store statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>@dataclass\nclass Stats:\n    \"\"\"Dataclass to store statistics.\"\"\"\n\n    means: Union[NDArray, tuple, list, None]\n    \"\"\"Mean of the data across channels.\"\"\"\n\n    stds: Union[NDArray, tuple, list, None]\n    \"\"\"Standard deviation of the data across channels.\"\"\"\n\n    def get_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return the means and standard deviations.\n\n        Returns\n        -------\n        tuple of two lists of floats\n            Means and standard deviations.\n        \"\"\"\n        if self.means is None or self.stds is None:\n            return [], []\n\n        return list(self.means), list(self.stds)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.means","title":"<code>means</code>  <code>instance-attribute</code>","text":"<p>Mean of the data across channels.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.stds","title":"<code>stds</code>  <code>instance-attribute</code>","text":"<p>Standard deviation of the data across channels.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Return the means and standard deviations.</p> <p>Returns:</p> Type Description <code>tuple of two lists of floats</code> <p>Means and standard deviations.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def get_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return the means and standard deviations.\n\n    Returns\n    -------\n    tuple of two lists of floats\n        Means and standard deviations.\n    \"\"\"\n    if self.means is None or self.stds is None:\n        return [], []\n\n    return list(self.means), list(self.stds)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_supervised","title":"<code>prepare_patches_supervised(train_files, target_files, axes, patch_size, read_source_func)</code>","text":"<p>Iterate over data source and create an array of patches and corresponding targets.</p> <p>The lists of Paths should be pre-sorted.</p> <p>Parameters:</p> Name Type Description Default <code>train_files</code> <code>list of pathlib.Path</code> <p>List of paths to training data.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>List of paths to target data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read the data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of patches.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_supervised(\n    train_files: list[Path],\n    target_files: list[Path],\n    axes: str,\n    patch_size: Union[list[int], tuple[int, ...]],\n    read_source_func: Callable,\n) -&gt; PatchedOutput:\n    \"\"\"\n    Iterate over data source and create an array of patches and corresponding targets.\n\n    The lists of Paths should be pre-sorted.\n\n    Parameters\n    ----------\n    train_files : list of pathlib.Path\n        List of paths to training data.\n    target_files : list of pathlib.Path\n        List of paths to target data.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n    read_source_func : Callable\n        Function to read the data.\n\n    Returns\n    -------\n    np.ndarray\n        Array of patches.\n    \"\"\"\n    means, stds, num_samples = 0, 0, 0\n    all_patches, all_targets = [], []\n    for train_filename, target_filename in zip(train_files, target_files, strict=False):\n        try:\n            sample: np.ndarray = read_source_func(train_filename, axes)\n            target: np.ndarray = read_source_func(target_filename, axes)\n            means += sample.mean()\n            stds += sample.std()\n            num_samples += 1\n\n            # reshape array\n            sample = reshape_array(sample, axes)\n            target = reshape_array(target, axes)\n\n            # generate patches, return a generator\n            patches, targets = extract_patches_sequential(\n                sample, patch_size=patch_size, target=target\n            )\n\n            # convert generator to list and add to all_patches\n            all_patches.append(patches)\n\n            # ensure targets are not None (type checking)\n            if targets is not None:\n                all_targets.append(targets)\n            else:\n                raise ValueError(f\"No target found for {target_filename}.\")\n\n        except Exception as e:\n            # emit warning and continue\n            logger.error(f\"Failed to read {train_filename} or {target_filename}: {e}\")\n\n    # raise error if no valid samples found\n    if num_samples == 0 or len(all_patches) == 0:\n        raise ValueError(\n            f\"No valid samples found in the input data: {train_files} and \"\n            f\"{target_files}.\"\n        )\n\n    image_means, image_stds = compute_normalization_stats(np.concatenate(all_patches))\n    target_means, target_stds = compute_normalization_stats(np.concatenate(all_targets))\n\n    patch_array: np.ndarray = np.concatenate(all_patches, axis=0)\n    target_array: np.ndarray = np.concatenate(all_targets, axis=0)\n    logger.info(f\"Extracted {patch_array.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patch_array,\n        target_array,\n        Stats(image_means, image_stds),\n        Stats(target_means, target_stds),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_supervised_array","title":"<code>prepare_patches_supervised_array(data, axes, data_target, patch_size)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method expects an array of shape SC(Z)YX, where S and C can be singleton dimensions.</p> <p>Patches returned are of shape SC(Z)YX, where S is now the patches dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>data_target</code> <code>ndarray</code> <p>Target data array.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding the source and target patches, with their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_supervised_array(\n    data: NDArray,\n    axes: str,\n    data_target: NDArray,\n    patch_size: Union[list[int], tuple[int]],\n) -&gt; PatchedOutput:\n    \"\"\"Iterate over data source and create an array of patches.\n\n    This method expects an array of shape SC(Z)YX, where S and C can be singleton\n    dimensions.\n\n    Patches returned are of shape SC(Z)YX, where S is now the patches dimension.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Input data array.\n    axes : str\n        Axes of the data.\n    data_target : numpy.ndarray\n        Target data array.\n    patch_size : list or tuple of int\n        Size of the patches.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding the source and target patches, with their statistics.\n    \"\"\"\n    # reshape array\n    reshaped_sample = reshape_array(data, axes)\n    reshaped_target = reshape_array(data_target, axes)\n\n    # compute statistics\n    image_means, image_stds = compute_normalization_stats(reshaped_sample)\n    target_means, target_stds = compute_normalization_stats(reshaped_target)\n\n    # generate patches, return a generator\n    patches, patch_targets = extract_patches_sequential(\n        reshaped_sample, patch_size=patch_size, target=reshaped_target\n    )\n\n    if patch_targets is None:\n        raise ValueError(\"No target extracted.\")\n\n    logger.info(f\"Extracted {patches.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patches,\n        patch_targets,\n        Stats(image_means, image_stds),\n        Stats(target_means, target_stds),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_unsupervised","title":"<code>prepare_patches_unsupervised(train_files, axes, patch_size, read_source_func)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method returns the mean and standard deviation of the image.</p> <p>Parameters:</p> Name Type Description Default <code>train_files</code> <code>list of pathlib.Path</code> <p>List of paths to training data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read the data.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding patches and their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_unsupervised(\n    train_files: list[Path],\n    axes: str,\n    patch_size: Union[list[int], tuple[int]],\n    read_source_func: Callable,\n) -&gt; PatchedOutput:\n    \"\"\"Iterate over data source and create an array of patches.\n\n    This method returns the mean and standard deviation of the image.\n\n    Parameters\n    ----------\n    train_files : list of pathlib.Path\n        List of paths to training data.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n    read_source_func : Callable\n        Function to read the data.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding patches and their statistics.\n    \"\"\"\n    means, stds, num_samples = 0, 0, 0\n    all_patches = []\n    for filename in train_files:\n        try:\n            sample: np.ndarray = read_source_func(filename, axes)\n            means += sample.mean()\n            stds += sample.std()\n            num_samples += 1\n\n            # reshape array\n            sample = reshape_array(sample, axes)\n\n            # generate patches, return a generator\n            patches, _ = extract_patches_sequential(sample, patch_size=patch_size)\n\n            # convert generator to list and add to all_patches\n            all_patches.append(patches)\n        except Exception as e:\n            # emit warning and continue\n            logger.error(f\"Failed to read {filename}: {e}\")\n\n    # raise error if no valid samples found\n    if num_samples == 0:\n        raise ValueError(f\"No valid samples found in the input data: {train_files}.\")\n\n    image_means, image_stds = compute_normalization_stats(np.concatenate(all_patches))\n\n    patch_array: np.ndarray = np.concatenate(all_patches)\n    logger.info(f\"Extracted {patch_array.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patch_array, None, Stats(image_means, image_stds), Stats((), ())\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_unsupervised_array","title":"<code>prepare_patches_unsupervised_array(data, axes, patch_size)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method expects an array of shape SC(Z)YX, where S and C can be singleton dimensions.</p> <p>Patches returned are of shape SC(Z)YX, where S is now the patches dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding the patches and their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_unsupervised_array(\n    data: NDArray,\n    axes: str,\n    patch_size: Union[list[int], tuple[int]],\n) -&gt; PatchedOutput:\n    \"\"\"\n    Iterate over data source and create an array of patches.\n\n    This method expects an array of shape SC(Z)YX, where S and C can be singleton\n    dimensions.\n\n    Patches returned are of shape SC(Z)YX, where S is now the patches dimension.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Input data array.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding the patches and their statistics.\n    \"\"\"\n    # reshape array\n    reshaped_sample = reshape_array(data, axes)\n\n    # calculate mean and std\n    means, stds = compute_normalization_stats(reshaped_sample)\n\n    # generate patches, return a generator\n    patches, _ = extract_patches_sequential(reshaped_sample, patch_size=patch_size)\n\n    return PatchedOutput(patches, None, Stats(means, stds), Stats((), ()))\n</code></pre>"},{"location":"reference/careamics/dataset/patching/random_patching/","title":"random_patching","text":"<p>Random patching utilities.</p>"},{"location":"reference/careamics/dataset/patching/random_patching/#careamics.dataset.patching.random_patching.extract_patches_random","title":"<code>extract_patches_random(arr, patch_size, target=None, seed=None)</code>","text":"<p>Generate patches from an array in a random manner.</p> <p>The method calculates how many patches the image can be divided into and then extracts an equal number of random patches.</p> <p>It returns a generator that yields the following:</p> <ul> <li>patch: np.ndarray, dimension C(Z)YX.</li> <li>target_patch: np.ndarray, dimension C(Z)YX, if the target is present, None     otherwise.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>tuple of int</code> <p>Patch sizes in each dimension.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target array, by default None.</p> <code>None</code> <code>seed</code> <code>int or None</code> <p>Random seed.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>Generator of patches.</p> Source code in <code>src/careamics/dataset/patching/random_patching.py</code> <pre><code>def extract_patches_random(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    target: np.ndarray | None = None,\n    seed: int | None = None,\n) -&gt; Generator[tuple[np.ndarray, np.ndarray | None], None, None]:\n    \"\"\"\n    Generate patches from an array in a random manner.\n\n    The method calculates how many patches the image can be divided into and then\n    extracts an equal number of random patches.\n\n    It returns a generator that yields the following:\n\n    - patch: np.ndarray, dimension C(Z)YX.\n    - target_patch: np.ndarray, dimension C(Z)YX, if the target is present, None\n        otherwise.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : tuple of int\n        Patch sizes in each dimension.\n    target : Optional[np.ndarray], optional\n        Target array, by default None.\n    seed : int or None, default=None\n        Random seed.\n\n    Yields\n    ------\n    Generator[np.ndarray, None, None]\n        Generator of patches.\n    \"\"\"\n    rng = np.random.default_rng(seed=seed)\n\n    is_3d_patch = len(patch_size) == 3\n\n    # patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    # Update patch size to encompass S and C dimensions\n    patch_size = [1, arr.shape[1], *patch_size]\n\n    # iterate over the number of samples (S or T)\n    for sample_idx in range(arr.shape[0]):\n        # get sample array\n        sample: np.ndarray = arr[sample_idx, ...]\n\n        # same for target\n        if target is not None:\n            target_sample: np.ndarray = target[sample_idx, ...]\n\n        # calculate the number of patches\n        n_patches = np.ceil(np.prod(sample.shape) / np.prod(patch_size)).astype(int)\n\n        # iterate over the number of patches\n        for _ in range(n_patches):\n            # get crop coordinates\n            crop_coords = [\n                rng.integers(0, sample.shape[i] - patch_size[1:][i], endpoint=True)\n                for i in range(len(patch_size[1:]))\n            ]\n\n            # extract patch\n            patch = (\n                sample[\n                    (\n                        ...,  # type: ignore\n                        *[  # type: ignore\n                            slice(c, c + patch_size[1:][i])\n                            for i, c in enumerate(crop_coords)\n                        ],\n                    )\n                ]\n                .copy()\n                .astype(np.float32)\n            )\n\n            # same for target\n            if target is not None:\n                target_patch = (\n                    target_sample[\n                        (\n                            ...,  # type: ignore\n                            *[  # type: ignore\n                                slice(c, c + patch_size[1:][i])\n                                for i, c in enumerate(crop_coords)\n                            ],\n                        )\n                    ]\n                    .copy()\n                    .astype(np.float32)\n                )\n                # return patch and target patch\n                yield patch, target_patch\n            else:\n                # return patch\n                yield patch, None\n</code></pre>"},{"location":"reference/careamics/dataset/patching/random_patching/#careamics.dataset.patching.random_patching.extract_patches_random_from_chunks","title":"<code>extract_patches_random_from_chunks(arr, patch_size, chunk_size, chunk_limit=None, seed=None)</code>","text":"<p>Generate patches from an array in a random manner.</p> <p>The method calculates how many patches the image can be divided into and then extracts an equal number of random patches.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Patch sizes in each dimension.</p> required <code>chunk_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Chunk sizes to load from the.</p> required <code>chunk_limit</code> <code>Optional[int]</code> <p>Number of chunks to load, by default None.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>Generator of patches.</p> Source code in <code>src/careamics/dataset/patching/random_patching.py</code> <pre><code>def extract_patches_random_from_chunks(\n    arr: zarr.Array,\n    patch_size: Union[list[int], tuple[int, ...]],\n    chunk_size: Union[list[int], tuple[int, ...]],\n    chunk_limit: int | None = None,\n    seed: int | None = None,\n) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"\n    Generate patches from an array in a random manner.\n\n    The method calculates how many patches the image can be divided into and then\n    extracts an equal number of random patches.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : Union[list[int], tuple[int, ...]]\n        Patch sizes in each dimension.\n    chunk_size : Union[list[int], tuple[int, ...]]\n        Chunk sizes to load from the.\n    chunk_limit : Optional[int], optional\n        Number of chunks to load, by default None.\n    seed : Optional[int], optional\n        Random seed, by default None.\n\n    Yields\n    ------\n    Generator[np.ndarray, None, None]\n        Generator of patches.\n    \"\"\"\n    is_3d_patch = len(patch_size) == 3\n\n    # Patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    rng = np.random.default_rng(seed=seed)\n    num_chunks = chunk_limit if chunk_limit else np.prod(arr._cdata_shape)\n\n    # Iterate over num chunks in the array\n    for _ in range(num_chunks):\n        chunk_crop_coords = [\n            rng.integers(0, max(0, arr.shape[i] - chunk_size[i]), endpoint=True)\n            for i in range(len(chunk_size))\n        ]\n        chunk = arr[\n            (\n                ...,\n                *[slice(c, c + chunk_size[i]) for i, c in enumerate(chunk_crop_coords)],\n            )\n        ].squeeze()\n\n        # Add a singleton dimension if the chunk does not have a sample dimension\n        if len(chunk.shape) == len(patch_size):\n            chunk = np.expand_dims(chunk, axis=0)\n\n        # Iterate over num samples (S)\n        for sample_idx in range(chunk.shape[0]):\n            spatial_chunk = chunk[sample_idx]\n            assert len(spatial_chunk.shape) == len(\n                patch_size\n            ), \"Requested chunk shape is not equal to patch size\"\n\n            n_patches = np.ceil(\n                np.prod(spatial_chunk.shape) / np.prod(patch_size)\n            ).astype(int)\n\n            # Iterate over the number of patches\n            for _ in range(n_patches):\n                patch_crop_coords = [\n                    rng.integers(\n                        0, spatial_chunk.shape[i] - patch_size[i], endpoint=True\n                    )\n                    for i in range(len(patch_size))\n                ]\n                patch = (\n                    spatial_chunk[\n                        (\n                            ...,\n                            *[\n                                slice(c, c + patch_size[i])\n                                for i, c in enumerate(patch_crop_coords)\n                            ],\n                        )\n                    ]\n                    .copy()\n                    .astype(np.float32)\n                )\n                yield patch\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/","title":"sequential_patching","text":"<p>Sequential patching functions.</p>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching.extract_patches_sequential","title":"<code>extract_patches_sequential(arr, patch_size, target=None)</code>","text":"<p>Generate patches from an array in a sequential manner.</p> <p>Array dimensions should be SC(Z)YX, where S and C can be singleton dimensions. The patches are generated sequentially and cover the whole array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>tuple[int]</code> <p>Patch sizes in each dimension.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target array, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>Patches.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def extract_patches_sequential(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    target: np.ndarray | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray | None]:\n    \"\"\"\n    Generate patches from an array in a sequential manner.\n\n    Array dimensions should be SC(Z)YX, where S and C can be singleton dimensions. The\n    patches are generated sequentially and cover the whole array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : tuple[int]\n        Patch sizes in each dimension.\n    target : Optional[np.ndarray], optional\n        Target array, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        Patches.\n    \"\"\"\n    is_3d_patch = len(patch_size) == 3\n\n    # Patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    # Update patch size to encompass S and C dimensions\n    patch_size = [1, arr.shape[1], *patch_size]\n\n    # Compute overlap\n    overlaps = _compute_overlap(arr_shape=arr.shape, patch_sizes=patch_size)\n\n    # Create view window and overlaps\n    window_steps = _compute_patch_steps(patch_sizes=patch_size, overlaps=overlaps)\n\n    output_shape = [\n        -1,\n    ] + patch_size[1:]\n\n    # Generate a view of the input array containing pre-calculated number of patches\n    # in each dimension with overlap.\n    # Resulting array is resized to (n_patches, C, Z, Y, X) or (n_patches, C, Y, X)\n    patches = _compute_patch_views(\n        arr,\n        window_shape=patch_size,\n        step=window_steps,\n        output_shape=output_shape,\n        target=target,\n    )\n\n    if target is not None:\n        # target was concatenated to patches in _compute_reshaped_view\n        return (\n            patches[:, 0, ...],\n            patches[:, 1, ...],\n        )  # TODO  in _compute_reshaped_view?\n    else:\n        return patches, None\n</code></pre>"},{"location":"reference/careamics/dataset/patching/validate_patch_dimension/","title":"validate_patch_dimension","text":"<p>Patch validation functions.</p>"},{"location":"reference/careamics/dataset/patching/validate_patch_dimension/#careamics.dataset.patching.validate_patch_dimension.validate_patch_dimensions","title":"<code>validate_patch_dimensions(arr, patch_size, is_3d_patch)</code>","text":"<p>Check patch size and array compatibility.</p> <p>This method validates the patch sizes with respect to the array dimensions:</p> <ul> <li>Patch must have two dimensions fewer than the array (S and C).</li> <li>Patch sizes are smaller than the corresponding array dimensions.</li> </ul> <p>If one of these conditions is not met, a ValueError is raised.</p> <p>This method should be called after inputs have been resized.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input array.</p> required <code>patch_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Size of the patches along each dimension of the array, except the first.</p> required <code>is_3d_patch</code> <code>bool</code> <p>Whether the patch is 3D or not.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is not consistent with the array shape (one more array dimension).</p> <code>ValueError</code> <p>If the patch size in Z is larger than the array dimension.</p> <code>ValueError</code> <p>If either of the patch sizes in X or Y is larger than the corresponding array dimension.</p> Source code in <code>src/careamics/dataset/patching/validate_patch_dimension.py</code> <pre><code>def validate_patch_dimensions(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    is_3d_patch: bool,\n) -&gt; None:\n    \"\"\"\n    Check patch size and array compatibility.\n\n    This method validates the patch sizes with respect to the array dimensions:\n\n    - Patch must have two dimensions fewer than the array (S and C).\n    - Patch sizes are smaller than the corresponding array dimensions.\n\n    If one of these conditions is not met, a ValueError is raised.\n\n    This method should be called after inputs have been resized.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array.\n    patch_size : Union[list[int], tuple[int, ...]]\n        Size of the patches along each dimension of the array, except the first.\n    is_3d_patch : bool\n        Whether the patch is 3D or not.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is not consistent with the array shape (one more array\n        dimension).\n    ValueError\n        If the patch size in Z is larger than the array dimension.\n    ValueError\n        If either of the patch sizes in X or Y is larger than the corresponding array\n        dimension.\n    \"\"\"\n    if len(patch_size) != len(arr.shape[2:]):\n        raise ValueError(\n            f\"There must be a patch size for each spatial dimensions \"\n            f\"(got {patch_size} patches for dims {arr.shape}). Check the axes order.\"\n        )\n\n    # Sanity checks on patch sizes versus array dimension\n    if is_3d_patch and patch_size[0] &gt; arr.shape[-3]:\n        raise ValueError(\n            f\"Z patch size is inconsistent with image shape \"\n            f\"(got {patch_size[0]} patches for dim {arr.shape[1]}). Check the axes \"\n            f\"order.\"\n        )\n\n    if patch_size[-2] &gt; arr.shape[-2] or patch_size[-1] &gt; arr.shape[-1]:\n        raise ValueError(\n            f\"At least one of YX patch dimensions is larger than the corresponding \"\n            f\"image dimension (got {patch_size} patches for dims {arr.shape[-2:]}). \"\n            f\"Check the axes order.\"\n        )\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/collate_tiles/","title":"collate_tiles","text":"<p>Collate function for tiling.</p>"},{"location":"reference/careamics/dataset/tiling/collate_tiles/#careamics.dataset.tiling.collate_tiles.collate_tiles","title":"<code>collate_tiles(batch)</code>","text":"<p>Collate tiles received from CAREamics prediction dataloader.</p> <p>CAREamics prediction dataloader returns tuples of arrays and TileInformation. In case of non-tiled data, this function will return the arrays. In case of tiled data, it will return the arrays, the last tile flag, the overlap crop coordinates and the stitch coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[tuple[ndarray, TileInformation], ...]</code> <p>Batch of tiles.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Collated batch.</p> Source code in <code>src/careamics/dataset/tiling/collate_tiles.py</code> <pre><code>def collate_tiles(batch: list[tuple[np.ndarray, TileInformation]]) -&gt; Any:\n    \"\"\"\n    Collate tiles received from CAREamics prediction dataloader.\n\n    CAREamics prediction dataloader returns tuples of arrays and TileInformation. In\n    case of non-tiled data, this function will return the arrays. In case of tiled data,\n    it will return the arrays, the last tile flag, the overlap crop coordinates and the\n    stitch coordinates.\n\n    Parameters\n    ----------\n    batch : list[tuple[np.ndarray, TileInformation], ...]\n        Batch of tiles.\n\n    Returns\n    -------\n    Any\n        Collated batch.\n    \"\"\"\n    new_batch = [tile for tile, _ in batch]\n    tiles_batch = [tile_info for _, tile_info in batch]\n\n    return default_collate(new_batch), tiles_batch\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/","title":"lvae_tiled_patching","text":"<p>Functions to reimplement the tiling in the Disentangle repository.</p>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_padding","title":"<code>compute_padding(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate padding to ensure stitched data comes from the center of a tile.</p> <p>Padding is added to an array with shape <code>data_shape</code> so that when tiles are stitched together, the data used always comes from the center of a tile, even for tiles at the boundaries of the array.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of (int, int)</code> <p>A tuple specifying the padding to add in each dimension, each element is a two element tuple specifying the padding to add before and after the data. This can be used as the <code>pad_width</code> argument to <code>numpy.pad</code>.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_padding(\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n) -&gt; tuple[tuple[int, int], ...]:\n    \"\"\"\n    Calculate padding to ensure stitched data comes from the center of a tile.\n\n    Padding is added to an array with shape `data_shape` so that when tiles are\n    stitched together, the data used always comes from the center of a tile, even for\n    tiles at the boundaries of the array.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n    Returns\n    -------\n    tuple of (int, int)\n        A tuple specifying the padding to add in each dimension, each element is a two\n        element tuple specifying the padding to add before and after the data. This\n        can be used as the `pad_width` argument to `numpy.pad`.\n    \"\"\"\n    tile_grid_shape = np.array(compute_tile_grid_shape(data_shape, tile_size, overlaps))\n    covered_shape = (tile_size - overlaps) * tile_grid_shape + overlaps\n\n    pad_before = overlaps // 2\n    pad_after = covered_shape - data_shape[-len(tile_size) :] - pad_before\n\n    return tuple(\n        (before, after) for before, after in zip(pad_before, pad_after, strict=False)\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_grid_shape","title":"<code>compute_tile_grid_shape(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate the number of tiles in each dimension.</p> <p>This can be thought of as a grid of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of int</code> <p>The number of tiles in each direction, ((Z, Y, X)).</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_grid_shape(\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n) -&gt; tuple[int, ...]:\n    \"\"\"Calculate the number of tiles in each dimension.\n\n    This can be thought of as a grid of tiles.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n    Returns\n    -------\n    tuple of int\n        The number of tiles in each direction, ((Z, Y, X)).\n    \"\"\"\n    shape = [0 for _ in range(len(tile_size))]\n    # assume spatial dimension are the last dimensions so iterate backwards\n    for i in range(-1, -len(tile_size) - 1, -1):\n        shape[i] = n_tiles_1d(data_shape[i], tile_size[i], overlaps[i])\n    return tuple(shape)\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_info","title":"<code>compute_tile_info(tile_grid_indices, data_shape, tile_size, overlaps, sample_id=0)</code>","text":"<p>Compute the tile information for a tile with the coordinates <code>tile_grid_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tile_grid_indices</code> <code>1D np.array of int</code> <p>The coordinates of the tile within the tile grid, ((Z), Y, X), i.e. for 2D tiling the coordinates for the second tile in the first row of tiles would be (0, 1).</p> required <code>data_shape</code> <code>1D np.array of int</code> <p>The shape of the data, should be (C, (Z), Y, X) where Z is optional.</p> required <code>tile_size</code> <code>1D np.array of int</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>1D np.array of int</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <code>sample_id</code> <code>int</code> <p>An ID to identify which sample a tile belongs to.</p> <code>0</code> <p>Returns:</p> Type Description <code>TileInformation</code> <p>Information that describes how to crop and stitch a tile to create a full image.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_info(\n    tile_grid_indices: NDArray[np.int_],\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n    sample_id: int = 0,\n) -&gt; TileInformation:\n    \"\"\"\n    Compute the tile information for a tile with the coordinates `tile_grid_indices`.\n\n    Parameters\n    ----------\n    tile_grid_indices : 1D np.array of int\n        The coordinates of the tile within the tile grid, ((Z), Y, X), i.e. for 2D\n        tiling the coordinates for the second tile in the first row of tiles would be\n        (0, 1).\n    data_shape : 1D np.array of int\n        The shape of the data, should be (C, (Z), Y, X) where Z is optional.\n    tile_size : 1D np.array of int\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : 1D np.array of int\n        Overlap values in each dimension, of length 2 or 3.\n    sample_id : int, default=0\n        An ID to identify which sample a tile belongs to.\n\n    Returns\n    -------\n    TileInformation\n        Information that describes how to crop and stitch a tile to create a full image.\n    \"\"\"\n    spatial_dims_shape = data_shape[-len(tile_size) :]\n\n    # The extent of the tile which will make up part of the stitched image.\n    stitch_size = tile_size - overlaps\n    stitch_coords_start = tile_grid_indices * stitch_size\n    stitch_coords_end = stitch_coords_start + stitch_size\n\n    tile_coords_start = stitch_coords_start - overlaps // 2\n\n    # --- replace out of bounds indices\n    out_of_lower_bound = stitch_coords_start &lt; 0\n    out_of_upper_bound = stitch_coords_end &gt; spatial_dims_shape\n    stitch_coords_start[out_of_lower_bound] = 0\n    stitch_coords_end[out_of_upper_bound] = spatial_dims_shape[out_of_upper_bound]\n\n    # --- calculate overlap crop coords\n    overlap_crop_coords_start = stitch_coords_start - tile_coords_start\n    overlap_crop_coords_end = overlap_crop_coords_start + (\n        stitch_coords_end - stitch_coords_start\n    )\n\n    # --- combine start and end\n    stitch_coords = tuple(\n        (start, end)\n        for start, end in zip(stitch_coords_start, stitch_coords_end, strict=False)\n    )\n    overlap_crop_coords = tuple(\n        (start, end)\n        for start, end in zip(\n            overlap_crop_coords_start, overlap_crop_coords_end, strict=False\n        )\n    )\n\n    # --- Check if last tile\n    tile_grid_shape = np.array(compute_tile_grid_shape(data_shape, tile_size, overlaps))\n    last_tile = (tile_grid_indices == (tile_grid_shape - 1)).all()\n\n    tile_info = TileInformation(\n        array_shape=data_shape,\n        last_tile=last_tile,\n        overlap_crop_coords=overlap_crop_coords,\n        stitch_coords=stitch_coords,\n        sample_id=sample_id,\n    )\n    return tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_info_legacy","title":"<code>compute_tile_info_legacy(grid_index_manager, index)</code>","text":"<p>Compute the tile information for a tile at a given dataset index.</p> <p>Parameters:</p> Name Type Description Default <code>grid_index_manager</code> <code>GridIndexManager</code> <p>The grid index manager that keeps track of tile locations.</p> required <code>index</code> <code>int</code> <p>The dataset index.</p> required <p>Returns:</p> Type Description <code>TileInformation</code> <p>Information that describes how to crop and stitch a tile to create a full image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>grid_index_manager.data_shape</code> does not have 4 or 5 dimensions.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_info_legacy(\n    grid_index_manager: GridIndexManager, index: int\n) -&gt; TileInformation:\n    \"\"\"\n    Compute the tile information for a tile at a given dataset index.\n\n    Parameters\n    ----------\n    grid_index_manager : GridIndexManager\n        The grid index manager that keeps track of tile locations.\n    index : int\n        The dataset index.\n\n    Returns\n    -------\n    TileInformation\n        Information that describes how to crop and stitch a tile to create a full image.\n\n    Raises\n    ------\n    ValueError\n        If `grid_index_manager.data_shape` does not have 4 or 5 dimensions.\n    \"\"\"\n    data_shape = np.array(grid_index_manager.data_shape)\n    if len(data_shape) == 5:\n        n_spatial_dims = 3\n    elif len(data_shape) == 4:\n        n_spatial_dims = 2\n    else:\n        raise ValueError(\"Data shape must have 4 or 5 dimensions, equating to SC(Z)YX.\")\n\n    stitch_coords_start = np.array(\n        grid_index_manager.get_location_from_dataset_idx(index)\n    )\n    stitch_coords_end = stitch_coords_start + np.array(grid_index_manager.grid_shape)\n\n    tile_coords_start = stitch_coords_start - grid_index_manager.patch_offset()\n\n    # --- replace out of bounds indices\n    out_of_lower_bound = stitch_coords_start &lt; 0\n    out_of_upper_bound = stitch_coords_end &gt; data_shape\n    stitch_coords_start[out_of_lower_bound] = 0\n    stitch_coords_end[out_of_upper_bound] = data_shape[out_of_upper_bound]\n\n    # TODO: TilingMode not in current version\n    # if grid_index_manager.tiling_mode == TilingMode.ShiftBoundary:\n    #     for dim in range(len(stitch_coords_start)):\n    #         if tile_coords_start[dim] == 0:\n    #             stitch_coords_start[dim] = 0\n    #         if tile_coords_end[dim] == grid_index_manager.data_shape[dim]:\n    #             tile_coords_end [dim]= grid_index_manager.data_shape[dim]\n\n    # --- calculate overlap crop coords\n    overlap_crop_coords_start = stitch_coords_start - tile_coords_start\n    overlap_crop_coords_end = overlap_crop_coords_start + (\n        stitch_coords_end - stitch_coords_start\n    )\n\n    last_tile = index == grid_index_manager.total_grid_count() - 1\n\n    # --- combine start and end\n    stitch_coords = tuple(\n        (start, end)\n        for start, end in zip(stitch_coords_start, stitch_coords_end, strict=False)\n    )\n    overlap_crop_coords = tuple(\n        (start, end)\n        for start, end in zip(\n            overlap_crop_coords_start, overlap_crop_coords_end, strict=False\n        )\n    )\n\n    tile_info = TileInformation(\n        array_shape=data_shape[1:],  # remove S dim\n        last_tile=last_tile,\n        overlap_crop_coords=overlap_crop_coords[-n_spatial_dims:],\n        stitch_coords=stitch_coords[-n_spatial_dims:],\n        sample_id=0,\n    )\n    return tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.extract_tiles","title":"<code>extract_tiles(arr, tile_size, overlaps, padding_kwargs=None)</code>","text":"<p>Generate tiles from the input array with specified overlap.</p> <p>The tiles cover the whole array; which will be additionally padded, to ensure that the section of the tile that contributes to the final image comes from the center of the tile.</p> <p>The method returns a generator that yields tuples of array and tile information, the latter includes whether the tile is the last one, the coordinates of the overlap crop, and the coordinates of the stitched tile.</p> <p>Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX, where C can be a singleton.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array of shape (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.ndarray of tuple</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>1D numpy.ndarray of tuple</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <code>padding_kwargs</code> <code>dict</code> <p>The arguments of <code>np.pad</code> after the first two arguments, <code>array</code> and <code>pad_width</code>. If not specified the default will be <code>{\"mode\": \"reflect\"}</code>. See <code>numpy.pad</code> docs: https://numpy.org/doc/stable/reference/generated/numpy.pad.html.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[Tuple[ndarray, TileInformation], None, None]</code> <p>Tile generator, yields the tile and additional information.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def extract_tiles(\n    arr: NDArray,\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n    padding_kwargs: dict[str, Any] | None = None,\n) -&gt; Generator[tuple[NDArray, TileInformation], None, None]:\n    \"\"\"Generate tiles from the input array with specified overlap.\n\n    The tiles cover the whole array; which will be additionally padded, to ensure that\n    the section of the tile that contributes to the final image comes from the center\n    of the tile.\n\n    The method returns a generator that yields tuples of array and tile information,\n    the latter includes whether the tile is the last one, the coordinates of the\n    overlap crop, and the coordinates of the stitched tile.\n\n    Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX,\n    where C can be a singleton.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Array of shape (S, C, (Z), Y, X).\n    tile_size : 1D numpy.ndarray of tuple\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : 1D numpy.ndarray of tuple\n        Overlap values in each dimension, of length 2 or 3.\n    padding_kwargs : dict, optional\n        The arguments of `np.pad` after the first two arguments, `array` and\n        `pad_width`. If not specified the default will be `{\"mode\": \"reflect\"}`. See\n        `numpy.pad` docs:\n        https://numpy.org/doc/stable/reference/generated/numpy.pad.html.\n\n    Yields\n    ------\n    Generator[Tuple[np.ndarray, TileInformation], None, None]\n        Tile generator, yields the tile and additional information.\n    \"\"\"\n    if padding_kwargs is None:\n        padding_kwargs = {\"mode\": \"reflect\"}\n\n    # Iterate over num samples (S)\n    for sample_idx in range(arr.shape[0]):\n        sample = arr[sample_idx, ...]\n        data_shape = np.array(sample.shape)\n\n        # add padding to ensure evenly spaced &amp; overlapping tiles.\n        spatial_padding = compute_padding(data_shape, tile_size, overlaps)\n        padding = ((0, 0), *spatial_padding)\n        sample = np.pad(sample, padding, **padding_kwargs)\n\n        # The number of tiles in each dimension, should be of length 2 or 3\n        tile_grid_shape = compute_tile_grid_shape(data_shape, tile_size, overlaps)\n        # itertools.product is equivalent of nested loops\n\n        stitch_size = tile_size - overlaps\n        for tile_grid_indices in itertools.product(\n            *[range(n) for n in tile_grid_shape]\n        ):\n\n            # calculate crop coordinates\n            crop_coords_start = np.array(tile_grid_indices) * stitch_size\n            crop_slices: tuple[Union[builtins.ellipsis, slice], ...] = (\n                ...,\n                *[\n                    slice(coords, coords + extent)\n                    for coords, extent in zip(\n                        crop_coords_start, tile_size, strict=False\n                    )\n                ],\n            )\n            tile = sample[crop_slices]\n\n            tile_info = compute_tile_info(\n                np.array(tile_grid_indices),\n                np.array(data_shape),\n                np.array(tile_size),\n                np.array(overlaps),\n                sample_idx,\n            )\n            # TODO: kinda weird this is a generator,\n            #   -&gt; doesn't really save memory ? Don't think there are any places the\n            #    tiles are not exracted all at the same time.\n            #   Although I guess it would make sense for a zarr tile extractor.\n            yield tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.n_tiles_1d","title":"<code>n_tiles_1d(axis_size, tile_size, overlap)</code>","text":"<p>Calculate the number of tiles in a specific dimension.</p> <p>Parameters:</p> Name Type Description Default <code>axis_size</code> <code>int</code> <p>The length of the data for in a specific dimension.</p> required <code>tile_size</code> <code>int</code> <p>The length of the tiles in a specific dimension.</p> required <code>overlap</code> <code>int</code> <p>The tile overlap in a specific dimension.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of tiles that fit in one dimension given the arguments.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def n_tiles_1d(axis_size: int, tile_size: int, overlap: int) -&gt; int:\n    \"\"\"Calculate the number of tiles in a specific dimension.\n\n    Parameters\n    ----------\n    axis_size : int\n        The length of the data for in a specific dimension.\n    tile_size : int\n        The length of the tiles in a specific dimension.\n    overlap : int\n        The tile overlap in a specific dimension.\n\n    Returns\n    -------\n    int\n        The number of tiles that fit in one dimension given the arguments.\n    \"\"\"\n    return int(np.ceil(axis_size / (tile_size - overlap)))\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.total_n_tiles","title":"<code>total_n_tiles(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate The total number of tiles over all dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The total number of tiles over all dimensions.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def total_n_tiles(\n    data_shape: tuple[int, ...], tile_size: tuple[int, ...], overlaps: tuple[int, ...]\n) -&gt; int:\n    \"\"\"Calculate The total number of tiles over all dimensions.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n\n    Returns\n    -------\n    int\n        The total number of tiles over all dimensions.\n    \"\"\"\n    result = 1\n    # assume spatial dimension are the last dimensions so iterate backwards\n    for i in range(-1, -len(tile_size) - 1, -1):\n        result = result * n_tiles_1d(data_shape[i], tile_size[i], overlaps[i])\n\n    return result\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/tiled_patching/","title":"tiled_patching","text":"<p>Tiled patching utilities.</p>"},{"location":"reference/careamics/dataset/tiling/tiled_patching/#careamics.dataset.tiling.tiled_patching.extract_tiles","title":"<code>extract_tiles(arr, tile_size, overlaps)</code>","text":"<p>Generate tiles from the input array with specified overlap.</p> <p>The tiles cover the whole array. The method returns a generator that yields tuples of array and tile information, the latter includes whether the tile is the last one, the coordinates of the overlap crop, and the coordinates of the stitched tile.</p> <p>Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX, where C can be a singleton.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array of shape (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>Union[list[int], tuple[int]]</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>Union[list[int], tuple[int]]</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <p>Yields:</p> Type Description <code>Generator[tuple[ndarray, TileInformation], None, None]</code> <p>Tile generator, yields the tile and additional information.</p> Source code in <code>src/careamics/dataset/tiling/tiled_patching.py</code> <pre><code>def extract_tiles(\n    arr: np.ndarray,\n    tile_size: Union[list[int], tuple[int, ...]],\n    overlaps: Union[list[int], tuple[int, ...]],\n) -&gt; Generator[tuple[np.ndarray, TileInformation], None, None]:\n    \"\"\"Generate tiles from the input array with specified overlap.\n\n    The tiles cover the whole array. The method returns a generator that yields\n    tuples of array and tile information, the latter includes whether\n    the tile is the last one, the coordinates of the overlap crop, and the coordinates\n    of the stitched tile.\n\n    Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX,\n    where C can be a singleton.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Array of shape (S, C, (Z), Y, X).\n    tile_size : Union[list[int], tuple[int]]\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : Union[list[int], tuple[int]]\n        Overlap values in each dimension, of length 2 or 3.\n\n    Yields\n    ------\n    Generator[tuple[np.ndarray, TileInformation], None, None]\n        Tile generator, yields the tile and additional information.\n    \"\"\"\n    # Iterate over num samples (S)\n    for sample_idx in range(arr.shape[0]):\n        sample: np.ndarray = arr[sample_idx, ...]\n\n        # Create a list of coordinates for cropping and stitching all axes.\n        # [crop coordinates, stitching coordinates, overlap crop coordinates]\n        # For axis of size 35 and patch size of 32 compute_crop_and_stitch_coords_1d\n        # will output ([(0, 32), (3, 35)], [(0, 20), (20, 35)], [(0, 20), (17, 32)])\n        crop_and_stitch_coords_list = [\n            _compute_crop_and_stitch_coords_1d(\n                sample.shape[i + 1], tile_size[i], overlaps[i]\n            )\n            for i in range(len(tile_size))\n        ]\n\n        # Rearrange crop coordinates from a list of coordinate pairs per axis to a list\n        # grouped by type.\n        all_crop_coords, all_stitch_coords, all_overlap_crop_coords = zip(\n            *crop_and_stitch_coords_list, strict=False\n        )\n\n        # Maximum tile index\n        max_tile_idx = np.prod([len(axis) for axis in all_crop_coords]) - 1\n\n        # Iterate over generated coordinate pairs:\n        for tile_idx, (crop_coords, stitch_coords, overlap_crop_coords) in enumerate(\n            zip(\n                itertools.product(*all_crop_coords),\n                itertools.product(*all_stitch_coords),\n                itertools.product(*all_overlap_crop_coords),\n                strict=False,\n            )\n        ):\n            # Extract tile from the sample\n            tile: np.ndarray = sample[\n                (..., *[slice(c[0], c[1]) for c in list(crop_coords)])  # type: ignore\n            ]\n\n            # Check if we are at the end of the sample by computing the length of the\n            # array that contains all the tiles\n            if tile_idx == max_tile_idx:\n                last_tile = True\n            else:\n                last_tile = False\n\n            # create tile information\n            tile_info = TileInformation(\n                array_shape=sample.shape,\n                last_tile=last_tile,\n                overlap_crop_coords=overlap_crop_coords,\n                stitch_coords=stitch_coords,\n                sample_id=sample_idx,\n            )\n\n            yield tile, tile_info\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/","title":"get_func","text":"<p>Module to get read functions.</p>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.ReadFunc","title":"<code>ReadFunc</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for type hinting read functions.</p> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>class ReadFunc(Protocol):\n    \"\"\"Protocol for type hinting read functions.\"\"\"\n\n    def __call__(self, file_path: Path, *args, **kwargs) -&gt; NDArray:\n        \"\"\"\n        Type hinted callables must match this function signature (not including self).\n\n        Parameters\n        ----------\n        file_path : pathlib.Path\n            Path to file.\n        *args\n            Other positional arguments.\n        **kwargs\n            Other keyword arguments.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.ReadFunc.__call__","title":"<code>__call__(file_path, *args, **kwargs)</code>","text":"<p>Type hinted callables must match this function signature (not including self).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>*args</code> <p>Other positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>def __call__(self, file_path: Path, *args, **kwargs) -&gt; NDArray:\n    \"\"\"\n    Type hinted callables must match this function signature (not including self).\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    *args\n        Other positional arguments.\n    **kwargs\n        Other keyword arguments.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.get_read_func","title":"<code>get_read_func(data_type)</code>","text":"<p>Get the read function for the data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Read function.</p> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>def get_read_func(data_type: Union[str, SupportedData]) -&gt; Callable:\n    \"\"\"\n    Get the read function for the data type.\n\n    Parameters\n    ----------\n    data_type : SupportedData\n        Data type.\n\n    Returns\n    -------\n    callable\n        Read function.\n    \"\"\"\n    if data_type in READ_FUNCS:\n        data_type = SupportedData(data_type)  # mypy complaining about dict key type\n        return READ_FUNCS[data_type]\n    else:\n        raise NotImplementedError(f\"Data type '{data_type}' is not supported.\")\n</code></pre>"},{"location":"reference/careamics/file_io/read/tiff/","title":"tiff","text":"<p>Functions to read tiff images.</p>"},{"location":"reference/careamics/file_io/read/tiff/#careamics.file_io.read.tiff.read_tiff","title":"<code>read_tiff(file_path, *args, **kwargs)</code>","text":"<p>Read a tiff file and return a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to a file.</p> required <code>*args</code> <code>list</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Resulting array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file failed to open.</p> <code>OSError</code> <p>If the file failed to open.</p> <code>ValueError</code> <p>If the file is not a valid tiff.</p> <code>ValueError</code> <p>If the data dimensions are incorrect.</p> <code>ValueError</code> <p>If the axes length is incorrect.</p> Source code in <code>src/careamics/file_io/read/tiff.py</code> <pre><code>def read_tiff(file_path: Path, *args: list, **kwargs: dict) -&gt; np.ndarray:\n    \"\"\"\n    Read a tiff file and return a numpy array.\n\n    Parameters\n    ----------\n    file_path : Path\n        Path to a file.\n    *args : list\n        Additional arguments.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    np.ndarray\n        Resulting array.\n\n    Raises\n    ------\n    ValueError\n        If the file failed to open.\n    OSError\n        If the file failed to open.\n    ValueError\n        If the file is not a valid tiff.\n    ValueError\n        If the data dimensions are incorrect.\n    ValueError\n        If the axes length is incorrect.\n    \"\"\"\n    if fnmatch(\n        file_path.suffix, SupportedData.get_extension_pattern(SupportedData.TIFF)\n    ):\n        try:\n            array = tifffile.imread(file_path)\n        except (ValueError, OSError) as e:\n            logging.exception(f\"Exception in file {file_path}: {e}, skipping it.\")\n            raise e\n    else:\n        raise ValueError(f\"File {file_path} is not a valid tiff.\")\n\n    return array\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/","title":"get_func","text":"<p>Module to get write functions.</p>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.WriteFunc","title":"<code>WriteFunc</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for type hinting write functions.</p> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>class WriteFunc(Protocol):\n    \"\"\"Protocol for type hinting write functions.\"\"\"\n\n    def __call__(self, file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Type hinted callables must match this function signature (not including self).\n\n        Parameters\n        ----------\n        file_path : pathlib.Path\n            Path to file.\n        img : numpy.ndarray\n            Image data to save.\n        *args\n            Other positional arguments.\n        **kwargs\n            Other keyword arguments.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.WriteFunc.__call__","title":"<code>__call__(file_path, img, *args, **kwargs)</code>","text":"<p>Type hinted callables must match this function signature (not including self).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>img</code> <code>ndarray</code> <p>Image data to save.</p> required <code>*args</code> <p>Other positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>def __call__(self, file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Type hinted callables must match this function signature (not including self).\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    img : numpy.ndarray\n        Image data to save.\n    *args\n        Other positional arguments.\n    **kwargs\n        Other keyword arguments.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.get_write_func","title":"<code>get_write_func(data_type)</code>","text":"<p>Get the write function for the data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>(tiff, custom)</code> <p>Data type.</p> <code>\"tiff\"</code> <p>Returns:</p> Type Description <code>callable</code> <p>Write function.</p> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>def get_write_func(data_type: SupportedWriteType) -&gt; WriteFunc:\n    \"\"\"\n    Get the write function for the data type.\n\n    Parameters\n    ----------\n    data_type : {\"tiff\", \"custom\"}\n        Data type.\n\n    Returns\n    -------\n    callable\n        Write function.\n    \"\"\"\n    # error raised here if not supported\n    data_type_ = SupportedData(data_type)  # new variable for mypy\n    # error if no write func.\n    if data_type_ not in WRITE_FUNCS:\n        raise NotImplementedError(f\"No write function for data type '{data_type}'.\")\n\n    return WRITE_FUNCS[data_type_]\n</code></pre>"},{"location":"reference/careamics/file_io/write/tiff/","title":"tiff","text":"<p>Write tiff function.</p>"},{"location":"reference/careamics/file_io/write/tiff/#careamics.file_io.write.tiff.write_tiff","title":"<code>write_tiff(file_path, img, *args, **kwargs)</code>","text":"<p>Write tiff files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>img</code> <code>ndarray</code> <p>Image data to save.</p> required <code>*args</code> <p>Positional arguments passed to <code>tifffile.imwrite</code>.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>tifffile.imwrite</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file extension of <code>file_path</code> does not match the Unix shell-style pattern '.tif'.</p> Source code in <code>src/careamics/file_io/write/tiff.py</code> <pre><code>def write_tiff(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n    # TODO: add link to tiffile docs for args kwrgs?\n    \"\"\"\n    Write tiff files.\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    img : numpy.ndarray\n        Image data to save.\n    *args\n        Positional arguments passed to `tifffile.imwrite`.\n    **kwargs\n        Keyword arguments passed to `tifffile.imwrite`.\n\n    Raises\n    ------\n    ValueError\n        When the file extension of `file_path` does not match the Unix shell-style\n        pattern '*.tif*'.\n    \"\"\"\n    if not fnmatch(\n        file_path.suffix, SupportedData.get_extension_pattern(SupportedData.TIFF)\n    ):\n        raise ValueError(\n            f\"Unexpected extension '{file_path.suffix}' for save file type 'tiff'.\"\n        )\n    tifffile.imwrite(file_path, img, *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/","title":"lightning_module","text":"<p>CAREamics Lightning module.</p>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule","title":"<code>FCNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CAREamics Lightning module.</p> <p>This class encapsulates the PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>AlgorithmModel or dict</code> <p>Algorithm configuration.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>PyTorch model.</p> <code>loss_func</code> <code>Module</code> <p>Loss function.</p> <code>optimizer_name</code> <code>str</code> <p>Optimizer name.</p> <code>optimizer_params</code> <code>dict</code> <p>Optimizer parameters.</p> <code>lr_scheduler_name</code> <code>str</code> <p>Learning rate scheduler name.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>class FCNModule(L.LightningModule):\n    \"\"\"\n    CAREamics Lightning module.\n\n    This class encapsulates the PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : AlgorithmModel or dict\n        Algorithm configuration.\n\n    Attributes\n    ----------\n    model : torch.nn.Module\n        PyTorch model.\n    loss_func : torch.nn.Module\n        Loss function.\n    optimizer_name : str\n        Optimizer name.\n    optimizer_params : dict\n        Optimizer parameters.\n    lr_scheduler_name : str\n        Learning rate scheduler name.\n    \"\"\"\n\n    def __init__(\n        self, algorithm_config: Union[UNetBasedAlgorithm, VAEBasedAlgorithm, dict]\n    ) -&gt; None:\n        \"\"\"Lightning module for CAREamics.\n\n        This class encapsulates the a PyTorch model along with the training, validation,\n        and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n        Parameters\n        ----------\n        algorithm_config : AlgorithmModel or dict\n            Algorithm configuration.\n        \"\"\"\n        super().__init__()\n\n        if isinstance(algorithm_config, dict):\n            algorithm_config = algorithm_factory(algorithm_config)\n\n        # create preprocessing, model and loss function\n        if isinstance(algorithm_config, N2VAlgorithm):\n            self.use_n2v = True\n            self.n2v_preprocess: N2VManipulateTorch | None = N2VManipulateTorch(\n                n2v_manipulate_config=algorithm_config.n2v_config\n            )\n        else:\n            self.use_n2v = False\n            self.n2v_preprocess = None\n\n        self.algorithm = algorithm_config.algorithm\n        self.model: torch.nn.Module = model_factory(algorithm_config.model)\n        self.loss_func = loss_factory(algorithm_config.loss)\n\n        # save optimizer and lr_scheduler names and parameters\n        self.optimizer_name = algorithm_config.optimizer.name\n        self.optimizer_params = algorithm_config.optimizer.parameters\n        self.lr_scheduler_name = algorithm_config.lr_scheduler.name\n        self.lr_scheduler_params = algorithm_config.lr_scheduler.parameters\n\n    def forward(self, x: Any) -&gt; Any:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : Any\n            Input tensor.\n\n        Returns\n        -------\n        Any\n            Output tensor.\n        \"\"\"\n        return self.model(x)\n\n    def training_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Training step.\n\n        Parameters\n        ----------\n        batch : torch.torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Loss value.\n        \"\"\"\n        x, *targets = batch\n        if self.use_n2v and self.n2v_preprocess is not None:\n            x_preprocessed, *aux = self.n2v_preprocess(x)\n        else:\n            x_preprocessed = x\n            aux = []\n\n        out = self.model(x_preprocessed)\n        loss = self.loss_func(out, *aux, *targets)\n        self.log(\n            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        optimizer = self.optimizers()\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        self.log(\"learning_rate\", current_lr, on_step=False, on_epoch=True, logger=True)\n        return loss\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; None:\n        \"\"\"Validation step.\n\n        Parameters\n        ----------\n        batch : torch.torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n        \"\"\"\n        x, *targets = batch\n        if self.use_n2v and self.n2v_preprocess is not None:\n            x_preprocessed, *aux = self.n2v_preprocess(x)\n        else:\n            x_preprocessed = x\n            aux = []\n\n        out = self.model(x_preprocessed)\n        val_loss = self.loss_func(out, *aux, *targets)\n\n        # log validation loss\n        self.log(\n            \"val_loss\",\n            val_loss,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n    def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Prediction step.\n\n        Parameters\n        ----------\n        batch : torch.torch.torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Model output.\n        \"\"\"\n        # TODO refactor when redoing datasets\n        # hacky way to determine if it is PredictDataModule, otherwise there is a\n        # circular import to solve with isinstance\n        from_prediction = hasattr(self._trainer.datamodule, \"tiled\")\n        is_tiled = (\n            len(batch) &gt; 1\n            and isinstance(batch[1], list)\n            and isinstance(batch[1][0], TileInformation)\n        )\n\n        # TODO add explanations for what is happening here\n        if is_tiled:\n            x, *aux = batch\n            if type(x) in [list, tuple]:\n                x = x[0]\n        else:\n            if type(batch) in [list, tuple]:\n                x = batch[0]  # TODO change, ugly way to deal with n2v refac\n            else:\n                x = batch\n            aux = []\n\n        # apply test-time augmentation if available\n        # TODO: probably wont work with batch size &gt; 1\n        if (\n            from_prediction\n            and self._trainer.datamodule.prediction_config.tta_transforms\n        ):\n            tta = ImageRestorationTTA()\n            augmented_batch = tta.forward(x)  # list of augmented tensors\n            augmented_output = []\n            for augmented in augmented_batch:\n                augmented_pred = self.model(augmented)\n                augmented_output.append(augmented_pred)\n            output = tta.backward(augmented_output)\n        else:\n            output = self.model(x)\n\n        # Denormalize the output\n        # TODO incompatible API between predict and train datasets\n        denorm = Denormalize(\n            image_means=(\n                self._trainer.datamodule.predict_dataset.image_means\n                if from_prediction\n                else self._trainer.datamodule.train_dataset.image_stats.means\n            ),\n            image_stds=(\n                self._trainer.datamodule.predict_dataset.image_stds\n                if from_prediction\n                else self._trainer.datamodule.train_dataset.image_stats.stds\n            ),\n        )\n        denormalized_output = denorm(patch=output.cpu().numpy())\n\n        if len(aux) &gt; 0:  # aux can be tiling information\n            return denormalized_output, *aux\n        else:\n            return denormalized_output\n\n    def configure_optimizers(self) -&gt; Any:\n        \"\"\"Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Any\n            Optimizer and learning rate scheduler.\n        \"\"\"\n        # instantiate optimizer\n        optimizer_func = get_optimizer(self.optimizer_name)\n        optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n        # and scheduler\n        scheduler_func = get_scheduler(self.lr_scheduler_name)\n        scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n        }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Lightning module for CAREamics.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>AlgorithmModel or dict</code> <p>Algorithm configuration.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def __init__(\n    self, algorithm_config: Union[UNetBasedAlgorithm, VAEBasedAlgorithm, dict]\n) -&gt; None:\n    \"\"\"Lightning module for CAREamics.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : AlgorithmModel or dict\n        Algorithm configuration.\n    \"\"\"\n    super().__init__()\n\n    if isinstance(algorithm_config, dict):\n        algorithm_config = algorithm_factory(algorithm_config)\n\n    # create preprocessing, model and loss function\n    if isinstance(algorithm_config, N2VAlgorithm):\n        self.use_n2v = True\n        self.n2v_preprocess: N2VManipulateTorch | None = N2VManipulateTorch(\n            n2v_manipulate_config=algorithm_config.n2v_config\n        )\n    else:\n        self.use_n2v = False\n        self.n2v_preprocess = None\n\n    self.algorithm = algorithm_config.algorithm\n    self.model: torch.nn.Module = model_factory(algorithm_config.model)\n    self.loss_func = loss_factory(algorithm_config.loss)\n\n    # save optimizer and lr_scheduler names and parameters\n    self.optimizer_name = algorithm_config.optimizer.name\n    self.optimizer_params = algorithm_config.optimizer.parameters\n    self.lr_scheduler_name = algorithm_config.lr_scheduler.name\n    self.lr_scheduler_params = algorithm_config.lr_scheduler.parameters\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Optimizer and learning rate scheduler.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Any:\n    \"\"\"Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Any\n        Optimizer and learning rate scheduler.\n    \"\"\"\n    # instantiate optimizer\n    optimizer_func = get_optimizer(self.optimizer_name)\n    optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n    # and scheduler\n    scheduler_func = get_scheduler(self.lr_scheduler_name)\n    scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n        \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n    }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Output tensor.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def forward(self, x: Any) -&gt; Any:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : Any\n        Input tensor.\n\n    Returns\n    -------\n    Any\n        Output tensor.\n    \"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Prediction step.\n\n    Parameters\n    ----------\n    batch : torch.torch.torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Model output.\n    \"\"\"\n    # TODO refactor when redoing datasets\n    # hacky way to determine if it is PredictDataModule, otherwise there is a\n    # circular import to solve with isinstance\n    from_prediction = hasattr(self._trainer.datamodule, \"tiled\")\n    is_tiled = (\n        len(batch) &gt; 1\n        and isinstance(batch[1], list)\n        and isinstance(batch[1][0], TileInformation)\n    )\n\n    # TODO add explanations for what is happening here\n    if is_tiled:\n        x, *aux = batch\n        if type(x) in [list, tuple]:\n            x = x[0]\n    else:\n        if type(batch) in [list, tuple]:\n            x = batch[0]  # TODO change, ugly way to deal with n2v refac\n        else:\n            x = batch\n        aux = []\n\n    # apply test-time augmentation if available\n    # TODO: probably wont work with batch size &gt; 1\n    if (\n        from_prediction\n        and self._trainer.datamodule.prediction_config.tta_transforms\n    ):\n        tta = ImageRestorationTTA()\n        augmented_batch = tta.forward(x)  # list of augmented tensors\n        augmented_output = []\n        for augmented in augmented_batch:\n            augmented_pred = self.model(augmented)\n            augmented_output.append(augmented_pred)\n        output = tta.backward(augmented_output)\n    else:\n        output = self.model(x)\n\n    # Denormalize the output\n    # TODO incompatible API between predict and train datasets\n    denorm = Denormalize(\n        image_means=(\n            self._trainer.datamodule.predict_dataset.image_means\n            if from_prediction\n            else self._trainer.datamodule.train_dataset.image_stats.means\n        ),\n        image_stds=(\n            self._trainer.datamodule.predict_dataset.image_stds\n            if from_prediction\n            else self._trainer.datamodule.train_dataset.image_stats.stds\n        ),\n    )\n    denormalized_output = denorm(patch=output.cpu().numpy())\n\n    if len(aux) &gt; 0:  # aux can be tiling information\n        return denormalized_output, *aux\n    else:\n        return denormalized_output\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loss value.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def training_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Training step.\n\n    Parameters\n    ----------\n    batch : torch.torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Loss value.\n    \"\"\"\n    x, *targets = batch\n    if self.use_n2v and self.n2v_preprocess is not None:\n        x_preprocessed, *aux = self.n2v_preprocess(x)\n    else:\n        x_preprocessed = x\n        aux = []\n\n    out = self.model(x_preprocessed)\n    loss = self.loss_func(out, *aux, *targets)\n    self.log(\n        \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n    )\n    optimizer = self.optimizers()\n    current_lr = optimizer.param_groups[0][\"lr\"]\n    self.log(\"learning_rate\", current_lr, on_step=False, on_epoch=True, logger=True)\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def validation_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; None:\n    \"\"\"Validation step.\n\n    Parameters\n    ----------\n    batch : torch.torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n    \"\"\"\n    x, *targets = batch\n    if self.use_n2v and self.n2v_preprocess is not None:\n        x_preprocessed, *aux = self.n2v_preprocess(x)\n    else:\n        x_preprocessed = x\n        aux = []\n\n    out = self.model(x_preprocessed)\n    val_loss = self.loss_func(out, *aux, *targets)\n\n    # log validation loss\n    self.log(\n        \"val_loss\",\n        val_loss,\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule","title":"<code>VAEModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CAREamics Lightning module.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>Union[VAEAlgorithmConfig, dict]</code> <p>Algorithm configuration.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>PyTorch model.</p> <code>loss_func</code> <code>Module</code> <p>Loss function.</p> <code>optimizer_name</code> <code>str</code> <p>Optimizer name.</p> <code>optimizer_params</code> <code>dict</code> <p>Optimizer parameters.</p> <code>lr_scheduler_name</code> <code>str</code> <p>Learning rate scheduler name.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>class VAEModule(L.LightningModule):\n    \"\"\"\n    CAREamics Lightning module.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : Union[VAEAlgorithmConfig, dict]\n        Algorithm configuration.\n\n    Attributes\n    ----------\n    model : nn.Module\n        PyTorch model.\n    loss_func : nn.Module\n        Loss function.\n    optimizer_name : str\n        Optimizer name.\n    optimizer_params : dict\n        Optimizer parameters.\n    lr_scheduler_name : str\n        Learning rate scheduler name.\n    \"\"\"\n\n    def __init__(self, algorithm_config: Union[VAEBasedAlgorithm, dict]) -&gt; None:\n        \"\"\"Lightning module for CAREamics.\n\n        This class encapsulates the a PyTorch model along with the training, validation,\n        and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n        Parameters\n        ----------\n        algorithm_config : Union[AlgorithmModel, dict]\n            Algorithm configuration.\n        \"\"\"\n        super().__init__()\n        # if loading from a checkpoint, AlgorithmModel needs to be instantiated\n        self.algorithm_config = (\n            VAEBasedAlgorithm(**algorithm_config)\n            if isinstance(algorithm_config, dict)\n            else algorithm_config\n        )\n\n        # TODO: log algorithm config\n        # self.save_hyperparameters(self.algorithm_config.model_dump())\n\n        # create model\n        self.model: torch.nn.Module = model_factory(self.algorithm_config.model)\n\n        # supervised_mode\n        self.supervised_mode = self.algorithm_config.is_supervised\n        # create loss function\n        self.noise_model: NoiseModel | None = noise_model_factory(\n            self.algorithm_config.noise_model\n        )\n\n        self.noise_model_likelihood: NoiseModelLikelihood | None = None\n        if self.algorithm_config.noise_model_likelihood is not None:\n            self.noise_model_likelihood = likelihood_factory(\n                config=self.algorithm_config.noise_model_likelihood,\n                noise_model=self.noise_model,\n            )\n\n        self.gaussian_likelihood: GaussianLikelihood | None = likelihood_factory(\n            self.algorithm_config.gaussian_likelihood\n        )\n\n        self.loss_parameters = self.algorithm_config.loss\n        self.loss_func = loss_factory(self.algorithm_config.loss.loss_type)\n\n        # save optimizer and lr_scheduler names and parameters\n        self.optimizer_name = self.algorithm_config.optimizer.name\n        self.optimizer_params = self.algorithm_config.optimizer.parameters\n        self.lr_scheduler_name = self.algorithm_config.lr_scheduler.name\n        self.lr_scheduler_params = self.algorithm_config.lr_scheduler.parameters\n\n        # initialize running PSNR\n        self.running_psnr = [\n            RunningPSNR() for _ in range(self.algorithm_config.model.output_channels)\n        ]\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs.\n\n        Returns\n        -------\n        tuple[torch.Tensor, dict[str, Any]]\n            A tuple with the output tensor and additional data from the top-down pass.\n        \"\"\"\n        return self.model(x)  # TODO Different model can have more than one output\n\n    def set_data_stats(self, data_mean, data_std):\n        \"\"\"Set data mean and std for the noise model likelihood.\n\n        Parameters\n        ----------\n        data_mean : float\n            Mean of the data.\n        data_std : float\n            Standard deviation of the data.\n        \"\"\"\n        if self.noise_model_likelihood is not None:\n            self.noise_model_likelihood.set_data_stats(data_mean, data_std)\n\n    def training_step(\n        self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: Any\n    ) -&gt; dict[str, torch.Tensor] | None:\n        \"\"\"Training step.\n\n        Parameters\n        ----------\n        batch : tuple[torch.Tensor, torch.Tensor]\n            Input batch. It is a tuple with the input tensor and the target tensor.\n            The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n            where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n            muSplit/denoiSplit).\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Loss value.\n        \"\"\"\n        x, *target = batch\n\n        # Forward pass\n        out = self.model(x)\n        if not self.supervised_mode:\n            target = x\n        else:\n            target = target[\n                0\n            ]  # hacky way to unpack. #TODO maybe should be fixed on the dataset level\n\n        # Update loss parameters\n        self.loss_parameters.kl_params.current_epoch = self.current_epoch\n\n        # Compute loss\n        if self.noise_model_likelihood is not None:\n            if (\n                self.noise_model_likelihood.data_mean is None\n                or self.noise_model_likelihood.data_std is None\n            ):\n                raise RuntimeError(\n                    \"NoiseModelLikelihood: data_mean and data_std must be set before\"\n                    \"training.\"\n                )\n        loss = self.loss_func(\n            model_outputs=out,\n            targets=target,\n            config=self.loss_parameters,\n            gaussian_likelihood=self.gaussian_likelihood,\n            noise_model_likelihood=self.noise_model_likelihood,\n        )\n\n        # Logging\n        # TODO: implement a separate logging method?\n        self.log_dict(loss, on_step=True, on_epoch=True)\n\n        try:\n            optimizer = self.optimizers()\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            self.log(\n                \"learning_rate\", current_lr, on_step=False, on_epoch=True, logger=True\n            )\n        except RuntimeError:\n            # This happens when the module is not attached to a trainer, e.g., in tests\n            pass\n        return loss\n\n    def validation_step(\n        self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: Any\n    ) -&gt; None:\n        \"\"\"Validation step.\n\n        Parameters\n        ----------\n        batch : tuple[torch.Tensor, torch.Tensor]\n            Input batch. It is a tuple with the input tensor and the target tensor.\n            The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n            where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n            muSplit/denoiSplit).\n        batch_idx : Any\n            Batch index.\n        \"\"\"\n        x, *target = batch\n\n        # Forward pass\n        out = self.model(x)\n        if not self.supervised_mode:\n            target = x\n        else:\n            target = target[\n                0\n            ]  # hacky way to unpack. #TODO maybe should be fixed on the datasel level\n        # Compute loss\n        loss = self.loss_func(\n            model_outputs=out,\n            targets=target,\n            config=self.loss_parameters,\n            gaussian_likelihood=self.gaussian_likelihood,\n            noise_model_likelihood=self.noise_model_likelihood,\n        )\n\n        # Logging\n        # Rename val_loss dict\n        loss = {\"_\".join([\"val\", k]): v for k, v in loss.items()}\n        self.log_dict(loss, on_epoch=True, prog_bar=True)\n        curr_psnr = self.compute_val_psnr(out, target)\n        for i, psnr in enumerate(curr_psnr):\n            self.log(f\"val_psnr_ch{i+1}_batch\", psnr, on_epoch=True)\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Validation epoch end.\"\"\"\n        psnr_ = self.reduce_running_psnr()\n        if psnr_ is not None:\n            self.log(\"val_psnr\", psnr_, on_epoch=True, prog_bar=True)\n        else:\n            self.log(\"val_psnr\", 0.0, on_epoch=True, prog_bar=True)\n\n    def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Prediction step.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Model output.\n        \"\"\"\n        if self.algorithm_config.algorithm == \"microsplit\":\n            x, *aux = batch\n            # Reset model for inference with spatial dimensions only (H, W)\n            self.model.reset_for_inference(x.shape[-2:])\n\n            rec_img_list = []\n            for _ in range(self.algorithm_config.mmse_count):\n                # get model output\n                rec, _ = self.model(x)\n\n                # get reconstructed img\n                if self.model.predict_logvar is None:\n                    rec_img = rec\n                    _logvar = torch.tensor([-1])\n                else:\n                    rec_img, _logvar = torch.chunk(rec, chunks=2, dim=1)\n                rec_img_list.append(rec_img.cpu().unsqueeze(0))  # add MMSE dim\n\n            # aggregate results\n            samples = torch.cat(rec_img_list, dim=0)\n            mmse_imgs = torch.mean(samples, dim=0)  # avg over MMSE dim\n            std_imgs = torch.std(samples, dim=0)  # std over MMSE dim\n\n            tile_prediction = mmse_imgs.cpu().numpy()\n            tile_std = std_imgs.cpu().numpy()\n\n            return tile_prediction, tile_std\n\n        else:\n            # Regular prediction logic\n            if self._trainer.datamodule.tiled:\n                # TODO tile_size should match model input size\n                x, *aux = batch\n                x = (\n                    x[0] if isinstance(x, list | tuple) else x\n                )  # TODO ugly, so far i don't know why x might be a list\n                self.model.reset_for_inference(x.shape)  # TODO should it be here ?\n            else:\n                x = batch[0] if isinstance(batch, list | tuple) else batch\n                aux = []\n                self.model.reset_for_inference(x.shape)\n\n            mmse_list = []\n            for _ in range(self.algorithm_config.mmse_count):\n                # apply test-time augmentation if available\n                if self._trainer.datamodule.prediction_config.tta_transforms:\n                    tta = ImageRestorationTTA()\n                    augmented_batch = tta.forward(x)  # list of augmented tensors\n                    augmented_output = []\n                    for augmented in augmented_batch:\n                        augmented_pred = self.model(augmented)\n                        augmented_output.append(augmented_pred)\n                    output = tta.backward(augmented_output)\n                else:\n                    output = self.model(x)\n\n                # taking the 1st element of the output, 2nd is std if\n                # predict_logvar==\"pixelwise\"\n                output = (\n                    output[0]\n                    if self.model.predict_logvar is None\n                    else output[0][:, 0:1, ...]\n                )\n                mmse_list.append(output)\n\n            mmse = torch.stack(mmse_list).mean(0)\n            std = torch.stack(mmse_list).std(0)  # TODO why?\n            # TODO better way to unpack if pred logvar\n            # Denormalize the output\n            denorm = Denormalize(\n                image_means=self._trainer.datamodule.predict_dataset.image_means,\n                image_stds=self._trainer.datamodule.predict_dataset.image_stds,\n            )\n\n            denormalized_output = denorm(patch=mmse.cpu().numpy())\n\n            if len(aux) &gt; 0:  # aux can be tiling information\n                return denormalized_output, std, *aux\n            else:\n                return denormalized_output, std\n\n    def configure_optimizers(self) -&gt; Any:\n        \"\"\"Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Any\n            Optimizer and learning rate scheduler.\n        \"\"\"\n        # instantiate optimizer\n        optimizer_func = get_optimizer(self.optimizer_name)\n        optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n        # and scheduler\n        scheduler_func = get_scheduler(self.lr_scheduler_name)\n        scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n        }\n\n    # TODO: find a way to move the following methods to a separate module\n    # TODO: this same operation is done in many other places, like in loss_func\n    # should we refactor LadderVAE so that it already outputs\n    # tuple(`mean`, `logvar`, `td_data`)?\n    def get_reconstructed_tensor(\n        self, model_outputs: tuple[torch.Tensor, dict[str, Any]]\n    ) -&gt; torch.Tensor:\n        \"\"\"Get the reconstructed tensor from the LVAE model outputs.\n\n        Parameters\n        ----------\n        model_outputs : tuple[torch.Tensor, dict[str, Any]]\n            Model outputs. It is a tuple with a tensor representing the predicted mean\n            and (optionally) logvar, and the top-down data dictionary.\n\n        Returns\n        -------\n        torch.Tensor\n            Reconstructed tensor, i.e., the predicted mean.\n        \"\"\"\n        predictions, _ = model_outputs\n        if self.model.predict_logvar is None:\n            return predictions\n        elif self.model.predict_logvar == \"pixelwise\":\n            return predictions.chunk(2, dim=1)[0]\n\n    def compute_val_psnr(\n        self,\n        model_output: tuple[torch.Tensor, dict[str, Any]],\n        target: torch.Tensor,\n        psnr_func: Callable = scale_invariant_psnr,\n    ) -&gt; list[float]:\n        \"\"\"Compute the PSNR for the current validation batch.\n\n        Parameters\n        ----------\n        model_output : tuple[torch.Tensor, dict[str, Any]]\n            Model output, a tuple with the predicted mean and (optionally) logvar,\n            and the top-down data dictionary.\n        target : torch.Tensor\n            Target tensor.\n        psnr_func : Callable, optional\n            PSNR function to use, by default `scale_invariant_psnr`.\n\n        Returns\n        -------\n        list[float]\n            PSNR for each channel in the current batch.\n        \"\"\"\n        # TODO check this! Related to is_supervised which is also wacky\n        out_channels = target.shape[1]\n\n        # get the reconstructed image\n        recons_img = self.get_reconstructed_tensor(model_output)\n\n        # update running psnr\n        for i in range(out_channels):\n            self.running_psnr[i].update(rec=recons_img[:, i], tar=target[:, i])\n\n        # compute psnr for each channel in the current batch\n        # TODO: this doesn't need do be a method of this class\n        # and hence can be moved to a separate module\n        return [\n            psnr_func(\n                gt=target[:, i].clone().detach().cpu().numpy(),\n                pred=recons_img[:, i].clone().detach().cpu().numpy(),\n            )\n            for i in range(out_channels)\n        ]\n\n    def reduce_running_psnr(self) -&gt; float | None:\n        \"\"\"Reduce the running PSNR statistics and reset the running PSNR.\n\n        Returns\n        -------\n        Optional[float]\n            Running PSNR averaged over the different output channels.\n        \"\"\"\n        psnr_arr = []  # type: ignore\n        for i in range(len(self.running_psnr)):\n            psnr = self.running_psnr[i].get()\n            if psnr is None:\n                psnr_arr = None  # type: ignore\n                break\n            psnr_arr.append(psnr.cpu().numpy())\n            self.running_psnr[i].reset()\n            # TODO: this line forces it to be a method of this class\n            # alternative is returning also the reset `running_psnr`\n        if psnr_arr is not None:\n            psnr = np.mean(psnr_arr)\n        return psnr\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Lightning module for CAREamics.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>Union[AlgorithmModel, dict]</code> <p>Algorithm configuration.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def __init__(self, algorithm_config: Union[VAEBasedAlgorithm, dict]) -&gt; None:\n    \"\"\"Lightning module for CAREamics.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : Union[AlgorithmModel, dict]\n        Algorithm configuration.\n    \"\"\"\n    super().__init__()\n    # if loading from a checkpoint, AlgorithmModel needs to be instantiated\n    self.algorithm_config = (\n        VAEBasedAlgorithm(**algorithm_config)\n        if isinstance(algorithm_config, dict)\n        else algorithm_config\n    )\n\n    # TODO: log algorithm config\n    # self.save_hyperparameters(self.algorithm_config.model_dump())\n\n    # create model\n    self.model: torch.nn.Module = model_factory(self.algorithm_config.model)\n\n    # supervised_mode\n    self.supervised_mode = self.algorithm_config.is_supervised\n    # create loss function\n    self.noise_model: NoiseModel | None = noise_model_factory(\n        self.algorithm_config.noise_model\n    )\n\n    self.noise_model_likelihood: NoiseModelLikelihood | None = None\n    if self.algorithm_config.noise_model_likelihood is not None:\n        self.noise_model_likelihood = likelihood_factory(\n            config=self.algorithm_config.noise_model_likelihood,\n            noise_model=self.noise_model,\n        )\n\n    self.gaussian_likelihood: GaussianLikelihood | None = likelihood_factory(\n        self.algorithm_config.gaussian_likelihood\n    )\n\n    self.loss_parameters = self.algorithm_config.loss\n    self.loss_func = loss_factory(self.algorithm_config.loss.loss_type)\n\n    # save optimizer and lr_scheduler names and parameters\n    self.optimizer_name = self.algorithm_config.optimizer.name\n    self.optimizer_params = self.algorithm_config.optimizer.parameters\n    self.lr_scheduler_name = self.algorithm_config.lr_scheduler.name\n    self.lr_scheduler_params = self.algorithm_config.lr_scheduler.parameters\n\n    # initialize running PSNR\n    self.running_psnr = [\n        RunningPSNR() for _ in range(self.algorithm_config.model.output_channels)\n    ]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.compute_val_psnr","title":"<code>compute_val_psnr(model_output, target, psnr_func=scale_invariant_psnr)</code>","text":"<p>Compute the PSNR for the current validation batch.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Model output, a tuple with the predicted mean and (optionally) logvar, and the top-down data dictionary.</p> required <code>target</code> <code>Tensor</code> <p>Target tensor.</p> required <code>psnr_func</code> <code>Callable</code> <p>PSNR function to use, by default <code>scale_invariant_psnr</code>.</p> <code>scale_invariant_psnr</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>PSNR for each channel in the current batch.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def compute_val_psnr(\n    self,\n    model_output: tuple[torch.Tensor, dict[str, Any]],\n    target: torch.Tensor,\n    psnr_func: Callable = scale_invariant_psnr,\n) -&gt; list[float]:\n    \"\"\"Compute the PSNR for the current validation batch.\n\n    Parameters\n    ----------\n    model_output : tuple[torch.Tensor, dict[str, Any]]\n        Model output, a tuple with the predicted mean and (optionally) logvar,\n        and the top-down data dictionary.\n    target : torch.Tensor\n        Target tensor.\n    psnr_func : Callable, optional\n        PSNR function to use, by default `scale_invariant_psnr`.\n\n    Returns\n    -------\n    list[float]\n        PSNR for each channel in the current batch.\n    \"\"\"\n    # TODO check this! Related to is_supervised which is also wacky\n    out_channels = target.shape[1]\n\n    # get the reconstructed image\n    recons_img = self.get_reconstructed_tensor(model_output)\n\n    # update running psnr\n    for i in range(out_channels):\n        self.running_psnr[i].update(rec=recons_img[:, i], tar=target[:, i])\n\n    # compute psnr for each channel in the current batch\n    # TODO: this doesn't need do be a method of this class\n    # and hence can be moved to a separate module\n    return [\n        psnr_func(\n            gt=target[:, i].clone().detach().cpu().numpy(),\n            pred=recons_img[:, i].clone().detach().cpu().numpy(),\n        )\n        for i in range(out_channels)\n    ]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Optimizer and learning rate scheduler.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Any:\n    \"\"\"Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Any\n        Optimizer and learning rate scheduler.\n    \"\"\"\n    # instantiate optimizer\n    optimizer_func = get_optimizer(self.optimizer_name)\n    optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n    # and scheduler\n    scheduler_func = get_scheduler(self.lr_scheduler_name)\n    scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n        \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n    }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Any]]</code> <p>A tuple with the output tensor and additional data from the top-down pass.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs.\n\n    Returns\n    -------\n    tuple[torch.Tensor, dict[str, Any]]\n        A tuple with the output tensor and additional data from the top-down pass.\n    \"\"\"\n    return self.model(x)  # TODO Different model can have more than one output\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.get_reconstructed_tensor","title":"<code>get_reconstructed_tensor(model_outputs)</code>","text":"<p>Get the reconstructed tensor from the LVAE model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Model outputs. It is a tuple with a tensor representing the predicted mean and (optionally) logvar, and the top-down data dictionary.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor, i.e., the predicted mean.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def get_reconstructed_tensor(\n    self, model_outputs: tuple[torch.Tensor, dict[str, Any]]\n) -&gt; torch.Tensor:\n    \"\"\"Get the reconstructed tensor from the LVAE model outputs.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Model outputs. It is a tuple with a tensor representing the predicted mean\n        and (optionally) logvar, and the top-down data dictionary.\n\n    Returns\n    -------\n    torch.Tensor\n        Reconstructed tensor, i.e., the predicted mean.\n    \"\"\"\n    predictions, _ = model_outputs\n    if self.model.predict_logvar is None:\n        return predictions\n    elif self.model.predict_logvar == \"pixelwise\":\n        return predictions.chunk(2, dim=1)[0]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Validation epoch end.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Validation epoch end.\"\"\"\n    psnr_ = self.reduce_running_psnr()\n    if psnr_ is not None:\n        self.log(\"val_psnr\", psnr_, on_epoch=True, prog_bar=True)\n    else:\n        self.log(\"val_psnr\", 0.0, on_epoch=True, prog_bar=True)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Prediction step.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Model output.\n    \"\"\"\n    if self.algorithm_config.algorithm == \"microsplit\":\n        x, *aux = batch\n        # Reset model for inference with spatial dimensions only (H, W)\n        self.model.reset_for_inference(x.shape[-2:])\n\n        rec_img_list = []\n        for _ in range(self.algorithm_config.mmse_count):\n            # get model output\n            rec, _ = self.model(x)\n\n            # get reconstructed img\n            if self.model.predict_logvar is None:\n                rec_img = rec\n                _logvar = torch.tensor([-1])\n            else:\n                rec_img, _logvar = torch.chunk(rec, chunks=2, dim=1)\n            rec_img_list.append(rec_img.cpu().unsqueeze(0))  # add MMSE dim\n\n        # aggregate results\n        samples = torch.cat(rec_img_list, dim=0)\n        mmse_imgs = torch.mean(samples, dim=0)  # avg over MMSE dim\n        std_imgs = torch.std(samples, dim=0)  # std over MMSE dim\n\n        tile_prediction = mmse_imgs.cpu().numpy()\n        tile_std = std_imgs.cpu().numpy()\n\n        return tile_prediction, tile_std\n\n    else:\n        # Regular prediction logic\n        if self._trainer.datamodule.tiled:\n            # TODO tile_size should match model input size\n            x, *aux = batch\n            x = (\n                x[0] if isinstance(x, list | tuple) else x\n            )  # TODO ugly, so far i don't know why x might be a list\n            self.model.reset_for_inference(x.shape)  # TODO should it be here ?\n        else:\n            x = batch[0] if isinstance(batch, list | tuple) else batch\n            aux = []\n            self.model.reset_for_inference(x.shape)\n\n        mmse_list = []\n        for _ in range(self.algorithm_config.mmse_count):\n            # apply test-time augmentation if available\n            if self._trainer.datamodule.prediction_config.tta_transforms:\n                tta = ImageRestorationTTA()\n                augmented_batch = tta.forward(x)  # list of augmented tensors\n                augmented_output = []\n                for augmented in augmented_batch:\n                    augmented_pred = self.model(augmented)\n                    augmented_output.append(augmented_pred)\n                output = tta.backward(augmented_output)\n            else:\n                output = self.model(x)\n\n            # taking the 1st element of the output, 2nd is std if\n            # predict_logvar==\"pixelwise\"\n            output = (\n                output[0]\n                if self.model.predict_logvar is None\n                else output[0][:, 0:1, ...]\n            )\n            mmse_list.append(output)\n\n        mmse = torch.stack(mmse_list).mean(0)\n        std = torch.stack(mmse_list).std(0)  # TODO why?\n        # TODO better way to unpack if pred logvar\n        # Denormalize the output\n        denorm = Denormalize(\n            image_means=self._trainer.datamodule.predict_dataset.image_means,\n            image_stds=self._trainer.datamodule.predict_dataset.image_stds,\n        )\n\n        denormalized_output = denorm(patch=mmse.cpu().numpy())\n\n        if len(aux) &gt; 0:  # aux can be tiling information\n            return denormalized_output, std, *aux\n        else:\n            return denormalized_output, std\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.reduce_running_psnr","title":"<code>reduce_running_psnr()</code>","text":"<p>Reduce the running PSNR statistics and reset the running PSNR.</p> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Running PSNR averaged over the different output channels.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def reduce_running_psnr(self) -&gt; float | None:\n    \"\"\"Reduce the running PSNR statistics and reset the running PSNR.\n\n    Returns\n    -------\n    Optional[float]\n        Running PSNR averaged over the different output channels.\n    \"\"\"\n    psnr_arr = []  # type: ignore\n    for i in range(len(self.running_psnr)):\n        psnr = self.running_psnr[i].get()\n        if psnr is None:\n            psnr_arr = None  # type: ignore\n            break\n        psnr_arr.append(psnr.cpu().numpy())\n        self.running_psnr[i].reset()\n        # TODO: this line forces it to be a method of this class\n        # alternative is returning also the reset `running_psnr`\n    if psnr_arr is not None:\n        psnr = np.mean(psnr_arr)\n    return psnr\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.set_data_stats","title":"<code>set_data_stats(data_mean, data_std)</code>","text":"<p>Set data mean and std for the noise model likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>data_mean</code> <code>float</code> <p>Mean of the data.</p> required <code>data_std</code> <code>float</code> <p>Standard deviation of the data.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def set_data_stats(self, data_mean, data_std):\n    \"\"\"Set data mean and std for the noise model likelihood.\n\n    Parameters\n    ----------\n    data_mean : float\n        Mean of the data.\n    data_std : float\n        Standard deviation of the data.\n    \"\"\"\n    if self.noise_model_likelihood is not None:\n        self.noise_model_likelihood.set_data_stats(data_mean, data_std)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple[Tensor, Tensor]</code> <p>Input batch. It is a tuple with the input tensor and the target tensor. The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X), where C is the number of target channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loss value.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def training_step(\n    self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: Any\n) -&gt; dict[str, torch.Tensor] | None:\n    \"\"\"Training step.\n\n    Parameters\n    ----------\n    batch : tuple[torch.Tensor, torch.Tensor]\n        Input batch. It is a tuple with the input tensor and the target tensor.\n        The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n        where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n        muSplit/denoiSplit).\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Loss value.\n    \"\"\"\n    x, *target = batch\n\n    # Forward pass\n    out = self.model(x)\n    if not self.supervised_mode:\n        target = x\n    else:\n        target = target[\n            0\n        ]  # hacky way to unpack. #TODO maybe should be fixed on the dataset level\n\n    # Update loss parameters\n    self.loss_parameters.kl_params.current_epoch = self.current_epoch\n\n    # Compute loss\n    if self.noise_model_likelihood is not None:\n        if (\n            self.noise_model_likelihood.data_mean is None\n            or self.noise_model_likelihood.data_std is None\n        ):\n            raise RuntimeError(\n                \"NoiseModelLikelihood: data_mean and data_std must be set before\"\n                \"training.\"\n            )\n    loss = self.loss_func(\n        model_outputs=out,\n        targets=target,\n        config=self.loss_parameters,\n        gaussian_likelihood=self.gaussian_likelihood,\n        noise_model_likelihood=self.noise_model_likelihood,\n    )\n\n    # Logging\n    # TODO: implement a separate logging method?\n    self.log_dict(loss, on_step=True, on_epoch=True)\n\n    try:\n        optimizer = self.optimizers()\n        current_lr = optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\", current_lr, on_step=False, on_epoch=True, logger=True\n        )\n    except RuntimeError:\n        # This happens when the module is not attached to a trainer, e.g., in tests\n        pass\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple[Tensor, Tensor]</code> <p>Input batch. It is a tuple with the input tensor and the target tensor. The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X), where C is the number of target channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def validation_step(\n    self, batch: tuple[torch.Tensor, torch.Tensor], batch_idx: Any\n) -&gt; None:\n    \"\"\"Validation step.\n\n    Parameters\n    ----------\n    batch : tuple[torch.Tensor, torch.Tensor]\n        Input batch. It is a tuple with the input tensor and the target tensor.\n        The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n        where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n        muSplit/denoiSplit).\n    batch_idx : Any\n        Batch index.\n    \"\"\"\n    x, *target = batch\n\n    # Forward pass\n    out = self.model(x)\n    if not self.supervised_mode:\n        target = x\n    else:\n        target = target[\n            0\n        ]  # hacky way to unpack. #TODO maybe should be fixed on the datasel level\n    # Compute loss\n    loss = self.loss_func(\n        model_outputs=out,\n        targets=target,\n        config=self.loss_parameters,\n        gaussian_likelihood=self.gaussian_likelihood,\n        noise_model_likelihood=self.noise_model_likelihood,\n    )\n\n    # Logging\n    # Rename val_loss dict\n    loss = {\"_\".join([\"val\", k]): v for k, v in loss.items()}\n    self.log_dict(loss, on_epoch=True, prog_bar=True)\n    curr_psnr = self.compute_val_psnr(out, target)\n    for i, psnr in enumerate(curr_psnr):\n        self.log(f\"val_psnr_ch{i+1}_batch\", psnr, on_epoch=True)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.create_careamics_module","title":"<code>create_careamics_module(algorithm, loss, architecture, use_n2v2=False, struct_n2v_axis='none', struct_n2v_span=5, model_parameters=None, optimizer='Adam', optimizer_parameters=None, lr_scheduler='ReduceLROnPlateau', lr_scheduler_parameters=None)</code>","text":"<p>Create a CAREamics Lightning module.</p> <p>This function exposes parameters used to create an AlgorithmModel instance, triggering parameters validation.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>SupportedAlgorithm or str</code> <p>Algorithm to use for training (see SupportedAlgorithm).</p> required <code>loss</code> <code>SupportedLoss or str</code> <p>Loss function to use for training (see SupportedLoss).</p> required <code>architecture</code> <code>SupportedArchitecture or str</code> <p>Model architecture to use for training (see SupportedArchitecture).</p> required <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2 or Noise2Void.</p> <code>False</code> <code>struct_n2v_axis</code> <code>\"horizontal\", \"vertical\", or \"none\"</code> <p>Axis of the StructN2V mask.</p> <code>\"none\"</code> <code>struct_n2v_span</code> <code>int</code> <p>Span of the StructN2V mask.</p> <code>5</code> <code>model_parameters</code> <code>dict</code> <p>Model parameters to use for training, by default {}. Model parameters are defined in the relevant <code>torch.nn.Module</code> class, or Pyddantic model (see <code>careamics.config.architectures</code>).</p> <code>None</code> <code>optimizer</code> <code>SupportedOptimizer or str</code> <p>Optimizer to use for training, by default \"Adam\" (see SupportedOptimizer).</p> <code>'Adam'</code> <code>optimizer_parameters</code> <code>dict</code> <p>Optimizer parameters to use for training, as defined in <code>torch.optim</code>, by default {}.</p> <code>None</code> <code>lr_scheduler</code> <code>SupportedScheduler or str</code> <p>Learning rate scheduler to use for training, by default \"ReduceLROnPlateau\" (see SupportedScheduler).</p> <code>'ReduceLROnPlateau'</code> <code>lr_scheduler_parameters</code> <code>dict</code> <p>Learning rate scheduler parameters to use for training, as defined in <code>torch.optim</code>, by default {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>CAREamicsModule</code> <p>CAREamics Lightning module.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def create_careamics_module(\n    algorithm: Union[SupportedAlgorithm, str],\n    loss: Union[SupportedLoss, str],\n    architecture: Union[SupportedArchitecture, str],\n    use_n2v2: bool = False,\n    struct_n2v_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_n2v_span: int = 5,\n    model_parameters: dict | None = None,\n    optimizer: Union[SupportedOptimizer, str] = \"Adam\",\n    optimizer_parameters: dict | None = None,\n    lr_scheduler: Union[SupportedScheduler, str] = \"ReduceLROnPlateau\",\n    lr_scheduler_parameters: dict | None = None,\n) -&gt; Union[FCNModule, VAEModule]:\n    \"\"\"Create a CAREamics Lightning module.\n\n    This function exposes parameters used to create an AlgorithmModel instance,\n    triggering parameters validation.\n\n    Parameters\n    ----------\n    algorithm : SupportedAlgorithm or str\n        Algorithm to use for training (see SupportedAlgorithm).\n    loss : SupportedLoss or str\n        Loss function to use for training (see SupportedLoss).\n    architecture : SupportedArchitecture or str\n        Model architecture to use for training (see SupportedArchitecture).\n    use_n2v2 : bool, default=False\n        Whether to use N2V2 or Noise2Void.\n    struct_n2v_axis : \"horizontal\", \"vertical\", or \"none\", default=\"none\"\n        Axis of the StructN2V mask.\n    struct_n2v_span : int, default=5\n        Span of the StructN2V mask.\n    model_parameters : dict, optional\n        Model parameters to use for training, by default {}. Model parameters are\n        defined in the relevant `torch.nn.Module` class, or Pyddantic model (see\n        `careamics.config.architectures`).\n    optimizer : SupportedOptimizer or str, optional\n        Optimizer to use for training, by default \"Adam\" (see SupportedOptimizer).\n    optimizer_parameters : dict, optional\n        Optimizer parameters to use for training, as defined in `torch.optim`, by\n        default {}.\n    lr_scheduler : SupportedScheduler or str, optional\n        Learning rate scheduler to use for training, by default \"ReduceLROnPlateau\"\n        (see SupportedScheduler).\n    lr_scheduler_parameters : dict, optional\n        Learning rate scheduler parameters to use for training, as defined in\n        `torch.optim`, by default {}.\n\n    Returns\n    -------\n    CAREamicsModule\n        CAREamics Lightning module.\n    \"\"\"\n    # TODO should use the same functions are in configuration_factory.py\n    # create an AlgorithmModel compatible dictionary\n    if lr_scheduler_parameters is None:\n        lr_scheduler_parameters = {}\n    if optimizer_parameters is None:\n        optimizer_parameters = {}\n    if model_parameters is None:\n        model_parameters = {}\n    algorithm_dict: dict[str, Any] = {\n        \"algorithm\": algorithm,\n        \"loss\": loss,\n        \"optimizer\": {\n            \"name\": optimizer,\n            \"parameters\": optimizer_parameters,\n        },\n        \"lr_scheduler\": {\n            \"name\": lr_scheduler,\n            \"parameters\": lr_scheduler_parameters,\n        },\n    }\n\n    model_dict = {\"architecture\": architecture}\n    model_dict.update(model_parameters)\n\n    # add model parameters to algorithm configuration\n    algorithm_dict[\"model\"] = model_dict\n\n    which_algo = algorithm_dict[\"algorithm\"]\n    if which_algo in UNetBasedAlgorithm.get_compatible_algorithms():\n        algorithm_cfg = algorithm_factory(algorithm_dict)\n\n        # if use N2V\n        if isinstance(algorithm_cfg, N2VAlgorithm):\n            algorithm_cfg.n2v_config.struct_mask_axis = struct_n2v_axis\n            algorithm_cfg.n2v_config.struct_mask_span = struct_n2v_span\n            algorithm_cfg.set_n2v2(use_n2v2)\n\n        return FCNModule(algorithm_cfg)\n    else:\n        raise NotImplementedError(\n            f\"Algorithm {which_algo} is not implemented or unknown.\"\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/","title":"microsplit_data_module","text":"<p>MicroSplit data module for training and validation.</p>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitDataModule","title":"<code>MicroSplitDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning DataModule for MicroSplit-style datasets.</p> <p>Matches the interface of TrainDataModule, but internally uses original MicroSplit dataset logic.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>MicroSplitDataConfig</code> <p>Configuration for the MicroSplit dataset.</p> required <code>train_data</code> <code>str</code> <p>Path to training data directory.</p> required <code>val_data</code> <code>str</code> <p>Path to validation data directory.</p> <code>None</code> <code>train_data_target</code> <code>str</code> <p>Path to training target data.</p> <code>None</code> <code>val_data_target</code> <code>str</code> <p>Path to validation target data.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>File extension filter.</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of data to use for validation, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of samples for validation split, by default 5.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Whether to use in-memory dataset, by default True.</p> <code>True</code> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>class MicroSplitDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule for MicroSplit-style datasets.\n\n    Matches the interface of TrainDataModule, but internally uses original MicroSplit\n    dataset logic.\n\n    Parameters\n    ----------\n    data_config : MicroSplitDataConfig\n        Configuration for the MicroSplit dataset.\n    train_data : str\n        Path to training data directory.\n    val_data : str, optional\n        Path to validation data directory.\n    train_data_target : str, optional\n        Path to training target data.\n    val_data_target : str, optional\n        Path to validation target data.\n    read_source_func : Callable, optional\n        Function to read source data.\n    extension_filter : str, optional\n        File extension filter.\n    val_percentage : float, optional\n        Percentage of data to use for validation, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of samples for validation split, by default 5.\n    use_in_memory : bool, optional\n        Whether to use in-memory dataset, by default True.\n    \"\"\"\n\n    def __init__(\n        self,\n        # Should be compatible with microSplit DatasetConfig\n        data_config: MicroSplitDataConfig,\n        train_data: str,\n        val_data: str | None = None,\n        train_data_target: str | None = None,\n        val_data_target: str | None = None,\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ):\n        \"\"\"Initialize MicroSplitDataModule.\n\n        Parameters\n        ----------\n        data_config : MicroSplitDataConfig\n            Configuration for the MicroSplit dataset.\n        train_data : str\n            Path to training data directory.\n        val_data : str, optional\n            Path to validation data directory.\n        train_data_target : str, optional\n            Path to training target data.\n        val_data_target : str, optional\n            Path to validation target data.\n        read_source_func : Callable, optional\n            Function to read source data.\n        extension_filter : str, optional\n            File extension filter.\n        val_percentage : float, optional\n            Percentage of data to use for validation, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of samples for validation split, by default 5.\n        use_in_memory : bool, optional\n            Whether to use in-memory dataset, by default True.\n        \"\"\"\n        super().__init__()\n        # Dataset selection logic (adapted from create_train_val_datasets)\n        self.train_config = data_config  # SHould configs be separated?\n        self.val_config = data_config\n        self.test_config = data_config\n\n        datapath = train_data\n        load_data_func = read_source_func\n\n        dataset_class = LCMultiChDloader  # TODO hardcoded for now\n\n        # Create datasets\n        self.train_dataset = dataset_class(\n            self.train_config,\n            datapath,\n            load_data_fn=load_data_func,\n            val_fraction=val_percentage,\n            test_fraction=0.1,\n        )\n        max_val = self.train_dataset.get_max_val()\n        self.val_config.max_val = max_val\n        if self.train_config.datasplit_type == DataSplitType.All:\n            self.val_config.datasplit_type = DataSplitType.All\n            self.test_config.datasplit_type = DataSplitType.All\n        self.val_dataset = dataset_class(\n            self.val_config,\n            datapath,\n            load_data_fn=load_data_func,\n            val_fraction=val_percentage,\n            test_fraction=0.1,\n        )\n        self.test_config.max_val = max_val\n        self.test_dataset = dataset_class(\n            self.test_config,\n            datapath,\n            load_data_fn=load_data_func,\n            val_fraction=val_percentage,\n            test_fraction=0.1,\n        )\n        mean_val, std_val = self.train_dataset.compute_mean_std()\n        self.train_dataset.set_mean_std(mean_val, std_val)\n        self.val_dataset.set_mean_std(mean_val, std_val)\n        self.test_dataset.set_mean_std(mean_val, std_val)\n        data_stats = self.train_dataset.get_mean_std()\n\n        # Store data statistics\n        self.data_stats = (\n            data_stats[0],\n            data_stats[1],\n        )  # TODO repeats old logic, revisit\n\n    def train_dataloader(self):\n        \"\"\"Create a dataloader for training.\n\n        Returns\n        -------\n        DataLoader\n            Training dataloader.\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            # TODO should be inside dataloader params?\n            batch_size=self.train_config.batch_size,\n            **self.train_config.train_dataloader_params,\n        )\n\n    def val_dataloader(self):\n        \"\"\"Create a dataloader for validation.\n\n        Returns\n        -------\n        DataLoader\n            Validation dataloader.\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.train_config.batch_size,\n            **self.val_config.val_dataloader_params,  # TODO duplicated\n        )\n\n    def get_data_stats(self):\n        \"\"\"Get data statistics.\n\n        Returns\n        -------\n        tuple[dict, dict]\n            A tuple containing two dictionaries:\n            - data_mean: mean values for input and target\n            - data_std: standard deviation values for input and target\n        \"\"\"\n        return self.data_stats, self.val_config.max_val  # TODO should be in the config?\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitDataModule.__init__","title":"<code>__init__(data_config, train_data, val_data=None, train_data_target=None, val_data_target=None, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_split=5, use_in_memory=True)</code>","text":"<p>Initialize MicroSplitDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>MicroSplitDataConfig</code> <p>Configuration for the MicroSplit dataset.</p> required <code>train_data</code> <code>str</code> <p>Path to training data directory.</p> required <code>val_data</code> <code>str</code> <p>Path to validation data directory.</p> <code>None</code> <code>train_data_target</code> <code>str</code> <p>Path to training target data.</p> <code>None</code> <code>val_data_target</code> <code>str</code> <p>Path to validation target data.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>File extension filter.</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of data to use for validation, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of samples for validation split, by default 5.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Whether to use in-memory dataset, by default True.</p> <code>True</code> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def __init__(\n    self,\n    # Should be compatible with microSplit DatasetConfig\n    data_config: MicroSplitDataConfig,\n    train_data: str,\n    val_data: str | None = None,\n    train_data_target: str | None = None,\n    val_data_target: str | None = None,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 5,\n    use_in_memory: bool = True,\n):\n    \"\"\"Initialize MicroSplitDataModule.\n\n    Parameters\n    ----------\n    data_config : MicroSplitDataConfig\n        Configuration for the MicroSplit dataset.\n    train_data : str\n        Path to training data directory.\n    val_data : str, optional\n        Path to validation data directory.\n    train_data_target : str, optional\n        Path to training target data.\n    val_data_target : str, optional\n        Path to validation target data.\n    read_source_func : Callable, optional\n        Function to read source data.\n    extension_filter : str, optional\n        File extension filter.\n    val_percentage : float, optional\n        Percentage of data to use for validation, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of samples for validation split, by default 5.\n    use_in_memory : bool, optional\n        Whether to use in-memory dataset, by default True.\n    \"\"\"\n    super().__init__()\n    # Dataset selection logic (adapted from create_train_val_datasets)\n    self.train_config = data_config  # SHould configs be separated?\n    self.val_config = data_config\n    self.test_config = data_config\n\n    datapath = train_data\n    load_data_func = read_source_func\n\n    dataset_class = LCMultiChDloader  # TODO hardcoded for now\n\n    # Create datasets\n    self.train_dataset = dataset_class(\n        self.train_config,\n        datapath,\n        load_data_fn=load_data_func,\n        val_fraction=val_percentage,\n        test_fraction=0.1,\n    )\n    max_val = self.train_dataset.get_max_val()\n    self.val_config.max_val = max_val\n    if self.train_config.datasplit_type == DataSplitType.All:\n        self.val_config.datasplit_type = DataSplitType.All\n        self.test_config.datasplit_type = DataSplitType.All\n    self.val_dataset = dataset_class(\n        self.val_config,\n        datapath,\n        load_data_fn=load_data_func,\n        val_fraction=val_percentage,\n        test_fraction=0.1,\n    )\n    self.test_config.max_val = max_val\n    self.test_dataset = dataset_class(\n        self.test_config,\n        datapath,\n        load_data_fn=load_data_func,\n        val_fraction=val_percentage,\n        test_fraction=0.1,\n    )\n    mean_val, std_val = self.train_dataset.compute_mean_std()\n    self.train_dataset.set_mean_std(mean_val, std_val)\n    self.val_dataset.set_mean_std(mean_val, std_val)\n    self.test_dataset.set_mean_std(mean_val, std_val)\n    data_stats = self.train_dataset.get_mean_std()\n\n    # Store data statistics\n    self.data_stats = (\n        data_stats[0],\n        data_stats[1],\n    )  # TODO repeats old logic, revisit\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitDataModule.get_data_stats","title":"<code>get_data_stats()</code>","text":"<p>Get data statistics.</p> <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>A tuple containing two dictionaries: - data_mean: mean values for input and target - data_std: standard deviation values for input and target</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def get_data_stats(self):\n    \"\"\"Get data statistics.\n\n    Returns\n    -------\n    tuple[dict, dict]\n        A tuple containing two dictionaries:\n        - data_mean: mean values for input and target\n        - data_std: standard deviation values for input and target\n    \"\"\"\n    return self.data_stats, self.val_config.max_val  # TODO should be in the config?\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create a dataloader for training.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Training dataloader.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def train_dataloader(self):\n    \"\"\"Create a dataloader for training.\n\n    Returns\n    -------\n    DataLoader\n        Training dataloader.\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        # TODO should be inside dataloader params?\n        batch_size=self.train_config.batch_size,\n        **self.train_config.train_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create a dataloader for validation.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Validation dataloader.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def val_dataloader(self):\n    \"\"\"Create a dataloader for validation.\n\n    Returns\n    -------\n    DataLoader\n        Validation dataloader.\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.train_config.batch_size,\n        **self.val_config.val_dataloader_params,  # TODO duplicated\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitPredictDataModule","title":"<code>MicroSplitPredictDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Lightning DataModule for MicroSplit-style prediction datasets.</p> <p>Matches the interface of PredictDataModule, but internally uses MicroSplit dataset logic for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>MicroSplitDataConfig</code> <p>Configuration for MicroSplit prediction.</p> required <code>pred_data</code> <code>str or Path or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types.</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters.</p> <code>None</code> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>class MicroSplitPredictDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule for MicroSplit-style prediction datasets.\n\n    Matches the interface of PredictDataModule, but internally uses MicroSplit\n    dataset logic for prediction.\n\n    Parameters\n    ----------\n    pred_config : MicroSplitDataConfig\n        Configuration for MicroSplit prediction.\n    pred_data : str or Path or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types.\n    dataloader_params : dict, optional\n        Dataloader parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        pred_config: MicroSplitDataConfig,\n        pred_data: Union[str, Path, NDArray],\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        dataloader_params: dict | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Constructor for MicroSplit prediction data module.\n\n        Parameters\n        ----------\n        pred_config : MicroSplitDataConfig\n            Configuration for MicroSplit prediction.\n        pred_data : str or Path or numpy.ndarray\n            Prediction data, can be a path to a folder, a file or a numpy array.\n        read_source_func : Callable, optional\n            Function to read custom types, by default None.\n        extension_filter : str, optional\n            Filter to filter file extensions for custom types, by default \"\".\n        dataloader_params : dict, optional\n            Dataloader parameters, by default {}.\n        \"\"\"\n        super().__init__()\n\n        if dataloader_params is None:\n            dataloader_params = {}\n        self.pred_config = pred_config\n        self.pred_data = pred_data\n        self.read_source_func = read_source_func or get_train_val_data\n        self.extension_filter = extension_filter\n        self.dataloader_params = dataloader_params\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n        # # TODO currently data preparation is handled in dataset creation, revisit!\n        pass\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Hook called at the beginning of predict.\n\n        Parameters\n        ----------\n        stage : Optional[str], optional\n            Stage, by default None.\n        \"\"\"\n        # Create prediction dataset using LCMultiChDloader\n        self.predict_dataset = LCMultiChDloader(\n            self.pred_config,\n            self.pred_data,\n            load_data_fn=self.read_source_func,\n            val_fraction=0.0,  # No validation split for prediction\n            test_fraction=1.0,  # No test split for prediction\n        )\n        self.predict_dataset.set_mean_std(*self.pred_config.data_stats)\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for prediction.\n\n        Returns\n        -------\n        DataLoader\n            Prediction dataloader.\n        \"\"\"\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.pred_config.batch_size,\n            **self.dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitPredictDataModule.__init__","title":"<code>__init__(pred_config, pred_data, read_source_func=None, extension_filter='', dataloader_params=None)</code>","text":"<p>Constructor for MicroSplit prediction data module.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>MicroSplitDataConfig</code> <p>Configuration for MicroSplit prediction.</p> required <code>pred_data</code> <code>str or Path or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters, by default {}.</p> <code>None</code> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def __init__(\n    self,\n    pred_config: MicroSplitDataConfig,\n    pred_data: Union[str, Path, NDArray],\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    dataloader_params: dict | None = None,\n) -&gt; None:\n    \"\"\"\n    Constructor for MicroSplit prediction data module.\n\n    Parameters\n    ----------\n    pred_config : MicroSplitDataConfig\n        Configuration for MicroSplit prediction.\n    pred_data : str or Path or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types, by default None.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types, by default \"\".\n    dataloader_params : dict, optional\n        Dataloader parameters, by default {}.\n    \"\"\"\n    super().__init__()\n\n    if dataloader_params is None:\n        dataloader_params = {}\n    self.pred_config = pred_config\n    self.pred_data = pred_data\n    self.read_source_func = read_source_func or get_train_val_data\n    self.extension_filter = extension_filter\n    self.dataloader_params = dataloader_params\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitPredictDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Prediction dataloader.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for prediction.\n\n    Returns\n    -------\n    DataLoader\n        Prediction dataloader.\n    \"\"\"\n    return DataLoader(\n        self.predict_dataset,\n        batch_size=self.pred_config.batch_size,\n        **self.dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitPredictDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Hook used to prepare the data before calling <code>setup</code>.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n    # # TODO currently data preparation is handled in dataset creation, revisit!\n    pass\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.MicroSplitPredictDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Hook called at the beginning of predict.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>Stage, by default None.</p> <code>None</code> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Hook called at the beginning of predict.\n\n    Parameters\n    ----------\n    stage : Optional[str], optional\n        Stage, by default None.\n    \"\"\"\n    # Create prediction dataset using LCMultiChDloader\n    self.predict_dataset = LCMultiChDloader(\n        self.pred_config,\n        self.pred_data,\n        load_data_fn=self.read_source_func,\n        val_fraction=0.0,  # No validation split for prediction\n        test_fraction=1.0,  # No test split for prediction\n    )\n    self.predict_dataset.set_mean_std(*self.pred_config.data_stats)\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.create_microsplit_predict_datamodule","title":"<code>create_microsplit_predict_datamodule(pred_data, tile_size, data_type, axes, batch_size=1, num_channels=2, depth3D=1, grid_size=None, multiscale_count=None, data_stats=None, tiling_mode=TilingMode.ShiftBoundary, read_source_func=None, extension_filter='', dataloader_params=None, **dataset_kwargs)</code>","text":"<p>Create a MicroSplitPredictDataModule for microSplit-style prediction datasets.</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>str or Path or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>tile_size</code> <code>tuple</code> <p>Size of one tile of data.</p> required <code>data_type</code> <code>DataType</code> <p>Type of the dataset (must be a DataType enum value).</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g., 'SYX').</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction dataloader.</p> <code>1</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input.</p> <code>2</code> <code>depth3D</code> <code>int</code> <p>Number of slices in 3D.</p> <code>1</code> <code>grid_size</code> <code>tuple</code> <p>Grid size for patch extraction.</p> <code>None</code> <code>multiscale_count</code> <code>int</code> <p>Number of LC scales.</p> <code>None</code> <code>data_stats</code> <code>tuple</code> <p>Data statistics, by default None.</p> <code>None</code> <code>tiling_mode</code> <code>TilingMode</code> <p>Tiling mode for patch extraction.</p> <code>ShiftBoundary</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>File extension filter.</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Parameters for prediction dataloader.</p> <code>None</code> <code>**dataset_kwargs</code> <p>Additional arguments passed to MicroSplitDataConfig.</p> <code>{}</code> <p>Returns:</p> Type Description <code>MicroSplitPredictDataModule</code> <p>Configured MicroSplitPredictDataModule instance.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def create_microsplit_predict_datamodule(\n    pred_data: Union[str, Path, NDArray],\n    tile_size: tuple,\n    data_type: DataType,\n    axes: str,\n    batch_size: int = 1,\n    num_channels: int = 2,\n    depth3D: int = 1,\n    grid_size: int | None = None,\n    multiscale_count: int | None = None,\n    data_stats: tuple | None = None,\n    tiling_mode: TilingMode = TilingMode.ShiftBoundary,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    dataloader_params: dict | None = None,\n    **dataset_kwargs,\n) -&gt; MicroSplitPredictDataModule:\n    \"\"\"\n    Create a MicroSplitPredictDataModule for microSplit-style prediction datasets.\n\n    Parameters\n    ----------\n    pred_data : str or Path or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    tile_size : tuple\n        Size of one tile of data.\n    data_type : DataType\n        Type of the dataset (must be a DataType enum value).\n    axes : str\n        Axes of the data (e.g., 'SYX').\n    batch_size : int, default=1\n        Batch size for prediction dataloader.\n    num_channels : int, default=2\n        Number of channels in the input.\n    depth3D : int, default=1\n        Number of slices in 3D.\n    grid_size : tuple, optional\n        Grid size for patch extraction.\n    multiscale_count : int, optional\n        Number of LC scales.\n    data_stats : tuple, optional\n        Data statistics, by default None.\n    tiling_mode : TilingMode, default=ShiftBoundary\n        Tiling mode for patch extraction.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, optional\n        File extension filter.\n    dataloader_params : dict, optional\n        Parameters for prediction dataloader.\n    **dataset_kwargs :\n        Additional arguments passed to MicroSplitDataConfig.\n\n    Returns\n    -------\n    MicroSplitPredictDataModule\n        Configured MicroSplitPredictDataModule instance.\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n\n    # Create prediction config with only valid parameters\n    prediction_config_params = {\n        \"data_type\": data_type,\n        \"image_size\": tile_size,\n        \"num_channels\": num_channels,\n        \"depth3D\": depth3D,\n        \"grid_size\": grid_size,\n        \"multiscale_lowres_count\": multiscale_count,\n        \"data_stats\": data_stats,\n        \"tiling_mode\": tiling_mode,\n        \"batch_size\": batch_size,\n        \"datasplit_type\": DataSplitType.Test,  # For prediction, use all data\n        **dataset_kwargs,\n    }\n\n    pred_config = MicroSplitDataConfig(**prediction_config_params)\n\n    # Remove batch_size from dataloader_params if present\n    if \"batch_size\" in dataloader_params:\n        del dataloader_params[\"batch_size\"]\n\n    return MicroSplitPredictDataModule(\n        pred_config=pred_config,\n        pred_data=pred_data,\n        read_source_func=(\n            read_source_func if read_source_func is not None else get_train_val_data\n        ),\n        extension_filter=extension_filter,\n        dataloader_params=dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.create_microsplit_train_datamodule","title":"<code>create_microsplit_train_datamodule(train_data, patch_size, data_type, axes, batch_size, val_data=None, num_channels=2, depth3D=1, grid_size=None, multiscale_count=None, tiling_mode=TilingMode.ShiftBoundary, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_split=5, use_in_memory=True, transforms=None, train_dataloader_params=None, val_dataloader_params=None, **dataset_kwargs)</code>","text":"<p>Create a MicroSplitDataModule for microSplit-style datasets.</p> <p>This includes config creation.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>str</code> <p>Path to training data.</p> required <code>patch_size</code> <code>tuple</code> <p>Size of one patch of data.</p> required <code>data_type</code> <code>DataType</code> <p>Type of the dataset (must be a DataType enum value).</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g., 'SYX').</p> required <code>batch_size</code> <code>int</code> <p>Batch size for dataloaders.</p> required <code>val_data</code> <code>str</code> <p>Path to validation data.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input.</p> <code>2</code> <code>depth3D</code> <code>int</code> <p>Number of slices in 3D.</p> <code>1</code> <code>grid_size</code> <code>tuple</code> <p>Grid size for patch extraction.</p> <code>None</code> <code>multiscale_count</code> <code>int</code> <p>Number of LC scales.</p> <code>None</code> <code>tiling_mode</code> <code>TilingMode</code> <p>Tiling mode for patch extraction.</p> <code>ShiftBoundary</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>File extension filter.</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of training data to use for validation.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches/files for validation split.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Use in-memory dataset if possible.</p> <code>True</code> <code>transforms</code> <code>list</code> <p>List of transforms to apply.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for training dataloader.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for validation dataloader.</p> <code>None</code> <code>**dataset_kwargs</code> <p>Additional arguments passed to DatasetConfig.</p> <code>{}</code> <p>Returns:</p> Type Description <code>MicroSplitDataModule</code> <p>Configured MicroSplitDataModule instance.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def create_microsplit_train_datamodule(\n    train_data: str,\n    patch_size: tuple,\n    data_type: DataType,\n    axes: str,  # TODO should be there after refactoring\n    batch_size: int,\n    val_data: str | None = None,\n    num_channels: int = 2,\n    depth3D: int = 1,\n    grid_size: tuple | None = None,\n    multiscale_count: int | None = None,\n    tiling_mode: TilingMode = TilingMode.ShiftBoundary,\n    read_source_func: Callable | None = None,  # TODO should be there after refactoring\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 5,\n    use_in_memory: bool = True,\n    transforms: list | None = None,  # TODO should it be here?\n    train_dataloader_params: dict | None = None,\n    val_dataloader_params: dict | None = None,\n    **dataset_kwargs,\n) -&gt; MicroSplitDataModule:\n    \"\"\"\n    Create a MicroSplitDataModule for microSplit-style datasets.\n\n    This includes config creation.\n\n    Parameters\n    ----------\n    train_data : str\n        Path to training data.\n    patch_size : tuple\n        Size of one patch of data.\n    data_type : DataType\n        Type of the dataset (must be a DataType enum value).\n    axes : str\n        Axes of the data (e.g., 'SYX').\n    batch_size : int\n        Batch size for dataloaders.\n    val_data : str, optional\n        Path to validation data.\n    num_channels : int, default=2\n        Number of channels in the input.\n    depth3D : int, default=1\n        Number of slices in 3D.\n    grid_size : tuple, optional\n        Grid size for patch extraction.\n    multiscale_count : int, optional\n        Number of LC scales.\n    tiling_mode : TilingMode, default=ShiftBoundary\n        Tiling mode for patch extraction.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, optional\n        File extension filter.\n    val_percentage : float, default=0.1\n        Percentage of training data to use for validation.\n    val_minimum_split : int, default=5\n        Minimum number of patches/files for validation split.\n    use_in_memory : bool, default=True\n        Use in-memory dataset if possible.\n    transforms : list, optional\n        List of transforms to apply.\n    train_dataloader_params : dict, optional\n        Parameters for training dataloader.\n    val_dataloader_params : dict, optional\n        Parameters for validation dataloader.\n    **dataset_kwargs :\n        Additional arguments passed to DatasetConfig.\n\n    Returns\n    -------\n    MicroSplitDataModule\n        Configured MicroSplitDataModule instance.\n    \"\"\"\n    # Create dataset configs with only valid parameters\n    dataset_config_params = {\n        \"data_type\": data_type,\n        \"image_size\": patch_size,\n        \"num_channels\": num_channels,\n        \"depth3D\": depth3D,\n        \"grid_size\": grid_size,\n        \"multiscale_lowres_count\": multiscale_count,\n        \"tiling_mode\": tiling_mode,\n        \"batch_size\": batch_size,\n        \"train_dataloader_params\": train_dataloader_params,\n        \"val_dataloader_params\": val_dataloader_params,\n        **dataset_kwargs,\n    }\n\n    train_config = MicroSplitDataConfig(\n        **dataset_config_params,\n        datasplit_type=DataSplitType.Train,\n    )\n    # val_config = MicroSplitDataConfig(\n    #     **dataset_config_params,\n    #     datasplit_type=DataSplitType.Val,\n    # )\n    # TODO, data config is duplicated here and in configuration\n\n    return MicroSplitDataModule(\n        data_config=train_config,\n        train_data=train_data,\n        val_data=val_data or train_data,\n        train_data_target=None,\n        val_data_target=None,\n        read_source_func=get_train_val_data,  # Use our wrapped function\n        extension_filter=extension_filter,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_split,\n        use_in_memory=use_in_memory,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.get_datasplit_tuples","title":"<code>get_datasplit_tuples(val_fraction, test_fraction, data_length)</code>","text":"<p>Get train/val/test indices for data splitting.</p> <p>Parameters:</p> Name Type Description Default <code>val_fraction</code> <code>float or None</code> <p>Fraction of data to use for validation.</p> required <code>test_fraction</code> <code>float or None</code> <p>Fraction of data to use for testing.</p> required <code>data_length</code> <code>int</code> <p>Total length of the dataset.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray]</code> <p>Training, validation, and test indices.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def get_datasplit_tuples(val_fraction, test_fraction, data_length):\n    \"\"\"Get train/val/test indices for data splitting.\n\n    Parameters\n    ----------\n    val_fraction : float or None\n        Fraction of data to use for validation.\n    test_fraction : float or None\n        Fraction of data to use for testing.\n    data_length : int\n        Total length of the dataset.\n\n    Returns\n    -------\n    tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]\n        Training, validation, and test indices.\n    \"\"\"\n    indices = np.arange(data_length)\n    np.random.shuffle(indices)\n\n    if val_fraction is None:\n        val_fraction = 0.0\n    if test_fraction is None:\n        test_fraction = 0.0\n\n    val_size = int(data_length * val_fraction)\n    test_size = int(data_length * test_fraction)\n    train_size = data_length - val_size - test_size\n\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size : train_size + val_size]\n    test_idx = indices[train_size + val_size :]\n\n    return train_idx, val_idx, test_idx\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.get_train_val_data","title":"<code>get_train_val_data(data_config, datadir, datasplit_type, val_fraction=None, test_fraction=None, allow_generation=None, **kwargs)</code>","text":"<p>Load and split data according to configuration.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>MicroSplitDataConfig</code> <p>Data configuration object.</p> required <code>datadir</code> <code>str or Path</code> <p>Path to the data directory.</p> required <code>datasplit_type</code> <code>DataSplitType</code> <p>Type of data split to return.</p> required <code>val_fraction</code> <code>float</code> <p>Fraction of data to use for validation.</p> <code>None</code> <code>test_fraction</code> <code>float</code> <p>Fraction of data to use for testing.</p> <code>None</code> <code>allow_generation</code> <code>bool</code> <p>Whether to allow data generation.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Split data array.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def get_train_val_data(\n    data_config,\n    datadir,\n    datasplit_type: DataSplitType,\n    val_fraction=None,\n    test_fraction=None,\n    allow_generation=None,\n    **kwargs,\n):\n    \"\"\"Load and split data according to configuration.\n\n    Parameters\n    ----------\n    data_config : MicroSplitDataConfig\n        Data configuration object.\n    datadir : str or Path\n        Path to the data directory.\n    datasplit_type : DataSplitType\n        Type of data split to return.\n    val_fraction : float, optional\n        Fraction of data to use for validation.\n    test_fraction : float, optional\n        Fraction of data to use for testing.\n    allow_generation : bool, optional\n        Whether to allow data generation.\n    **kwargs\n        Additional keyword arguments.\n\n    Returns\n    -------\n    numpy.ndarray\n        Split data array.\n    \"\"\"\n    data = load_data(datadir)\n    train_idx, val_idx, test_idx = get_datasplit_tuples(\n        val_fraction, test_fraction, len(data)\n    )\n\n    if datasplit_type == DataSplitType.All:\n        data = data.astype(np.float64)\n    elif datasplit_type == DataSplitType.Train:\n        data = data[train_idx].astype(np.float64)\n    elif datasplit_type == DataSplitType.Val:\n        data = data[val_idx].astype(np.float64)\n    elif datasplit_type == DataSplitType.Test:\n        # TODO this is only used for prediction, and only because old dataset uses it\n        data = data[test_idx].astype(np.float64)\n    else:\n        raise Exception(\"invalid datasplit\")\n\n    return data\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.load_data","title":"<code>load_data(datadir)</code>","text":"<p>Load data from a directory containing channel subdirectories with image files.</p> <p>Parameters:</p> Name Type Description Default <code>datadir</code> <code>str or Path</code> <p>Path to the data directory containing channel subdirectories.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Stacked array of all channels' data.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def load_data(datadir):\n    \"\"\"Load data from a directory containing channel subdirectories with image files.\n\n    Parameters\n    ----------\n    datadir : str or Path\n        Path to the data directory containing channel subdirectories.\n\n    Returns\n    -------\n    numpy.ndarray\n        Stacked array of all channels' data.\n    \"\"\"\n    data_path = Path(datadir)\n\n    channel_dirs = sorted(p for p in data_path.iterdir() if p.is_dir())\n    channels_data = []\n\n    for channel_dir in channel_dirs:\n        image_files = sorted(f for f in channel_dir.iterdir() if f.is_file())\n        channel_images = [load_one_file(image_path) for image_path in image_files]\n\n        channel_stack = np.concatenate(\n            channel_images, axis=0\n        )  # FIXME: this line works iff images have\n        # a singleton channel dimension. Specify in the notebook or change with\n        # `torch.stack`??\n        channels_data.append(channel_stack)\n\n    final_data = np.stack(channels_data, axis=-1)\n    return final_data\n</code></pre>"},{"location":"reference/careamics/lightning/microsplit_data_module/#careamics.lightning.microsplit_data_module.load_one_file","title":"<code>load_one_file(fpath)</code>","text":"<p>Load a single 2D image file.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str or Path</code> <p>Path to the image file.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Reshaped image data.</p> Source code in <code>src/careamics/lightning/microsplit_data_module.py</code> <pre><code>def load_one_file(fpath):\n    \"\"\"Load a single 2D image file.\n\n    Parameters\n    ----------\n    fpath : str or Path\n        Path to the image file.\n\n    Returns\n    -------\n    numpy.ndarray\n        Reshaped image data.\n    \"\"\"\n    data = tifffile.imread(fpath)\n    if len(data.shape) == 2:\n        axes = \"YX\"\n    elif len(data.shape) == 3:\n        axes = \"SYX\"\n    elif len(data.shape) == 4:\n        axes = \"STYX\"\n    else:\n        raise ValueError(f\"Invalid data shape: {data.shape}\")\n    data = reshape_array(data, axes)\n    data = data.reshape(-1, data.shape[-2], data.shape[-1])\n    return data\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/","title":"predict_data_module","text":"<p>Prediction Lightning data modules.</p>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule","title":"<code>PredictDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>CAREamics Lightning prediction data module.</p> <p>The data module can be used with Path, str or numpy arrays. The data can be either a folder containing images or a single file.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>InferenceModel</code> <p>Pydantic model for CAREamics prediction configuration.</p> required <code>pred_data</code> <code>Path or str or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters, by default {}.</p> <code>None</code> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>class PredictDataModule(L.LightningDataModule):\n    \"\"\"\n    CAREamics Lightning prediction data module.\n\n    The data module can be used with Path, str or numpy arrays. The data can be either\n    a folder containing images or a single file.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    pred_config : InferenceModel\n        Pydantic model for CAREamics prediction configuration.\n    pred_data : pathlib.Path or str or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types, by default None.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types, by default \"\".\n    dataloader_params : dict, optional\n        Dataloader parameters, by default {}.\n    \"\"\"\n\n    def __init__(\n        self,\n        pred_config: InferenceConfig,\n        pred_data: Union[Path, str, NDArray],\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        dataloader_params: dict | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        The data module can be used with Path, str or numpy arrays. The data can be\n        either a folder containing images or a single file.\n\n        To read custom data types, you can set `data_type` to `custom` in `data_config`\n        and provide a function that returns a numpy array from a path as\n        `read_source_func` parameter. The function will receive a Path object and\n        an axies string as arguments, the axes being derived from the `data_config`.\n\n        You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n        \"*.czi\") to filter the files extension using `extension_filter`.\n\n        Parameters\n        ----------\n        pred_config : InferenceModel\n            Pydantic model for CAREamics prediction configuration.\n        pred_data : pathlib.Path or str or numpy.ndarray\n            Prediction data, can be a path to a folder, a file or a numpy array.\n        read_source_func : Callable, optional\n            Function to read custom types, by default None.\n        extension_filter : str, optional\n            Filter to filter file extensions for custom types, by default \"\".\n        dataloader_params : dict, optional\n            Dataloader parameters, by default {}.\n\n        Raises\n        ------\n        ValueError\n            If the data type is `custom` and no `read_source_func` is provided.\n        ValueError\n            If the data type is `array` and the input is not a numpy array.\n        ValueError\n            If the data type is `tiff` and the input is neither a Path nor a str.\n        \"\"\"\n        if dataloader_params is None:\n            dataloader_params = {}\n        if dataloader_params is None:\n            dataloader_params = {}\n        super().__init__()\n\n        # check that a read source function is provided for custom types\n        if pred_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n            raise ValueError(\n                f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n                f\"specifying a `read_source_func` and an `extension_filer`.\"\n            )\n\n        # check correct input type\n        if (\n            isinstance(pred_data, np.ndarray)\n            and pred_config.data_type != SupportedData.ARRAY\n        ):\n            raise ValueError(\n                f\"Received a numpy array as input, but the data type was set to \"\n                f\"{pred_config.data_type}. Set the data type \"\n                f\"to {SupportedData.ARRAY} to predict on numpy arrays.\"\n            )\n\n        # and that Path or str are passed, if tiff file type specified\n        elif (isinstance(pred_data, Path) or isinstance(pred_config, str)) and (\n            pred_config.data_type != SupportedData.TIFF\n            and pred_config.data_type != SupportedData.CUSTOM\n        ):\n            raise ValueError(\n                f\"Received a path as input, but the data type was neither set to \"\n                f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n                f\" to {SupportedData.TIFF} or \"\n                f\"{SupportedData.CUSTOM} to predict on files.\"\n            )\n\n        # configuration data\n        self.prediction_config = pred_config\n        self.data_type = pred_config.data_type\n        self.batch_size = pred_config.batch_size\n        self.dataloader_params = dataloader_params\n\n        self.pred_data = pred_data\n        self.tile_size = pred_config.tile_size\n        self.tile_overlap = pred_config.tile_overlap\n\n        # check if it is tiled\n        self.tiled = self.tile_size is not None and self.tile_overlap is not None\n\n        # read source function\n        if pred_config.data_type == SupportedData.CUSTOM:\n            # mypy check\n            assert read_source_func is not None\n\n            self.read_source_func: Callable = read_source_func\n        elif pred_config.data_type != SupportedData.ARRAY:\n            self.read_source_func = get_read_func(pred_config.data_type)\n\n        self.extension_filter = extension_filter\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n        # if the data is a Path or a str\n        if not isinstance(self.pred_data, np.ndarray):\n            self.pred_files = list_files(\n                self.pred_data, self.data_type, self.extension_filter\n            )\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Hook called at the beginning of predict.\n\n        Parameters\n        ----------\n        stage : Optional[str], optional\n            Stage, by default None.\n        \"\"\"\n        # if numpy array\n        if self.data_type == SupportedData.ARRAY:\n            if self.tiled:\n                self.predict_dataset: PredictDatasetType = InMemoryTiledPredDataset(\n                    prediction_config=self.prediction_config,\n                    inputs=self.pred_data,\n                )\n            else:\n                self.predict_dataset = InMemoryPredDataset(\n                    prediction_config=self.prediction_config,\n                    inputs=self.pred_data,\n                )\n        else:\n            if self.tiled:\n                self.predict_dataset = IterableTiledPredDataset(\n                    prediction_config=self.prediction_config,\n                    src_files=self.pred_files,\n                    read_source_func=self.read_source_func,\n                )\n            else:\n                self.predict_dataset = IterablePredDataset(\n                    prediction_config=self.prediction_config,\n                    src_files=self.pred_files,\n                    read_source_func=self.read_source_func,\n                )\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for prediction.\n\n        Returns\n        -------\n        DataLoader\n            Prediction dataloader.\n        \"\"\"\n        # For tiled predictions, we need to ensure tiles are processed in order\n        # to avoid stitching artifacts. Multi-worker processing can return batches\n        # out of order, so we disable it for tiled predictions.\n        dataloader_params = self.dataloader_params.copy()\n        if self.tiled:\n            dataloader_params[\"num_workers\"] = 0\n\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.batch_size,\n            collate_fn=collate_tiles if self.tiled else None,\n            **dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.__init__","title":"<code>__init__(pred_config, pred_data, read_source_func=None, extension_filter='', dataloader_params=None)</code>","text":"<p>Constructor.</p> <p>The data module can be used with Path, str or numpy arrays. The data can be either a folder containing images or a single file.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>InferenceModel</code> <p>Pydantic model for CAREamics prediction configuration.</p> required <code>pred_data</code> <code>Path or str or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters, by default {}.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data type is <code>custom</code> and no <code>read_source_func</code> is provided.</p> <code>ValueError</code> <p>If the data type is <code>array</code> and the input is not a numpy array.</p> <code>ValueError</code> <p>If the data type is <code>tiff</code> and the input is neither a Path nor a str.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def __init__(\n    self,\n    pred_config: InferenceConfig,\n    pred_data: Union[Path, str, NDArray],\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    dataloader_params: dict | None = None,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    The data module can be used with Path, str or numpy arrays. The data can be\n    either a folder containing images or a single file.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    pred_config : InferenceModel\n        Pydantic model for CAREamics prediction configuration.\n    pred_data : pathlib.Path or str or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types, by default None.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types, by default \"\".\n    dataloader_params : dict, optional\n        Dataloader parameters, by default {}.\n\n    Raises\n    ------\n    ValueError\n        If the data type is `custom` and no `read_source_func` is provided.\n    ValueError\n        If the data type is `array` and the input is not a numpy array.\n    ValueError\n        If the data type is `tiff` and the input is neither a Path nor a str.\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n    if dataloader_params is None:\n        dataloader_params = {}\n    super().__init__()\n\n    # check that a read source function is provided for custom types\n    if pred_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n        raise ValueError(\n            f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n            f\"specifying a `read_source_func` and an `extension_filer`.\"\n        )\n\n    # check correct input type\n    if (\n        isinstance(pred_data, np.ndarray)\n        and pred_config.data_type != SupportedData.ARRAY\n    ):\n        raise ValueError(\n            f\"Received a numpy array as input, but the data type was set to \"\n            f\"{pred_config.data_type}. Set the data type \"\n            f\"to {SupportedData.ARRAY} to predict on numpy arrays.\"\n        )\n\n    # and that Path or str are passed, if tiff file type specified\n    elif (isinstance(pred_data, Path) or isinstance(pred_config, str)) and (\n        pred_config.data_type != SupportedData.TIFF\n        and pred_config.data_type != SupportedData.CUSTOM\n    ):\n        raise ValueError(\n            f\"Received a path as input, but the data type was neither set to \"\n            f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n            f\" to {SupportedData.TIFF} or \"\n            f\"{SupportedData.CUSTOM} to predict on files.\"\n        )\n\n    # configuration data\n    self.prediction_config = pred_config\n    self.data_type = pred_config.data_type\n    self.batch_size = pred_config.batch_size\n    self.dataloader_params = dataloader_params\n\n    self.pred_data = pred_data\n    self.tile_size = pred_config.tile_size\n    self.tile_overlap = pred_config.tile_overlap\n\n    # check if it is tiled\n    self.tiled = self.tile_size is not None and self.tile_overlap is not None\n\n    # read source function\n    if pred_config.data_type == SupportedData.CUSTOM:\n        # mypy check\n        assert read_source_func is not None\n\n        self.read_source_func: Callable = read_source_func\n    elif pred_config.data_type != SupportedData.ARRAY:\n        self.read_source_func = get_read_func(pred_config.data_type)\n\n    self.extension_filter = extension_filter\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Prediction dataloader.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for prediction.\n\n    Returns\n    -------\n    DataLoader\n        Prediction dataloader.\n    \"\"\"\n    # For tiled predictions, we need to ensure tiles are processed in order\n    # to avoid stitching artifacts. Multi-worker processing can return batches\n    # out of order, so we disable it for tiled predictions.\n    dataloader_params = self.dataloader_params.copy()\n    if self.tiled:\n        dataloader_params[\"num_workers\"] = 0\n\n    return DataLoader(\n        self.predict_dataset,\n        batch_size=self.batch_size,\n        collate_fn=collate_tiles if self.tiled else None,\n        **dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Hook used to prepare the data before calling <code>setup</code>.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n    # if the data is a Path or a str\n    if not isinstance(self.pred_data, np.ndarray):\n        self.pred_files = list_files(\n            self.pred_data, self.data_type, self.extension_filter\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Hook called at the beginning of predict.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>Stage, by default None.</p> <code>None</code> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Hook called at the beginning of predict.\n\n    Parameters\n    ----------\n    stage : Optional[str], optional\n        Stage, by default None.\n    \"\"\"\n    # if numpy array\n    if self.data_type == SupportedData.ARRAY:\n        if self.tiled:\n            self.predict_dataset: PredictDatasetType = InMemoryTiledPredDataset(\n                prediction_config=self.prediction_config,\n                inputs=self.pred_data,\n            )\n        else:\n            self.predict_dataset = InMemoryPredDataset(\n                prediction_config=self.prediction_config,\n                inputs=self.pred_data,\n            )\n    else:\n        if self.tiled:\n            self.predict_dataset = IterableTiledPredDataset(\n                prediction_config=self.prediction_config,\n                src_files=self.pred_files,\n                read_source_func=self.read_source_func,\n            )\n        else:\n            self.predict_dataset = IterablePredDataset(\n                prediction_config=self.prediction_config,\n                src_files=self.pred_files,\n                read_source_func=self.read_source_func,\n            )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.create_predict_datamodule","title":"<code>create_predict_datamodule(pred_data, data_type, axes, image_means, image_stds, tile_size=None, tile_overlap=None, batch_size=1, tta_transforms=True, read_source_func=None, extension_filter='', dataloader_params=None)</code>","text":"<p>Create a CAREamics prediction Lightning datamodule.</p> <p>This function is used to explicitly pass the parameters usually contained in an <code>inference_model</code> configuration.</p> <p>Since the lightning datamodule has no access to the model, make sure that the parameters passed to the datamodule are consistent with the model's requirements and are coherent. This can be done by creating a <code>Configuration</code> object beforehand and passing its parameters to the different Lightning modules.</p> <p>The data module can be used with Path, str or numpy arrays. To use array data, set <code>data_type</code> to <code>array</code> and pass a numpy array to <code>train_data</code>.</p> <p>By default, CAREamics only supports types defined in <code>careamics.config.support.SupportedData</code>. To read custom data types, you can set <code>data_type</code> to <code>custom</code> and provide a function that returns a numpy array from a path. Additionally, pass a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.jpeg\") to filter the files extension using <code>extension_filter</code>.</p> <p>In <code>dataloader_params</code>, you can pass any parameter accepted by PyTorch dataloaders, except for <code>batch_size</code>, which is set by the <code>batch_size</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>str or Path or ndarray</code> <p>Prediction data.</p> required <code>data_type</code> <code>(array, tiff, custom)</code> <p>Data type, see <code>SupportedData</code> for available options.</p> <code>\"array\"</code> <code>axes</code> <code>str</code> <p>Axes of the data, chosen among SCZYX.</p> required <code>image_means</code> <code>list of float</code> <p>Mean values for normalization, only used if Normalization is defined.</p> required <code>image_stds</code> <code>list of float</code> <p>Std values for normalization, only used if Normalization is defined.</p> required <code>tile_size</code> <code>tuple of int</code> <p>Tile size, 2D or 3D tile size.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Tile overlap, 2D or 3D tile overlap.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>1</code> <code>tta_transforms</code> <code>bool</code> <p>Use test time augmentation, by default True.</p> <code>True</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Pytorch dataloader parameters, by default {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>PredictDataModule</code> <p>CAREamics prediction datamodule.</p> Notes <p>If you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def create_predict_datamodule(\n    pred_data: Union[str, Path, NDArray],\n    data_type: Union[Literal[\"array\", \"tiff\", \"custom\"], SupportedData],\n    axes: str,\n    image_means: list[float],\n    image_stds: list[float],\n    tile_size: tuple[int, ...] | None = None,\n    tile_overlap: tuple[int, ...] | None = None,\n    batch_size: int = 1,\n    tta_transforms: bool = True,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    dataloader_params: dict | None = None,\n) -&gt; PredictDataModule:\n    \"\"\"Create a CAREamics prediction Lightning datamodule.\n\n    This function is used to explicitly pass the parameters usually contained in an\n    `inference_model` configuration.\n\n    Since the lightning datamodule has no access to the model, make sure that the\n    parameters passed to the datamodule are consistent with the model's requirements\n    and are coherent. This can be done by creating a `Configuration` object beforehand\n    and passing its parameters to the different Lightning modules.\n\n    The data module can be used with Path, str or numpy arrays. To use array data, set\n    `data_type` to `array` and pass a numpy array to `train_data`.\n\n    By default, CAREamics only supports types defined in\n    `careamics.config.support.SupportedData`. To read custom data types, you can set\n    `data_type` to `custom` and provide a function that returns a numpy array from a\n    path. Additionally, pass a `fnmatch` and `Path.rglob` compatible expression\n    (e.g. \"*.jpeg\") to filter the files extension using `extension_filter`.\n\n    In `dataloader_params`, you can pass any parameter accepted by PyTorch\n    dataloaders, except for `batch_size`, which is set by the `batch_size`\n    parameter.\n\n    Parameters\n    ----------\n    pred_data : str or pathlib.Path or numpy.ndarray\n        Prediction data.\n    data_type : {\"array\", \"tiff\", \"custom\"}\n        Data type, see `SupportedData` for available options.\n    axes : str\n        Axes of the data, chosen among SCZYX.\n    image_means : list of float\n        Mean values for normalization, only used if Normalization is defined.\n    image_stds : list of float\n        Std values for normalization, only used if Normalization is defined.\n    tile_size : tuple of int, optional\n        Tile size, 2D or 3D tile size.\n    tile_overlap : tuple of int, optional\n        Tile overlap, 2D or 3D tile overlap.\n    batch_size : int\n        Batch size.\n    tta_transforms : bool, optional\n        Use test time augmentation, by default True.\n    read_source_func : Callable, optional\n        Function to read the source data, used if `data_type` is `custom`, by\n        default None.\n    extension_filter : str, optional\n        Filter for file extensions, used if `data_type` is `custom`, by default \"\".\n    dataloader_params : dict, optional\n        Pytorch dataloader parameters, by default {}.\n\n    Returns\n    -------\n    PredictDataModule\n        CAREamics prediction datamodule.\n\n    Notes\n    -----\n    If you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n\n    prediction_dict: dict[str, Any] = {\n        \"data_type\": data_type,\n        \"tile_size\": tile_size,\n        \"tile_overlap\": tile_overlap,\n        \"axes\": axes,\n        \"image_means\": image_means,\n        \"image_stds\": image_stds,\n        \"tta_transforms\": tta_transforms,\n        \"batch_size\": batch_size,\n    }\n\n    # validate configuration\n    prediction_config = InferenceConfig(**prediction_dict)\n\n    # sanity check on the dataloader parameters\n    if \"batch_size\" in dataloader_params:\n        # remove it\n        del dataloader_params[\"batch_size\"]\n\n    return PredictDataModule(\n        pred_config=prediction_config,\n        pred_data=pred_data,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        dataloader_params=dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/","title":"train_data_module","text":"<p>Training and validation Lightning data modules.</p>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule","title":"<code>TrainDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>CAREamics Ligthning training and validation data module.</p> <p>The data module can be used with Path, str or numpy arrays. In the case of numpy arrays, it loads and computes all the patches in memory. For Path and str inputs, it calculates the total file size and estimate whether it can fit in memory. If it does not, it iterates through the files. This behaviour can be deactivated by setting <code>use_in_memory</code> to False, in which case it will always use the iterating dataset to train on a Path or str.</p> <p>The data can be either a folder containing images or a single file.</p> <p>Validation can be omitted, in which case the validation data is extracted from the training data. The percentage of the training data to use for validation, as well as the minimum number of patches or files to split from the training data can be set using <code>val_percentage</code> and <code>val_minimum_split</code>, respectively.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataModel</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data, can be a path to a folder, a file or a numpy array.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>train_data_target</code> <code>Path or str or ndarray</code> <p>Training target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>val_data_target</code> <code>Path or str or ndarray</code> <p>Validation target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, by default None. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\". Only used for <code>custom</code> data types (see DataModel).</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, by default 0.1. Only used if <code>val_data</code> is None.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, by default 5. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data_config</code> <code>DataModel</code> <p>CAREamics data configuration.</p> <code>data_type</code> <code>SupportedData</code> <p>Expected data type, one of \"tiff\", \"array\" or \"custom\".</p> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>use_in_memory</code> <code>bool</code> <p>Whether to use in memory dataset if possible.</p> <code>train_data</code> <code>Path or ndarray</code> <p>Training data.</p> <code>val_data</code> <code>Path or ndarray</code> <p>Validation data.</p> <code>train_data_target</code> <code>Path or ndarray</code> <p>Training target data.</p> <code>val_data_target</code> <code>Path or ndarray</code> <p>Validation target data.</p> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, if no validation data is provided.</p> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, if no validation data is provided.</p> <code>read_source_func</code> <code>Optional[Callable]</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>.</p> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>class TrainDataModule(L.LightningDataModule):\n    \"\"\"\n    CAREamics Ligthning training and validation data module.\n\n    The data module can be used with Path, str or numpy arrays. In the case of\n    numpy arrays, it loads and computes all the patches in memory. For Path and str\n    inputs, it calculates the total file size and estimate whether it can fit in\n    memory. If it does not, it iterates through the files. This behaviour can be\n    deactivated by setting `use_in_memory` to False, in which case it will\n    always use the iterating dataset to train on a Path or str.\n\n    The data can be either a folder containing images or a single file.\n\n    Validation can be omitted, in which case the validation data is extracted from\n    the training data. The percentage of the training data to use for validation,\n    as well as the minimum number of patches or files to split from the training\n    data can be set using `val_percentage` and `val_minimum_split`, respectively.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    data_config : DataModel\n        Pydantic model for CAREamics data configuration.\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data, can be a path to a folder, a file or a numpy array.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    train_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    val_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, can be a path to a folder, a file or a numpy array,\n        by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, by default None. Only used for `custom`\n        data type (see DataModel).\n    extension_filter : str, optional\n        Filter for file extensions, by default \"\". Only used for `custom` data types\n        (see DataModel).\n    val_percentage : float, optional\n        Percentage of the training data to use for validation, by default 0.1. Only\n        used if `val_data` is None.\n    val_minimum_split : int, optional\n        Minimum number of patches or files to split from the training data for\n        validation, by default 5. Only used if `val_data` is None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Attributes\n    ----------\n    data_config : DataModel\n        CAREamics data configuration.\n    data_type : SupportedData\n        Expected data type, one of \"tiff\", \"array\" or \"custom\".\n    batch_size : int\n        Batch size.\n    use_in_memory : bool\n        Whether to use in memory dataset if possible.\n    train_data : pathlib.Path or numpy.ndarray\n        Training data.\n    val_data : pathlib.Path or numpy.ndarray\n        Validation data.\n    train_data_target : pathlib.Path or numpy.ndarray\n        Training target data.\n    val_data_target : pathlib.Path or numpy.ndarray\n        Validation target data.\n    val_percentage : float\n        Percentage of the training data to use for validation, if no validation data is\n        provided.\n    val_minimum_split : int\n        Minimum number of patches or files to split from the training data for\n        validation, if no validation data is provided.\n    read_source_func : Optional[Callable]\n        Function to read the source data, used if `data_type` is `custom`.\n    extension_filter : str\n        Filter for file extensions, used if `data_type` is `custom`.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        train_data: Union[Path, str, NDArray],\n        val_data: Union[Path, str, NDArray] | None = None,\n        train_data_target: Union[Path, str, NDArray] | None = None,\n        val_data_target: Union[Path, str, NDArray] | None = None,\n        read_source_func: Callable | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        data_config : DataModel\n            Pydantic model for CAREamics data configuration.\n        train_data : pathlib.Path or str or numpy.ndarray\n            Training data, can be a path to a folder, a file or a numpy array.\n        val_data : pathlib.Path or str or numpy.ndarray, optional\n            Validation data, can be a path to a folder, a file or a numpy array, by\n            default None.\n        train_data_target : pathlib.Path or str or numpy.ndarray, optional\n            Training target data, can be a path to a folder, a file or a numpy array, by\n            default None.\n        val_data_target : pathlib.Path or str or numpy.ndarray, optional\n            Validation target data, can be a path to a folder, a file or a numpy array,\n            by default None.\n        read_source_func : Callable, optional\n            Function to read the source data, by default None. Only used for `custom`\n            data type (see DataModel).\n        extension_filter : str, optional\n            Filter for file extensions, by default \"\". Only used for `custom` data types\n            (see DataModel).\n        val_percentage : float, optional\n            Percentage of the training data to use for validation, by default 0.1. Only\n            used if `val_data` is None.\n        val_minimum_split : int, optional\n            Minimum number of patches or files to split from the training data for\n            validation, by default 5. Only used if `val_data` is None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n\n        Raises\n        ------\n        NotImplementedError\n            Raised if target data is provided.\n        ValueError\n            If the input types are mixed (e.g. Path and numpy.ndarray).\n        ValueError\n            If the data type is `custom` and no `read_source_func` is provided.\n        ValueError\n            If the data type is `array` and the input is not a numpy array.\n        ValueError\n            If the data type is `tiff` and the input is neither a Path nor a str.\n        \"\"\"\n        super().__init__()\n\n        # check input types coherence (no mixed types)\n        inputs = [train_data, val_data, train_data_target, val_data_target]\n        types_set = {type(i) for i in inputs}\n        if len(types_set) &gt; 2:  # None + expected type\n            raise ValueError(\n                f\"Inputs for `train_data`, `val_data`, `train_data_target` and \"\n                f\"`val_data_target` must be of the same type or None. Got \"\n                f\"{types_set}.\"\n            )\n\n        # check that a read source function is provided for custom types\n        if data_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n            raise ValueError(\n                f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n                f\"specifying a `read_source_func` and an `extension_filer`.\"\n            )\n\n        # check correct input type\n        if (\n            isinstance(train_data, np.ndarray)\n            and data_config.data_type != SupportedData.ARRAY\n        ):\n            raise ValueError(\n                f\"Received a numpy array as input, but the data type was set to \"\n                f\"{data_config.data_type}. Set the data type in the configuration \"\n                f\"to {SupportedData.ARRAY} to train on numpy arrays.\"\n            )\n\n        # and that Path or str are passed, if tiff file type specified\n        elif (isinstance(train_data, Path) or isinstance(train_data, str)) and (\n            data_config.data_type != SupportedData.TIFF\n            and data_config.data_type != SupportedData.CUSTOM\n        ):\n            raise ValueError(\n                f\"Received a path as input, but the data type was neither set to \"\n                f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n                f\"in the configuration to {SupportedData.TIFF} or \"\n                f\"{SupportedData.CUSTOM} to train on files.\"\n            )\n\n        # configuration\n        self.data_config: DataConfig = data_config\n        self.data_type: str = data_config.data_type\n        self.batch_size: int = data_config.batch_size\n        self.use_in_memory: bool = use_in_memory\n\n        # data: make data Path or np.ndarray, use type annotations for mypy\n        self.train_data: Union[Path, NDArray] = (\n            Path(train_data) if isinstance(train_data, str) else train_data\n        )\n\n        self.val_data: Union[Path, NDArray] = (\n            Path(val_data) if isinstance(val_data, str) else val_data\n        )\n\n        self.train_data_target: Union[Path, NDArray] = (\n            Path(train_data_target)\n            if isinstance(train_data_target, str)\n            else train_data_target\n        )\n\n        self.val_data_target: Union[Path, NDArray] = (\n            Path(val_data_target)\n            if isinstance(val_data_target, str)\n            else val_data_target\n        )\n\n        # validation split\n        self.val_percentage = val_percentage\n        self.val_minimum_split = val_minimum_split\n\n        # read source function corresponding to the requested type\n        if data_config.data_type == SupportedData.CUSTOM.value:\n            # mypy check\n            assert read_source_func is not None\n\n            self.read_source_func: Callable = read_source_func\n\n        elif data_config.data_type != SupportedData.ARRAY:\n            self.read_source_func = get_read_func(data_config.data_type)\n\n        self.extension_filter: str = extension_filter\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Hook used to prepare the data before calling `setup`.\n\n        Here, we only need to examine the data if it was provided as a str or a Path.\n\n        TODO: from lightning doc:\n        prepare_data is called from the main process. It is not recommended to assign\n        state here (e.g. self.x = y) since it is called on a single process and if you\n        assign states here then they won't be available for other processes.\n\n        https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n        \"\"\"\n        # if the data is a Path or a str\n        if (\n            not isinstance(self.train_data, np.ndarray)\n            and not isinstance(self.val_data, np.ndarray)\n            and not isinstance(self.train_data_target, np.ndarray)\n            and not isinstance(self.val_data_target, np.ndarray)\n        ):\n            # list training files\n            self.train_files = list_files(\n                self.train_data, self.data_type, self.extension_filter\n            )\n            self.train_files_size = get_files_size(self.train_files)\n\n            # list validation files\n            if self.val_data is not None:\n                self.val_files = list_files(\n                    self.val_data, self.data_type, self.extension_filter\n                )\n\n            # same for target data\n            if self.train_data_target is not None:\n                self.train_target_files: list[Path] = list_files(\n                    self.train_data_target, self.data_type, self.extension_filter\n                )\n\n                # verify that they match the training data\n                validate_source_target_files(self.train_files, self.train_target_files)\n\n            if self.val_data_target is not None:\n                self.val_target_files = list_files(\n                    self.val_data_target, self.data_type, self.extension_filter\n                )\n\n                # verify that they match the validation data\n                validate_source_target_files(self.val_files, self.val_target_files)\n\n    def setup(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Hook called at the beginning of fit, validate, or predict.\n\n        Parameters\n        ----------\n        *args : Any\n            Unused.\n        **kwargs : Any\n            Unused.\n        \"\"\"\n        # if numpy array\n        if self.data_type == SupportedData.ARRAY:\n            # mypy checks\n            assert isinstance(self.train_data, np.ndarray)\n            if self.train_data_target is not None:\n                assert isinstance(self.train_data_target, np.ndarray)\n\n            # train dataset\n            self.train_dataset: DatasetType = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.train_data,\n                input_target=self.train_data_target,\n            )\n\n            # validation dataset\n            if self.val_data is not None:\n                # mypy checks\n                assert isinstance(self.val_data, np.ndarray)\n                if self.val_data_target is not None:\n                    assert isinstance(self.val_data_target, np.ndarray)\n\n                # create its own dataset\n                self.val_dataset: DatasetType = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.val_data,\n                    input_target=self.val_data_target,\n                )\n            else:\n                # extract validation from the training patches\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_patches=self.val_minimum_split,\n                )\n\n        # else we read files\n        else:\n            # Heuristics, if the file size is smaller than 80% of the RAM,\n            # we run the training in memory, otherwise we switch to iterable dataset\n            # The switch is deactivated if use_in_memory is False\n            if self.use_in_memory and self.train_files_size &lt; get_ram_size() * 0.8:\n                # train dataset\n                self.train_dataset = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.train_files,\n                    input_target=(\n                        self.train_target_files if self.train_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n\n                # validation dataset\n                if self.val_data is not None:\n                    self.val_dataset = InMemoryDataset(\n                        data_config=self.data_config,\n                        inputs=self.val_files,\n                        input_target=(\n                            self.val_target_files if self.val_data_target else None\n                        ),\n                        read_source_func=self.read_source_func,\n                    )\n                else:\n                    # split dataset\n                    self.val_dataset = self.train_dataset.split_dataset(\n                        percentage=self.val_percentage,\n                        minimum_patches=self.val_minimum_split,\n                    )\n\n            # else if the data is too large, load file by file during training\n            else:\n                # create training dataset\n                self.train_dataset = PathIterableDataset(\n                    data_config=self.data_config,\n                    src_files=self.train_files,\n                    target_files=(\n                        self.train_target_files if self.train_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n\n                # create validation dataset\n                if self.val_data is not None:\n                    # create its own dataset\n                    self.val_dataset = PathIterableDataset(\n                        data_config=self.data_config,\n                        src_files=self.val_files,\n                        target_files=(\n                            self.val_target_files if self.val_data_target else None\n                        ),\n                        read_source_func=self.read_source_func,\n                    )\n                elif len(self.train_files) &lt;= self.val_minimum_split:\n                    raise ValueError(\n                        f\"Not enough files to split a minimum of \"\n                        f\"{self.val_minimum_split} files, got {len(self.train_files)} \"\n                        f\"files.\"\n                    )\n                else:\n                    # extract validation from the training patches\n                    self.val_dataset = self.train_dataset.split_dataset(\n                        percentage=self.val_percentage,\n                        minimum_number=self.val_minimum_split,\n                    )\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        Returns\n        -------\n        tuple of list\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.train_dataset.get_data_statistics()\n\n    def train_dataloader(self) -&gt; Any:\n        \"\"\"\n        Create a dataloader for training.\n\n        Returns\n        -------\n        Any\n            Training dataloader.\n        \"\"\"\n        train_dataloader_params = self.data_config.train_dataloader_params.copy()\n\n        # NOTE: When next-gen datasets are completed this can be removed\n        # iterable dataset cannot be shuffled\n        if isinstance(self.train_dataset, IterableDataset):\n            del train_dataloader_params[\"shuffle\"]\n\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            **train_dataloader_params,\n        )\n\n    def val_dataloader(self) -&gt; Any:\n        \"\"\"\n        Create a dataloader for validation.\n\n        Returns\n        -------\n        Any\n            Validation dataloader.\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            **self.data_config.val_dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.__init__","title":"<code>__init__(data_config, train_data, val_data=None, train_data_target=None, val_data_target=None, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_split=5, use_in_memory=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataModel</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data, can be a path to a folder, a file or a numpy array.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>train_data_target</code> <code>Path or str or ndarray</code> <p>Training target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>val_data_target</code> <code>Path or str or ndarray</code> <p>Validation target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, by default None. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\". Only used for <code>custom</code> data types (see DataModel).</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, by default 0.1. Only used if <code>val_data</code> is None.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, by default 5. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Raised if target data is provided.</p> <code>ValueError</code> <p>If the input types are mixed (e.g. Path and numpy.ndarray).</p> <code>ValueError</code> <p>If the data type is <code>custom</code> and no <code>read_source_func</code> is provided.</p> <code>ValueError</code> <p>If the data type is <code>array</code> and the input is not a numpy array.</p> <code>ValueError</code> <p>If the data type is <code>tiff</code> and the input is neither a Path nor a str.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    train_data: Union[Path, str, NDArray],\n    val_data: Union[Path, str, NDArray] | None = None,\n    train_data_target: Union[Path, str, NDArray] | None = None,\n    val_data_target: Union[Path, str, NDArray] | None = None,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 5,\n    use_in_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    data_config : DataModel\n        Pydantic model for CAREamics data configuration.\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data, can be a path to a folder, a file or a numpy array.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    train_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    val_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, can be a path to a folder, a file or a numpy array,\n        by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, by default None. Only used for `custom`\n        data type (see DataModel).\n    extension_filter : str, optional\n        Filter for file extensions, by default \"\". Only used for `custom` data types\n        (see DataModel).\n    val_percentage : float, optional\n        Percentage of the training data to use for validation, by default 0.1. Only\n        used if `val_data` is None.\n    val_minimum_split : int, optional\n        Minimum number of patches or files to split from the training data for\n        validation, by default 5. Only used if `val_data` is None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Raises\n    ------\n    NotImplementedError\n        Raised if target data is provided.\n    ValueError\n        If the input types are mixed (e.g. Path and numpy.ndarray).\n    ValueError\n        If the data type is `custom` and no `read_source_func` is provided.\n    ValueError\n        If the data type is `array` and the input is not a numpy array.\n    ValueError\n        If the data type is `tiff` and the input is neither a Path nor a str.\n    \"\"\"\n    super().__init__()\n\n    # check input types coherence (no mixed types)\n    inputs = [train_data, val_data, train_data_target, val_data_target]\n    types_set = {type(i) for i in inputs}\n    if len(types_set) &gt; 2:  # None + expected type\n        raise ValueError(\n            f\"Inputs for `train_data`, `val_data`, `train_data_target` and \"\n            f\"`val_data_target` must be of the same type or None. Got \"\n            f\"{types_set}.\"\n        )\n\n    # check that a read source function is provided for custom types\n    if data_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n        raise ValueError(\n            f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n            f\"specifying a `read_source_func` and an `extension_filer`.\"\n        )\n\n    # check correct input type\n    if (\n        isinstance(train_data, np.ndarray)\n        and data_config.data_type != SupportedData.ARRAY\n    ):\n        raise ValueError(\n            f\"Received a numpy array as input, but the data type was set to \"\n            f\"{data_config.data_type}. Set the data type in the configuration \"\n            f\"to {SupportedData.ARRAY} to train on numpy arrays.\"\n        )\n\n    # and that Path or str are passed, if tiff file type specified\n    elif (isinstance(train_data, Path) or isinstance(train_data, str)) and (\n        data_config.data_type != SupportedData.TIFF\n        and data_config.data_type != SupportedData.CUSTOM\n    ):\n        raise ValueError(\n            f\"Received a path as input, but the data type was neither set to \"\n            f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n            f\"in the configuration to {SupportedData.TIFF} or \"\n            f\"{SupportedData.CUSTOM} to train on files.\"\n        )\n\n    # configuration\n    self.data_config: DataConfig = data_config\n    self.data_type: str = data_config.data_type\n    self.batch_size: int = data_config.batch_size\n    self.use_in_memory: bool = use_in_memory\n\n    # data: make data Path or np.ndarray, use type annotations for mypy\n    self.train_data: Union[Path, NDArray] = (\n        Path(train_data) if isinstance(train_data, str) else train_data\n    )\n\n    self.val_data: Union[Path, NDArray] = (\n        Path(val_data) if isinstance(val_data, str) else val_data\n    )\n\n    self.train_data_target: Union[Path, NDArray] = (\n        Path(train_data_target)\n        if isinstance(train_data_target, str)\n        else train_data_target\n    )\n\n    self.val_data_target: Union[Path, NDArray] = (\n        Path(val_data_target)\n        if isinstance(val_data_target, str)\n        else val_data_target\n    )\n\n    # validation split\n    self.val_percentage = val_percentage\n    self.val_minimum_split = val_minimum_split\n\n    # read source function corresponding to the requested type\n    if data_config.data_type == SupportedData.CUSTOM.value:\n        # mypy check\n        assert read_source_func is not None\n\n        self.read_source_func: Callable = read_source_func\n\n    elif data_config.data_type != SupportedData.ARRAY:\n        self.read_source_func = get_read_func(data_config.data_type)\n\n    self.extension_filter: str = extension_filter\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>Returns:</p> Type Description <code>tuple of list</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    Returns\n    -------\n    tuple of list\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.train_dataset.get_data_statistics()\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Hook used to prepare the data before calling <code>setup</code>.</p> <p>Here, we only need to examine the data if it was provided as a str or a Path.</p> <p>TODO: from lightning doc: prepare_data is called from the main process. It is not recommended to assign state here (e.g. self.x = y) since it is called on a single process and if you assign states here then they won't be available for other processes.</p> <p>https://lightning.ai/docs/pytorch/stable/data/datamodule.html</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Hook used to prepare the data before calling `setup`.\n\n    Here, we only need to examine the data if it was provided as a str or a Path.\n\n    TODO: from lightning doc:\n    prepare_data is called from the main process. It is not recommended to assign\n    state here (e.g. self.x = y) since it is called on a single process and if you\n    assign states here then they won't be available for other processes.\n\n    https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n    \"\"\"\n    # if the data is a Path or a str\n    if (\n        not isinstance(self.train_data, np.ndarray)\n        and not isinstance(self.val_data, np.ndarray)\n        and not isinstance(self.train_data_target, np.ndarray)\n        and not isinstance(self.val_data_target, np.ndarray)\n    ):\n        # list training files\n        self.train_files = list_files(\n            self.train_data, self.data_type, self.extension_filter\n        )\n        self.train_files_size = get_files_size(self.train_files)\n\n        # list validation files\n        if self.val_data is not None:\n            self.val_files = list_files(\n                self.val_data, self.data_type, self.extension_filter\n            )\n\n        # same for target data\n        if self.train_data_target is not None:\n            self.train_target_files: list[Path] = list_files(\n                self.train_data_target, self.data_type, self.extension_filter\n            )\n\n            # verify that they match the training data\n            validate_source_target_files(self.train_files, self.train_target_files)\n\n        if self.val_data_target is not None:\n            self.val_target_files = list_files(\n                self.val_data_target, self.data_type, self.extension_filter\n            )\n\n            # verify that they match the validation data\n            validate_source_target_files(self.val_files, self.val_target_files)\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.setup","title":"<code>setup(*args, **kwargs)</code>","text":"<p>Hook called at the beginning of fit, validate, or predict.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def setup(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Hook called at the beginning of fit, validate, or predict.\n\n    Parameters\n    ----------\n    *args : Any\n        Unused.\n    **kwargs : Any\n        Unused.\n    \"\"\"\n    # if numpy array\n    if self.data_type == SupportedData.ARRAY:\n        # mypy checks\n        assert isinstance(self.train_data, np.ndarray)\n        if self.train_data_target is not None:\n            assert isinstance(self.train_data_target, np.ndarray)\n\n        # train dataset\n        self.train_dataset: DatasetType = InMemoryDataset(\n            data_config=self.data_config,\n            inputs=self.train_data,\n            input_target=self.train_data_target,\n        )\n\n        # validation dataset\n        if self.val_data is not None:\n            # mypy checks\n            assert isinstance(self.val_data, np.ndarray)\n            if self.val_data_target is not None:\n                assert isinstance(self.val_data_target, np.ndarray)\n\n            # create its own dataset\n            self.val_dataset: DatasetType = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.val_data,\n                input_target=self.val_data_target,\n            )\n        else:\n            # extract validation from the training patches\n            self.val_dataset = self.train_dataset.split_dataset(\n                percentage=self.val_percentage,\n                minimum_patches=self.val_minimum_split,\n            )\n\n    # else we read files\n    else:\n        # Heuristics, if the file size is smaller than 80% of the RAM,\n        # we run the training in memory, otherwise we switch to iterable dataset\n        # The switch is deactivated if use_in_memory is False\n        if self.use_in_memory and self.train_files_size &lt; get_ram_size() * 0.8:\n            # train dataset\n            self.train_dataset = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.train_files,\n                input_target=(\n                    self.train_target_files if self.train_data_target else None\n                ),\n                read_source_func=self.read_source_func,\n            )\n\n            # validation dataset\n            if self.val_data is not None:\n                self.val_dataset = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.val_files,\n                    input_target=(\n                        self.val_target_files if self.val_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n            else:\n                # split dataset\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_patches=self.val_minimum_split,\n                )\n\n        # else if the data is too large, load file by file during training\n        else:\n            # create training dataset\n            self.train_dataset = PathIterableDataset(\n                data_config=self.data_config,\n                src_files=self.train_files,\n                target_files=(\n                    self.train_target_files if self.train_data_target else None\n                ),\n                read_source_func=self.read_source_func,\n            )\n\n            # create validation dataset\n            if self.val_data is not None:\n                # create its own dataset\n                self.val_dataset = PathIterableDataset(\n                    data_config=self.data_config,\n                    src_files=self.val_files,\n                    target_files=(\n                        self.val_target_files if self.val_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n            elif len(self.train_files) &lt;= self.val_minimum_split:\n                raise ValueError(\n                    f\"Not enough files to split a minimum of \"\n                    f\"{self.val_minimum_split} files, got {len(self.train_files)} \"\n                    f\"files.\"\n                )\n            else:\n                # extract validation from the training patches\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_number=self.val_minimum_split,\n                )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create a dataloader for training.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Training dataloader.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def train_dataloader(self) -&gt; Any:\n    \"\"\"\n    Create a dataloader for training.\n\n    Returns\n    -------\n    Any\n        Training dataloader.\n    \"\"\"\n    train_dataloader_params = self.data_config.train_dataloader_params.copy()\n\n    # NOTE: When next-gen datasets are completed this can be removed\n    # iterable dataset cannot be shuffled\n    if isinstance(self.train_dataset, IterableDataset):\n        del train_dataloader_params[\"shuffle\"]\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        **train_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create a dataloader for validation.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Validation dataloader.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def val_dataloader(self) -&gt; Any:\n    \"\"\"\n    Create a dataloader for validation.\n\n    Returns\n    -------\n    Any\n        Validation dataloader.\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        **self.data_config.val_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.create_train_datamodule","title":"<code>create_train_datamodule(train_data, data_type, patch_size, axes, batch_size, val_data=None, transforms=None, train_target_data=None, val_target_data=None, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_patches=5, train_dataloader_params=None, val_dataloader_params=None, use_in_memory=True)</code>","text":"<p>Create a TrainDataModule.</p> <p>This function is used to explicitly pass the parameters usually contained in a <code>GenericDataConfig</code> to a TrainDataModule.</p> <p>Since the lightning datamodule has no access to the model, make sure that the parameters passed to the datamodule are consistent with the model's requirements and are coherent.</p> <p>The default augmentations are XY flip and XY rotation. To use a different set of transformations, you can pass a list of transforms to <code>transforms</code>.</p> <p>The data module can be used with Path, str or numpy arrays. In the case of numpy arrays, it loads and computes all the patches in memory. For Path and str inputs, it calculates the total file size and estimate whether it can fit in memory. If it does not, it iterates through the files. This behaviour can be deactivated by setting <code>use_in_memory</code> to False, in which case it will always use the iterating dataset to train on a Path or str.</p> <p>To use array data, set <code>data_type</code> to <code>array</code> and pass a numpy array to <code>train_data</code>.</p> <p>By default, CAREamics only supports types defined in <code>careamics.config.support.SupportedData</code>. To read custom data types, you can set <code>data_type</code> to <code>custom</code> and provide a function that returns a numpy array from a path. Additionally, pass a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.jpeg\") to filter the files extension using <code>extension_filter</code>.</p> <p>In the absence of validation data, the validation data is extracted from the training data. The percentage of the training data to use for validation, as well as the minimum number of patches to split from the training data for validation can be set using <code>val_percentage</code> and <code>val_minimum_patches</code>, respectively.</p> <p>In <code>dataloader_params</code>, you can pass any parameter accepted by PyTorch dataloaders, except for <code>batch_size</code>, which is set by the <code>batch_size</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data.</p> required <code>data_type</code> <code>(array, tiff, custom)</code> <p>Data type, see <code>SupportedData</code> for available options.</p> <code>\"array\"</code> <code>patch_size</code> <code>list of int</code> <p>Patch size, 2D or 3D patch size.</p> required <code>axes</code> <code>str</code> <p>Axes of the data, chosen amongst SCZYX.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, by default None.</p> <code>None</code> <code>transforms</code> <code>list of Transforms</code> <p>List of transforms to apply to training patches. If None, default transforms are applied.</p> <code>None</code> <code>train_target_data</code> <code>Path or str or ndarray</code> <p>Training target data, by default None.</p> <code>None</code> <code>val_target_data</code> <code>Path or str or ndarray</code> <p>Validation target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>, by default \"\".</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation if no validation data is given, by default 0.1.</p> <code>0.1</code> <code>val_minimum_patches</code> <code>int</code> <p>Minimum number of patches to split from the training data for validation if no validation data is given, by default 5.</p> <code>5</code> <code>train_dataloader_params</code> <code>dict</code> <p>Pytorch dataloader parameters for the training data, by default {}.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Pytorch dataloader parameters for the validation data, by default {}.</p> <code>None</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>TrainDataModule</code> <p>CAREamics training Lightning data module.</p> <p>Examples:</p> <p>Create a TrainingDataModule with default transforms with a numpy array:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=my_array,\n...     data_type=\"array\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n... )\n</code></pre> <p>For custom data types (those not supported by CAREamics), then one can pass a read function and a filter for the files extension:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt;\n&gt;&gt;&gt; def read_npy(path):\n...     return np.load(path)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=\"path/to/data\",\n...     data_type=\"custom\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n...     read_source_func=read_npy,\n...     extension_filter=\"*.npy\",\n... )\n</code></pre> <p>If you want to use a different set of transformations, you can pass a list of transforms:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; from careamics.config.support import SupportedTransform\n&gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n&gt;&gt;&gt; my_transforms = [\n...     XYFlipModel(flip_y=False),\n... ]\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=my_array,\n...     data_type=\"array\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n...     transforms=my_transforms,\n... )\n</code></pre> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def create_train_datamodule(\n    train_data: Union[str, Path, NDArray],\n    data_type: Union[Literal[\"array\", \"tiff\", \"custom\"], SupportedData],\n    patch_size: list[int],\n    axes: str,\n    batch_size: int,\n    val_data: Union[str, Path, NDArray] | None = None,\n    transforms: list[TransformModel] | None = None,\n    train_target_data: Union[str, Path, NDArray] | None = None,\n    val_target_data: Union[str, Path, NDArray] | None = None,\n    read_source_func: Callable | None = None,\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_patches: int = 5,\n    train_dataloader_params: dict | None = None,\n    val_dataloader_params: dict | None = None,\n    use_in_memory: bool = True,\n) -&gt; TrainDataModule:\n    \"\"\"Create a TrainDataModule.\n\n    This function is used to explicitly pass the parameters usually contained in a\n    `GenericDataConfig` to a TrainDataModule.\n\n    Since the lightning datamodule has no access to the model, make sure that the\n    parameters passed to the datamodule are consistent with the model's requirements and\n    are coherent.\n\n    The default augmentations are XY flip and XY rotation. To use a different set of\n    transformations, you can pass a list of transforms to `transforms`.\n\n    The data module can be used with Path, str or numpy arrays. In the case of\n    numpy arrays, it loads and computes all the patches in memory. For Path and str\n    inputs, it calculates the total file size and estimate whether it can fit in\n    memory. If it does not, it iterates through the files. This behaviour can be\n    deactivated by setting `use_in_memory` to False, in which case it will\n    always use the iterating dataset to train on a Path or str.\n\n    To use array data, set `data_type` to `array` and pass a numpy array to\n    `train_data`.\n\n    By default, CAREamics only supports types defined in\n    `careamics.config.support.SupportedData`. To read custom data types, you can set\n    `data_type` to `custom` and provide a function that returns a numpy array from a\n    path. Additionally, pass a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.jpeg\") to filter the files extension using `extension_filter`.\n\n    In the absence of validation data, the validation data is extracted from the\n    training data. The percentage of the training data to use for validation, as well as\n    the minimum number of patches to split from the training data for validation can be\n    set using `val_percentage` and `val_minimum_patches`, respectively.\n\n    In `dataloader_params`, you can pass any parameter accepted by PyTorch dataloaders,\n    except for `batch_size`, which is set by the `batch_size` parameter.\n\n    Parameters\n    ----------\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data.\n    data_type : {\"array\", \"tiff\", \"custom\"}\n        Data type, see `SupportedData` for available options.\n    patch_size : list of int\n        Patch size, 2D or 3D patch size.\n    axes : str\n        Axes of the data, chosen amongst SCZYX.\n    batch_size : int\n        Batch size.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, by default None.\n    transforms : list of Transforms, optional\n        List of transforms to apply to training patches. If None, default transforms\n        are applied.\n    train_target_data : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, by default None.\n    val_target_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, used if `data_type` is `custom`, by\n        default None.\n    extension_filter : str, optional\n        Filter for file extensions, used if `data_type` is `custom`, by default \"\".\n    val_percentage : float, optional\n        Percentage of the training data to use for validation if no validation data\n        is given, by default 0.1.\n    val_minimum_patches : int, optional\n        Minimum number of patches to split from the training data for validation if\n        no validation data is given, by default 5.\n    train_dataloader_params : dict, optional\n        Pytorch dataloader parameters for the training data, by default {}.\n    val_dataloader_params : dict, optional\n        Pytorch dataloader parameters for the validation data, by default {}.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Returns\n    -------\n    TrainDataModule\n        CAREamics training Lightning data module.\n\n    Examples\n    --------\n    Create a TrainingDataModule with default transforms with a numpy array:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=my_array,\n    ...     data_type=\"array\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ... )\n\n    For custom data types (those not supported by CAREamics), then one can pass a read\n    function and a filter for the files extension:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; def read_npy(path):\n    ...     return np.load(path)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=\"path/to/data\",\n    ...     data_type=\"custom\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ...     read_source_func=read_npy,\n    ...     extension_filter=\"*.npy\",\n    ... )\n\n    If you want to use a different set of transformations, you can pass a list of\n    transforms:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; from careamics.config.support import SupportedTransform\n    &gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n    &gt;&gt;&gt; my_transforms = [\n    ...     XYFlipModel(flip_y=False),\n    ... ]\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=my_array,\n    ...     data_type=\"array\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ...     transforms=my_transforms,\n    ... )\n    \"\"\"\n    if train_dataloader_params is None:\n        train_dataloader_params = {\"shuffle\": True}\n\n    if val_dataloader_params is None:\n        val_dataloader_params = {\"shuffle\": False}\n\n    data_dict: dict[str, Any] = {\n        \"mode\": \"train\",\n        \"data_type\": data_type,\n        \"patch_size\": patch_size,\n        \"axes\": axes,\n        \"batch_size\": batch_size,\n        \"train_dataloader_params\": train_dataloader_params,\n        \"val_dataloader_params\": val_dataloader_params,\n    }\n\n    # if transforms are passed (otherwise it will use the default ones)\n    if transforms is not None:\n        data_dict[\"transforms\"] = transforms\n\n    # instantiate data configuration\n    data_config = DataConfig(**data_dict)\n\n    # sanity check on the dataloader parameters\n    if \"batch_size\" in train_dataloader_params:\n        # remove it\n        del train_dataloader_params[\"batch_size\"]\n\n    if \"batch_size\" in val_dataloader_params:\n        # remove it\n        del val_dataloader_params[\"batch_size\"]\n\n    return TrainDataModule(\n        data_config=data_config,\n        train_data=train_data,\n        val_data=val_data,\n        train_data_target=train_target_data,\n        val_data_target=val_target_data,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_patches,\n        use_in_memory=use_in_memory,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/data_stats_callback/","title":"data_stats_callback","text":"<p>Data statistics callback.</p>"},{"location":"reference/careamics/lightning/callbacks/data_stats_callback/#careamics.lightning.callbacks.data_stats_callback.DataStatsCallback","title":"<code>DataStatsCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to update model's data statistics from datamodule.</p> <p>This callback ensures that the model has access to the data statistics (mean and std) calculated by the datamodule before training starts.</p> Source code in <code>src/careamics/lightning/callbacks/data_stats_callback.py</code> <pre><code>class DataStatsCallback(Callback):\n    \"\"\"Callback to update model's data statistics from datamodule.\n\n    This callback ensures that the model has access to the data statistics (mean and\n    std) calculated by the datamodule before training starts.\n    \"\"\"\n\n    def setup(self, trainer: L.Trainer, module: L.LightningModule, stage: str) -&gt; None:\n        \"\"\"Called when trainer is setting up.\n\n        Parameters\n        ----------\n        trainer : Lightning.Trainer\n            The trainer instance.\n        module : Lightning.LightningModule\n            The model being trained.\n        stage : str\n            The current stage of training (e.g., 'fit', 'validate', 'test', 'predict').\n        \"\"\"\n        if stage == \"fit\":\n            # Get data statistics from datamodule\n            (data_mean, data_std), _ = trainer.datamodule.get_data_stats()\n\n            # Set data statistics in the model's likelihood module\n            module.noise_model_likelihood.set_data_stats(\n                data_mean=data_mean[\"target\"], data_std=data_std[\"target\"]\n            )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/data_stats_callback/#careamics.lightning.callbacks.data_stats_callback.DataStatsCallback.setup","title":"<code>setup(trainer, module, stage)</code>","text":"<p>Called when trainer is setting up.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer instance.</p> required <code>module</code> <code>LightningModule</code> <p>The model being trained.</p> required <code>stage</code> <code>str</code> <p>The current stage of training (e.g., 'fit', 'validate', 'test', 'predict').</p> required Source code in <code>src/careamics/lightning/callbacks/data_stats_callback.py</code> <pre><code>def setup(self, trainer: L.Trainer, module: L.LightningModule, stage: str) -&gt; None:\n    \"\"\"Called when trainer is setting up.\n\n    Parameters\n    ----------\n    trainer : Lightning.Trainer\n        The trainer instance.\n    module : Lightning.LightningModule\n        The model being trained.\n    stage : str\n        The current stage of training (e.g., 'fit', 'validate', 'test', 'predict').\n    \"\"\"\n    if stage == \"fit\":\n        # Get data statistics from datamodule\n        (data_mean, data_std), _ = trainer.datamodule.get_data_stats()\n\n        # Set data statistics in the model's likelihood module\n        module.noise_model_likelihood.set_data_stats(\n            data_mean=data_mean[\"target\"], data_std=data_std[\"target\"]\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/","title":"hyperparameters_callback","text":"<p>Callback saving CAREamics configuration as hyperparameters in the model.</p>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback","title":"<code>HyperParametersCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback allowing saving CAREamics configuration as hyperparameters in the model.</p> <p>This allows saving the configuration as dictionary in the checkpoints, and loading it subsequently in a CAREamist instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>class HyperParametersCallback(Callback):\n    \"\"\"\n    Callback allowing saving CAREamics configuration as hyperparameters in the model.\n\n    This allows saving the configuration as dictionary in the checkpoints, and\n    loading it subsequently in a CAREamist instance.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n\n    Attributes\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n    \"\"\"\n\n    def __init__(self, config: Configuration) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        config : Configuration\n            CAREamics configuration to be saved as hyperparameter in the model.\n        \"\"\"\n        self.config = config\n\n    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"\n        Update the hyperparameters of the model with the configuration on train start.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer, unused.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        pl_module.hparams.update(self.config.model_dump())\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> required Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>def __init__(self, config: Configuration) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Update the hyperparameters of the model with the configuration on train start.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer, unused.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"\n    Update the hyperparameters of the model with the configuration on train start.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer, unused.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    pl_module.hparams.update(self.config.model_dump())\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/","title":"progress_bar_callback","text":"<p>Progressbar callback.</p>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback","title":"<code>ProgressBarCallback</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Progress bar for training and validation steps.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>class ProgressBarCallback(TQDMProgressBar):\n    \"\"\"Progress bar for training and validation steps.\"\"\"\n\n    def init_train_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for training.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        bar = tqdm(\n            desc=\"Training\",\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=True,\n            file=sys.stdout,\n            smoothing=0,\n        )\n        return bar\n\n    def init_validation_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for validation.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        # The main progress bar doesn't exist in `trainer.validate()`\n        has_main_bar = self.train_progress_bar is not None\n        bar = tqdm(\n            desc=\"Validating\",\n            position=(2 * self.process_position + has_main_bar),\n            disable=self.is_disabled,\n            leave=False,\n            dynamic_ncols=True,\n            file=sys.stdout,\n        )\n        return bar\n\n    def init_test_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for testing.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        bar = tqdm(\n            desc=\"Testing\",\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=False,\n            ncols=100,\n            file=sys.stdout,\n        )\n        return bar\n\n    def get_metrics(\n        self, trainer: Trainer, pl_module: LightningModule\n    ) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n        \"\"\"Override this to customize the metrics displayed in the progress bar.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            The trainer object.\n        pl_module : LightningModule\n            The LightningModule object, unused.\n\n        Returns\n        -------\n        dict\n            A dictionary with the metrics to display in the progress bar.\n        \"\"\"\n        pbar_metrics = trainer.progress_bar_metrics\n        return {**pbar_metrics}\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.get_metrics","title":"<code>get_metrics(trainer, pl_module)</code>","text":"<p>Override this to customize the metrics displayed in the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule object, unused.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the metrics to display in the progress bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def get_metrics(\n    self, trainer: Trainer, pl_module: LightningModule\n) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n    \"\"\"Override this to customize the metrics displayed in the progress bar.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        The trainer object.\n    pl_module : LightningModule\n        The LightningModule object, unused.\n\n    Returns\n    -------\n    dict\n        A dictionary with the metrics to display in the progress bar.\n    \"\"\"\n    pbar_metrics = trainer.progress_bar_metrics\n    return {**pbar_metrics}\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_test_tqdm","title":"<code>init_test_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for testing.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_test_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for testing.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    bar = tqdm(\n        desc=\"Testing\",\n        position=(2 * self.process_position),\n        disable=self.is_disabled,\n        leave=True,\n        dynamic_ncols=False,\n        ncols=100,\n        file=sys.stdout,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_train_tqdm","title":"<code>init_train_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for training.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_train_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for training.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    bar = tqdm(\n        desc=\"Training\",\n        position=(2 * self.process_position),\n        disable=self.is_disabled,\n        leave=True,\n        dynamic_ncols=True,\n        file=sys.stdout,\n        smoothing=0,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_validation_tqdm","title":"<code>init_validation_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for validation.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_validation_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for validation.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    # The main progress bar doesn't exist in `trainer.validate()`\n    has_main_bar = self.train_progress_bar is not None\n    bar = tqdm(\n        desc=\"Validating\",\n        position=(2 * self.process_position + has_main_bar),\n        disable=self.is_disabled,\n        leave=False,\n        dynamic_ncols=True,\n        file=sys.stdout,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/","title":"file_path_utils","text":"<p>Module containing file path utilities for <code>WriteStrategy</code> to use.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/#careamics.lightning.callbacks.prediction_writer_callback.file_path_utils.create_write_file_path","title":"<code>create_write_file_path(dirpath, file_path, write_extension)</code>","text":"<p>Create the file name for the output file.</p> <p>Takes the original file path, changes the directory to <code>dirpath</code> and changes the extension to <code>write_extension</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>Path</code> <p>The output directory to write file to.</p> required <code>file_path</code> <code>Path</code> <p>The original file path.</p> required <code>write_extension</code> <code>str</code> <p>The extension that output files should have.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The output file path.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils.py</code> <pre><code>def create_write_file_path(\n    dirpath: Path, file_path: Path, write_extension: str\n) -&gt; Path:\n    \"\"\"\n    Create the file name for the output file.\n\n    Takes the original file path, changes the directory to `dirpath` and changes\n    the extension to `write_extension`.\n\n    Parameters\n    ----------\n    dirpath : pathlib.Path\n        The output directory to write file to.\n    file_path : pathlib.Path\n        The original file path.\n    write_extension : str\n        The extension that output files should have.\n\n    Returns\n    -------\n    Path\n        The output file path.\n    \"\"\"\n    file_name = Path(file_path.stem).with_suffix(write_extension)\n    file_path = dirpath / file_name\n    return file_path\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/#careamics.lightning.callbacks.prediction_writer_callback.file_path_utils.get_sample_file_path","title":"<code>get_sample_file_path(dataset, sample_id)</code>","text":"<p>Get the file path for a particular sample.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>IterableTiledPredDataset or IterablePredDataset</code> <p>Dataset.</p> required <code>sample_id</code> <code>int</code> <p>Sample ID, the index of the file in the dataset <code>dataset</code>.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The file path corresponding to the sample with the ID <code>sample_id</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils.py</code> <pre><code>def get_sample_file_path(\n    dataset: Union[IterableTiledPredDataset, IterablePredDataset], sample_id: int\n) -&gt; Path:\n    \"\"\"\n    Get the file path for a particular sample.\n\n    Parameters\n    ----------\n    dataset : IterableTiledPredDataset or IterablePredDataset\n        Dataset.\n    sample_id : int\n        Sample ID, the index of the file in the dataset `dataset`.\n\n    Returns\n    -------\n    Path\n        The file path corresponding to the sample with the ID `sample_id`.\n    \"\"\"\n    return dataset.data_files[sample_id]\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/","title":"prediction_writer_callback","text":"<p>Module containing <code>PredictionWriterCallback</code> class.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback","title":"<code>PredictionWriterCallback</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>A PyTorch Lightning callback to save predictions.</p> <p>Parameters:</p> Name Type Description Default <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> required <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> <p>Attributes:</p> Name Type Description <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> <code>dirpath</code> <code>pathlib.Path, default=\"predictions\"</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>writing_predictions</code> <code>bool</code> <p>If writing predictions is turned on or off.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>class PredictionWriterCallback(BasePredictionWriter):\n    \"\"\"\n    A PyTorch Lightning callback to save predictions.\n\n    Parameters\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n\n    Attributes\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : pathlib.Path, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n    writing_predictions : bool\n        If writing predictions is turned on or off.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_strategy: WriteStrategy,\n        dirpath: Union[Path, str] = \"predictions\",\n    ):\n        \"\"\"\n        A PyTorch Lightning callback to save predictions.\n\n        Parameters\n        ----------\n        write_strategy : WriteStrategy\n            A strategy for writing predictions.\n        dirpath : pathlib.Path or str, default=\"predictions\"\n            The path to the directory where prediction outputs will be saved. If\n            `dirpath` is not absolute it is assumed to be relative to current working\n            directory.\n        \"\"\"\n        super().__init__(write_interval=\"batch\")\n\n        # Toggle for CAREamist to switch off saving if desired\n        self.writing_predictions: bool = True\n\n        self.write_strategy: WriteStrategy = write_strategy\n\n        # forward declaration\n        self.dirpath: Path\n        # attribute initialisation\n        self._init_dirpath(dirpath)\n\n    @classmethod\n    def from_write_func_params(\n        cls,\n        write_type: SupportedWriteType,\n        tiled: bool,\n        write_func: WriteFunc | None = None,\n        write_extension: str | None = None,\n        write_func_kwargs: dict[str, Any] | None = None,\n        dirpath: Union[Path, str] = \"predictions\",\n    ) -&gt; PredictionWriterCallback:  # TODO: change type hint to self (find out how)\n        \"\"\"\n        Initialize a `PredictionWriterCallback` from write function parameters.\n\n        This will automatically create a `WriteStrategy` to be passed to the\n        initialization of `PredictionWriterCallback`.\n\n        Parameters\n        ----------\n        write_type : {\"tiff\", \"custom\"}\n            The data type to save as, includes custom.\n        tiled : bool\n            Whether the prediction will be tiled or not.\n        write_func : WriteFunc, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` a function to save the data must be passed. See notes below.\n        write_extension : str, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` an extension to save the data with must be passed.\n        write_func_kwargs : dict of {{str: any}}, optional\n            Additional keyword arguments to be passed to the save function.\n        dirpath : pathlib.Path or str, default=\"predictions\"\n            The path to the directory where prediction outputs will be saved. If\n            `dirpath` is not absolute it is assumed to be relative to current working\n            directory.\n\n        Returns\n        -------\n        PredictionWriterCallback\n            Callback for writing predictions.\n        \"\"\"\n        write_strategy = create_write_strategy(\n            write_type=write_type,\n            tiled=tiled,\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n        return cls(write_strategy=write_strategy, dirpath=dirpath)\n\n    def _init_dirpath(self, dirpath):\n        \"\"\"\n        Initialize directory path. Should only be called from `__init__`.\n\n        Parameters\n        ----------\n        dirpath : pathlib.Path\n            See `__init__` description.\n        \"\"\"\n        dirpath = Path(dirpath)\n        if not dirpath.is_absolute():\n            dirpath = Path.cwd() / dirpath\n            logger.warning(\n                \"Prediction output directory is not absolute, absolute path assumed to\"\n                f\"be '{dirpath}'\"\n            )\n        self.dirpath = dirpath\n\n    def setup(self, trainer: Trainer, pl_module: LightningModule, stage: str) -&gt; None:\n        \"\"\"\n        Create the prediction output directory when predict begins.\n\n        Called when fit, validate, test, predict, or tune begins.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        stage : str\n            Stage of training e.g. 'predict', 'fit', 'validate'.\n        \"\"\"\n        super().setup(trainer, pl_module, stage)\n        if stage == \"predict\":\n            # make prediction output directory\n            logger.info(\"Making prediction output directory.\")\n            self.dirpath.mkdir(parents=True, exist_ok=True)\n\n    def write_on_batch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,  # TODO: change to expected type\n        batch_indices: Sequence[int] | None,\n        batch: Any,  # TODO: change to expected type\n        batch_idx: int,\n        dataloader_idx: int,\n    ) -&gt; None:\n        \"\"\"\n        Write predictions at the end of a batch.\n\n        The method of prediction is determined by the attribute `write_strategy`.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        prediction : Any\n            Prediction outputs of `batch`.\n        batch_indices : sequence of Any, optional\n            Batch indices.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        \"\"\"\n        # if writing prediction is turned off\n        if not self.writing_predictions:\n            return\n\n        dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dataloader: DataLoader = (\n            dataloaders[dataloader_idx]\n            if isinstance(dataloaders, list)\n            else dataloaders\n        )\n        dataset: ValidPredDatasets = dataloader.dataset\n        if not (\n            isinstance(dataset, IterablePredDataset)\n            or isinstance(dataset, IterableTiledPredDataset)\n        ):\n            # Note: Error will be raised before here from the source type\n            # This is for extra redundancy of errors.\n            raise TypeError(\n                \"Prediction dataset has to be `IterableTiledPredDataset` or \"\n                \"`IterablePredDataset`. Cannot be `InMemoryPredDataset` because \"\n                \"filenames are taken from the original file.\"\n            )\n\n        self.write_strategy.write_batch(\n            trainer=trainer,\n            pl_module=pl_module,\n            prediction=prediction,\n            batch_indices=batch_indices,\n            batch=batch,\n            batch_idx=batch_idx,\n            dataloader_idx=dataloader_idx,\n            dirpath=self.dirpath,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.__init__","title":"<code>__init__(write_strategy, dirpath='predictions')</code>","text":"<p>A PyTorch Lightning callback to save predictions.</p> <p>Parameters:</p> Name Type Description Default <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> required <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def __init__(\n    self,\n    write_strategy: WriteStrategy,\n    dirpath: Union[Path, str] = \"predictions\",\n):\n    \"\"\"\n    A PyTorch Lightning callback to save predictions.\n\n    Parameters\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : pathlib.Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n    \"\"\"\n    super().__init__(write_interval=\"batch\")\n\n    # Toggle for CAREamist to switch off saving if desired\n    self.writing_predictions: bool = True\n\n    self.write_strategy: WriteStrategy = write_strategy\n\n    # forward declaration\n    self.dirpath: Path\n    # attribute initialisation\n    self._init_dirpath(dirpath)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.from_write_func_params","title":"<code>from_write_func_params(write_type, tiled, write_func=None, write_extension=None, write_func_kwargs=None, dirpath='predictions')</code>  <code>classmethod</code>","text":"<p>Initialize a <code>PredictionWriterCallback</code> from write function parameters.</p> <p>This will automatically create a <code>WriteStrategy</code> to be passed to the initialization of <code>PredictionWriterCallback</code>.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>('tiff', 'custom')</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>tiled</code> <code>bool</code> <p>Whether the prediction will be tiled or not.</p> required <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {{str: any}}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> <p>Returns:</p> Type Description <code>PredictionWriterCallback</code> <p>Callback for writing predictions.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>@classmethod\ndef from_write_func_params(\n    cls,\n    write_type: SupportedWriteType,\n    tiled: bool,\n    write_func: WriteFunc | None = None,\n    write_extension: str | None = None,\n    write_func_kwargs: dict[str, Any] | None = None,\n    dirpath: Union[Path, str] = \"predictions\",\n) -&gt; PredictionWriterCallback:  # TODO: change type hint to self (find out how)\n    \"\"\"\n    Initialize a `PredictionWriterCallback` from write function parameters.\n\n    This will automatically create a `WriteStrategy` to be passed to the\n    initialization of `PredictionWriterCallback`.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    tiled : bool\n        Whether the prediction will be tiled or not.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func_kwargs : dict of {{str: any}}, optional\n        Additional keyword arguments to be passed to the save function.\n    dirpath : pathlib.Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n\n    Returns\n    -------\n    PredictionWriterCallback\n        Callback for writing predictions.\n    \"\"\"\n    write_strategy = create_write_strategy(\n        write_type=write_type,\n        tiled=tiled,\n        write_func=write_func,\n        write_extension=write_extension,\n        write_func_kwargs=write_func_kwargs,\n    )\n    return cls(write_strategy=write_strategy, dirpath=dirpath)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.setup","title":"<code>setup(trainer, pl_module, stage)</code>","text":"<p>Create the prediction output directory when predict begins.</p> <p>Called when fit, validate, test, predict, or tune begins.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>stage</code> <code>str</code> <p>Stage of training e.g. 'predict', 'fit', 'validate'.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: str) -&gt; None:\n    \"\"\"\n    Create the prediction output directory when predict begins.\n\n    Called when fit, validate, test, predict, or tune begins.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    stage : str\n        Stage of training e.g. 'predict', 'fit', 'validate'.\n    \"\"\"\n    super().setup(trainer, pl_module, stage)\n    if stage == \"predict\":\n        # make prediction output directory\n        logger.info(\"Making prediction output directory.\")\n        self.dirpath.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.write_on_batch_end","title":"<code>write_on_batch_end(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx)</code>","text":"<p>Write predictions at the end of a batch.</p> <p>The method of prediction is determined by the attribute <code>write_strategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>prediction</code> <code>Any</code> <p>Prediction outputs of <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of Any</code> <p>Batch indices.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def write_on_batch_end(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,  # TODO: change to expected type\n    batch_indices: Sequence[int] | None,\n    batch: Any,  # TODO: change to expected type\n    batch_idx: int,\n    dataloader_idx: int,\n) -&gt; None:\n    \"\"\"\n    Write predictions at the end of a batch.\n\n    The method of prediction is determined by the attribute `write_strategy`.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    prediction : Any\n        Prediction outputs of `batch`.\n    batch_indices : sequence of Any, optional\n        Batch indices.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    \"\"\"\n    # if writing prediction is turned off\n    if not self.writing_predictions:\n        return\n\n    dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dataloader: DataLoader = (\n        dataloaders[dataloader_idx]\n        if isinstance(dataloaders, list)\n        else dataloaders\n    )\n    dataset: ValidPredDatasets = dataloader.dataset\n    if not (\n        isinstance(dataset, IterablePredDataset)\n        or isinstance(dataset, IterableTiledPredDataset)\n    ):\n        # Note: Error will be raised before here from the source type\n        # This is for extra redundancy of errors.\n        raise TypeError(\n            \"Prediction dataset has to be `IterableTiledPredDataset` or \"\n            \"`IterablePredDataset`. Cannot be `InMemoryPredDataset` because \"\n            \"filenames are taken from the original file.\"\n        )\n\n    self.write_strategy.write_batch(\n        trainer=trainer,\n        pl_module=pl_module,\n        prediction=prediction,\n        batch_indices=batch_indices,\n        batch=batch,\n        batch_idx=batch_idx,\n        dataloader_idx=dataloader_idx,\n        dirpath=self.dirpath,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/","title":"write_strategy","text":"<p>Module containing different strategies for writing predictions.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles","title":"<code>CacheTiles</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>A write strategy that will cache tiles.</p> <p>Tiles are cached until a whole image is predicted on. Then the stitched prediction is saved.</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required <p>Attributes:</p> Name Type Description <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> <code>tile_cache</code> <code>list of numpy.ndarray</code> <p>Tiles cached for stitching prediction.</p> <code>tile_info_cache</code> <code>list of TileInformation</code> <p>Cached tile information for stitching prediction.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class CacheTiles(WriteStrategy):\n    \"\"\"\n    A write strategy that will cache tiles.\n\n    Tiles are cached until a whole image is predicted on. Then the stitched\n    prediction is saved.\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n\n    Attributes\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    tile_cache : list of numpy.ndarray\n        Tiles cached for stitching prediction.\n    tile_info_cache : list of TileInformation\n        Cached tile information for stitching prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_func: WriteFunc,\n        write_extension: str,\n        write_func_kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        A write strategy that will cache tiles.\n\n        Tiles are cached until a whole image is predicted on. Then the stitched\n        prediction is saved.\n\n        Parameters\n        ----------\n        write_func : WriteFunc\n            Function used to save predictions.\n        write_extension : str\n            Extension added to prediction file paths.\n        write_func_kwargs : dict of {str: Any}\n            Extra kwargs to pass to `write_func`.\n        \"\"\"\n        super().__init__()\n\n        self.write_func: WriteFunc = write_func\n        self.write_extension: str = write_extension\n        self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n        # where tiles will be cached until a whole image has been predicted\n        self.tile_cache: list[NDArray] = []\n        self.tile_info_cache: list[TileInformation] = []\n\n    @property\n    def last_tiles(self) -&gt; list[bool]:\n        \"\"\"\n        List of bool to determine whether each tile in the cache is the last tile.\n\n        Returns\n        -------\n        list of bool\n            Whether each tile in the tile cache is the last tile.\n        \"\"\"\n        return [tile_info.last_tile for tile_info in self.tile_info_cache]\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: tuple[NDArray, list[TileInformation]],\n        batch_indices: Sequence[int] | None,\n        batch: tuple[NDArray, list[TileInformation]],\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Cache tiles until the last tile is predicted; save the stitched prediction.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n        \"\"\"\n        dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dataloader: DataLoader = (\n            dataloaders[dataloader_idx]\n            if isinstance(dataloaders, list)\n            else dataloaders\n        )\n        dataset: IterableTiledPredDataset = dataloader.dataset\n        if not isinstance(dataset, IterableTiledPredDataset):\n            raise TypeError(\"Prediction dataset is not `IterableTiledPredDataset`.\")\n\n        # cache tiles (batches are split into single samples)\n        self.tile_cache.extend(np.split(prediction[0], prediction[0].shape[0]))\n        self.tile_info_cache.extend(prediction[1])\n\n        # save stitched prediction\n        if self._has_last_tile():\n\n            # get image tiles and remove them from the cache\n            tiles, tile_infos = self._get_image_tiles()\n            self._clear_cache()\n\n            # stitch prediction\n            prediction_image = stitch_prediction_single(\n                tiles=tiles, tile_infos=tile_infos\n            )\n\n            # write prediction\n            sample_id = tile_infos[0].sample_id  # need this to select correct file name\n            input_file_path = get_sample_file_path(dataset=dataset, sample_id=sample_id)\n            file_path = create_write_file_path(\n                dirpath=dirpath,\n                file_path=input_file_path,\n                write_extension=self.write_extension,\n            )\n            self.write_func(\n                file_path=file_path, img=prediction_image[0], **self.write_func_kwargs\n            )\n\n    def _has_last_tile(self) -&gt; bool:\n        \"\"\"\n        Whether a last tile is contained in the cached tiles.\n\n        Returns\n        -------\n        bool\n            Whether a last tile is contained in the cached tiles.\n        \"\"\"\n        return any(self.last_tiles)\n\n    def _clear_cache(self) -&gt; None:\n        \"\"\"Remove the tiles in the cache up to the first last tile.\"\"\"\n        index = self._last_tile_index()\n        self.tile_cache = self.tile_cache[index + 1 :]\n        self.tile_info_cache = self.tile_info_cache[index + 1 :]\n\n    def _last_tile_index(self) -&gt; int:\n        \"\"\"\n        Find the index of the last tile in the tile cache.\n\n        Returns\n        -------\n        int\n            Index of last tile.\n\n        Raises\n        ------\n        ValueError\n            If there is no last tile in the tile cache.\n        \"\"\"\n        last_tiles = self.last_tiles\n        if not any(last_tiles):\n            raise ValueError(\"No last tile in the tile cache.\")\n        index = np.where(last_tiles)[0][0]\n        return index\n\n    def _get_image_tiles(self) -&gt; tuple[list[NDArray], list[TileInformation]]:\n        \"\"\"\n        Get the tiles corresponding to a single image.\n\n        Returns\n        -------\n        tuple of (list of numpy.ndarray, list of TileInformation)\n            Tiles and tile information to stitch together a full image.\n        \"\"\"\n        index = self._last_tile_index()\n        tiles = self.tile_cache[: index + 1]\n        tile_infos = self.tile_info_cache[: index + 1]\n        return tiles, tile_infos\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.last_tiles","title":"<code>last_tiles</code>  <code>property</code>","text":"<p>List of bool to determine whether each tile in the cache is the last tile.</p> <p>Returns:</p> Type Description <code>list of bool</code> <p>Whether each tile in the tile cache is the last tile.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.__init__","title":"<code>__init__(write_func, write_extension, write_func_kwargs)</code>","text":"<p>A write strategy that will cache tiles.</p> <p>Tiles are cached until a whole image is predicted on. Then the stitched prediction is saved.</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def __init__(\n    self,\n    write_func: WriteFunc,\n    write_extension: str,\n    write_func_kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    A write strategy that will cache tiles.\n\n    Tiles are cached until a whole image is predicted on. Then the stitched\n    prediction is saved.\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n    super().__init__()\n\n    self.write_func: WriteFunc = write_func\n    self.write_extension: str = write_extension\n    self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n    # where tiles will be cached until a whole image has been predicted\n    self.tile_cache: list[NDArray] = []\n    self.tile_info_cache: list[TileInformation] = []\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Cache tiles until the last tile is predicted; save the stitched prediction.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: tuple[NDArray, list[TileInformation]],\n    batch_indices: Sequence[int] | None,\n    batch: tuple[NDArray, list[TileInformation]],\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Cache tiles until the last tile is predicted; save the stitched prediction.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n    \"\"\"\n    dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dataloader: DataLoader = (\n        dataloaders[dataloader_idx]\n        if isinstance(dataloaders, list)\n        else dataloaders\n    )\n    dataset: IterableTiledPredDataset = dataloader.dataset\n    if not isinstance(dataset, IterableTiledPredDataset):\n        raise TypeError(\"Prediction dataset is not `IterableTiledPredDataset`.\")\n\n    # cache tiles (batches are split into single samples)\n    self.tile_cache.extend(np.split(prediction[0], prediction[0].shape[0]))\n    self.tile_info_cache.extend(prediction[1])\n\n    # save stitched prediction\n    if self._has_last_tile():\n\n        # get image tiles and remove them from the cache\n        tiles, tile_infos = self._get_image_tiles()\n        self._clear_cache()\n\n        # stitch prediction\n        prediction_image = stitch_prediction_single(\n            tiles=tiles, tile_infos=tile_infos\n        )\n\n        # write prediction\n        sample_id = tile_infos[0].sample_id  # need this to select correct file name\n        input_file_path = get_sample_file_path(dataset=dataset, sample_id=sample_id)\n        file_path = create_write_file_path(\n            dirpath=dirpath,\n            file_path=input_file_path,\n            write_extension=self.write_extension,\n        )\n        self.write_func(\n            file_path=file_path, img=prediction_image[0], **self.write_func_kwargs\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage","title":"<code>WriteImage</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>A strategy for writing image predictions (i.e. un-tiled predictions).</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required <p>Attributes:</p> Name Type Description <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteImage(WriteStrategy):\n    \"\"\"\n    A strategy for writing image predictions (i.e. un-tiled predictions).\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n\n    Attributes\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_func: WriteFunc,\n        write_extension: str,\n        write_func_kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        A strategy for writing image predictions (i.e. un-tiled predictions).\n\n        Parameters\n        ----------\n        write_func : WriteFunc\n            Function used to save predictions.\n        write_extension : str\n            Extension added to prediction file paths.\n        write_func_kwargs : dict of {str: Any}\n            Extra kwargs to pass to `write_func`.\n        \"\"\"\n        super().__init__()\n\n        self.write_func: WriteFunc = write_func\n        self.write_extension: str = write_extension\n        self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: NDArray,\n        batch_indices: Sequence[int] | None,\n        batch: NDArray,\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Save full images.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n\n        Raises\n        ------\n        TypeError\n            If trainer prediction dataset is not `IterablePredDataset`.\n        \"\"\"\n        dls: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dl: DataLoader = dls[dataloader_idx] if isinstance(dls, list) else dls\n        ds: IterablePredDataset = dl.dataset\n        if not isinstance(ds, IterablePredDataset):\n            raise TypeError(\"Prediction dataset is not `IterablePredDataset`.\")\n\n        for i in range(prediction.shape[0]):\n            prediction_image = prediction[0]\n            sample_id = batch_idx * dl.batch_size + i\n            input_file_path = get_sample_file_path(dataset=ds, sample_id=sample_id)\n            file_path = create_write_file_path(\n                dirpath=dirpath,\n                file_path=input_file_path,\n                write_extension=self.write_extension,\n            )\n            self.write_func(\n                file_path=file_path, img=prediction_image, **self.write_func_kwargs\n            )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage.__init__","title":"<code>__init__(write_func, write_extension, write_func_kwargs)</code>","text":"<p>A strategy for writing image predictions (i.e. un-tiled predictions).</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def __init__(\n    self,\n    write_func: WriteFunc,\n    write_extension: str,\n    write_func_kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    A strategy for writing image predictions (i.e. un-tiled predictions).\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n    super().__init__()\n\n    self.write_func: WriteFunc = write_func\n    self.write_extension: str = write_extension\n    self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Save full images.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If trainer prediction dataset is not <code>IterablePredDataset</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: NDArray,\n    batch_indices: Sequence[int] | None,\n    batch: NDArray,\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Save full images.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n\n    Raises\n    ------\n    TypeError\n        If trainer prediction dataset is not `IterablePredDataset`.\n    \"\"\"\n    dls: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dl: DataLoader = dls[dataloader_idx] if isinstance(dls, list) else dls\n    ds: IterablePredDataset = dl.dataset\n    if not isinstance(ds, IterablePredDataset):\n        raise TypeError(\"Prediction dataset is not `IterablePredDataset`.\")\n\n    for i in range(prediction.shape[0]):\n        prediction_image = prediction[0]\n        sample_id = batch_idx * dl.batch_size + i\n        input_file_path = get_sample_file_path(dataset=ds, sample_id=sample_id)\n        file_path = create_write_file_path(\n            dirpath=dirpath,\n            file_path=input_file_path,\n            write_extension=self.write_extension,\n        )\n        self.write_func(\n            file_path=file_path, img=prediction_image, **self.write_func_kwargs\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteStrategy","title":"<code>WriteStrategy</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for write strategy classes.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteStrategy(Protocol):\n    \"\"\"Protocol for write strategy classes.\"\"\"\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,  # TODO: change to expected type\n        batch_indices: Sequence[int] | None,\n        batch: Any,  # TODO: change to expected type\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        WriteStrategy subclasses must contain this function to write a batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteStrategy.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>WriteStrategy subclasses must contain this function to write a batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,  # TODO: change to expected type\n    batch_indices: Sequence[int] | None,\n    batch: Any,  # TODO: change to expected type\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    WriteStrategy subclasses must contain this function to write a batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteTilesZarr","title":"<code>WriteTilesZarr</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>Strategy to write tiles to Zarr file.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteTilesZarr(WriteStrategy):\n    \"\"\"Strategy to write tiles to Zarr file.\"\"\"\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,\n        batch_indices: Sequence[int] | None,\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Write tiles to zarr file.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteTilesZarr.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Write tiles to zarr file.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,\n    batch_indices: Sequence[int] | None,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Write tiles to zarr file.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n\n    Raises\n    ------\n    NotImplementedError\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/","title":"write_strategy_factory","text":"<p>Module containing convenience function to create <code>WriteStrategy</code>.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.create_write_strategy","title":"<code>create_write_strategy(write_type, tiled, write_func=None, write_extension=None, write_func_kwargs=None)</code>","text":"<p>Create a write strategy from convenient parameters.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>tiled</code> <code>bool</code> <p>Whether the prediction will be tiled or not.</p> required <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {str: any}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <p>Returns:</p> Type Description <code>WriteStrategy</code> <p>A strategy for writing predicions.</p> Notes <p>The <code>write_func</code> function signature must match that of the example below     <pre><code>write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n</code></pre></p> <p>The <code>write_func_kwargs</code> will be passed to the <code>write_func</code> doing the following:     <pre><code>write_func(file_path=file_path, img=img, **kwargs)\n</code></pre></p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def create_write_strategy(\n    write_type: SupportedWriteType,\n    tiled: bool,\n    write_func: WriteFunc | None = None,\n    write_extension: str | None = None,\n    write_func_kwargs: dict[str, Any] | None = None,\n) -&gt; WriteStrategy:\n    \"\"\"\n    Create a write strategy from convenient parameters.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    tiled : bool\n        Whether the prediction will be tiled or not.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func_kwargs : dict of {str: any}, optional\n        Additional keyword arguments to be passed to the save function.\n\n    Returns\n    -------\n    WriteStrategy\n        A strategy for writing predicions.\n\n    Notes\n    -----\n    The `write_func` function signature must match that of the example below\n        ```\n        write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n        ```\n\n    The `write_func_kwargs` will be passed to the `write_func` doing the following:\n        ```\n        write_func(file_path=file_path, img=img, **kwargs)\n        ```\n    \"\"\"\n    if write_func_kwargs is None:\n        write_func_kwargs = {}\n\n    write_strategy: WriteStrategy\n    if not tiled:\n        write_func = select_write_func(write_type=write_type, write_func=write_func)\n        write_extension = select_write_extension(\n            write_type=write_type, write_extension=write_extension\n        )\n        write_strategy = WriteImage(\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n    else:\n        # select CacheTiles or WriteTilesZarr (when implemented)\n        write_strategy = _create_tiled_write_strategy(\n            write_type=write_type,\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n\n    return write_strategy\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.select_write_extension","title":"<code>select_write_extension(write_type, write_extension=None)</code>","text":"<p>Return an extension to add to file paths.</p> <p>If <code>write_type</code> is \"custom\" then <code>write_extension</code>, otherwise the known write extension is selected.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The extension to be added to file paths.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>self.save_type=\"custom\"</code> but <code>save_extension</code> has not been given.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def select_write_extension(\n    write_type: SupportedWriteType, write_extension: str | None = None\n) -&gt; str:\n    \"\"\"\n    Return an extension to add to file paths.\n\n    If `write_type` is \"custom\" then `write_extension`, otherwise the known\n    write extension is selected.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n\n    Returns\n    -------\n    str\n        The extension to be added to file paths.\n\n    Raises\n    ------\n    ValueError\n        If `self.save_type=\"custom\"` but `save_extension` has not been given.\n    \"\"\"\n    write_type_: SupportedData = SupportedData(write_type)  # new variable for mypy\n    if write_type_ == SupportedData.CUSTOM:\n        if write_extension is None:\n            raise ValueError(\"A save extension must be provided for custom data types.\")\n        else:\n            write_extension = write_extension\n    else:\n        # kind of a weird pattern -&gt; reason to move get_extension from SupportedData\n        write_extension = write_type_.get_extension(write_type_)\n    return write_extension\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.select_write_func","title":"<code>select_write_func(write_type, write_func=None)</code>","text":"<p>Return a function to write images.</p> <p>If <code>write_type</code> is \"custom\" then <code>write_func</code>, otherwise the known write function is selected.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <p>Returns:</p> Type Description <code>WriteFunc</code> <p>A function for writing images.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>write_type=\"custom\"</code> but <code>write_func</code> has not been given.</p> Notes <p>The <code>write_func</code> function signature must match that of the example below     <pre><code>write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n</code></pre></p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def select_write_func(\n    write_type: SupportedWriteType, write_func: WriteFunc | None = None\n) -&gt; WriteFunc:\n    \"\"\"\n    Return a function to write images.\n\n    If `write_type` is \"custom\" then `write_func`, otherwise the known write function\n    is selected.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n\n    Returns\n    -------\n    WriteFunc\n        A function for writing images.\n\n    Raises\n    ------\n    ValueError\n        If `write_type=\"custom\"` but `write_func` has not been given.\n\n    Notes\n    -----\n    The `write_func` function signature must match that of the example below\n        ```\n        write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n        ```\n    \"\"\"\n    if write_type == SupportedData.CUSTOM:\n        if write_func is None:\n            raise ValueError(\n                \"A save function must be provided for custom data types.\"\n                # TODO: link to how save functions should be implemented\n            )\n        else:\n            write_func = write_func\n    else:\n        write_func = get_write_func(write_type)\n    return write_func\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/","title":"data_module","text":"<p>Next-Generation CAREamics DataModule.</p>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.InputType","title":"<code>InputType = Union[ItemType, list[ItemType], None]</code>  <code>module-attribute</code>","text":"<p>Type of input data passed to the dataset.</p>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.ItemType","title":"<code>ItemType = Union[Path, str, NDArray[Any]]</code>  <code>module-attribute</code>","text":"<p>Type of input items passed to the dataset.</p>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule","title":"<code>CareamicsDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Data module for Careamics dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataConfig</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Optional[InputType]</code> <p>Training data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>train_data_target</code> <code>Optional[InputType]</code> <p>Training data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>train_data_mask</code> <code>InputType (when filtering is needed)</code> <p>Training data mask, can be a path to a folder, a list of paths, or a numpy array. Used for coordinate filtering. Only required when using coordinate-based patch filtering.</p> <code>None</code> <code>val_data</code> <code>Optional[InputType]</code> <p>Validation data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>val_data_target</code> <code>Optional[InputType]</code> <p>Validation data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>pred_data</code> <code>Optional[InputType]</code> <p>Prediction data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>pred_data_target</code> <code>Optional[InputType]</code> <p>Prediction data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>read_source_func</code> <code>Optional[Callable]</code> <p>Function to read the source data. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>read_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>The kwargs for the read source function.</p> <code>None</code> <code>image_stack_loader</code> <code>Optional[ImageStackLoader]</code> <p>The image stack loader.</p> <code>None</code> <code>image_stack_loader_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>The image stack loader kwargs.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions. Only used for <code>custom</code> data types (see DataModel).</p> <code>\"\"</code> <code>val_percentage</code> <code>Optional[float]</code> <p>Percentage of the training data to use for validation. Only used if <code>val_data</code> is None.</p> <code>None</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Load data in memory dataset if possible, by default True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>config</code> <code>DataConfig</code> <p>Pydantic model for CAREamics data configuration.</p> <code>data_type</code> <code>str</code> <p>Type of data, one of SupportedData.</p> <code>batch_size</code> <code>int</code> <p>Batch size for the dataloaders.</p> <code>use_in_memory</code> <code>bool</code> <p>Whether to load data in memory if possible.</p> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\".</p> <code>read_source_func</code> <code>Optional[Callable], default=None</code> <p>Function to read the source data.</p> <code>read_kwargs</code> <code>Optional[dict[str, Any]], default=None</code> <p>The kwargs for the read source function.</p> <code>val_percentage</code> <code>Optional[float]</code> <p>Percentage of the training data to use for validation.</p> <code>val_minimum_split</code> <code>int, default=5</code> <p>Minimum number of patches or files to split from the training data for validation.</p> <code>train_data</code> <code>Optional[Any]</code> <p>Training data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>train_data_target</code> <code>Optional[Any]</code> <p>Training data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>train_data_mask</code> <code>Optional[Any]</code> <p>Training data mask, can be a path to a folder, a list of paths, or a numpy array.</p> <code>val_data</code> <code>Optional[Any]</code> <p>Validation data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>val_data_target</code> <code>Optional[Any]</code> <p>Validation data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>pred_data</code> <code>Optional[Any]</code> <p>Prediction data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>pred_data_target</code> <code>Optional[Any]</code> <p>Prediction data target, can be a path to a folder, a list of paths, or a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If at least one of train_data, val_data or pred_data is not provided.</p> <code>ValueError</code> <p>If input and target data types are not consistent.</p> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>class CareamicsDataModule(L.LightningDataModule):\n    \"\"\"Data module for Careamics dataset.\n\n    Parameters\n    ----------\n    data_config : DataConfig\n        Pydantic model for CAREamics data configuration.\n    train_data : Optional[InputType]\n        Training data, can be a path to a folder, a list of paths, or a numpy array.\n    train_data_target : Optional[InputType]\n        Training data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    train_data_mask : InputType (when filtering is needed)\n        Training data mask, can be a path to a folder,\n        a list of paths, or a numpy array. Used for coordinate filtering.\n        Only required when using coordinate-based patch filtering.\n    val_data : Optional[InputType]\n        Validation data, can be a path to a folder,\n        a list of paths, or a numpy array.\n    val_data_target : Optional[InputType]\n        Validation data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    pred_data : Optional[InputType]\n        Prediction data, can be a path to a folder, a list of paths,\n        or a numpy array.\n    pred_data_target : Optional[InputType]\n        Prediction data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    read_source_func : Optional[Callable], default=None\n        Function to read the source data. Only used for `custom`\n        data type (see DataModel).\n    read_kwargs : Optional[dict[str, Any]]\n        The kwargs for the read source function.\n    image_stack_loader : Optional[ImageStackLoader]\n        The image stack loader.\n    image_stack_loader_kwargs : Optional[dict[str, Any]]\n        The image stack loader kwargs.\n    extension_filter : str, default=\"\"\n        Filter for file extensions. Only used for `custom` data types\n        (see DataModel).\n    val_percentage : Optional[float]\n        Percentage of the training data to use for validation. Only\n        used if `val_data` is None.\n    val_minimum_split : int, default=5\n        Minimum number of patches or files to split from the training data for\n        validation. Only used if `val_data` is None.\n    use_in_memory : bool\n        Load data in memory dataset if possible, by default True.\n\n\n    Attributes\n    ----------\n    config : DataConfig\n        Pydantic model for CAREamics data configuration.\n    data_type : str\n        Type of data, one of SupportedData.\n    batch_size : int\n        Batch size for the dataloaders.\n    use_in_memory : bool\n        Whether to load data in memory if possible.\n    extension_filter : str\n        Filter for file extensions, by default \"\".\n    read_source_func : Optional[Callable], default=None\n        Function to read the source data.\n    read_kwargs : Optional[dict[str, Any]], default=None\n        The kwargs for the read source function.\n    val_percentage : Optional[float]\n        Percentage of the training data to use for validation.\n    val_minimum_split : int, default=5\n        Minimum number of patches or files to split from the training data for\n        validation.\n    train_data : Optional[Any]\n        Training data, can be a path to a folder, a list of paths, or a numpy array.\n    train_data_target : Optional[Any]\n        Training data target, can be a path to a folder, a list of paths, or a numpy\n        array.\n    train_data_mask : Optional[Any]\n        Training data mask, can be a path to a folder, a list of paths, or a numpy\n        array.\n    val_data : Optional[Any]\n        Validation data, can be a path to a folder, a list of paths, or a numpy array.\n    val_data_target : Optional[Any]\n        Validation data target, can be a path to a folder, a list of paths, or a numpy\n        array.\n    pred_data : Optional[Any]\n        Prediction data, can be a path to a folder, a list of paths, or a numpy array.\n    pred_data_target : Optional[Any]\n        Prediction data target, can be a path to a folder, a list of paths, or a numpy\n        array.\n\n    Raises\n    ------\n    ValueError\n        If at least one of train_data, val_data or pred_data is not provided.\n    ValueError\n        If input and target data types are not consistent.\n    \"\"\"\n\n    # standard use (no mask)\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: InputType | None = None,\n        train_data_target: InputType | None = None,\n        val_data: InputType | None = None,\n        val_data_target: InputType | None = None,\n        pred_data: InputType | None = None,\n        pred_data_target: InputType | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    # with training mask for filtering\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: InputType | None = None,\n        train_data_target: InputType | None = None,\n        train_data_mask: InputType,\n        val_data: InputType | None = None,\n        val_data_target: InputType | None = None,\n        pred_data: InputType | None = None,\n        pred_data_target: InputType | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    # custom read function (no mask)\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: InputType | None = None,\n        train_data_target: InputType | None = None,\n        val_data: InputType | None = None,\n        val_data_target: InputType | None = None,\n        pred_data: InputType | None = None,\n        pred_data_target: InputType | None = None,\n        read_source_func: Callable,\n        read_kwargs: dict[str, Any] | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    # custom read function with training mask\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: InputType | None = None,\n        train_data_target: InputType | None = None,\n        train_data_mask: InputType,\n        val_data: InputType | None = None,\n        val_data_target: InputType | None = None,\n        pred_data: InputType | None = None,\n        pred_data_target: InputType | None = None,\n        read_source_func: Callable,\n        read_kwargs: dict[str, Any] | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    # image stack loader (no mask)\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: Any | None = None,\n        train_data_target: Any | None = None,\n        val_data: Any | None = None,\n        val_data_target: Any | None = None,\n        pred_data: Any | None = None,\n        pred_data_target: Any | None = None,\n        image_stack_loader: ImageStackLoader,\n        image_stack_loader_kwargs: dict[str, Any] | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    # image stack loader with training mask\n    @overload\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: Any | None = None,\n        train_data_target: Any | None = None,\n        train_data_mask: Any,\n        val_data: Any | None = None,\n        val_data_target: Any | None = None,\n        pred_data: Any | None = None,\n        pred_data_target: Any | None = None,\n        image_stack_loader: ImageStackLoader,\n        image_stack_loader_kwargs: dict[str, Any] | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None: ...\n\n    def __init__(\n        self,\n        data_config: NGDataConfig,\n        *,\n        train_data: Any | None = None,\n        train_data_target: Any | None = None,\n        train_data_mask: Any | None = None,\n        val_data: Any | None = None,\n        val_data_target: Any | None = None,\n        pred_data: Any | None = None,\n        pred_data_target: Any | None = None,\n        read_source_func: Callable | None = None,\n        read_kwargs: dict[str, Any] | None = None,\n        image_stack_loader: ImageStackLoader | None = None,\n        image_stack_loader_kwargs: dict[str, Any] | None = None,\n        extension_filter: str = \"\",\n        val_percentage: float | None = None,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Data module for Careamics dataset initialization.\n\n        Create a lightning datamodule that handles creating datasets for training,\n        validation, and prediction.\n\n        Parameters\n        ----------\n        data_config : NGDataConfig\n            Pydantic model for CAREamics data configuration.\n        train_data : Optional[InputType]\n            Training data, can be a path to a folder, a list of paths, or a numpy array.\n        train_data_target : Optional[InputType]\n            Training data target, can be a path to a folder,\n            a list of paths, or a numpy array.\n        train_data_mask : InputType (when filtering is needed)\n            Training data mask, can be a path to a folder,\n            a list of paths, or a numpy array. Used for coordinate filtering.\n            Only required when using coordinate-based patch filtering.\n        val_data : Optional[InputType]\n            Validation data, can be a path to a folder,\n            a list of paths, or a numpy array.\n        val_data_target : Optional[InputType]\n            Validation data target, can be a path to a folder,\n            a list of paths, or a numpy array.\n        pred_data : Optional[InputType]\n            Prediction data, can be a path to a folder, a list of paths,\n            or a numpy array.\n        pred_data_target : Optional[InputType]\n            Prediction data target, can be a path to a folder,\n            a list of paths, or a numpy array.\n        read_source_func : Optional[Callable]\n            Function to read the source data, by default None. Only used for `custom`\n            data type (see DataModel).\n        read_kwargs : Optional[dict[str, Any]]\n            The kwargs for the read source function.\n        image_stack_loader : Optional[ImageStackLoader]\n            The image stack loader.\n        image_stack_loader_kwargs : Optional[dict[str, Any]]\n            The image stack loader kwargs.\n        extension_filter : str\n            Filter for file extensions, by default \"\". Only used for `custom` data types\n            (see DataModel).\n        val_percentage : Optional[float]\n            Percentage of the training data to use for validation. Only\n            used if `val_data` is None.\n        val_minimum_split : int\n            Minimum number of patches or files to split from the training data for\n            validation, by default 5. Only used if `val_data` is None.\n        use_in_memory : bool\n            Load data in memory dataset if possible, by default True.\n        \"\"\"\n        super().__init__()\n\n        if train_data is None and val_data is None and pred_data is None:\n            raise ValueError(\n                \"At least one of train_data, val_data or pred_data must be provided.\"\n            )\n\n        self.config: NGDataConfig = data_config\n        self.data_type: str = data_config.data_type\n        self.batch_size: int = data_config.batch_size\n        self.use_in_memory: bool = use_in_memory\n        self.extension_filter: str = extension_filter\n        self.read_source_func = read_source_func\n        self.read_kwargs = read_kwargs\n        self.image_stack_loader = image_stack_loader\n        self.image_stack_loader_kwargs = image_stack_loader_kwargs\n\n        # TODO: implement the validation split logic\n        self.val_percentage = val_percentage\n        self.val_minimum_split = val_minimum_split\n        if self.val_percentage is not None:\n            raise NotImplementedError(\"Validation split not implemented\")\n\n        self.train_data, self.train_data_target = self._initialize_data_pair(\n            train_data, train_data_target\n        )\n        self.train_data_mask, _ = self._initialize_data_pair(train_data_mask, None)\n\n        self.val_data, self.val_data_target = self._initialize_data_pair(\n            val_data, val_data_target\n        )\n\n        # The pred_data_target can be needed to count metrics on the prediction\n        self.pred_data, self.pred_data_target = self._initialize_data_pair(\n            pred_data, pred_data_target\n        )\n\n    def _validate_input_target_type_consistency(\n        self,\n        input_data: InputType,\n        target_data: InputType | None,\n    ) -&gt; None:\n        \"\"\"Validate if the input and target data types are consistent.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n        \"\"\"\n        if input_data is not None and target_data is not None:\n            if not isinstance(input_data, type(target_data)):\n                raise ValueError(\n                    f\"Inputs for input and target must be of the same type or None. \"\n                    f\"Got {type(input_data)} and {type(target_data)}.\"\n                )\n        if isinstance(input_data, list) and isinstance(target_data, list):\n            if len(input_data) != len(target_data):\n                raise ValueError(\n                    f\"Inputs and targets must have the same length. \"\n                    f\"Got {len(input_data)} and {len(target_data)}.\"\n                )\n            if not isinstance(input_data[0], type(target_data[0])):\n                raise ValueError(\n                    f\"Inputs and targets must have the same type. \"\n                    f\"Got {type(input_data[0])} and {type(target_data[0])}.\"\n                )\n\n    def _list_files_in_directory(\n        self,\n        input_data,\n        target_data=None,\n    ) -&gt; tuple[list[Path], list[Path] | None]:\n        \"\"\"List files from input and target directories.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (list[Path], Optional[list[Path]])\n            A tuple containing lists of file paths for input and target data.\n            If target_data is None, the second element will be None.\n        \"\"\"\n        input_data = Path(input_data)\n        input_files = list_files(input_data, self.data_type, self.extension_filter)\n        if target_data is None:\n            return input_files, None\n        else:\n            target_data = Path(target_data)\n            target_files = list_files(\n                target_data, self.data_type, self.extension_filter\n            )\n            validate_source_target_files(input_files, target_files)\n            return input_files, target_files\n\n    def _convert_paths_to_pathlib(\n        self,\n        input_data,\n        target_data=None,\n    ) -&gt; tuple[list[Path], list[Path] | None]:\n        \"\"\"Create a list of file paths from the input and target data.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (list[Path], Optional[list[Path]])\n            A tuple containing lists of file paths for input and target data.\n            If target_data is None, the second element will be None.\n        \"\"\"\n        input_files = [\n            Path(item) if isinstance(item, str) else item for item in input_data\n        ]\n        if target_data is None:\n            return input_files, None\n        else:\n            target_files = [\n                Path(item) if isinstance(item, str) else item for item in target_data\n            ]\n            validate_source_target_files(input_files, target_files)\n            return input_files, target_files\n\n    def _validate_array_input(\n        self,\n        input_data: InputType,\n        target_data: InputType | None,\n    ) -&gt; tuple[Any, Any]:\n        \"\"\"Validate if the input data is a numpy array.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (Any, Any)\n            A tuple containing the input and target.\n        \"\"\"\n        if isinstance(input_data, np.ndarray):\n            input_array = [input_data]\n            target_array = [target_data] if target_data is not None else None\n            return input_array, target_array\n        elif isinstance(input_data, list):\n            return input_data, target_data\n        else:\n            raise ValueError(\n                f\"Unsupported input type for {self.data_type}: {type(input_data)}\"\n            )\n\n    def _validate_path_input(\n        self, input_data: InputType, target_data: InputType | None\n    ) -&gt; tuple[list[Path], list[Path] | None]:\n        \"\"\"Validate if the input data is a path or a list of paths.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (list[Path], Optional[list[Path]])\n            A tuple containing lists of file paths for input and target data.\n            If target_data is None, the second element will be None.\n        \"\"\"\n        if isinstance(input_data, str | Path):\n            if target_data is not None:\n                assert isinstance(target_data, str | Path)\n            input_list, target_list = self._list_files_in_directory(\n                input_data, target_data\n            )\n            return input_list, target_list\n        elif isinstance(input_data, list):\n            if target_data is not None:\n                assert isinstance(target_data, list)\n            input_list, target_list = self._convert_paths_to_pathlib(\n                input_data, target_data\n            )\n            return input_list, target_list\n        else:\n            raise ValueError(\n                f\"Unsupported input type for {self.data_type}: {type(input_data)}\"\n            )\n\n    def _validate_custom_input(self, input_data, target_data) -&gt; tuple[Any, Any]:\n        \"\"\"Convert custom input data to a list of file paths.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be a path to a folder, a list of paths, or a numpy array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (Any, Any)\n            A tuple containing lists of file paths for input and target data.\n            If target_data is None, the second element will be None.\n        \"\"\"\n        if self.image_stack_loader is not None:\n            return input_data, target_data\n        elif isinstance(input_data, str | Path):\n            if target_data is not None:\n                assert isinstance(target_data, str | Path)\n            input_list, target_list = self._list_files_in_directory(\n                input_data, target_data\n            )\n            return input_list, target_list\n        elif isinstance(input_data, list):\n            if isinstance(input_data[0], str | Path):\n                if target_data is not None:\n                    assert isinstance(target_data, list)\n                input_list, target_list = self._convert_paths_to_pathlib(\n                    input_data, target_data\n                )\n                return input_list, target_list\n        else:\n            raise ValueError(\n                f\"If using {self.data_type}, pass a custom \"\n                f\"image_stack_loader or read_source_func\"\n            )\n        return input_data, target_data\n\n    def _initialize_data_pair(\n        self,\n        input_data: InputType | None,\n        target_data: InputType | None,\n    ) -&gt; tuple[Any, Any]:\n        \"\"\"\n        Initialize a pair of input and target data.\n\n        Parameters\n        ----------\n        input_data : InputType\n            Input data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n        target_data : Optional[InputType]\n            Target data, can be None, a path to a folder, a list of paths, or a numpy\n            array.\n\n        Returns\n        -------\n        (list of numpy.ndarray or list of pathlib.Path, None or list of numpy.ndarray or\n        list of pathlib.Path)\n            A tuple containing the initialized input and target data. For file paths,\n            returns lists of Path objects. For numpy arrays, returns the arrays\n            directly.\n        \"\"\"\n        if input_data is None:\n            return None, None\n\n        self._validate_input_target_type_consistency(input_data, target_data)\n\n        if self.data_type == SupportedData.ARRAY:\n            if isinstance(input_data, np.ndarray):\n                return self._validate_array_input(input_data, target_data)\n            elif isinstance(input_data, list):\n                if isinstance(input_data[0], np.ndarray):\n                    return self._validate_array_input(input_data, target_data)\n                else:\n                    raise ValueError(\n                        f\"Unsupported input type for {self.data_type}: \"\n                        f\"{type(input_data[0])}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unsupported input type for {self.data_type}: {type(input_data)}\"\n                )\n        elif self.data_type in (SupportedData.TIFF, SupportedData.CZI):\n            if isinstance(input_data, str | Path):\n                return self._validate_path_input(input_data, target_data)\n            elif isinstance(input_data, list):\n                if isinstance(input_data[0], str | Path):\n                    return self._validate_path_input(input_data, target_data)\n                else:\n                    raise ValueError(\n                        f\"Unsupported input type for {self.data_type}: \"\n                        f\"{type(input_data[0])}\"\n                    )\n            else:\n                raise ValueError(\n                    f\"Unsupported input type for {self.data_type}: {type(input_data)}\"\n                )\n        elif self.data_type == SupportedData.CUSTOM:\n            return self._validate_custom_input(input_data, target_data)\n        else:\n            raise NotImplementedError(f\"Unsupported data type: {self.data_type}\")\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"\n        Setup datasets.\n\n        Lightning hook that is called at the beginning of fit (train + validate),\n        validate, test, or predict. Creates the datasets for a given stage.\n\n        Parameters\n        ----------\n        stage : str\n            The stage to set up datasets for.\n            Is either 'fit', 'validate', 'test', or 'predict'.\n\n        Raises\n        ------\n        NotImplementedError\n            If stage is not one of \"fit\", \"validate\" or \"predict\".\n        \"\"\"\n        if stage == \"fit\":\n            self.train_dataset = create_dataset(\n                mode=Mode.TRAINING,\n                inputs=self.train_data,\n                targets=self.train_data_target,\n                masks=self.train_data_mask,\n                config=self.config,\n                in_memory=self.use_in_memory,\n                read_func=self.read_source_func,\n                read_kwargs=self.read_kwargs,\n                image_stack_loader=self.image_stack_loader,\n                image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n            )\n            # TODO: ugly, need to find a better solution\n            self.stats = self.train_dataset.input_stats\n            self.config.set_means_and_stds(\n                self.train_dataset.input_stats.means,\n                self.train_dataset.input_stats.stds,\n                self.train_dataset.target_stats.means,\n                self.train_dataset.target_stats.stds,\n            )\n            self.val_dataset = create_dataset(\n                mode=Mode.VALIDATING,\n                inputs=self.val_data,\n                targets=self.val_data_target,\n                config=self.config,\n                in_memory=self.use_in_memory,\n                read_func=self.read_source_func,\n                read_kwargs=self.read_kwargs,\n                image_stack_loader=self.image_stack_loader,\n                image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n            )\n        elif stage == \"validate\":\n            self.val_dataset = create_dataset(\n                mode=Mode.VALIDATING,\n                inputs=self.val_data,\n                targets=self.val_data_target,\n                config=self.config,\n                in_memory=self.use_in_memory,\n                read_func=self.read_source_func,\n                read_kwargs=self.read_kwargs,\n                image_stack_loader=self.image_stack_loader,\n                image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n            )\n            self.stats = self.val_dataset.input_stats\n        elif stage == \"predict\":\n            self.predict_dataset = create_dataset(\n                mode=Mode.PREDICTING,\n                inputs=self.pred_data,\n                targets=self.pred_data_target,\n                config=self.config,\n                in_memory=self.use_in_memory,\n                read_func=self.read_source_func,\n                read_kwargs=self.read_kwargs,\n                image_stack_loader=self.image_stack_loader,\n                image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n            )\n            self.stats = self.predict_dataset.input_stats\n        else:\n            raise NotImplementedError(f\"Stage {stage} not implemented\")\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for training.\n\n        Returns\n        -------\n        DataLoader\n            Training dataloader.\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            collate_fn=default_collate,\n            **self.config.train_dataloader_params,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for validation.\n\n        Returns\n        -------\n        DataLoader\n            Validation dataloader.\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            collate_fn=default_collate,\n            **self.config.val_dataloader_params,\n        )\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for prediction.\n\n        Returns\n        -------\n        DataLoader\n            Prediction dataloader.\n        \"\"\"\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.batch_size,\n            collate_fn=default_collate,\n            **self.config.test_dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule.__init__","title":"<code>__init__(data_config, *, train_data=None, train_data_target=None, train_data_mask=None, val_data=None, val_data_target=None, pred_data=None, pred_data_target=None, read_source_func=None, read_kwargs=None, image_stack_loader=None, image_stack_loader_kwargs=None, extension_filter='', val_percentage=None, val_minimum_split=5, use_in_memory=True)</code>","text":"<pre><code>__init__(data_config: NGDataConfig, *, train_data: InputType | None = None, train_data_target: InputType | None = None, val_data: InputType | None = None, val_data_target: InputType | None = None, pred_data: InputType | None = None, pred_data_target: InputType | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre><pre><code>__init__(data_config: NGDataConfig, *, train_data: InputType | None = None, train_data_target: InputType | None = None, train_data_mask: InputType, val_data: InputType | None = None, val_data_target: InputType | None = None, pred_data: InputType | None = None, pred_data_target: InputType | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre><pre><code>__init__(data_config: NGDataConfig, *, train_data: InputType | None = None, train_data_target: InputType | None = None, val_data: InputType | None = None, val_data_target: InputType | None = None, pred_data: InputType | None = None, pred_data_target: InputType | None = None, read_source_func: Callable, read_kwargs: dict[str, Any] | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre><pre><code>__init__(data_config: NGDataConfig, *, train_data: InputType | None = None, train_data_target: InputType | None = None, train_data_mask: InputType, val_data: InputType | None = None, val_data_target: InputType | None = None, pred_data: InputType | None = None, pred_data_target: InputType | None = None, read_source_func: Callable, read_kwargs: dict[str, Any] | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre><pre><code>__init__(data_config: NGDataConfig, *, train_data: Any | None = None, train_data_target: Any | None = None, val_data: Any | None = None, val_data_target: Any | None = None, pred_data: Any | None = None, pred_data_target: Any | None = None, image_stack_loader: ImageStackLoader, image_stack_loader_kwargs: dict[str, Any] | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre><pre><code>__init__(data_config: NGDataConfig, *, train_data: Any | None = None, train_data_target: Any | None = None, train_data_mask: Any, val_data: Any | None = None, val_data_target: Any | None = None, pred_data: Any | None = None, pred_data_target: Any | None = None, image_stack_loader: ImageStackLoader, image_stack_loader_kwargs: dict[str, Any] | None = None, extension_filter: str = '', val_percentage: float | None = None, val_minimum_split: int = 5, use_in_memory: bool = True) -&gt; None\n</code></pre> <p>Data module for Careamics dataset initialization.</p> <p>Create a lightning datamodule that handles creating datasets for training, validation, and prediction.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>NGDataConfig</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Optional[InputType]</code> <p>Training data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>train_data_target</code> <code>Optional[InputType]</code> <p>Training data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>train_data_mask</code> <code>InputType (when filtering is needed)</code> <p>Training data mask, can be a path to a folder, a list of paths, or a numpy array. Used for coordinate filtering. Only required when using coordinate-based patch filtering.</p> <code>None</code> <code>val_data</code> <code>Optional[InputType]</code> <p>Validation data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>val_data_target</code> <code>Optional[InputType]</code> <p>Validation data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>pred_data</code> <code>Optional[InputType]</code> <p>Prediction data, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>pred_data_target</code> <code>Optional[InputType]</code> <p>Prediction data target, can be a path to a folder, a list of paths, or a numpy array.</p> <code>None</code> <code>read_source_func</code> <code>Optional[Callable]</code> <p>Function to read the source data, by default None. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>read_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>The kwargs for the read source function.</p> <code>None</code> <code>image_stack_loader</code> <code>Optional[ImageStackLoader]</code> <p>The image stack loader.</p> <code>None</code> <code>image_stack_loader_kwargs</code> <code>Optional[dict[str, Any]]</code> <p>The image stack loader kwargs.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\". Only used for <code>custom</code> data types (see DataModel).</p> <code>''</code> <code>val_percentage</code> <code>Optional[float]</code> <p>Percentage of the training data to use for validation. Only used if <code>val_data</code> is None.</p> <code>None</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, by default 5. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Load data in memory dataset if possible, by default True.</p> <code>True</code> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>def __init__(\n    self,\n    data_config: NGDataConfig,\n    *,\n    train_data: Any | None = None,\n    train_data_target: Any | None = None,\n    train_data_mask: Any | None = None,\n    val_data: Any | None = None,\n    val_data_target: Any | None = None,\n    pred_data: Any | None = None,\n    pred_data_target: Any | None = None,\n    read_source_func: Callable | None = None,\n    read_kwargs: dict[str, Any] | None = None,\n    image_stack_loader: ImageStackLoader | None = None,\n    image_stack_loader_kwargs: dict[str, Any] | None = None,\n    extension_filter: str = \"\",\n    val_percentage: float | None = None,\n    val_minimum_split: int = 5,\n    use_in_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Data module for Careamics dataset initialization.\n\n    Create a lightning datamodule that handles creating datasets for training,\n    validation, and prediction.\n\n    Parameters\n    ----------\n    data_config : NGDataConfig\n        Pydantic model for CAREamics data configuration.\n    train_data : Optional[InputType]\n        Training data, can be a path to a folder, a list of paths, or a numpy array.\n    train_data_target : Optional[InputType]\n        Training data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    train_data_mask : InputType (when filtering is needed)\n        Training data mask, can be a path to a folder,\n        a list of paths, or a numpy array. Used for coordinate filtering.\n        Only required when using coordinate-based patch filtering.\n    val_data : Optional[InputType]\n        Validation data, can be a path to a folder,\n        a list of paths, or a numpy array.\n    val_data_target : Optional[InputType]\n        Validation data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    pred_data : Optional[InputType]\n        Prediction data, can be a path to a folder, a list of paths,\n        or a numpy array.\n    pred_data_target : Optional[InputType]\n        Prediction data target, can be a path to a folder,\n        a list of paths, or a numpy array.\n    read_source_func : Optional[Callable]\n        Function to read the source data, by default None. Only used for `custom`\n        data type (see DataModel).\n    read_kwargs : Optional[dict[str, Any]]\n        The kwargs for the read source function.\n    image_stack_loader : Optional[ImageStackLoader]\n        The image stack loader.\n    image_stack_loader_kwargs : Optional[dict[str, Any]]\n        The image stack loader kwargs.\n    extension_filter : str\n        Filter for file extensions, by default \"\". Only used for `custom` data types\n        (see DataModel).\n    val_percentage : Optional[float]\n        Percentage of the training data to use for validation. Only\n        used if `val_data` is None.\n    val_minimum_split : int\n        Minimum number of patches or files to split from the training data for\n        validation, by default 5. Only used if `val_data` is None.\n    use_in_memory : bool\n        Load data in memory dataset if possible, by default True.\n    \"\"\"\n    super().__init__()\n\n    if train_data is None and val_data is None and pred_data is None:\n        raise ValueError(\n            \"At least one of train_data, val_data or pred_data must be provided.\"\n        )\n\n    self.config: NGDataConfig = data_config\n    self.data_type: str = data_config.data_type\n    self.batch_size: int = data_config.batch_size\n    self.use_in_memory: bool = use_in_memory\n    self.extension_filter: str = extension_filter\n    self.read_source_func = read_source_func\n    self.read_kwargs = read_kwargs\n    self.image_stack_loader = image_stack_loader\n    self.image_stack_loader_kwargs = image_stack_loader_kwargs\n\n    # TODO: implement the validation split logic\n    self.val_percentage = val_percentage\n    self.val_minimum_split = val_minimum_split\n    if self.val_percentage is not None:\n        raise NotImplementedError(\"Validation split not implemented\")\n\n    self.train_data, self.train_data_target = self._initialize_data_pair(\n        train_data, train_data_target\n    )\n    self.train_data_mask, _ = self._initialize_data_pair(train_data_mask, None)\n\n    self.val_data, self.val_data_target = self._initialize_data_pair(\n        val_data, val_data_target\n    )\n\n    # The pred_data_target can be needed to count metrics on the prediction\n    self.pred_data, self.pred_data_target = self._initialize_data_pair(\n        pred_data, pred_data_target\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Prediction dataloader.</p> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for prediction.\n\n    Returns\n    -------\n    DataLoader\n        Prediction dataloader.\n    \"\"\"\n    return DataLoader(\n        self.predict_dataset,\n        batch_size=self.batch_size,\n        collate_fn=default_collate,\n        **self.config.test_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Setup datasets.</p> <p>Lightning hook that is called at the beginning of fit (train + validate), validate, test, or predict. Creates the datasets for a given stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>The stage to set up datasets for. Is either 'fit', 'validate', 'test', or 'predict'.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If stage is not one of \"fit\", \"validate\" or \"predict\".</p> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"\n    Setup datasets.\n\n    Lightning hook that is called at the beginning of fit (train + validate),\n    validate, test, or predict. Creates the datasets for a given stage.\n\n    Parameters\n    ----------\n    stage : str\n        The stage to set up datasets for.\n        Is either 'fit', 'validate', 'test', or 'predict'.\n\n    Raises\n    ------\n    NotImplementedError\n        If stage is not one of \"fit\", \"validate\" or \"predict\".\n    \"\"\"\n    if stage == \"fit\":\n        self.train_dataset = create_dataset(\n            mode=Mode.TRAINING,\n            inputs=self.train_data,\n            targets=self.train_data_target,\n            masks=self.train_data_mask,\n            config=self.config,\n            in_memory=self.use_in_memory,\n            read_func=self.read_source_func,\n            read_kwargs=self.read_kwargs,\n            image_stack_loader=self.image_stack_loader,\n            image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n        )\n        # TODO: ugly, need to find a better solution\n        self.stats = self.train_dataset.input_stats\n        self.config.set_means_and_stds(\n            self.train_dataset.input_stats.means,\n            self.train_dataset.input_stats.stds,\n            self.train_dataset.target_stats.means,\n            self.train_dataset.target_stats.stds,\n        )\n        self.val_dataset = create_dataset(\n            mode=Mode.VALIDATING,\n            inputs=self.val_data,\n            targets=self.val_data_target,\n            config=self.config,\n            in_memory=self.use_in_memory,\n            read_func=self.read_source_func,\n            read_kwargs=self.read_kwargs,\n            image_stack_loader=self.image_stack_loader,\n            image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n        )\n    elif stage == \"validate\":\n        self.val_dataset = create_dataset(\n            mode=Mode.VALIDATING,\n            inputs=self.val_data,\n            targets=self.val_data_target,\n            config=self.config,\n            in_memory=self.use_in_memory,\n            read_func=self.read_source_func,\n            read_kwargs=self.read_kwargs,\n            image_stack_loader=self.image_stack_loader,\n            image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n        )\n        self.stats = self.val_dataset.input_stats\n    elif stage == \"predict\":\n        self.predict_dataset = create_dataset(\n            mode=Mode.PREDICTING,\n            inputs=self.pred_data,\n            targets=self.pred_data_target,\n            config=self.config,\n            in_memory=self.use_in_memory,\n            read_func=self.read_source_func,\n            read_kwargs=self.read_kwargs,\n            image_stack_loader=self.image_stack_loader,\n            image_stack_loader_kwargs=self.image_stack_loader_kwargs,\n        )\n        self.stats = self.predict_dataset.input_stats\n    else:\n        raise NotImplementedError(f\"Stage {stage} not implemented\")\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create a dataloader for training.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Training dataloader.</p> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for training.\n\n    Returns\n    -------\n    DataLoader\n        Training dataloader.\n    \"\"\"\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        collate_fn=default_collate,\n        **self.config.train_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/data_module/#careamics.lightning.dataset_ng.data_module.CareamicsDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create a dataloader for validation.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Validation dataloader.</p> Source code in <code>src/careamics/lightning/dataset_ng/data_module.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for validation.\n\n    Returns\n    -------\n    DataLoader\n        Validation dataloader.\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        collate_fn=default_collate,\n        **self.config.val_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/care_module/","title":"care_module","text":"<p>CARE Lightning DataModule.</p>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/care_module/#careamics.lightning.dataset_ng.lightning_modules.care_module.CAREModule","title":"<code>CAREModule</code>","text":"<p>               Bases: <code>UnetModule</code></p> <p>CAREamics PyTorch Lightning module for CARE algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>CAREAlgorithm or dict</code> <p>Configuration for the CARE algorithm, either as a CAREAlgorithm instance or a dictionary.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/care_module.py</code> <pre><code>class CAREModule(UnetModule):\n    \"\"\"CAREamics PyTorch Lightning module for CARE algorithm.\n\n    Parameters\n    ----------\n    algorithm_config : CAREAlgorithm or dict\n        Configuration for the CARE algorithm, either as a CAREAlgorithm instance or a\n        dictionary.\n    \"\"\"\n\n    def __init__(self, algorithm_config: Union[CAREAlgorithm, dict]) -&gt; None:\n        \"\"\"Instantiate CARE DataModule.\n\n        Parameters\n        ----------\n        algorithm_config : CAREAlgorithm or dict\n            Configuration for the CARE algorithm, either as a CAREAlgorithm instance or\n            a dictionary.\n        \"\"\"\n        super().__init__(algorithm_config)\n        assert isinstance(\n            algorithm_config, CAREAlgorithm | N2NAlgorithm\n        ), \"algorithm_config must be a CAREAlgorithm or a N2NAlgorithm\"\n        loss = algorithm_config.loss\n        if loss == SupportedLoss.MAE:\n            self.loss_func: Callable = mae_loss\n        elif loss == SupportedLoss.MSE:\n            self.loss_func = mse_loss\n        else:\n            raise ValueError(f\"Unsupported loss for Care: {loss}\")\n\n    def training_step(\n        self,\n        batch: tuple[ImageRegionData, ImageRegionData],\n        batch_idx: Any,\n    ) -&gt; Any:\n        \"\"\"Training step for CARE module.\n\n        Parameters\n        ----------\n        batch : (ImageRegionData, ImageRegionData)\n            A tuple containing the input data and the target data.\n        batch_idx : Any\n            The index of the current batch in the training loop.\n\n        Returns\n        -------\n        Any\n            The loss value computed for the current batch.\n        \"\"\"\n        # TODO: add validation to determine if target is initialized\n        x, target = batch[0], batch[1]\n\n        prediction = self.model(x.data)\n        loss = self.loss_func(prediction, target.data)\n\n        self._log_training_stats(loss, batch_size=x.data.shape[0])\n\n        return loss\n\n    def validation_step(\n        self,\n        batch: tuple[ImageRegionData, ImageRegionData],\n        batch_idx: Any,\n    ) -&gt; None:\n        \"\"\"Validation step for CARE module.\n\n        Parameters\n        ----------\n        batch : (ImageRegionData, ImageRegionData)\n            A tuple containing the input data and the target data.\n        batch_idx : Any\n            The index of the current batch in the training loop.\n        \"\"\"\n        x, target = batch[0], batch[1]\n\n        prediction = self.model(x.data)\n        val_loss = self.loss_func(prediction, target.data)\n        self.metrics(prediction, target.data)\n        self._log_validation_stats(val_loss, batch_size=x.data.shape[0])\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/care_module/#careamics.lightning.dataset_ng.lightning_modules.care_module.CAREModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Instantiate CARE DataModule.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>CAREAlgorithm or dict</code> <p>Configuration for the CARE algorithm, either as a CAREAlgorithm instance or a dictionary.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/care_module.py</code> <pre><code>def __init__(self, algorithm_config: Union[CAREAlgorithm, dict]) -&gt; None:\n    \"\"\"Instantiate CARE DataModule.\n\n    Parameters\n    ----------\n    algorithm_config : CAREAlgorithm or dict\n        Configuration for the CARE algorithm, either as a CAREAlgorithm instance or\n        a dictionary.\n    \"\"\"\n    super().__init__(algorithm_config)\n    assert isinstance(\n        algorithm_config, CAREAlgorithm | N2NAlgorithm\n    ), \"algorithm_config must be a CAREAlgorithm or a N2NAlgorithm\"\n    loss = algorithm_config.loss\n    if loss == SupportedLoss.MAE:\n        self.loss_func: Callable = mae_loss\n    elif loss == SupportedLoss.MSE:\n        self.loss_func = mse_loss\n    else:\n        raise ValueError(f\"Unsupported loss for Care: {loss}\")\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/care_module/#careamics.lightning.dataset_ng.lightning_modules.care_module.CAREModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for CARE module.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>(ImageRegionData, ImageRegionData)</code> <p>A tuple containing the input data and the target data.</p> required <code>batch_idx</code> <code>Any</code> <p>The index of the current batch in the training loop.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loss value computed for the current batch.</p> Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/care_module.py</code> <pre><code>def training_step(\n    self,\n    batch: tuple[ImageRegionData, ImageRegionData],\n    batch_idx: Any,\n) -&gt; Any:\n    \"\"\"Training step for CARE module.\n\n    Parameters\n    ----------\n    batch : (ImageRegionData, ImageRegionData)\n        A tuple containing the input data and the target data.\n    batch_idx : Any\n        The index of the current batch in the training loop.\n\n    Returns\n    -------\n    Any\n        The loss value computed for the current batch.\n    \"\"\"\n    # TODO: add validation to determine if target is initialized\n    x, target = batch[0], batch[1]\n\n    prediction = self.model(x.data)\n    loss = self.loss_func(prediction, target.data)\n\n    self._log_training_stats(loss, batch_size=x.data.shape[0])\n\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/care_module/#careamics.lightning.dataset_ng.lightning_modules.care_module.CAREModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for CARE module.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>(ImageRegionData, ImageRegionData)</code> <p>A tuple containing the input data and the target data.</p> required <code>batch_idx</code> <code>Any</code> <p>The index of the current batch in the training loop.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/care_module.py</code> <pre><code>def validation_step(\n    self,\n    batch: tuple[ImageRegionData, ImageRegionData],\n    batch_idx: Any,\n) -&gt; None:\n    \"\"\"Validation step for CARE module.\n\n    Parameters\n    ----------\n    batch : (ImageRegionData, ImageRegionData)\n        A tuple containing the input data and the target data.\n    batch_idx : Any\n        The index of the current batch in the training loop.\n    \"\"\"\n    x, target = batch[0], batch[1]\n\n    prediction = self.model(x.data)\n    val_loss = self.loss_func(prediction, target.data)\n    self.metrics(prediction, target.data)\n    self._log_validation_stats(val_loss, batch_size=x.data.shape[0])\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/n2v_module/","title":"n2v_module","text":"<p>Noise2Void Lightning DataModule.</p>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/n2v_module/#careamics.lightning.dataset_ng.lightning_modules.n2v_module.N2VModule","title":"<code>N2VModule</code>","text":"<p>               Bases: <code>UnetModule</code></p> <p>CAREamics PyTorch Lightning module for N2V algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>N2VAlgorithm or dict</code> <p>Configuration for the N2V algorithm, either as an N2VAlgorithm instance or a dictionary.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/n2v_module.py</code> <pre><code>class N2VModule(UnetModule):\n    \"\"\"CAREamics PyTorch Lightning module for N2V algorithm.\n\n    Parameters\n    ----------\n    algorithm_config : N2VAlgorithm or dict\n        Configuration for the N2V algorithm, either as an N2VAlgorithm instance or a\n        dictionary.\n    \"\"\"\n\n    def __init__(self, algorithm_config: Union[N2VAlgorithm, dict]) -&gt; None:\n        \"\"\"Instantiate N2V DataModule.\n\n        Parameters\n        ----------\n        algorithm_config : N2VAlgorithm or dict\n            Configuration for the N2V algorithm, either as an N2VAlgorithm instance or a\n            dictionary.\n        \"\"\"\n        super().__init__(algorithm_config)\n\n        assert isinstance(\n            algorithm_config, N2VAlgorithm\n        ), \"algorithm_config must be a N2VAlgorithm\"\n\n        self.n2v_manipulate = N2VManipulateTorch(\n            n2v_manipulate_config=algorithm_config.n2v_config\n        )\n        self.loss_func = n2v_loss\n\n    def _load_best_checkpoint(self) -&gt; None:\n        \"\"\"Load the best checkpoint for N2V model.\"\"\"\n        logger.warning(\n            \"Loading best checkpoint for N2V model. Note that for N2V, \"\n            \"the checkpoint with the best validation metrics may not necessarily \"\n            \"have the best denoising performance.\"\n        )\n        super()._load_best_checkpoint()\n\n    def training_step(\n        self,\n        batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n        batch_idx: Any,\n    ) -&gt; Any:\n        \"\"\"Training step for N2V model.\n\n        Parameters\n        ----------\n        batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n            A tuple containing the input data and the target data.\n        batch_idx : Any\n            The index of the current batch in the training loop.\n\n        Returns\n        -------\n        Any\n            The loss value for the current training step.\n        \"\"\"\n        x = batch[0]\n        x_masked, x_original, mask = self.n2v_manipulate(x.data)\n        prediction = self.model(x_masked)\n        loss = self.loss_func(prediction, x_original, mask)\n\n        self._log_training_stats(loss, batch_size=x.data.shape[0])\n\n        return loss\n\n    def validation_step(\n        self,\n        batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n        batch_idx: Any,\n    ) -&gt; None:\n        \"\"\"Validation step for N2V model.\n\n        Parameters\n        ----------\n        batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n            A tuple containing the input data and the target data.\n        batch_idx : Any\n            The index of the current batch in the training loop.\n        \"\"\"\n        x = batch[0]\n\n        x_masked, x_original, mask = self.n2v_manipulate(x.data)\n        prediction = self.model(x_masked)\n\n        val_loss = self.loss_func(prediction, x_original, mask)\n        self.metrics(prediction, x_original)\n        self._log_validation_stats(val_loss, batch_size=x.data.shape[0])\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/n2v_module/#careamics.lightning.dataset_ng.lightning_modules.n2v_module.N2VModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Instantiate N2V DataModule.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>N2VAlgorithm or dict</code> <p>Configuration for the N2V algorithm, either as an N2VAlgorithm instance or a dictionary.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/n2v_module.py</code> <pre><code>def __init__(self, algorithm_config: Union[N2VAlgorithm, dict]) -&gt; None:\n    \"\"\"Instantiate N2V DataModule.\n\n    Parameters\n    ----------\n    algorithm_config : N2VAlgorithm or dict\n        Configuration for the N2V algorithm, either as an N2VAlgorithm instance or a\n        dictionary.\n    \"\"\"\n    super().__init__(algorithm_config)\n\n    assert isinstance(\n        algorithm_config, N2VAlgorithm\n    ), \"algorithm_config must be a N2VAlgorithm\"\n\n    self.n2v_manipulate = N2VManipulateTorch(\n        n2v_manipulate_config=algorithm_config.n2v_config\n    )\n    self.loss_func = n2v_loss\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/n2v_module/#careamics.lightning.dataset_ng.lightning_modules.n2v_module.N2VModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step for N2V model.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ImageRegionData or (ImageRegionData, ImageRegionData)</code> <p>A tuple containing the input data and the target data.</p> required <code>batch_idx</code> <code>Any</code> <p>The index of the current batch in the training loop.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loss value for the current training step.</p> Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/n2v_module.py</code> <pre><code>def training_step(\n    self,\n    batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n    batch_idx: Any,\n) -&gt; Any:\n    \"\"\"Training step for N2V model.\n\n    Parameters\n    ----------\n    batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n        A tuple containing the input data and the target data.\n    batch_idx : Any\n        The index of the current batch in the training loop.\n\n    Returns\n    -------\n    Any\n        The loss value for the current training step.\n    \"\"\"\n    x = batch[0]\n    x_masked, x_original, mask = self.n2v_manipulate(x.data)\n    prediction = self.model(x_masked)\n    loss = self.loss_func(prediction, x_original, mask)\n\n    self._log_training_stats(loss, batch_size=x.data.shape[0])\n\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/n2v_module/#careamics.lightning.dataset_ng.lightning_modules.n2v_module.N2VModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step for N2V model.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ImageRegionData or (ImageRegionData, ImageRegionData)</code> <p>A tuple containing the input data and the target data.</p> required <code>batch_idx</code> <code>Any</code> <p>The index of the current batch in the training loop.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/n2v_module.py</code> <pre><code>def validation_step(\n    self,\n    batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n    batch_idx: Any,\n) -&gt; None:\n    \"\"\"Validation step for N2V model.\n\n    Parameters\n    ----------\n    batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n        A tuple containing the input data and the target data.\n    batch_idx : Any\n        The index of the current batch in the training loop.\n    \"\"\"\n    x = batch[0]\n\n    x_masked, x_original, mask = self.n2v_manipulate(x.data)\n    prediction = self.model(x_masked)\n\n    val_loss = self.loss_func(prediction, x_original, mask)\n    self.metrics(prediction, x_original)\n    self._log_validation_stats(val_loss, batch_size=x.data.shape[0])\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/","title":"unet_module","text":"<p>Generic UNet Lightning DataModule.</p>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/#careamics.lightning.dataset_ng.lightning_modules.unet_module.UnetModule","title":"<code>UnetModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CAREamics PyTorch Lightning module for UNet based algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, or dict</code> <p>Configuration for the algorithm, either as an instance of a specific algorithm class or a dictionary that can be converted to an algorithm instance.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/unet_module.py</code> <pre><code>class UnetModule(L.LightningModule):\n    \"\"\"CAREamics PyTorch Lightning module for UNet based algorithms.\n\n    Parameters\n    ----------\n    algorithm_config : CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, or dict\n        Configuration for the algorithm, either as an instance of a specific algorithm\n        class or a dictionary that can be converted to an algorithm instance.\n    \"\"\"\n\n    def __init__(\n        self, algorithm_config: Union[CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, dict]\n    ) -&gt; None:\n        \"\"\"Instantiate UNet DataModule.\n\n        Parameters\n        ----------\n        algorithm_config : CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, or dict\n            Configuration for the algorithm, either as an instance of a specific\n            algorithm class or a dictionary that can be converted to an algorithm\n            instance.\n        \"\"\"\n        super().__init__()\n\n        if isinstance(algorithm_config, dict):\n            algorithm_config = algorithm_factory(algorithm_config)\n\n        self.config = algorithm_config\n        self.model: nn.Module = UNet(**algorithm_config.model.model_dump())\n\n        self._best_checkpoint_loaded = False\n\n        # TODO: how to support metric evaluation better\n        self.metrics = MetricCollection(PeakSignalNoiseRatio())\n\n    def forward(self, x: Any) -&gt; Any:\n        \"\"\"Default forward method.\n\n        Parameters\n        ----------\n        x : Any\n            Input data.\n\n        Returns\n        -------\n        Any\n            Output from the model.\n        \"\"\"\n        return self.model(x)\n\n    def _log_training_stats(self, loss: Any, batch_size: Any) -&gt; None:\n        \"\"\"Log training statistics.\n\n        Parameters\n        ----------\n        loss : Any\n            The loss value for the current training step.\n        batch_size : Any\n            The size of the batch used in the current training step.\n        \"\"\"\n        self.log(\n            \"train_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n            batch_size=batch_size,\n        )\n\n        optimizer = self.optimizers()\n        if isinstance(optimizer, list):\n            current_lr = optimizer[0].param_groups[0][\"lr\"]\n        else:\n            current_lr = optimizer.param_groups[0][\"lr\"]\n        self.log(\n            \"learning_rate\",\n            current_lr,\n            on_step=False,\n            on_epoch=True,\n            logger=True,\n            batch_size=batch_size,\n        )\n\n    def _log_validation_stats(self, loss: Any, batch_size: Any) -&gt; None:\n        \"\"\"Log validation statistics.\n\n        Parameters\n        ----------\n        loss : Any\n            The loss value for the current validation step.\n        batch_size : Any\n            The size of the batch used in the current validation step.\n        \"\"\"\n        self.log(\n            \"val_loss\",\n            loss,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n            batch_size=batch_size,\n        )\n        self.log_dict(self.metrics, on_step=False, on_epoch=True, batch_size=batch_size)\n\n    def _load_best_checkpoint(self) -&gt; None:\n        \"\"\"Load the best checkpoint from the trainer's checkpoint callback.\"\"\"\n        if (\n            not hasattr(self.trainer, \"checkpoint_callback\")\n            or self.trainer.checkpoint_callback is None\n        ):\n            logger.warning(\"No checkpoint callback found, cannot load best checkpoint.\")\n            return\n\n        best_model_path = self.trainer.checkpoint_callback.best_model_path\n        if best_model_path and best_model_path != \"\":\n            logger.info(f\"Loading best checkpoint from: {best_model_path}\")\n            model_state = torch.load(best_model_path, weights_only=True)[\"state_dict\"]\n            self.load_state_dict(model_state)\n        else:\n            logger.warning(\"No best checkpoint found.\")\n\n    def predict_step(\n        self,\n        batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n        batch_idx: Any,\n        load_best_checkpoint=False,\n    ) -&gt; Any:\n        \"\"\"Default predict step.\n\n        Parameters\n        ----------\n        batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n            A tuple containing the input data and optionally the target data.\n        batch_idx : Any\n            The index of the current batch in the prediction loop.\n        load_best_checkpoint : bool, default=False\n            Whether to load the best checkpoint before making predictions.\n\n        Returns\n        -------\n        Any\n            The output batch containing the predictions.\n        \"\"\"\n        if self._best_checkpoint_loaded is False and load_best_checkpoint:\n            self._load_best_checkpoint()\n            self._best_checkpoint_loaded = True\n\n        x = batch[0]\n        # TODO: add TTA\n        prediction = self.model(x.data).cpu().numpy()\n\n        means = self._trainer.datamodule.stats.means\n        stds = self._trainer.datamodule.stats.stds\n        denormalize = Denormalize(\n            image_means=means,\n            image_stds=stds,\n        )\n        denormalized_output = denormalize(prediction)\n\n        output_batch = ImageRegionData(\n            data=denormalized_output,\n            source=x.source,\n            data_shape=x.data_shape,\n            dtype=x.dtype,\n            axes=x.axes,\n            region_spec=x.region_spec,\n        )\n        return output_batch\n\n    def configure_optimizers(self) -&gt; Any:\n        \"\"\"Configure optimizers.\n\n        Returns\n        -------\n        Any\n            A dictionary containing the optimizer and learning rate scheduler.\n        \"\"\"\n        optimizer_func = get_optimizer(self.config.optimizer.name)\n        optimizer = optimizer_func(\n            self.model.parameters(), **self.config.optimizer.parameters\n        )\n\n        scheduler_func = get_scheduler(self.config.lr_scheduler.name)\n        scheduler = scheduler_func(optimizer, **self.config.lr_scheduler.parameters)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n        }\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/#careamics.lightning.dataset_ng.lightning_modules.unet_module.UnetModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Instantiate UNet DataModule.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, or dict</code> <p>Configuration for the algorithm, either as an instance of a specific algorithm class or a dictionary that can be converted to an algorithm instance.</p> required Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/unet_module.py</code> <pre><code>def __init__(\n    self, algorithm_config: Union[CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, dict]\n) -&gt; None:\n    \"\"\"Instantiate UNet DataModule.\n\n    Parameters\n    ----------\n    algorithm_config : CAREAlgorithm, N2VAlgorithm, N2NAlgorithm, or dict\n        Configuration for the algorithm, either as an instance of a specific\n        algorithm class or a dictionary that can be converted to an algorithm\n        instance.\n    \"\"\"\n    super().__init__()\n\n    if isinstance(algorithm_config, dict):\n        algorithm_config = algorithm_factory(algorithm_config)\n\n    self.config = algorithm_config\n    self.model: nn.Module = UNet(**algorithm_config.model.model_dump())\n\n    self._best_checkpoint_loaded = False\n\n    # TODO: how to support metric evaluation better\n    self.metrics = MetricCollection(PeakSignalNoiseRatio())\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/#careamics.lightning.dataset_ng.lightning_modules.unet_module.UnetModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A dictionary containing the optimizer and learning rate scheduler.</p> Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/unet_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Any:\n    \"\"\"Configure optimizers.\n\n    Returns\n    -------\n    Any\n        A dictionary containing the optimizer and learning rate scheduler.\n    \"\"\"\n    optimizer_func = get_optimizer(self.config.optimizer.name)\n    optimizer = optimizer_func(\n        self.model.parameters(), **self.config.optimizer.parameters\n    )\n\n    scheduler_func = get_scheduler(self.config.lr_scheduler.name)\n    scheduler = scheduler_func(optimizer, **self.config.lr_scheduler.parameters)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n        \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n    }\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/#careamics.lightning.dataset_ng.lightning_modules.unet_module.UnetModule.forward","title":"<code>forward(x)</code>","text":"<p>Default forward method.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Input data.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Output from the model.</p> Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/unet_module.py</code> <pre><code>def forward(self, x: Any) -&gt; Any:\n    \"\"\"Default forward method.\n\n    Parameters\n    ----------\n    x : Any\n        Input data.\n\n    Returns\n    -------\n    Any\n        Output from the model.\n    \"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"reference/careamics/lightning/dataset_ng/lightning_modules/unet_module/#careamics.lightning.dataset_ng.lightning_modules.unet_module.UnetModule.predict_step","title":"<code>predict_step(batch, batch_idx, load_best_checkpoint=False)</code>","text":"<p>Default predict step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>ImageRegionData or (ImageRegionData, ImageRegionData)</code> <p>A tuple containing the input data and optionally the target data.</p> required <code>batch_idx</code> <code>Any</code> <p>The index of the current batch in the prediction loop.</p> required <code>load_best_checkpoint</code> <code>bool</code> <p>Whether to load the best checkpoint before making predictions.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The output batch containing the predictions.</p> Source code in <code>src/careamics/lightning/dataset_ng/lightning_modules/unet_module.py</code> <pre><code>def predict_step(\n    self,\n    batch: Union[tuple[ImageRegionData], tuple[ImageRegionData, ImageRegionData]],\n    batch_idx: Any,\n    load_best_checkpoint=False,\n) -&gt; Any:\n    \"\"\"Default predict step.\n\n    Parameters\n    ----------\n    batch : ImageRegionData or (ImageRegionData, ImageRegionData)\n        A tuple containing the input data and optionally the target data.\n    batch_idx : Any\n        The index of the current batch in the prediction loop.\n    load_best_checkpoint : bool, default=False\n        Whether to load the best checkpoint before making predictions.\n\n    Returns\n    -------\n    Any\n        The output batch containing the predictions.\n    \"\"\"\n    if self._best_checkpoint_loaded is False and load_best_checkpoint:\n        self._load_best_checkpoint()\n        self._best_checkpoint_loaded = True\n\n    x = batch[0]\n    # TODO: add TTA\n    prediction = self.model(x.data).cpu().numpy()\n\n    means = self._trainer.datamodule.stats.means\n    stds = self._trainer.datamodule.stats.stds\n    denormalize = Denormalize(\n        image_means=means,\n        image_stds=stds,\n    )\n    denormalized_output = denormalize(prediction)\n\n    output_batch = ImageRegionData(\n        data=denormalized_output,\n        source=x.source,\n        data_shape=x.data_shape,\n        dtype=x.dtype,\n        axes=x.axes,\n        region_spec=x.region_spec,\n    )\n    return output_batch\n</code></pre>"},{"location":"reference/careamics/losses/loss_factory/","title":"loss_factory","text":"<p>Loss factory module.</p> <p>This module contains a factory function for creating loss functions.</p>"},{"location":"reference/careamics/losses/loss_factory/#careamics.losses.loss_factory.FCNLossParameters","title":"<code>FCNLossParameters</code>  <code>dataclass</code>","text":"<p>Dataclass for FCN loss.</p> Source code in <code>src/careamics/losses/loss_factory.py</code> <pre><code>@dataclass\nclass FCNLossParameters:\n    \"\"\"Dataclass for FCN loss.\"\"\"\n\n    # TODO check\n    prediction: tensor\n    targets: tensor\n    mask: tensor\n    current_epoch: int\n    loss_weight: float\n</code></pre>"},{"location":"reference/careamics/losses/loss_factory/#careamics.losses.loss_factory.loss_factory","title":"<code>loss_factory(loss)</code>","text":"<p>Return loss function.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Union[SupportedLoss, str]</code> <p>Requested loss.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Loss function.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the loss is unknown.</p> Source code in <code>src/careamics/losses/loss_factory.py</code> <pre><code>def loss_factory(loss: Union[SupportedLoss, str]) -&gt; Callable:\n    \"\"\"Return loss function.\n\n    Parameters\n    ----------\n    loss : Union[SupportedLoss, str]\n        Requested loss.\n\n    Returns\n    -------\n    Callable\n        Loss function.\n\n    Raises\n    ------\n    NotImplementedError\n        If the loss is unknown.\n    \"\"\"\n    if loss == SupportedLoss.N2V:\n        return n2v_loss\n\n    # elif loss_type == SupportedLoss.PN2V:\n    #     return pn2v_loss\n\n    elif loss == SupportedLoss.MAE:\n        return mae_loss\n\n    elif loss == SupportedLoss.MSE:\n        return mse_loss\n\n    elif loss == SupportedLoss.HDN:\n        return hdn_loss\n\n    elif loss == SupportedLoss.MUSPLIT:\n        return musplit_loss\n\n    elif loss == SupportedLoss.DENOISPLIT:\n        return denoisplit_loss\n\n    elif loss == SupportedLoss.DENOISPLIT_MUSPLIT:\n        return denoisplit_musplit_loss\n\n    else:\n        raise NotImplementedError(f\"Loss {loss} is not yet supported.\")\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/","title":"losses","text":"<p>Loss submodule.</p> <p>This submodule contains the various losses used in CAREamics.</p>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.mae_loss","title":"<code>mae_loss(samples, labels, *args)</code>","text":"<p>N2N Loss function described in to J Lehtinen et al 2018.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Tensor</code> <p>Raw patches.</p> required <code>labels</code> <code>Tensor</code> <p>Different subset of noisy patches.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def mae_loss(samples: torch.Tensor, labels: torch.Tensor, *args) -&gt; torch.Tensor:\n    \"\"\"\n    N2N Loss function described in to J Lehtinen et al 2018.\n\n    Parameters\n    ----------\n    samples : torch.Tensor\n        Raw patches.\n    labels : torch.Tensor\n        Different subset of noisy patches.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    loss = L1Loss()\n    return loss(samples, labels)\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.mse_loss","title":"<code>mse_loss(source, target, *args)</code>","text":"<p>Mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>Source patches.</p> required <code>target</code> <code>Tensor</code> <p>Target patches.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def mse_loss(source: torch.Tensor, target: torch.Tensor, *args) -&gt; torch.Tensor:\n    \"\"\"\n    Mean squared error loss.\n\n    Parameters\n    ----------\n    source : torch.Tensor\n        Source patches.\n    target : torch.Tensor\n        Target patches.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    loss = MSELoss()\n    return loss(source, target)\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.n2v_loss","title":"<code>n2v_loss(manipulated_batch, original_batch, masks, *args)</code>","text":"<p>N2V Loss function described in A Krull et al 2018.</p> <p>Parameters:</p> Name Type Description Default <code>manipulated_batch</code> <code>Tensor</code> <p>Batch after manipulation function applied.</p> required <code>original_batch</code> <code>Tensor</code> <p>Original images.</p> required <code>masks</code> <code>Tensor</code> <p>Coordinates of changed pixels.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def n2v_loss(\n    manipulated_batch: torch.Tensor,\n    original_batch: torch.Tensor,\n    masks: torch.Tensor,\n    *args,\n) -&gt; torch.Tensor:\n    \"\"\"\n    N2V Loss function described in A Krull et al 2018.\n\n    Parameters\n    ----------\n    manipulated_batch : torch.Tensor\n        Batch after manipulation function applied.\n    original_batch : torch.Tensor\n        Original images.\n    masks : torch.Tensor\n        Coordinates of changed pixels.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    errors = (original_batch - manipulated_batch) ** 2\n    # Average over pixels and batch\n    loss = torch.sum(errors * masks) / torch.sum(masks)\n    return loss  # TODO change output to dict ?\n</code></pre>"},{"location":"reference/careamics/losses/lvae/loss_utils/","title":"loss_utils","text":""},{"location":"reference/careamics/losses/lvae/loss_utils/#careamics.losses.lvae.loss_utils.free_bits_kl","title":"<code>free_bits_kl(kl, free_bits, batch_average=False, eps=1e-06)</code>","text":"<p>Compute free-bits version of KL divergence.</p> <p>This function ensures that the KL doesn't go to zero for any latent dimension. Hence, it contributes to use latent variables more efficiently, leading to better representation learning.</p> <p>NOTE: Takes in the KL with shape (batch size, layers), returns the KL with free bits (for optimization) with shape (layers,), which is the average free-bits KL per layer in the current batch. If batch_average is False (default), the free bits are per layer and per batch element. Otherwise, the free bits are still per layer, but are assigned on average to the whole batch. In both cases, the batch average is returned, so it's simply a matter of doing mean(clamp(KL)) or clamp(mean(KL)).</p> <p>Parameters:</p> Name Type Description Default <code>kl</code> <code>Tensor</code> <p>The KL divergence tensor with shape (batch size, layers).</p> required <code>free_bits</code> <code>float</code> <p>The free bits value. Set to 0.0 to disable free bits.</p> required <code>batch_average</code> <code>bool</code> <p>Whether to average over the batch before clamping to <code>free_bits</code>.</p> <code>False</code> <code>eps</code> <code>float</code> <p>A small value to avoid numerical instability.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The free-bits version of the KL divergence with shape (layers,).</p> Source code in <code>src/careamics/losses/lvae/loss_utils.py</code> <pre><code>def free_bits_kl(\n    kl: torch.Tensor, free_bits: float, batch_average: bool = False, eps: float = 1e-6\n) -&gt; torch.Tensor:\n    \"\"\"Compute free-bits version of KL divergence.\n\n    This function ensures that the KL doesn't go to zero for any latent dimension.\n    Hence, it contributes to use latent variables more efficiently, leading to\n    better representation learning.\n\n    NOTE:\n    Takes in the KL with shape (batch size, layers), returns the KL with\n    free bits (for optimization) with shape (layers,), which is the average\n    free-bits KL per layer in the current batch.\n    If batch_average is False (default), the free bits are per layer and\n    per batch element. Otherwise, the free bits are still per layer, but\n    are assigned on average to the whole batch. In both cases, the batch\n    average is returned, so it's simply a matter of doing mean(clamp(KL))\n    or clamp(mean(KL)).\n\n    Parameters\n    ----------\n    kl : torch.Tensor\n        The KL divergence tensor with shape (batch size, layers).\n    free_bits : float\n        The free bits value. Set to 0.0 to disable free bits.\n    batch_average : bool\n        Whether to average over the batch before clamping to `free_bits`.\n    eps : float\n        A small value to avoid numerical instability.\n\n    Returns\n    -------\n    torch.Tensor\n        The free-bits version of the KL divergence with shape (layers,).\n    \"\"\"\n    assert kl.dim() == 2\n    if free_bits &lt; eps:\n        return kl.mean(0)\n    if batch_average:\n        return kl.mean(0).clamp(min=free_bits)\n    return kl.clamp(min=free_bits).mean(0)\n</code></pre>"},{"location":"reference/careamics/losses/lvae/loss_utils/#careamics.losses.lvae.loss_utils.get_kl_weight","title":"<code>get_kl_weight(kl_annealing, kl_start, kl_annealtime, kl_weight, current_epoch)</code>","text":"<p>Compute the weight of the KL loss in case of annealing.</p> <p>Parameters:</p> Name Type Description Default <code>kl_annealing</code> <code>bool</code> <p>Whether to use KL annealing.</p> required <code>kl_start</code> <code>int</code> <p>The epoch at which to start</p> required <code>kl_annealtime</code> <code>int</code> <p>The number of epochs for which annealing is applied.</p> required <code>kl_weight</code> <code>float</code> <p>The weight for the KL loss. If <code>None</code>, the weight is computed using annealing, else it is set to a default of 1.</p> required <code>current_epoch</code> <code>int</code> <p>The current epoch.</p> required Source code in <code>src/careamics/losses/lvae/loss_utils.py</code> <pre><code>def get_kl_weight(\n    kl_annealing: bool,\n    kl_start: int,\n    kl_annealtime: int,\n    kl_weight: float,\n    current_epoch: int,\n) -&gt; float:\n    \"\"\"Compute the weight of the KL loss in case of annealing.\n\n    Parameters\n    ----------\n    kl_annealing : bool\n        Whether to use KL annealing.\n    kl_start : int\n        The epoch at which to start\n    kl_annealtime : int\n        The number of epochs for which annealing is applied.\n    kl_weight : float\n        The weight for the KL loss. If `None`, the weight is computed\n        using annealing, else it is set to a default of 1.\n    current_epoch : int\n        The current epoch.\n    \"\"\"\n    if kl_annealing:\n        # calculate relative weight\n        kl_weight = (current_epoch - kl_start) * (1.0 / kl_annealtime)\n        # clamp to [0,1]\n        kl_weight = min(max(0.0, kl_weight), 1.0)\n\n        # if the final weight is given, then apply that weight on top of it\n        if kl_weight is not None:\n            kl_weight = kl_weight * kl_weight\n    elif kl_weight is not None:\n        return kl_weight\n    else:\n        kl_weight = 1.0\n    return kl_weight\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/","title":"losses","text":"<p>Methods for Loss Computation.</p>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.denoisplit_loss","title":"<code>denoisplit_loss(model_outputs, targets, config, gaussian_likelihood=None, noise_model_likelihood=None)</code>","text":"<p>Loss function for DenoiSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function containing all loss hyperparameters.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> <code>None</code> <code>noise_model_likelihood</code> <code>NoiseModelLikelihood</code> <p>The noise model likelihood object.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def denoisplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: GaussianLikelihood | None = None,\n    noise_model_likelihood: NoiseModelLikelihood | None = None,\n) -&gt; dict[str, torch.Tensor] | None:\n    \"\"\"Loss function for DenoiSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function containing all loss hyperparameters.\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : NoiseModelLikelihood\n        The noise model likelihood object.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    assert noise_model_likelihood is not None\n\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = config.reconstruction_weight * get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=noise_model_likelihood,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    kl_weight = get_kl_weight(\n        config.kl_params.annealing,\n        config.kl_params.start,\n        config.kl_params.annealtime,\n        config.kl_weight,\n        config.kl_params.current_epoch,\n    )\n    kl_loss = (\n        _get_kl_divergence_loss_denoisplit(\n            topdown_data=td_data,\n            img_shape=targets.shape[2:],\n            kl_type=config.kl_params.loss_type,\n        )\n        * kl_weight\n    )\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.denoisplit_musplit_loss","title":"<code>denoisplit_musplit_loss(model_outputs, targets, config, gaussian_likelihood, noise_model_likelihood)</code>","text":"<p>Loss function for DenoiSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function containing all loss hyperparameters.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> required <code>noise_model_likelihood</code> <code>NoiseModelLikelihood</code> <p>The noise model likelihood object.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def denoisplit_musplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: GaussianLikelihood,\n    noise_model_likelihood: NoiseModelLikelihood,\n) -&gt; dict[str, torch.Tensor] | None:\n    \"\"\"Loss function for DenoiSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function containing all loss hyperparameters.\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : NoiseModelLikelihood\n        The noise model likelihood object.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = _reconstruction_loss_musplit_denoisplit(\n        predictions=predictions,\n        targets=targets,\n        nm_likelihood=noise_model_likelihood,\n        gaussian_likelihood=gaussian_likelihood,\n        nm_weight=config.denoisplit_weight,\n        gaussian_weight=config.musplit_weight,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    # NOTE: 'kl' key stands for the 'kl_samplewise' key in the TopDownLayer class.\n    # The different naming comes from `top_down_pass()` method in the LadderVAE.\n    denoisplit_kl = _get_kl_divergence_loss_denoisplit(\n        topdown_data=td_data,\n        img_shape=targets.shape[2:],\n        kl_type=config.kl_params.loss_type,\n    )\n    musplit_kl = _get_kl_divergence_loss_musplit(\n        topdown_data=td_data,\n        img_shape=targets.shape[2:],\n        kl_type=config.kl_params.loss_type,\n    )\n    kl_loss = (\n        config.denoisplit_weight * denoisplit_kl + config.musplit_weight * musplit_kl\n    )\n    # TODO `kl_weight` is hardcoded (???)\n    kl_loss = config.kl_weight * kl_loss\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.get_kl_divergence_loss","title":"<code>get_kl_divergence_loss(kl_type, topdown_data, rescaling, aggregation, free_bits_coeff, img_shape=None)</code>","text":"<p>Compute the KL divergence loss.</p> <p>NOTE: Description of <code>rescaling</code> methods: - If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). In this way they have the same magnitude across layers. - If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial dimensions. In this way, the lower layers have a larger KL-loss value compared to the higher layers, since the latent space and hence the KL tensor has more entries. Specifically, at hierarchy <code>i</code>, the total KL loss is larger by a factor (128/i**2).</p> <p>NOTE: the type of <code>aggregation</code> determines the magnitude of the KL-loss. Clearly, \"sum\" aggregation results in a larger KL-loss value compared to \"mean\" by a factor of <code>n_layers</code>.</p> <p>NOTE: recall that sample-wise KL is obtained by summing over all dimensions, including Z. Also recall that in current 3D implementation of LVAE, no downsampling is done on Z. Therefore, to avoid emphasizing KL loss too much, we divide it by the Z dimension of input image in every case.</p> <p>Parameters:</p> Name Type Description Default <code>kl_type</code> <code>Literal['kl', 'kl_restricted']</code> <p>The type of KL divergence loss to compute.</p> required <code>topdown_data</code> <code>dict[str, Tensor]</code> <p>A dictionary containing information computed for each layer during the top-down pass. The dictionary must include the following keys: - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,). - \"z\": The sampled latents for each layer. Shape of each tensor is (B, layers, <code>z_dims[i]</code>, H, W).</p> required <code>rescaling</code> <code>Literal['latent_dim', 'image_dim']</code> <p>The rescaling method used for the KL-loss values. If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial dimensions.</p> required <code>aggregation</code> <code>Literal['mean', 'sum']</code> <p>The aggregation method used to combine the KL-loss values across layers. If \"mean\", the KL-loss values are averaged across layers. If \"sum\", the KL-loss values are summed across layers.</p> required <code>free_bits_coeff</code> <code>float</code> <p>The free bits coefficient used for the KL-loss computation.</p> required <code>img_shape</code> <code>Optional[tuple[int]]</code> <p>The shape of the input image to the LVAE model. Shape is ([Z], Y, X).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>kl_loss</code> <code>Tensor</code> <p>The KL divergence loss. Shape is (1, ).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def get_kl_divergence_loss(\n    kl_type: Literal[\"kl\", \"kl_restricted\"],\n    topdown_data: dict[str, torch.Tensor],\n    rescaling: Literal[\"latent_dim\", \"image_dim\"],\n    aggregation: Literal[\"mean\", \"sum\"],\n    free_bits_coeff: float,\n    img_shape: tuple[int] | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the KL divergence loss.\n\n    NOTE: Description of `rescaling` methods:\n    - If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space\n    dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). In this way they\n    have the same magnitude across layers.\n    - If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial\n    dimensions. In this way, the lower layers have a larger KL-loss value compared to\n    the higher layers, since the latent space and hence the KL tensor has more entries.\n    Specifically, at hierarchy `i`, the total KL loss is larger by a factor (128/i**2).\n\n    NOTE: the type of `aggregation` determines the magnitude of the KL-loss. Clearly,\n    \"sum\" aggregation results in a larger KL-loss value compared to \"mean\" by a factor\n    of `n_layers`.\n\n    NOTE: recall that sample-wise KL is obtained by summing over all dimensions,\n    including Z. Also recall that in current 3D implementation of LVAE, no downsampling\n    is done on Z. Therefore, to avoid emphasizing KL loss too much, we divide it\n    by the Z dimension of input image in every case.\n\n    Parameters\n    ----------\n    kl_type : Literal[\"kl\", \"kl_restricted\"]\n        The type of KL divergence loss to compute.\n    topdown_data : dict[str, torch.Tensor]\n        A dictionary containing information computed for each layer during the top-down\n        pass. The dictionary must include the following keys:\n        - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,).\n        - \"z\": The sampled latents for each layer. Shape of each tensor is\n        (B, layers, `z_dims[i]`, H, W).\n    rescaling : Literal[\"latent_dim\", \"image_dim\"]\n        The rescaling method used for the KL-loss values. If \"latent_dim\", the KL-loss\n        values are rescaled w.r.t. the latent space dimensions (spatial + number of\n        channels, i.e., (C, [Z], Y, X)). If \"image_dim\", the KL-loss values are\n        rescaled w.r.t. the input image spatial dimensions.\n    aggregation : Literal[\"mean\", \"sum\"]\n        The aggregation method used to combine the KL-loss values across layers. If\n        \"mean\", the KL-loss values are averaged across layers. If \"sum\", the KL-loss\n        values are summed across layers.\n    free_bits_coeff : float\n        The free bits coefficient used for the KL-loss computation.\n    img_shape : Optional[tuple[int]]\n        The shape of the input image to the LVAE model. Shape is ([Z], Y, X).\n\n    Returns\n    -------\n    kl_loss : torch.Tensor\n        The KL divergence loss. Shape is (1, ).\n    \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_data[kl_type]],\n        dim=1,\n    )  # shape: (B, n_layers)\n\n    # Apply free bits (&amp; batch average)\n    kl = free_bits_kl(kl, free_bits_coeff)  # shape: (n_layers,)\n\n    # In 3D case, rescale by Z dim\n    # TODO If we have downsampling in Z dimension, then this needs to change.\n    if len(img_shape) == 3:\n        kl = kl / img_shape[0]\n\n    # Rescaling\n    if rescaling == \"latent_dim\":\n        for i in range(len(kl)):\n            latent_dim = topdown_data[\"z\"][i].shape[1:]\n            norm_factor = np.prod(latent_dim)\n            kl[i] = kl[i] / norm_factor\n    elif rescaling == \"image_dim\":\n        kl = kl / np.prod(img_shape[-2:])\n\n    # Aggregation\n    if aggregation == \"mean\":\n        kl = kl.mean()  # shape: (1,)\n    elif aggregation == \"sum\":\n        kl = kl.sum()  # shape: (1,)\n\n    return kl\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.get_reconstruction_loss","title":"<code>get_reconstruction_loss(reconstruction, target, likelihood_obj)</code>","text":"<p>Compute the reconstruction loss (negative log-likelihood).</p> <p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Tensor</code> <p>The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), where C is the number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>target</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, C, [Z], Y, X), where C is the number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>likelihood_obj</code> <code>Likelihood</code> <p>The likelihood object used to compute the reconstruction loss.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The recontruction loss (negative log-likelihood).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def get_reconstruction_loss(\n    reconstruction: torch.Tensor,\n    target: torch.Tensor,\n    likelihood_obj: Likelihood,\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Compute the reconstruction loss (negative log-likelihood).\n\n    Parameters\n    ----------\n    reconstruction: torch.Tensor\n        The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), where C is the\n        number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).\n    target: torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, C, [Z], Y, X), where C is the number of output channels\n        (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).\n    likelihood_obj: Likelihood\n        The likelihood object used to compute the reconstruction loss.\n\n    Returns\n    -------\n    torch.Tensor\n        The recontruction loss (negative log-likelihood).\n    \"\"\"\n    # Compute Log likelihood\n    ll, _ = likelihood_obj(reconstruction, target)  # shape: (B, C, [Z], Y, X)\n    return -1 * ll.mean()\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.hdn_loss","title":"<code>hdn_loss(model_outputs, targets, config, gaussian_likelihood, noise_model_likelihood)</code>","text":"<p>Loss function for HDN.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. In this case we use the input patch itself as target. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function containing all loss hyperparameters.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> required <code>noise_model_likelihood</code> <code>NoiseModelLikelihood</code> <p>The noise model likelihood object.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def hdn_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: GaussianLikelihood | None,\n    noise_model_likelihood: NoiseModelLikelihood | None,\n) -&gt; dict[str, torch.Tensor] | None:\n    \"\"\"Loss function for HDN.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. In this case we use\n        the input patch itself as target. Shape is (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function containing all loss hyperparameters.\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : NoiseModelLikelihood\n        The noise model likelihood object.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    if gaussian_likelihood is not None:\n        likelihood = gaussian_likelihood\n    elif noise_model_likelihood is not None:\n        likelihood = noise_model_likelihood\n    else:\n        raise ValueError(\"Invalid likelihood object.\")\n    # TODO refactor loss signature\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = config.reconstruction_weight * get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=likelihood,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    kl_weight = get_kl_weight(\n        config.kl_params.annealing,\n        config.kl_params.start,\n        config.kl_params.annealtime,\n        config.kl_weight,\n        config.kl_params.current_epoch,\n    )\n    kl_loss = (\n        _get_kl_divergence_loss_denoisplit(\n            topdown_data=td_data,\n            img_shape=targets.shape[2:],\n            kl_type=config.kl_params.loss_type,\n        )\n        * kl_weight\n    )\n\n    net_loss = recons_loss + kl_loss  # TODO add check that losses coefs sum to 1\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.musplit_loss","title":"<code>musplit_loss(model_outputs, targets, config, gaussian_likelihood, noise_model_likelihood=None)</code>","text":"<p>Loss function for muSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function (e.g., KL hyperparameters, likelihood module, noise model, etc.).</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> required <code>noise_model_likelihood</code> <code>Optional[NoiseModelLikelihood]</code> <p>The noise model likelihood object. Not used here.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def musplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: GaussianLikelihood | None,\n    noise_model_likelihood: NoiseModelLikelihood | None = None,  # TODO: ugly\n) -&gt; dict[str, torch.Tensor] | None:\n    \"\"\"Loss function for muSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function (e.g., KL hyperparameters, likelihood module,\n        noise model, etc.).\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : Optional[NoiseModelLikelihood]\n        The noise model likelihood object. Not used here.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    assert gaussian_likelihood is not None\n\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = config.reconstruction_weight * get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=gaussian_likelihood,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    kl_weight = get_kl_weight(\n        config.kl_params.annealing,\n        config.kl_params.start,\n        config.kl_params.annealtime,\n        config.kl_weight,\n        config.kl_params.current_epoch,\n    )\n    kl_loss = (\n        _get_kl_divergence_loss_musplit(\n            topdown_data=td_data,\n            img_shape=targets.shape[2:],\n            kl_type=config.kl_params.loss_type,\n        )\n        * kl_weight\n    )\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/","title":"calibration","text":""},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration","title":"<code>Calibration</code>","text":"<p>Calibrate the uncertainty computed over samples from LVAE model.</p> <p>Calibration is done by learning a scalar that maps the pixel-wise standard deviation of the the predicted samples into the actual prediction error.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>class Calibration:\n    \"\"\"Calibrate the uncertainty computed over samples from LVAE model.\n\n    Calibration is done by learning a scalar that maps the pixel-wise standard\n    deviation of the the predicted samples into the actual prediction error.\n    \"\"\"\n\n    def __init__(self, num_bins: int = 15):\n        self._bins = num_bins\n        self._bin_boundaries = None\n\n    def compute_bin_boundaries(self, predict_std: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute the bin boundaries for `num_bins` bins and predicted std values.\"\"\"\n        min_std = np.min(predict_std)\n        max_std = np.max(predict_std)\n        return np.linspace(min_std, max_std, self._bins + 1)\n\n    def compute_stats(\n        self, pred: np.ndarray, pred_std: np.ndarray, target: np.ndarray\n    ) -&gt; dict[int, dict[str, Union[np.ndarray, list]]]:\n        \"\"\"\n        It computes the bin-wise RMSE and RMV for each channel of the predicted image.\n\n        Recall that:\n            - RMSE = np.sqrt((pred - target)**2 / num_pixels)\n            - RMV = np.sqrt(np.mean(pred_std**2))\n\n        ALGORITHM\n        - For each channel:\n            - Given the bin boundaries, assign pixels of `std_ch` array to a specific bin index.\n            - For each bin index:\n                - Compute the RMSE, RMV, and number of pixels for that bin.\n\n        NOTE: each channel of the predicted image/logvar has its own stats.\n\n        Parameters\n        ----------\n        pred: np.ndarray\n            Predicted patches, shape (n, h, w, c).\n        pred_std: np.ndarray\n            Std computed over the predicted patches, shape (n, h, w, c).\n        target: np.ndarray\n            Target GT image, shape (n, h, w, c).\n        \"\"\"\n        self._bin_boundaries = {}\n        stats_dict = {}\n        for ch_idx in range(pred.shape[-1]):\n            stats_dict[ch_idx] = {\n                \"bin_count\": [],\n                \"rmv\": [],\n                \"rmse\": [],\n                \"bin_boundaries\": None,\n                \"bin_matrix\": [],\n                \"rmse_err\": [],\n            }\n            pred_ch = pred[..., ch_idx]\n            std_ch = pred_std[..., ch_idx]\n            target_ch = target[..., ch_idx]\n            boundaries = self.compute_bin_boundaries(std_ch)\n            stats_dict[ch_idx][\"bin_boundaries\"] = boundaries\n            bin_matrix = np.digitize(std_ch.reshape(-1), boundaries)\n            bin_matrix = bin_matrix.reshape(std_ch.shape)\n            stats_dict[ch_idx][\"bin_matrix\"] = bin_matrix\n            error = (pred_ch - target_ch) ** 2\n            for bin_idx in range(1, 1 + self._bins):\n                bin_mask = bin_matrix == bin_idx\n                bin_error = error[bin_mask]\n                bin_size = np.sum(bin_mask)\n                bin_error = (\n                    np.sqrt(np.sum(bin_error) / bin_size) if bin_size &gt; 0 else None\n                )\n                stderr = (\n                    np.std(error[bin_mask]) / np.sqrt(bin_size)\n                    if bin_size &gt; 0\n                    else None\n                )\n                rmse_stderr = np.sqrt(stderr) if stderr is not None else None\n\n                bin_var = np.mean(std_ch[bin_mask] ** 2)\n                stats_dict[ch_idx][\"rmse\"].append(bin_error)\n                stats_dict[ch_idx][\"rmse_err\"].append(rmse_stderr)\n                stats_dict[ch_idx][\"rmv\"].append(np.sqrt(bin_var))\n                stats_dict[ch_idx][\"bin_count\"].append(bin_size)\n        self.stats_dict = stats_dict\n        return stats_dict\n\n    def get_calibrated_factor_for_stdev(\n        self,\n        pred: Optional[np.ndarray] = None,\n        pred_std: Optional[np.ndarray] = None,\n        target: Optional[np.ndarray] = None,\n        q_s: float = 0.00001,\n        q_e: float = 0.99999,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calibrate the uncertainty by multiplying the predicted std with a scalar.\n\n        Parameters\n        ----------\n        stats_dict : dict[int, dict[str, Union[np.ndarray, list]]]\n            Dictionary containing the stats for each channel.\n        q_s : float, optional\n            Start quantile, by default 0.00001.\n        q_e : float, optional\n            End quantile, by default 0.99999.\n\n        Returns\n        -------\n        dict[str, float]\n            Calibrated factor for each channel (slope + intercept).\n        \"\"\"\n        if not hasattr(self, \"stats_dict\"):\n            print(\"No stats found. Computing stats...\")\n            if any(v is None for v in [pred, pred_std, target]):\n                raise ValueError(\"pred, pred_std, and target must be provided.\")\n            self.stats_dict = self.compute_stats(\n                pred=pred, pred_std=pred_std, target=target\n            )\n        outputs = {}\n        for ch_idx in self.stats_dict.keys():\n            y = self.stats_dict[ch_idx][\"rmse\"]\n            x = self.stats_dict[ch_idx][\"rmv\"]\n            count = self.stats_dict[ch_idx][\"bin_count\"]\n\n            first_idx = get_first_index(count, q_s)\n            last_idx = get_last_index(count, q_e)\n            x = x[first_idx:-last_idx]\n            y = y[first_idx:-last_idx]\n            slope, intercept, *_ = stats.linregress(x, y)\n            output = {\"scalar\": slope, \"offset\": intercept}\n            outputs[ch_idx] = output\n        factors = self.get_factors_array(factors_dict=outputs)\n        return outputs, factors\n\n    def get_factors_array(self, factors_dict: list[dict]):\n        \"\"\"Get the calibration factors as a numpy array.\"\"\"\n        calib_scalar = [factors_dict[i][\"scalar\"] for i in range(len(factors_dict))]\n        calib_scalar = np.array(calib_scalar).reshape(1, 1, 1, -1)\n        calib_offset = [\n            factors_dict[i].get(\"offset\", 0.0) for i in range(len(factors_dict))\n        ]\n        calib_offset = np.array(calib_offset).reshape(1, 1, 1, -1)\n        return {\"scalar\": calib_scalar, \"offset\": calib_offset}\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.compute_bin_boundaries","title":"<code>compute_bin_boundaries(predict_std)</code>","text":"<p>Compute the bin boundaries for <code>num_bins</code> bins and predicted std values.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def compute_bin_boundaries(self, predict_std: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the bin boundaries for `num_bins` bins and predicted std values.\"\"\"\n    min_std = np.min(predict_std)\n    max_std = np.max(predict_std)\n    return np.linspace(min_std, max_std, self._bins + 1)\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.compute_stats","title":"<code>compute_stats(pred, pred_std, target)</code>","text":"<p>It computes the bin-wise RMSE and RMV for each channel of the predicted image.</p> <p>Recall that:     - RMSE = np.sqrt((pred - target)2 / num_pixels)     - RMV = np.sqrt(np.mean(pred_std2))</p> <p>ALGORITHM - For each channel:     - Given the bin boundaries, assign pixels of <code>std_ch</code> array to a specific bin index.     - For each bin index:         - Compute the RMSE, RMV, and number of pixels for that bin.</p> <p>NOTE: each channel of the predicted image/logvar has its own stats.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>ndarray</code> <p>Predicted patches, shape (n, h, w, c).</p> required <code>pred_std</code> <code>ndarray</code> <p>Std computed over the predicted patches, shape (n, h, w, c).</p> required <code>target</code> <code>ndarray</code> <p>Target GT image, shape (n, h, w, c).</p> required Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def compute_stats(\n    self, pred: np.ndarray, pred_std: np.ndarray, target: np.ndarray\n) -&gt; dict[int, dict[str, Union[np.ndarray, list]]]:\n    \"\"\"\n    It computes the bin-wise RMSE and RMV for each channel of the predicted image.\n\n    Recall that:\n        - RMSE = np.sqrt((pred - target)**2 / num_pixels)\n        - RMV = np.sqrt(np.mean(pred_std**2))\n\n    ALGORITHM\n    - For each channel:\n        - Given the bin boundaries, assign pixels of `std_ch` array to a specific bin index.\n        - For each bin index:\n            - Compute the RMSE, RMV, and number of pixels for that bin.\n\n    NOTE: each channel of the predicted image/logvar has its own stats.\n\n    Parameters\n    ----------\n    pred: np.ndarray\n        Predicted patches, shape (n, h, w, c).\n    pred_std: np.ndarray\n        Std computed over the predicted patches, shape (n, h, w, c).\n    target: np.ndarray\n        Target GT image, shape (n, h, w, c).\n    \"\"\"\n    self._bin_boundaries = {}\n    stats_dict = {}\n    for ch_idx in range(pred.shape[-1]):\n        stats_dict[ch_idx] = {\n            \"bin_count\": [],\n            \"rmv\": [],\n            \"rmse\": [],\n            \"bin_boundaries\": None,\n            \"bin_matrix\": [],\n            \"rmse_err\": [],\n        }\n        pred_ch = pred[..., ch_idx]\n        std_ch = pred_std[..., ch_idx]\n        target_ch = target[..., ch_idx]\n        boundaries = self.compute_bin_boundaries(std_ch)\n        stats_dict[ch_idx][\"bin_boundaries\"] = boundaries\n        bin_matrix = np.digitize(std_ch.reshape(-1), boundaries)\n        bin_matrix = bin_matrix.reshape(std_ch.shape)\n        stats_dict[ch_idx][\"bin_matrix\"] = bin_matrix\n        error = (pred_ch - target_ch) ** 2\n        for bin_idx in range(1, 1 + self._bins):\n            bin_mask = bin_matrix == bin_idx\n            bin_error = error[bin_mask]\n            bin_size = np.sum(bin_mask)\n            bin_error = (\n                np.sqrt(np.sum(bin_error) / bin_size) if bin_size &gt; 0 else None\n            )\n            stderr = (\n                np.std(error[bin_mask]) / np.sqrt(bin_size)\n                if bin_size &gt; 0\n                else None\n            )\n            rmse_stderr = np.sqrt(stderr) if stderr is not None else None\n\n            bin_var = np.mean(std_ch[bin_mask] ** 2)\n            stats_dict[ch_idx][\"rmse\"].append(bin_error)\n            stats_dict[ch_idx][\"rmse_err\"].append(rmse_stderr)\n            stats_dict[ch_idx][\"rmv\"].append(np.sqrt(bin_var))\n            stats_dict[ch_idx][\"bin_count\"].append(bin_size)\n    self.stats_dict = stats_dict\n    return stats_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.get_calibrated_factor_for_stdev","title":"<code>get_calibrated_factor_for_stdev(pred=None, pred_std=None, target=None, q_s=1e-05, q_e=0.99999)</code>","text":"<p>Calibrate the uncertainty by multiplying the predicted std with a scalar.</p> <p>Parameters:</p> Name Type Description Default <code>stats_dict</code> <code>dict[int, dict[str, Union[ndarray, list]]]</code> <p>Dictionary containing the stats for each channel.</p> required <code>q_s</code> <code>float</code> <p>Start quantile, by default 0.00001.</p> <code>1e-05</code> <code>q_e</code> <code>float</code> <p>End quantile, by default 0.99999.</p> <code>0.99999</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Calibrated factor for each channel (slope + intercept).</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def get_calibrated_factor_for_stdev(\n    self,\n    pred: Optional[np.ndarray] = None,\n    pred_std: Optional[np.ndarray] = None,\n    target: Optional[np.ndarray] = None,\n    q_s: float = 0.00001,\n    q_e: float = 0.99999,\n) -&gt; dict[str, float]:\n    \"\"\"Calibrate the uncertainty by multiplying the predicted std with a scalar.\n\n    Parameters\n    ----------\n    stats_dict : dict[int, dict[str, Union[np.ndarray, list]]]\n        Dictionary containing the stats for each channel.\n    q_s : float, optional\n        Start quantile, by default 0.00001.\n    q_e : float, optional\n        End quantile, by default 0.99999.\n\n    Returns\n    -------\n    dict[str, float]\n        Calibrated factor for each channel (slope + intercept).\n    \"\"\"\n    if not hasattr(self, \"stats_dict\"):\n        print(\"No stats found. Computing stats...\")\n        if any(v is None for v in [pred, pred_std, target]):\n            raise ValueError(\"pred, pred_std, and target must be provided.\")\n        self.stats_dict = self.compute_stats(\n            pred=pred, pred_std=pred_std, target=target\n        )\n    outputs = {}\n    for ch_idx in self.stats_dict.keys():\n        y = self.stats_dict[ch_idx][\"rmse\"]\n        x = self.stats_dict[ch_idx][\"rmv\"]\n        count = self.stats_dict[ch_idx][\"bin_count\"]\n\n        first_idx = get_first_index(count, q_s)\n        last_idx = get_last_index(count, q_e)\n        x = x[first_idx:-last_idx]\n        y = y[first_idx:-last_idx]\n        slope, intercept, *_ = stats.linregress(x, y)\n        output = {\"scalar\": slope, \"offset\": intercept}\n        outputs[ch_idx] = output\n    factors = self.get_factors_array(factors_dict=outputs)\n    return outputs, factors\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.get_factors_array","title":"<code>get_factors_array(factors_dict)</code>","text":"<p>Get the calibration factors as a numpy array.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def get_factors_array(self, factors_dict: list[dict]):\n    \"\"\"Get the calibration factors as a numpy array.\"\"\"\n    calib_scalar = [factors_dict[i][\"scalar\"] for i in range(len(factors_dict))]\n    calib_scalar = np.array(calib_scalar).reshape(1, 1, 1, -1)\n    calib_offset = [\n        factors_dict[i].get(\"offset\", 0.0) for i in range(len(factors_dict))\n    ]\n    calib_offset = np.array(calib_offset).reshape(1, 1, 1, -1)\n    return {\"scalar\": calib_scalar, \"offset\": calib_offset}\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/","title":"eval_utils","text":"<p>This script provides methods to evaluate the performance of the LVAE model. It includes functions to:     - make predictions,     - quantify the performance of the model     - create plots to visualize the results.</p>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.PatchLocation","title":"<code>PatchLocation</code>","text":"<p>Encapsulates t_idx and spatial location.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>class PatchLocation:\n    \"\"\"\n    Encapsulates t_idx and spatial location.\n    \"\"\"\n\n    def __init__(self, h_idx_range, w_idx_range, t_idx):\n        self.t = t_idx\n        self.h_start, self.h_end = h_idx_range\n        self.w_start, self.w_end = w_idx_range\n\n    def __str__(self):\n        msg = f\"T:{self.t} [{self.h_start}-{self.h_end}) [{self.w_start}-{self.w_end}) \"\n        return msg\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.TilingMode","title":"<code>TilingMode</code>","text":"<p>Enum for the tiling mode.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>class TilingMode:\n    \"\"\"\n    Enum for the tiling mode.\n    \"\"\"\n\n    TrimBoundary = 0\n    PadBoundary = 1\n    ShiftBoundary = 2\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.add_psnr_str","title":"<code>add_psnr_str(ax_, psnr)</code>","text":"<p>Add psnr string to the axes</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def add_psnr_str(ax_, psnr):\n    \"\"\"\n    Add psnr string to the axes\n    \"\"\"\n    textstr = f\"PSNR\\n{psnr}\"\n    props = dict(boxstyle=\"round\", facecolor=\"gray\", alpha=0.5)\n    # place a text box in upper left in axes coords\n    ax_.text(\n        0.05,\n        0.95,\n        textstr,\n        transform=ax_.transAxes,\n        fontsize=11,\n        verticalalignment=\"top\",\n        bbox=props,\n        color=\"white\",\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.clean_ax","title":"<code>clean_ax(ax)</code>","text":"<p>Helper function to remove ticks from axes in plots.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def clean_ax(ax):\n    \"\"\"\n    Helper function to remove ticks from axes in plots.\n    \"\"\"\n    # 2D or 1D axes are of type np.ndarray\n    if isinstance(ax, np.ndarray):\n        for one_ax in ax:\n            clean_ax(one_ax)\n        return\n\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    ax.tick_params(left=False, right=False, top=False, bottom=False)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_eval_output_dir","title":"<code>get_eval_output_dir(saveplotsdir, patch_size, mmse_count=50)</code>","text":"<p>Given the path to a root directory to save plots, patch size, and mmse count, it returns the specific directory to save the plots.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_eval_output_dir(\n    saveplotsdir: str, patch_size: int, mmse_count: int = 50\n) -&gt; str:\n    \"\"\"\n    Given the path to a root directory to save plots, patch size, and mmse count,\n    it returns the specific directory to save the plots.\n    \"\"\"\n    eval_out_dir = os.path.join(\n        saveplotsdir, f\"eval_outputs/patch_{patch_size}_mmse_{mmse_count}\"\n    )\n    os.makedirs(eval_out_dir, exist_ok=True)\n    print(eval_out_dir)\n    return eval_out_dir\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_fractional_change","title":"<code>get_fractional_change(target, prediction, max_val=None)</code>","text":"<p>Get relative difference between target and prediction.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_fractional_change(target, prediction, max_val=None):\n    \"\"\"\n    Get relative difference between target and prediction.\n    \"\"\"\n    if max_val is None:\n        max_val = target.max()\n    return (target - prediction) / max_val\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_location_from_idx","title":"<code>get_location_from_idx(dset, dset_input_idx, pred_h, pred_w)</code>","text":"<p>For a given idx of the dataset, it returns where exactly in the dataset, does this prediction lies. Note that this prediction also has padded pixels and so a subset of it will be used in the final prediction. Which time frame, which spatial location (h_start, h_end, w_start,w_end) Args:     dset:     dset_input_idx:     pred_h:     pred_w:</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_location_from_idx(dset, dset_input_idx, pred_h, pred_w):\n    \"\"\"\n    For a given idx of the dataset, it returns where exactly in the dataset, does this\n    prediction lies. Note that this prediction also has padded pixels and so a subset of\n    it will be used in the final prediction. Which time frame, which spatial location\n    (h_start, h_end, w_start,w_end)\n    Args:\n        dset:\n        dset_input_idx:\n        pred_h:\n        pred_w:\n\n    Returns\n    -------\n    \"\"\"\n    extra_padding = dset.per_side_overlap_pixelcount()\n    htw = dset.get_idx_manager().hwt_from_idx(\n        dset_input_idx, grid_size=dset.get_grid_size()\n    )\n    return _get_location(extra_padding, htw, pred_h, pred_w)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_predictions","title":"<code>get_predictions(model, dset, batch_size, tile_size=None, grid_size=None, mmse_count=1, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for the entire dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VAEModule</code> <p>Lightning model used for prediction.</p> required <code>dset</code> <code>Dataset</code> <p>Dataset to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for prediction.</p> required <code>loss_type</code> <p>Type of reconstruction loss used by the model, by default <code>None</code>.</p> required <code>mmse_count</code> <code>int</code> <p>Number of samples to generate for each input and then to average over for MMSE estimation, by default 1.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for DataLoader, by default 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray, List[float]]</code> <p>Tuple containing:     - predictions: Predicted images for the dataset.     - predictions_std: Standard deviation of the predicted images.     - logvar_arr: Log variance of the predicted images.     - losses: Reconstruction losses for the predictions.     - psnr: PSNR values for the predictions.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_predictions(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    mmse_count: int = 1,\n    num_workers: int = 4,\n) -&gt; tuple[dict, dict, dict]:\n    \"\"\"Get patch-wise predictions from a model for the entire dataset.\n\n    Parameters\n    ----------\n    model : VAEModule\n        Lightning model used for prediction.\n    dset : Dataset\n        Dataset to predict on.\n    batch_size : int\n        Batch size to use for prediction.\n    loss_type :\n        Type of reconstruction loss used by the model, by default `None`.\n    mmse_count : int, optional\n        Number of samples to generate for each input and then to average over for\n        MMSE estimation, by default 1.\n    num_workers : int, optional\n        Number of workers to use for DataLoader, by default 4.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[float]]\n        Tuple containing:\n            - predictions: Predicted images for the dataset.\n            - predictions_std: Standard deviation of the predicted images.\n            - logvar_arr: Log variance of the predicted images.\n            - losses: Reconstruction losses for the predictions.\n            - psnr: PSNR values for the predictions.\n    \"\"\"\n    if hasattr(dset, \"dsets\"):\n        multifile_stitched_predictions = {}\n        multifile_stitched_stds = {}\n        for d in dset.dsets:\n            stitched_predictions, stitched_stds = get_single_file_mmse(\n                model=model,\n                dset=d,\n                batch_size=batch_size,\n                tile_size=tile_size,\n                grid_size=grid_size,\n                mmse_count=mmse_count,\n                num_workers=num_workers,\n            )\n            # get filename without extension and path\n            filename = d._fpath.name\n            multifile_stitched_predictions[filename] = stitched_predictions\n            multifile_stitched_stds[filename] = stitched_stds\n        return (\n            multifile_stitched_predictions,\n            multifile_stitched_stds,\n        )\n    else:\n        stitched_predictions, stitched_stds = get_single_file_mmse(\n            model=model,\n            dset=dset,\n            batch_size=batch_size,\n            tile_size=tile_size,\n            grid_size=grid_size,\n            mmse_count=mmse_count,\n            num_workers=num_workers,\n        )\n        # TODO stitching still not working properly for weirdly shaped images\n        # get filename without extension and path\n        # TODO in the ref ds this is the name of a folder not file :(\n        filename = dset._fpath.name\n        return (\n            {filename: stitched_predictions},\n            {filename: stitched_stds},\n        )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_psnr_str","title":"<code>get_psnr_str(tar_hsnr, pred, col_idx)</code>","text":"<p>Compute PSNR between the ground truth (<code>tar_hsnr</code>) and the predicted image (<code>pred</code>).</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_psnr_str(tar_hsnr, pred, col_idx):\n    \"\"\"\n    Compute PSNR between the ground truth (`tar_hsnr`) and the predicted image (`pred`).\n    \"\"\"\n    psnr = scale_invariant_psnr(tar_hsnr[col_idx][None], pred[col_idx][None]).item()\n\n    return f\"{psnr:.1f}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_single_file_mmse","title":"<code>get_single_file_mmse(model, dset, batch_size, tile_size=None, grid_size=None, mmse_count=1, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for a single file dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_single_file_mmse(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    mmse_count: int = 1,\n    num_workers: int = 4,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get patch-wise predictions from a model for a single file dataset.\"\"\"\n    device = get_device()\n\n    dloader = DataLoader(\n        dset,\n        pin_memory=False,\n        num_workers=num_workers,\n        shuffle=False,\n        batch_size=batch_size,\n    )\n    if tile_size and grid_size:\n        dset.set_img_sz(tile_size, grid_size)\n\n    model.eval()\n    model.to(device)\n    tile_mmse = []\n    tile_stds = []\n    logvar_arr = []\n    with torch.no_grad():\n        for batch in tqdm(dloader, desc=\"Predicting tiles\"):\n            inp, tar = batch\n            inp = inp.to(device)\n            tar = tar.to(device)\n\n            rec_img_list = []\n            for _ in range(mmse_count):\n\n                # get model output\n                rec, _ = model(inp)\n\n                # get reconstructed img\n                if model.model.predict_logvar is None:\n                    rec_img = rec\n                    logvar = torch.tensor([-1])\n                else:\n                    rec_img, logvar = torch.chunk(rec, chunks=2, dim=1)\n                rec_img_list.append(rec_img.cpu().unsqueeze(0))  # add MMSE dim\n                logvar_arr.append(logvar.cpu().numpy())  # Why do we need this ?\n\n            # aggregate results\n            samples = torch.cat(rec_img_list, dim=0)\n            mmse_imgs = torch.mean(samples, dim=0)  # avg over MMSE dim\n            std_imgs = torch.std(samples, dim=0)  # std over MMSE dim\n\n            tile_mmse.append(mmse_imgs.cpu().numpy())\n            tile_stds.append(std_imgs.cpu().numpy())\n\n    tiles_arr = np.concatenate(tile_mmse, axis=0)\n    tile_stds = np.concatenate(tile_stds, axis=0)\n    # TODO temporary hack, because of the stupid jupyter!\n    # If a user reruns a cell with class definition, isinstance will return False\n    if str(MultiChDloaderRef).split(\".\")[-1] == str(dset.__class__).split(\".\")[-1]:\n        stitch_func = stitch_predictions_general\n    else:\n        stitch_func = stitch_predictions_new\n    stitched_predictions = stitch_func(tiles_arr, dset)\n    stitched_stds = stitch_func(tile_stds, dset)\n    return stitched_predictions, stitched_stds\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_single_file_predictions","title":"<code>get_single_file_predictions(model, dset, batch_size, tile_size=None, grid_size=None, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for a single file dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_single_file_predictions(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    num_workers: int = 4,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get patch-wise predictions from a model for a single file dataset.\"\"\"\n    if tile_size and grid_size:\n        dset.set_img_sz(tile_size, grid_size)\n\n    device = get_device()\n\n    dloader = DataLoader(\n        dset,\n        pin_memory=False,\n        num_workers=num_workers,\n        shuffle=False,\n        batch_size=batch_size,\n    )\n    model.eval()\n    model.to(device)\n    tiles = []\n    logvar_arr = []\n    with torch.no_grad():\n        for batch in tqdm(dloader, desc=\"Predicting tiles\"):\n            inp, tar = batch\n            inp = inp.to(device)\n            tar = tar.to(device)\n\n            # get model output\n            rec, _ = model(inp)\n\n            # get reconstructed img\n            if model.model.predict_logvar is None:\n                rec_img = rec\n                logvar = torch.tensor([-1])\n            else:\n                rec_img, logvar = torch.chunk(rec, chunks=2, dim=1)\n            logvar_arr.append(logvar.cpu().numpy())  # Why do we need this ?\n\n            tiles.append(rec_img.cpu().numpy())\n\n    tile_samples = np.concatenate(tiles, axis=0)\n    return stitch_predictions_new(tile_samples, dset)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_zero_centered_midval","title":"<code>get_zero_centered_midval(error)</code>","text":"<p>When done this way, the midval ensures that the colorbar is centered at 0. (Don't know how, but it works ;))</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_zero_centered_midval(error):\n    \"\"\"\n    When done this way, the midval ensures that the colorbar is centered at 0. (Don't\n    know how, but it works ;))\n    \"\"\"\n    vmax = error.max()\n    vmin = error.min()\n    midval = 1 - vmax / (vmax + abs(vmin))\n    return midval\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.plot_calibration","title":"<code>plot_calibration(ax, calibration_stats)</code>","text":"<p>To plot calibration statistics (RMV vs RMSE).</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def plot_calibration(ax, calibration_stats):\n    \"\"\"\n    To plot calibration statistics (RMV vs RMSE).\n    \"\"\"\n    first_idx = get_first_index(calibration_stats[0][\"bin_count\"], 0.001)\n    last_idx = get_last_index(calibration_stats[0][\"bin_count\"], 0.999)\n    ax.plot(\n        calibration_stats[0][\"rmv\"][first_idx:-last_idx],\n        calibration_stats[0][\"rmse\"][first_idx:-last_idx],\n        \"o\",\n        label=r\"$\\hat{C}_0$\",\n    )\n\n    first_idx = get_first_index(calibration_stats[1][\"bin_count\"], 0.001)\n    last_idx = get_last_index(calibration_stats[1][\"bin_count\"], 0.999)\n    ax.plot(\n        calibration_stats[1][\"rmv\"][first_idx:-last_idx],\n        calibration_stats[1][\"rmse\"][first_idx:-last_idx],\n        \"o\",\n        label=r\"$\\hat{C}_1$\",\n    )\n\n    ax.set_xlabel(\"RMV\")\n    ax.set_ylabel(\"RMSE\")\n    ax.legend()\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.plot_error","title":"<code>plot_error(target, prediction, cmap=matplotlib.cm.coolwarm, ax=None, max_val=None)</code>","text":"<p>Plot the relative difference between target and prediction. NOTE: The plot is overlapped to the prediction image (in gray scale). NOTE: The colorbar is centered at 0.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def plot_error(target, prediction, cmap=matplotlib.cm.coolwarm, ax=None, max_val=None):\n    \"\"\"\n    Plot the relative difference between target and prediction.\n    NOTE: The plot is overlapped to the prediction image (in gray scale).\n    NOTE: The colorbar is centered at 0.\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(6, 6))\n\n    # Relative difference between target and prediction\n    rel_diff = get_fractional_change(target, prediction, max_val=max_val)\n    midval = get_zero_centered_midval(rel_diff)\n    shifted_cmap = shiftedColorMap(\n        cmap, start=0, midpoint=midval, stop=1.0, name=\"shiftedcmap\"\n    )\n    ax.imshow(prediction, cmap=\"gray\")\n    img_err = ax.imshow(rel_diff, cmap=shifted_cmap, alpha=1)\n    plt.colorbar(img_err, ax=ax)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.shiftedColorMap","title":"<code>shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap')</code>","text":"<p>Adapted from https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in- matplotlib</p> <p>Function to offset the \"center\" of a colormap. Useful for data with a negative min and positive max and you want the middle of the colormap's dynamic range to be at zero.</p> Input <p>cmap : The matplotlib colormap to be altered   start : Offset from lowest point in the colormap's range.       Defaults to 0.0 (no lower offset). Should be between       0.0 and <code>midpoint</code>.   midpoint : The new center of the colormap. Defaults to       0.5 (no shift). Should be between 0.0 and 1.0. In       general, this should be  1 - vmax / (vmax + abs(vmin))       For example if your data range from -15.0 to +5.0 and       you want the center of the colormap at 0.0, <code>midpoint</code>       should be set to  1 - 5/(5 + 15)) or 0.75   stop : Offset from highest point in the colormap's range.       Defaults to 1.0 (no upper offset). Should be between       <code>midpoint</code> and 1.0.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name=\"shiftedcmap\"):\n    \"\"\"\n    Adapted from\n    https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in-\n    matplotlib\n\n    Function to offset the \"center\" of a colormap. Useful for\n    data with a negative min and positive max and you want the\n    middle of the colormap's dynamic range to be at zero.\n\n    Input\n    -----\n      cmap : The matplotlib colormap to be altered\n      start : Offset from lowest point in the colormap's range.\n          Defaults to 0.0 (no lower offset). Should be between\n          0.0 and `midpoint`.\n      midpoint : The new center of the colormap. Defaults to\n          0.5 (no shift). Should be between 0.0 and 1.0. In\n          general, this should be  1 - vmax / (vmax + abs(vmin))\n          For example if your data range from -15.0 to +5.0 and\n          you want the center of the colormap at 0.0, `midpoint`\n          should be set to  1 - 5/(5 + 15)) or 0.75\n      stop : Offset from highest point in the colormap's range.\n          Defaults to 1.0 (no upper offset). Should be between\n          `midpoint` and 1.0.\n    \"\"\"\n    cdict = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n    # regular index to compute the colors\n    reg_index = np.linspace(start, stop, 257)\n    mid_idx = len(reg_index) // 2\n    # shifted index to match the data\n    shift_index = np.hstack(\n        [\n            np.linspace(0.0, midpoint, 128, endpoint=False),\n            np.linspace(midpoint, 1.0, 129, endpoint=True),\n        ]\n    )\n\n    for ri, si in zip(reg_index, shift_index):\n        r, g, b, a = cmap(ri)\n        a = np.abs(ri - reg_index[mid_idx]) / reg_index[mid_idx]\n        # print(a)\n        cdict[\"red\"].append((si, r, r))\n        cdict[\"green\"].append((si, g, g))\n        cdict[\"blue\"].append((si, b, b))\n        cdict[\"alpha\"].append((si, a, a))\n\n    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n    matplotlib.colormaps.register(cmap=newcmap, force=True)\n\n    return newcmap\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.show_for_one","title":"<code>show_for_one(idx, val_dset, highsnr_val_dset, model, calibration_stats, mmse_count=5, patch_size=256, num_samples=2, baseline_preds=None)</code>","text":"<p>Given an index, it plots the input, target, reconstructed images and the difference image. Note the the difference image is computed with respect to a ground truth image, obtained from the high SNR dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def show_for_one(\n    idx,\n    val_dset,\n    highsnr_val_dset,\n    model,\n    calibration_stats,\n    mmse_count=5,\n    patch_size=256,\n    num_samples=2,\n    baseline_preds=None,\n):\n    \"\"\"\n    Given an index, it plots the input, target, reconstructed images and the difference\n    image.\n    Note the the difference image is computed with respect to a ground truth image,\n    obtained from the high SNR dataset.\n    \"\"\"\n    highsnr_val_dset.set_img_sz(patch_size, 64)\n    highsnr_val_dset.disable_noise()\n    _, tar_hsnr = highsnr_val_dset[idx]\n    inp, tar, recon_img_list = get_predictions(\n        idx, val_dset, model, mmse_count=mmse_count, patch_size=patch_size\n    )\n    plot_crops(\n        inp,\n        tar,\n        tar_hsnr,\n        recon_img_list,\n        calibration_stats,\n        num_samples=num_samples,\n        baseline_preds=baseline_preds,\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.stitch_predictions","title":"<code>stitch_predictions(predictions, dset, smoothening_pixelcount=0)</code>","text":"<p>Args:     smoothening_pixelcount: number of pixels which can be interpolated</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def stitch_predictions(predictions, dset, smoothening_pixelcount=0):\n    \"\"\"\n    Args:\n        smoothening_pixelcount: number of pixels which can be interpolated\n    \"\"\"\n    assert smoothening_pixelcount &gt;= 0 and isinstance(smoothening_pixelcount, int)\n    extra_padding = dset.per_side_overlap_pixelcount()\n    # if there are more channels, use all of them.\n    shape = list(dset.get_data_shape())\n    shape[-1] = max(shape[-1], predictions.shape[1])\n\n    output = np.zeros(shape, dtype=predictions.dtype)\n    frame_shape = dset.get_data_shape()[1:3]\n    for dset_input_idx in range(predictions.shape[0]):\n        loc = get_location_from_idx(\n            dset, dset_input_idx, predictions.shape[-2], predictions.shape[-1]\n        )\n\n        mask = None\n        cropped_pred_list = []\n        for ch_idx in range(predictions.shape[1]):\n            # class i\n            cropped_pred_i = remove_pad(\n                predictions[dset_input_idx, ch_idx],\n                loc,\n                extra_padding,\n                smoothening_pixelcount,\n                frame_shape,\n            )\n\n            if mask is None:\n                # NOTE: don't need to compute it for every patch.\n                assert (\n                    smoothening_pixelcount == 0\n                ), \"For smoothing,enable the get_smoothing_mask. It is disabled since I\"\n                \"don't use it and it needs modification to work with non-square images\"\n                mask = 1\n                # mask = _get_smoothing_mask(cropped_pred_i.shape,\n                # smoothening_pixelcount, loc, frame_size)\n\n            cropped_pred_list.append(cropped_pred_i)\n\n        loc = update_loc_for_final_insertion(loc, extra_padding, smoothening_pixelcount)\n        for ch_idx in range(predictions.shape[1]):\n            output[loc.t, loc.h_start : loc.h_end, loc.w_start : loc.w_end, ch_idx] += (\n                cropped_pred_list[ch_idx] * mask\n            )\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.stitch_predictions_general","title":"<code>stitch_predictions_general(predictions, dset)</code>","text":"<p>Stitching for the dataset with multiple files of different shape.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def stitch_predictions_general(predictions, dset):\n    \"\"\"Stitching for the dataset with multiple files of different shape.\"\"\"\n    mng = dset.idx_manager\n\n    # TODO assert all shapes are equal len\n    # adjust number of channels to match with prediction shape #TODO ugly, refac!\n    shapes = []\n    for shape in dset.get_data_shapes()[0]:\n        shapes.append((predictions.shape[1],) + shape[1:])\n\n    output = [np.zeros(shape, dtype=predictions.dtype) for shape in shapes]\n    # frame_shape = dset.get_data_shape()[:-1]\n    for patch_idx in range(predictions.shape[0]):\n        # grid start, grid end\n        # channel_idx is 0 because during prediction we're only use one channel.\n        # # TODO revisit this\n        # 0th dimension is sample index in the output list\n        grid_coords = np.array(\n            mng.get_location_from_patch_idx(channel_idx=0, patch_idx=patch_idx),\n            dtype=int,\n        )\n        sample_idx = grid_coords[0]\n        grid_start = grid_coords[1:]\n        # from here on, coordinates are relative to the sample(file in the list of\n        # inputs)\n        grid_end = grid_start + mng.grid_shape\n\n        # patch start, patch end\n        patch_start = grid_start - mng.patch_offset()\n        patch_end = patch_start + mng.patch_shape\n\n        # valid grid start, valid grid end\n        valid_grid_start = np.array([max(0, x) for x in grid_start], dtype=int)\n        valid_grid_end = np.array(\n            [min(x, y) for x, y in zip(grid_end, shapes[sample_idx])], dtype=int\n        )\n\n        if mng.tiling_mode == TilingMode.ShiftBoundary:\n            for dim in range(len(valid_grid_start)):\n                if patch_start[dim] == 0:\n                    valid_grid_start[dim] = 0\n                if patch_end[dim] == mng.data_shape[dim]:\n                    valid_grid_end[dim] = mng.data_shape[dim]\n\n        # relative start, relative end. This will be used on pred_tiled\n        relative_start = valid_grid_start - patch_start\n        relative_end = relative_start + (valid_grid_end - valid_grid_start)\n\n        for ch_idx in range(predictions.shape[1]):\n            if len(output[sample_idx].shape) == 3:\n                # starting from 1 because 0th dimension is channel relative to input\n                # channel dimension for stitched output is relative to model output\n                output[sample_idx][\n                    ch_idx,\n                    valid_grid_start[1] : valid_grid_end[1],\n                    valid_grid_start[2] : valid_grid_end[2],\n                ] = predictions[patch_idx][\n                    ch_idx,\n                    relative_start[1] : relative_end[1],\n                    relative_start[2] : relative_end[2],\n                ]\n            elif len(output[sample_idx].shape) == 4:\n                assert (\n                    valid_grid_end[0] - valid_grid_start[0] == 1\n                ), \"Only one frame is supported\"\n                output[\n                    ch_idx,\n                    valid_grid_start[0],\n                    valid_grid_end[1] : valid_grid_end[1],\n                    valid_grid_start[2] : valid_grid_end[2],\n                    valid_grid_start[3] : valid_grid_end[3],\n                ] = predictions[patch_idx][\n                    ch_idx,\n                    relative_start[1] : relative_end[1],\n                    relative_start[2] : relative_end[2],\n                    relative_start[3] : relative_end[3],\n                ]\n            else:\n                raise ValueError(f\"Unsupported shape {output.shape}\")\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.stitch_predictions_new","title":"<code>stitch_predictions_new(predictions, dset)</code>","text":"<p>Args:     smoothening_pixelcount: number of pixels which can be interpolated</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def stitch_predictions_new(predictions, dset):\n    \"\"\"\n    Args:\n        smoothening_pixelcount: number of pixels which can be interpolated\n    \"\"\"\n    # Commented out since it is not used as of now\n    # if isinstance(dset, MultiFileDset):\n    #     cum_count = 0\n    #     output = []\n    #     for dset in dset.dsets:\n    #         cnt = dset.idx_manager.total_grid_count()\n    #         output.append(\n    #             stitch_predictions(predictions[cum_count:cum_count + cnt], dset))\n    #         cum_count += cnt\n    #     return output\n\n    # else:\n    mng = dset.idx_manager\n\n    # if there are more channels, use all of them.\n    shape = list(dset.get_data_shape())\n    shape[-1] = max(shape[-1], predictions.shape[1])\n\n    output = np.zeros(shape, dtype=predictions.dtype)\n    # frame_shape = dset.get_data_shape()[:-1]\n    for dset_idx in range(predictions.shape[0]):\n        # loc = get_location_from_idx(dset, dset_idx, predictions.shape[-2],\n        # predictions.shape[-1])\n        # grid start, grid end\n        gs = np.array(mng.get_location_from_dataset_idx(dset_idx), dtype=int)\n        ge = gs + mng.grid_shape\n\n        # patch start, patch end\n        ps = gs - mng.patch_offset()\n        pe = ps + mng.patch_shape\n        # print('PS')\n        # print(ps)\n        # print(pe)\n\n        # valid grid start, valid grid end\n        vgs = np.array([max(0, x) for x in gs], dtype=int)\n        vge = np.array([min(x, y) for x, y in zip(ge, mng.data_shape)], dtype=int)\n        # assert np.all(vgs == gs)\n        # assert np.all(vge == ge) # TODO comented out this shit cuz I have no interest\n        # to dig why it's failing at this point !\n        # print('VGS')\n        # print(gs)\n        # print(ge)\n\n        if mng.tiling_mode == TilingMode.ShiftBoundary:\n            for dim in range(len(vgs)):\n                if ps[dim] == 0:\n                    vgs[dim] = 0\n                if pe[dim] == mng.data_shape[dim]:\n                    vge[dim] = mng.data_shape[dim]\n\n        # relative start, relative end. This will be used on pred_tiled\n        rs = vgs - ps\n        re = rs + (vge - vgs)\n        # print('RS')\n        # print(rs)\n        # print(re)\n\n        # print(output.shape)\n        # print(predictions.shape)\n        for ch_idx in range(predictions.shape[1]):\n            if len(output.shape) == 4:\n                # channel dimension is the last one.\n                output[vgs[0] : vge[0], vgs[1] : vge[1], vgs[2] : vge[2], ch_idx] = (\n                    predictions[dset_idx][ch_idx, rs[1] : re[1], rs[2] : re[2]]\n                )\n            elif len(output.shape) == 5:\n                # channel dimension is the last one.\n                assert vge[0] - vgs[0] == 1, \"Only one frame is supported\"\n                output[\n                    vgs[0], vgs[1] : vge[1], vgs[2] : vge[2], vgs[3] : vge[3], ch_idx\n                ] = predictions[dset_idx][\n                    ch_idx, rs[1] : re[1], rs[2] : re[2], rs[3] : re[3]\n                ]\n            else:\n                raise ValueError(f\"Unsupported shape {output.shape}\")\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/get_config/","title":"get_config","text":"<p>Here there are functions to define a config file.</p>"},{"location":"reference/careamics/lvae_training/lightning_module/","title":"lightning_module","text":"<p>Lightning Module for LadderVAE.</p>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight","title":"<code>LadderVAELight</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>class LadderVAELight(L.LightningModule):\n\n    def __init__(\n        self,\n        config: ml_collections.ConfigDict,\n        data_mean: Dict[str, torch.Tensor],\n        data_std: Dict[str, torch.Tensor],\n        target_ch: int,\n    ):\n        \"\"\"\n        Here we will do the following:\n            - initialize the model (from LadderVAE class)\n            - initialize the parameters related to the training and loss.\n\n        NOTE:\n        Some of the model attributes are defined in the model object itself, while some others will be defined here.\n        Note that all the attributes related to the training and loss that were already defined in the model object\n        are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr).\n        The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).\n\n        NOTE: HC stands for Hard Coded attribute.\n        \"\"\"\n        super().__init__()\n\n        self.data_mean = data_mean\n        self.data_std = data_std\n        self.target_ch = target_ch\n\n        # Initialize LVAE model\n        self.model = LadderVAE(\n            data_mean=data_mean, data_std=data_std, config=config, target_ch=target_ch\n        )\n\n        ##### Define attributes from config #####\n        self.workdir = config.workdir\n        self._input_is_sum = False\n        self.kl_loss_formulation = config.loss.kl_loss_formulation\n        assert self.kl_loss_formulation in [\n            None,\n            \"\",\n            \"usplit\",\n            \"denoisplit\",\n            \"denoisplit_usplit\",\n        ], f\"\"\"\n            Invalid kl_loss_formulation. {self.kl_loss_formulation}\"\"\"\n\n        ##### Define loss attributes #####\n        # Parameters already defined in the model object\n        self.loss_type = self.model.loss_type\n        self._denoisplit_w = self._usplit_w = None\n        if self.loss_type == LossType.DenoiSplitMuSplit:\n            self._usplit_w = 0\n            self._denoisplit_w = 1 - self._usplit_w\n            assert self._denoisplit_w + self._usplit_w == 1\n        self._restricted_kl = self.model._restricted_kl\n\n        # General loss parameters\n        self.channel_1_w = 1\n        self.channel_2_w = 1\n\n        # About Reconsruction Loss\n        self.reconstruction_mode = False\n        self.skip_nboundary_pixels_from_loss = None\n        self.reconstruction_weight = 1.0\n        self._exclusion_loss_weight = 0\n        self.ch1_recons_w = 1\n        self.ch2_recons_w = 1\n        self.enable_mixed_rec = False\n        self.mixed_rec_w_step = 0\n\n        # About KL Loss\n        self.kl_weight = 1.0  # HC\n        self.usplit_kl_weight = None  # HC\n        self.free_bits = 1.0  # HC\n        self.kl_annealing = False  # HC\n        self.kl_annealtime = self.kl_start = None\n        if self.kl_annealing:\n            self.kl_annealtime = 10  # HC\n            self.kl_start = -1  # HC\n\n        ##### Define training attributes #####\n        self.lr = config.training.lr\n        self.lr_scheduler_patience = config.training.lr_scheduler_patience\n        self.lr_scheduler_monitor = config.model.get(\"monitor\", \"val_loss\")\n        self.lr_scheduler_mode = MetricMonitor(self.lr_scheduler_monitor).mode()\n\n        # Initialize object for keeping track of PSNR for each output channel\n        self.channels_psnr = [RunningPSNR() for _ in range(self.model.target_ch)]\n\n    def forward(self, x: Any) -&gt; Any:\n        return self.model(x)\n\n    def training_step(\n        self, batch: torch.Tensor, batch_idx: int, enable_logging: bool = True\n    ) -&gt; Dict[str, torch.Tensor]:\n\n        if self.current_epoch == 0 and batch_idx == 0:\n            self.log(\"val_psnr\", 1.0, on_epoch=True)\n\n        # Pre-processing of inputs\n        x, target = batch[:2]\n        self.set_params_to_same_device_as(x)\n        x_normalized = self.normalize_input(x)\n        if self.reconstruction_mode:  # just for experimental purpose\n            target_normalized = x_normalized[:, :1].repeat(1, 2, 1, 1)\n            target = None\n            mask = None\n        else:\n            target_normalized = self.normalize_target(target)\n            mask = ~((target == 0).reshape(len(target), -1).all(dim=1))\n\n        # Forward pass\n        out, td_data = self.forward(x_normalized)\n\n        if (\n            self.model.encoder_no_padding_mode\n            and out.shape[-2:] != target_normalized.shape[-2:]\n        ):\n            target_normalized = F.center_crop(target_normalized, out.shape[-2:])\n\n        # Loss Computations\n        # mask = torch.isnan(target.reshape(len(x), -1)).all(dim=1)\n        recons_loss_dict, imgs = self.get_reconstruction_loss(\n            reconstruction=out,\n            target=target_normalized,\n            input=x_normalized,\n            splitting_mask=mask,\n            return_predicted_img=True,\n        )\n\n        # This `if` is not used by default config\n        if self.skip_nboundary_pixels_from_loss:\n            pad = self.skip_nboundary_pixels_from_loss\n            target_normalized = target_normalized[:, :, pad:-pad, pad:-pad]\n\n        recons_loss = recons_loss_dict[\"loss\"] * self.reconstruction_weight\n\n        if torch.isnan(recons_loss).any():\n            recons_loss = 0.0\n\n        if self.model.non_stochastic_version:\n            kl_loss = torch.Tensor([0.0]).cuda()\n            net_loss = recons_loss\n        else:\n            if self.loss_type == LossType.DenoiSplitMuSplit:\n                msg = f\"For the loss type {LossType.name(self.loss_type)}, kl_loss_formulation must be denoisplit_usplit\"\n                assert self.kl_loss_formulation == \"denoisplit_usplit\", msg\n                assert self._denoisplit_w is not None and self._usplit_w is not None\n\n                kl_key_denoisplit = \"kl_restricted\" if self._restricted_kl else \"kl\"\n                # NOTE: 'kl' key stands for the 'kl_samplewise' key in the TopDownLayer class.\n                # The different naming comes from `top_down_pass()` method in the LadderVAE class.\n                denoisplit_kl = self.get_kl_divergence_loss(\n                    topdown_layer_data_dict=td_data, kl_key=kl_key_denoisplit\n                )\n                usplit_kl = self.get_kl_divergence_loss_usplit(\n                    topdown_layer_data_dict=td_data\n                )\n                kl_loss = (\n                    self._denoisplit_w * denoisplit_kl + self._usplit_w * usplit_kl\n                )\n                kl_loss = self.kl_weight * kl_loss\n\n                recons_loss = self.reconstruction_loss_musplit_denoisplit(\n                    out, target_normalized\n                )\n                # recons_loss = self._denoisplit_w * recons_loss_nm + self._usplit_w * recons_loss_gm\n\n            elif self.kl_loss_formulation == \"usplit\":\n                kl_loss = self.get_kl_weight() * self.get_kl_divergence_loss_usplit(\n                    td_data\n                )\n            elif self.kl_loss_formulation in [\"\", \"denoisplit\"]:\n                kl_loss = self.get_kl_weight() * self.get_kl_divergence_loss(td_data)\n            net_loss = recons_loss + kl_loss\n\n        # Logging\n        if enable_logging:\n            for i, x in enumerate(td_data[\"debug_qvar_max\"]):\n                self.log(f\"qvar_max:{i}\", x.item(), on_epoch=True)\n\n            self.log(\"reconstruction_loss\", recons_loss_dict[\"loss\"], on_epoch=True)\n            self.log(\"kl_loss\", kl_loss, on_epoch=True)\n            self.log(\"training_loss\", net_loss, on_epoch=True)\n            self.log(\"lr\", self.lr, on_epoch=True)\n            if self.model._tethered_ch2_scalar is not None:\n                self.log(\n                    \"tethered_ch2_scalar\",\n                    self.model._tethered_ch2_scalar,\n                    on_epoch=True,\n                )\n                self.log(\n                    \"tethered_ch1_scalar\",\n                    self.model._tethered_ch1_scalar,\n                    on_epoch=True,\n                )\n\n            # self.log('grad_norm_bottom_up', self.grad_norm_bottom_up, on_epoch=True)\n            # self.log('grad_norm_top_down', self.grad_norm_top_down, on_epoch=True)\n\n        output = {\n            \"loss\": net_loss,\n            \"reconstruction_loss\": (\n                recons_loss.detach()\n                if isinstance(recons_loss, torch.Tensor)\n                else recons_loss\n            ),\n            \"kl_loss\": kl_loss.detach(),\n        }\n        # https://github.com/openai/vdvae/blob/main/train.py#L26\n        if torch.isnan(net_loss).any():\n            return None\n\n        return output\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int):\n        # Pre-processing of inputs\n        x, target = batch[:2]\n        self.set_params_to_same_device_as(x)\n        x_normalized = self.normalize_input(x)\n        if self.reconstruction_mode:  # only for experimental purpose\n            target_normalized = x_normalized[:, :1].repeat(1, 2, 1, 1)\n            target = None\n            mask = None\n        else:\n            target_normalized = self.normalize_target(target)\n            mask = ~((target == 0).reshape(len(target), -1).all(dim=1))\n\n        # Forward pass\n        out, _ = self.forward(x_normalized)\n\n        if self.model.predict_logvar is not None:\n            out_mean, _ = out.chunk(2, dim=1)\n        else:\n            out_mean = out\n\n        if (\n            self.model.encoder_no_padding_mode\n            and out.shape[-2:] != target_normalized.shape[-2:]\n        ):\n            target_normalized = F.center_crop(target_normalized, out.shape[-2:])\n\n        if self.loss_type == LossType.DenoiSplitMuSplit:\n            recons_loss = self.reconstruction_loss_musplit_denoisplit(\n                out, target_normalized\n            )\n            recons_loss_dict = {\"loss\": recons_loss}\n            recons_img = out_mean\n        else:\n            # Metrics computation\n            recons_loss_dict, recons_img = self.get_reconstruction_loss(\n                reconstruction=out_mean,\n                target=target_normalized,\n                input=x_normalized,\n                splitting_mask=mask,\n                return_predicted_img=True,\n            )\n\n        # This `if` is not used by default config\n        if self.skip_nboundary_pixels_from_loss:\n            pad = self.skip_nboundary_pixels_from_loss\n            target_normalized = target_normalized[:, :, pad:-pad, pad:-pad]\n\n        channels_rinvpsnr = []\n        for i in range(target_normalized.shape[1]):\n            self.channels_psnr[i].update(recons_img[:, i], target_normalized[:, i])\n            psnr = RangeInvariantPsnr(\n                target_normalized[:, i].clone(), recons_img[:, i].clone()\n            )\n            channels_rinvpsnr.append(psnr)\n            psnr = torch_nanmean(psnr).item()\n            self.log(f\"val_psnr_l{i+1}\", psnr, on_epoch=True)\n\n        recons_loss = recons_loss_dict[\"loss\"]\n        if torch.isnan(recons_loss).any():\n            return\n\n        self.log(\"val_loss\", recons_loss, on_epoch=True)\n        # self.log('val_psnr', (val_psnr_l1 + val_psnr_l2) / 2, on_epoch=True)\n\n        # if batch_idx == 0 and self.power_of_2(self.current_epoch):\n        #     all_samples = []\n        #     for i in range(20):\n        #         sample, _ = self(x_normalized[0:1, ...])\n        #         sample = self.likelihood.get_mean_lv(sample)[0]\n        #         all_samples.append(sample[None])\n\n        #     all_samples = torch.cat(all_samples, dim=0)\n        #     all_samples = all_samples * self.data_std + self.data_mean\n        #     all_samples = all_samples.cpu()\n        #     img_mmse = torch.mean(all_samples, dim=0)[0]\n        #     self.log_images_for_tensorboard(all_samples[:, 0, 0, ...], target[0, 0, ...], img_mmse[0], 'label1')\n        #     self.log_images_for_tensorboard(all_samples[:, 0, 1, ...], target[0, 1, ...], img_mmse[1], 'label2')\n\n        # return net_loss\n\n    def on_validation_epoch_end(self):\n        psnr_arr = []\n        for i in range(len(self.channels_psnr)):\n            psnr = self.channels_psnr[i].get()\n            if psnr is None:\n                psnr_arr = None\n                break\n            psnr_arr.append(psnr.cpu().numpy())\n            self.channels_psnr[i].reset()\n\n        if psnr_arr is not None:\n            psnr = np.mean(psnr_arr)\n            self.log(\"val_psnr\", psnr, on_epoch=True)\n        else:\n            self.log(\"val_psnr\", 0.0, on_epoch=True)\n\n        if self.mixed_rec_w_step:\n            self.mixed_rec_w = max(self.mixed_rec_w - self.mixed_rec_w_step, 0.0)\n            self.log(\"mixed_rec_w\", self.mixed_rec_w, on_epoch=True)\n\n    def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n        raise NotImplementedError(\"predict_step is not implemented\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adamax(self.parameters(), lr=self.lr, weight_decay=0)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            self.lr_scheduler_mode,\n            patience=self.lr_scheduler_patience,\n            factor=0.5,\n            min_lr=1e-12,\n            verbose=True,\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": self.lr_scheduler_monitor,\n        }\n\n    ##### REQUIRED Methods for Loss Computation #####\n    def get_reconstruction_loss(\n        self,\n        reconstruction: torch.Tensor,\n        target: torch.Tensor,\n        input: torch.Tensor,\n        splitting_mask: torch.Tensor = None,\n        return_predicted_img: bool = False,\n        likelihood_obj: LikelihoodModule = None,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Parameters\n        ----------\n        reconstruction: torch.Tensor,\n        target: torch.Tensor\n        input: torch.Tensor\n        splitting_mask: torch.Tensor = None\n            A boolean tensor that indicates which items to keep for reconstruction loss computation.\n            If `None`, all the elements of the items are considered (i.e., the mask is all `True`).\n        return_predicted_img: bool = False\n        likelihood_obj: LikelihoodModule = None\n        \"\"\"\n        output = self._get_reconstruction_loss_vector(\n            reconstruction=reconstruction,\n            target=target,\n            input=input,\n            return_predicted_img=return_predicted_img,\n            likelihood_obj=likelihood_obj,\n        )\n        loss_dict = output[0] if return_predicted_img else output\n\n        if splitting_mask is None:\n            splitting_mask = torch.ones_like(loss_dict[\"loss\"]).bool()\n\n        # print(len(target) - (torch.isnan(loss_dict['loss'])).sum())\n\n        loss_dict[\"loss\"] = loss_dict[\"loss\"][splitting_mask].sum() / len(\n            reconstruction\n        )\n        for i in range(1, 1 + target.shape[1]):\n            key = f\"ch{i}_loss\"\n            loss_dict[key] = loss_dict[key][splitting_mask].sum() / len(reconstruction)\n\n        if \"mixed_loss\" in loss_dict:\n            loss_dict[\"mixed_loss\"] = torch.mean(loss_dict[\"mixed_loss\"])\n        if return_predicted_img:\n            assert len(output) == 2\n            return loss_dict, output[1]\n        else:\n            return loss_dict\n\n    def _get_reconstruction_loss_vector(\n        self,\n        reconstruction: torch.Tensor,\n        target: torch.Tensor,\n        input: torch.Tensor,\n        return_predicted_img: bool = False,\n        likelihood_obj: LikelihoodModule = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        return_predicted_img: bool\n            If set to `True`, the besides the loss, the reconstructed image is also returned.\n            Default is `False`.\n        \"\"\"\n        output = {\n            \"loss\": None,\n            \"mixed_loss\": None,\n        }\n\n        for i in range(1, 1 + target.shape[1]):\n            output[f\"ch{i}_loss\"] = None\n\n        if likelihood_obj is None:\n            likelihood_obj = self.model.likelihood\n\n        # Log likelihood\n        ll, like_dict = likelihood_obj(reconstruction, target)\n        ll = self._get_weighted_likelihood(ll)\n        if (\n            self.skip_nboundary_pixels_from_loss is not None\n            and self.skip_nboundary_pixels_from_loss &gt; 0\n        ):\n            pad = self.skip_nboundary_pixels_from_loss\n            ll = ll[:, :, pad:-pad, pad:-pad]\n            like_dict[\"params\"][\"mean\"] = like_dict[\"params\"][\"mean\"][\n                :, :, pad:-pad, pad:-pad\n            ]\n\n        # assert ll.shape[1] == 2, f\"Change the code below to handle &gt;2 channels first. ll.shape {ll.shape}\"\n        output = {\"loss\": compute_batch_mean(-1 * ll)}\n        if ll.shape[1] &gt; 1:\n            for i in range(1, 1 + target.shape[1]):\n                output[f\"ch{i}_loss\"] = compute_batch_mean(-ll[:, i - 1])\n        else:\n            assert ll.shape[1] == 1\n            output[\"ch1_loss\"] = output[\"loss\"]\n            output[\"ch2_loss\"] = output[\"loss\"]\n\n        if (\n            self.channel_1_w is not None\n            and self.channel_2_w is not None\n            and (self.channel_1_w != 1 or self.channel_2_w != 1)\n        ):\n            assert ll.shape[1] == 2, \"Only 2 channels are supported for now.\"\n            output[\"loss\"] = (\n                self.channel_1_w * output[\"ch1_loss\"]\n                + self.channel_2_w * output[\"ch2_loss\"]\n            ) / (self.channel_1_w + self.channel_2_w)\n\n        # This `if` is not used by default config\n        if self.enable_mixed_rec:\n            mixed_pred, mixed_logvar = self.get_mixed_prediction(\n                like_dict[\"params\"][\"mean\"],\n                like_dict[\"params\"][\"logvar\"],\n                self.data_mean,\n                self.data_std,\n            )\n            if (\n                self.model._multiscale_count is not None\n                and self.model._multiscale_count &gt; 1\n            ):\n                assert input.shape[1] == self.model._multiscale_count\n                input = input[:, :1]\n\n            assert (\n                input.shape == mixed_pred.shape\n            ), \"No fucking room for vectorization induced bugs.\"\n            mixed_recons_ll = self.model.likelihood.log_likelihood(\n                input, {\"mean\": mixed_pred, \"logvar\": mixed_logvar}\n            )\n            output[\"mixed_loss\"] = compute_batch_mean(-1 * mixed_recons_ll)\n\n        # This `if` is not used by default config\n        if self._exclusion_loss_weight:\n            raise NotImplementedError(\n                \"Exclusion loss is not well defined here, so it should not be used.\"\n            )\n            imgs = like_dict[\"params\"][\"mean\"]\n            exclusion_loss = compute_exclusion_loss(imgs[:, :1], imgs[:, 1:])\n            output[\"exclusion_loss\"] = exclusion_loss\n\n        if return_predicted_img:\n            return output, like_dict[\"params\"][\"mean\"]\n\n        return output\n\n    def reconstruction_loss_musplit_denoisplit(self, out, target_normalized):\n        if self.model.predict_logvar is not None:\n            out_mean, _ = out.chunk(2, dim=1)\n        else:\n            out_mean = out\n\n        recons_loss_nm = (\n            -1 * self.model.likelihood_NM(out_mean, target_normalized)[0].mean()\n        )\n        recons_loss_gm = -1 * self.model.likelihood_gm(out, target_normalized)[0].mean()\n        recons_loss = (\n            self._denoisplit_w * recons_loss_nm + self._usplit_w * recons_loss_gm\n        )\n        return recons_loss\n\n    def _get_weighted_likelihood(self, ll):\n        \"\"\"\n        Each of the channels gets multiplied with a different weight.\n        \"\"\"\n        if self.ch1_recons_w == 1 and self.ch2_recons_w == 1:\n            return ll\n\n        assert ll.shape[1] == 2, \"This function is only for 2 channel images\"\n\n        mask1 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n        mask1[:, 0] = 1\n        mask2 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n        mask2[:, 1] = 1\n\n        return ll * mask1 * self.ch1_recons_w + ll * mask2 * self.ch2_recons_w\n\n    def get_kl_weight(self):\n        \"\"\"\n        KL loss can be weighted depending whether any annealing procedure is used.\n        This function computes the weight of the KL loss in case of annealing.\n        \"\"\"\n        if self.kl_annealing == True:\n            # calculate relative weight\n            kl_weight = (self.current_epoch - self.kl_start) * (\n                1.0 / self.kl_annealtime\n            )\n            # clamp to [0,1]\n            kl_weight = min(max(0.0, kl_weight), 1.0)\n\n            # if the final weight is given, then apply that weight on top of it\n            if self.kl_weight is not None:\n                kl_weight = kl_weight * self.kl_weight\n        elif self.kl_weight is not None:\n            return self.kl_weight\n        else:\n            kl_weight = 1.0\n        return kl_weight\n\n    def get_kl_divergence_loss_usplit(\n        self, topdown_layer_data_dict: Dict[str, torch.Tensor]\n    ) -&gt; torch.Tensor:\n        \"\"\" \"\"\"\n        kl = torch.cat(\n            [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[\"kl\"]], dim=1\n        )\n        # NOTE: kl.shape = (16,4) 16 is batch size. 4 is number of layers.\n        # Values are sum() and so are of the order 30000\n        # Example values: 30626.6758, 31028.8145, 29509.8809, 29945.4922, 28919.1875, 29075.2988\n\n        nlayers = kl.shape[1]\n        for i in range(nlayers):\n            # topdown_layer_data_dict['z'][2].shape[-3:] = 128 * 32 * 32\n            norm_factor = np.prod(topdown_layer_data_dict[\"z\"][i].shape[-3:])\n            # if self._restricted_kl:\n            #     pow = np.power(2,min(i + 1, self._multiscale_count-1))\n            #     norm_factor /= pow * pow\n\n            kl[:, i] = kl[:, i] / norm_factor\n\n        kl_loss = free_bits_kl(kl, 0.0).mean()\n        return kl_loss\n\n    def get_kl_divergence_loss(self, topdown_layer_data_dict, kl_key=\"kl\"):\n        \"\"\"\n        kl[i] for each i has length batch_size\n        resulting kl shape: (batch_size, layers)\n        \"\"\"\n        kl = torch.cat(\n            [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[kl_key]],\n            dim=1,\n        )\n\n        # As compared to uSplit kl divergence,\n        # more by a factor of 4 just because we do sum and not mean.\n        kl_loss = free_bits_kl(kl, self.free_bits).sum()\n        # NOTE: at each hierarchy, it is more by a factor of 128/i**2).\n        # 128/(2*2) = 32 (bottommost layer)\n        # 128/(4*4) = 8\n        # 128/(8*8) = 2\n        # 128/(16*16) = 0.5 (topmost layer)\n\n        # Normalize the KL-loss w.r.t. the  latent space\n        kl_loss = kl_loss / np.prod(self.model.img_shape)\n        return kl_loss\n\n    ##### UTILS Methods #####\n    def normalize_input(self, x):\n        if self.model.normalized_input:\n            return x\n        return (x - self.data_mean[\"input\"].mean()) / self.data_std[\"input\"].mean()\n\n    def normalize_target(self, target, batch=None):\n        return (target - self.data_mean[\"target\"]) / self.data_std[\"target\"]\n\n    def unnormalize_target(self, target_normalized):\n        return target_normalized * self.data_std[\"target\"] + self.data_mean[\"target\"]\n\n    ##### ADDITIONAL Methods #####\n    # def log_images_for_tensorboard(self, pred, target, img_mmse, label):\n    #     clamped_pred = torch.clamp((pred - pred.min()) / (pred.max() - pred.min()), 0, 1)\n    #     clamped_mmse = torch.clamp((img_mmse - img_mmse.min()) / (img_mmse.max() - img_mmse.min()), 0, 1)\n    #     if target is not None:\n    #         clamped_input = torch.clamp((target - target.min()) / (target.max() - target.min()), 0, 1)\n    #         img = wandb.Image(clamped_input[None].cpu().numpy())\n    #         self.logger.experiment.log({f'target_for{label}': img})\n    #         # self.trainer.logger.experiment.add_image(f'target_for{label}', clamped_input[None], self.current_epoch)\n    #     for i in range(3):\n    #         # self.trainer.logger.experiment.add_image(f'{label}/sample_{i}', clamped_pred[i:i + 1], self.current_epoch)\n    #         img = wandb.Image(clamped_pred[i:i + 1].cpu().numpy())\n    #         self.logger.experiment.log({f'{label}/sample_{i}': img})\n\n    #     img = wandb.Image(clamped_mmse[None].cpu().numpy())\n    #     self.trainer.logger.experiment.log({f'{label}/mmse (100 samples)': img})\n\n    @property\n    def global_step(self) -&gt; int:\n        \"\"\"Global step.\"\"\"\n        return self._global_step\n\n    def increment_global_step(self):\n        \"\"\"Increments global step by 1.\"\"\"\n        self._global_step += 1\n\n    def set_params_to_same_device_as(self, correct_device_tensor: torch.Tensor):\n\n        self.model.likelihood.set_params_to_same_device_as(correct_device_tensor)\n        if isinstance(self.data_mean, torch.Tensor):\n            if self.data_mean.device != correct_device_tensor.device:\n                self.data_mean = self.data_mean.to(correct_device_tensor.device)\n                self.data_std = self.data_std.to(correct_device_tensor.device)\n        elif isinstance(self.data_mean, dict):\n            for k, v in self.data_mean.items():\n                if v.device != correct_device_tensor.device:\n                    self.data_mean[k] = v.to(correct_device_tensor.device)\n                    self.data_std[k] = self.data_std[k].to(correct_device_tensor.device)\n\n    def get_mixed_prediction(\n        self, prediction, prediction_logvar, data_mean, data_std, channel_weights=None\n    ):\n        pred_unorm = prediction * data_std[\"target\"] + data_mean[\"target\"]\n        if channel_weights is None:\n            channel_weights = 1\n\n        if self._input_is_sum:\n            mixed_prediction = torch.sum(\n                pred_unorm * channel_weights, dim=1, keepdim=True\n            )\n        else:\n            mixed_prediction = torch.mean(\n                pred_unorm * channel_weights, dim=1, keepdim=True\n            )\n\n        mixed_prediction = (mixed_prediction - data_mean[\"input\"].mean()) / data_std[\n            \"input\"\n        ].mean()\n\n        if prediction_logvar is not None:\n            if data_std[\"target\"].shape == data_std[\"input\"].shape and torch.all(\n                data_std[\"target\"] == data_std[\"input\"]\n            ):\n                assert channel_weights == 1\n                logvar = prediction_logvar\n            else:\n                var = torch.exp(prediction_logvar)\n                var = var * (data_std[\"target\"] / data_std[\"input\"]) ** 2\n                if channel_weights != 1:\n                    var = var * torch.square(channel_weights)\n\n                # sum of variance.\n                mixed_var = 0\n                for i in range(var.shape[1]):\n                    mixed_var += var[:, i : i + 1]\n\n                logvar = torch.log(mixed_var)\n        else:\n            logvar = None\n        return mixed_prediction, logvar\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.global_step","title":"<code>global_step</code>  <code>property</code>","text":"<p>Global step.</p>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.__init__","title":"<code>__init__(config, data_mean, data_std, target_ch)</code>","text":"<p>Here we will do the following:     - initialize the model (from LadderVAE class)     - initialize the parameters related to the training and loss.</p> <p>NOTE: Some of the model attributes are defined in the model object itself, while some others will be defined here. Note that all the attributes related to the training and loss that were already defined in the model object are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr). The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).</p> <p>NOTE: HC stands for Hard Coded attribute.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def __init__(\n    self,\n    config: ml_collections.ConfigDict,\n    data_mean: Dict[str, torch.Tensor],\n    data_std: Dict[str, torch.Tensor],\n    target_ch: int,\n):\n    \"\"\"\n    Here we will do the following:\n        - initialize the model (from LadderVAE class)\n        - initialize the parameters related to the training and loss.\n\n    NOTE:\n    Some of the model attributes are defined in the model object itself, while some others will be defined here.\n    Note that all the attributes related to the training and loss that were already defined in the model object\n    are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr).\n    The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).\n\n    NOTE: HC stands for Hard Coded attribute.\n    \"\"\"\n    super().__init__()\n\n    self.data_mean = data_mean\n    self.data_std = data_std\n    self.target_ch = target_ch\n\n    # Initialize LVAE model\n    self.model = LadderVAE(\n        data_mean=data_mean, data_std=data_std, config=config, target_ch=target_ch\n    )\n\n    ##### Define attributes from config #####\n    self.workdir = config.workdir\n    self._input_is_sum = False\n    self.kl_loss_formulation = config.loss.kl_loss_formulation\n    assert self.kl_loss_formulation in [\n        None,\n        \"\",\n        \"usplit\",\n        \"denoisplit\",\n        \"denoisplit_usplit\",\n    ], f\"\"\"\n        Invalid kl_loss_formulation. {self.kl_loss_formulation}\"\"\"\n\n    ##### Define loss attributes #####\n    # Parameters already defined in the model object\n    self.loss_type = self.model.loss_type\n    self._denoisplit_w = self._usplit_w = None\n    if self.loss_type == LossType.DenoiSplitMuSplit:\n        self._usplit_w = 0\n        self._denoisplit_w = 1 - self._usplit_w\n        assert self._denoisplit_w + self._usplit_w == 1\n    self._restricted_kl = self.model._restricted_kl\n\n    # General loss parameters\n    self.channel_1_w = 1\n    self.channel_2_w = 1\n\n    # About Reconsruction Loss\n    self.reconstruction_mode = False\n    self.skip_nboundary_pixels_from_loss = None\n    self.reconstruction_weight = 1.0\n    self._exclusion_loss_weight = 0\n    self.ch1_recons_w = 1\n    self.ch2_recons_w = 1\n    self.enable_mixed_rec = False\n    self.mixed_rec_w_step = 0\n\n    # About KL Loss\n    self.kl_weight = 1.0  # HC\n    self.usplit_kl_weight = None  # HC\n    self.free_bits = 1.0  # HC\n    self.kl_annealing = False  # HC\n    self.kl_annealtime = self.kl_start = None\n    if self.kl_annealing:\n        self.kl_annealtime = 10  # HC\n        self.kl_start = -1  # HC\n\n    ##### Define training attributes #####\n    self.lr = config.training.lr\n    self.lr_scheduler_patience = config.training.lr_scheduler_patience\n    self.lr_scheduler_monitor = config.model.get(\"monitor\", \"val_loss\")\n    self.lr_scheduler_mode = MetricMonitor(self.lr_scheduler_monitor).mode()\n\n    # Initialize object for keeping track of PSNR for each output channel\n    self.channels_psnr = [RunningPSNR() for _ in range(self.model.target_ch)]\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_divergence_loss","title":"<code>get_kl_divergence_loss(topdown_layer_data_dict, kl_key='kl')</code>","text":"<p>kl[i] for each i has length batch_size resulting kl shape: (batch_size, layers)</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_divergence_loss(self, topdown_layer_data_dict, kl_key=\"kl\"):\n    \"\"\"\n    kl[i] for each i has length batch_size\n    resulting kl shape: (batch_size, layers)\n    \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[kl_key]],\n        dim=1,\n    )\n\n    # As compared to uSplit kl divergence,\n    # more by a factor of 4 just because we do sum and not mean.\n    kl_loss = free_bits_kl(kl, self.free_bits).sum()\n    # NOTE: at each hierarchy, it is more by a factor of 128/i**2).\n    # 128/(2*2) = 32 (bottommost layer)\n    # 128/(4*4) = 8\n    # 128/(8*8) = 2\n    # 128/(16*16) = 0.5 (topmost layer)\n\n    # Normalize the KL-loss w.r.t. the  latent space\n    kl_loss = kl_loss / np.prod(self.model.img_shape)\n    return kl_loss\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_divergence_loss_usplit","title":"<code>get_kl_divergence_loss_usplit(topdown_layer_data_dict)</code>","text":"Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_divergence_loss_usplit(\n    self, topdown_layer_data_dict: Dict[str, torch.Tensor]\n) -&gt; torch.Tensor:\n    \"\"\" \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[\"kl\"]], dim=1\n    )\n    # NOTE: kl.shape = (16,4) 16 is batch size. 4 is number of layers.\n    # Values are sum() and so are of the order 30000\n    # Example values: 30626.6758, 31028.8145, 29509.8809, 29945.4922, 28919.1875, 29075.2988\n\n    nlayers = kl.shape[1]\n    for i in range(nlayers):\n        # topdown_layer_data_dict['z'][2].shape[-3:] = 128 * 32 * 32\n        norm_factor = np.prod(topdown_layer_data_dict[\"z\"][i].shape[-3:])\n        # if self._restricted_kl:\n        #     pow = np.power(2,min(i + 1, self._multiscale_count-1))\n        #     norm_factor /= pow * pow\n\n        kl[:, i] = kl[:, i] / norm_factor\n\n    kl_loss = free_bits_kl(kl, 0.0).mean()\n    return kl_loss\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_weight","title":"<code>get_kl_weight()</code>","text":"<p>KL loss can be weighted depending whether any annealing procedure is used. This function computes the weight of the KL loss in case of annealing.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_weight(self):\n    \"\"\"\n    KL loss can be weighted depending whether any annealing procedure is used.\n    This function computes the weight of the KL loss in case of annealing.\n    \"\"\"\n    if self.kl_annealing == True:\n        # calculate relative weight\n        kl_weight = (self.current_epoch - self.kl_start) * (\n            1.0 / self.kl_annealtime\n        )\n        # clamp to [0,1]\n        kl_weight = min(max(0.0, kl_weight), 1.0)\n\n        # if the final weight is given, then apply that weight on top of it\n        if self.kl_weight is not None:\n            kl_weight = kl_weight * self.kl_weight\n    elif self.kl_weight is not None:\n        return self.kl_weight\n    else:\n        kl_weight = 1.0\n    return kl_weight\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_reconstruction_loss","title":"<code>get_reconstruction_loss(reconstruction, target, input, splitting_mask=None, return_predicted_img=False, likelihood_obj=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Tensor</code> required <code>target</code> <code>Tensor</code> required <code>input</code> <code>Tensor</code> required <code>splitting_mask</code> <code>Tensor</code> <p>A boolean tensor that indicates which items to keep for reconstruction loss computation. If <code>None</code>, all the elements of the items are considered (i.e., the mask is all <code>True</code>).</p> <code>None</code> <code>return_predicted_img</code> <code>bool</code> <code>False</code> <code>likelihood_obj</code> <code>LikelihoodModule</code> <code>None</code> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_reconstruction_loss(\n    self,\n    reconstruction: torch.Tensor,\n    target: torch.Tensor,\n    input: torch.Tensor,\n    splitting_mask: torch.Tensor = None,\n    return_predicted_img: bool = False,\n    likelihood_obj: LikelihoodModule = None,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Parameters\n    ----------\n    reconstruction: torch.Tensor,\n    target: torch.Tensor\n    input: torch.Tensor\n    splitting_mask: torch.Tensor = None\n        A boolean tensor that indicates which items to keep for reconstruction loss computation.\n        If `None`, all the elements of the items are considered (i.e., the mask is all `True`).\n    return_predicted_img: bool = False\n    likelihood_obj: LikelihoodModule = None\n    \"\"\"\n    output = self._get_reconstruction_loss_vector(\n        reconstruction=reconstruction,\n        target=target,\n        input=input,\n        return_predicted_img=return_predicted_img,\n        likelihood_obj=likelihood_obj,\n    )\n    loss_dict = output[0] if return_predicted_img else output\n\n    if splitting_mask is None:\n        splitting_mask = torch.ones_like(loss_dict[\"loss\"]).bool()\n\n    # print(len(target) - (torch.isnan(loss_dict['loss'])).sum())\n\n    loss_dict[\"loss\"] = loss_dict[\"loss\"][splitting_mask].sum() / len(\n        reconstruction\n    )\n    for i in range(1, 1 + target.shape[1]):\n        key = f\"ch{i}_loss\"\n        loss_dict[key] = loss_dict[key][splitting_mask].sum() / len(reconstruction)\n\n    if \"mixed_loss\" in loss_dict:\n        loss_dict[\"mixed_loss\"] = torch.mean(loss_dict[\"mixed_loss\"])\n    if return_predicted_img:\n        assert len(output) == 2\n        return loss_dict, output[1]\n    else:\n        return loss_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.increment_global_step","title":"<code>increment_global_step()</code>","text":"<p>Increments global step by 1.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def increment_global_step(self):\n    \"\"\"Increments global step by 1.\"\"\"\n    self._global_step += 1\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/","title":"metrics","text":"<p>This script contains the functions/classes to compute loss and metrics used to train and evaluate the performance of the model.</p>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR","title":"<code>RunningPSNR</code>","text":"<p>This class allows to compute the running PSNR during validation step in training. In this way it is possible to compute the PSNR on the entire validation set one batch at the time.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>class RunningPSNR:\n    \"\"\"\n    This class allows to compute the running PSNR during validation step in training.\n    In this way it is possible to compute the PSNR on the entire validation set one batch at the time.\n    \"\"\"\n\n    def __init__(self):\n        # number of elements seen so far during the epoch\n        self.N = None\n        # running sum of the MSE over the self.N elements seen so far\n        self.mse_sum = None\n        # running max and min values of the self.N target images seen so far\n        self.max = self.min = None\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Used to reset the running PSNR (usually called at the end of each epoch).\n        \"\"\"\n        self.mse_sum = 0\n        self.N = 0\n        self.max = self.min = None\n\n    def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n        \"\"\"\n        Given a batch of reconstructed and target images, it updates the MSE and.\n\n        Parameters\n        ----------\n        rec: torch.Tensor\n            Batch of reconstructed images (B, H, W).\n        tar: torch.Tensor\n            Batch of target images (B, H, W).\n        \"\"\"\n        ins_max = torch.max(tar).item()\n        ins_min = torch.min(tar).item()\n        if self.max is None:\n            assert self.min is None\n            self.max = ins_max\n            self.min = ins_min\n        else:\n            self.max = max(self.max, ins_max)\n            self.min = min(self.min, ins_min)\n\n        mse = (rec - tar) ** 2\n        elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n        self.mse_sum += torch.nansum(elementwise_mse)\n        self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n\n    def get(self):\n        \"\"\"\n        The get the actual PSNR value given the running statistics.\n        \"\"\"\n        if self.N == 0 or self.N is None:\n            return None\n        rmse = torch.sqrt(self.mse_sum / self.N)\n        return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.get","title":"<code>get()</code>","text":"<p>The get the actual PSNR value given the running statistics.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def get(self):\n    \"\"\"\n    The get the actual PSNR value given the running statistics.\n    \"\"\"\n    if self.N == 0 or self.N is None:\n        return None\n    rmse = torch.sqrt(self.mse_sum / self.N)\n    return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.reset","title":"<code>reset()</code>","text":"<p>Used to reset the running PSNR (usually called at the end of each epoch).</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Used to reset the running PSNR (usually called at the end of each epoch).\n    \"\"\"\n    self.mse_sum = 0\n    self.N = 0\n    self.max = self.min = None\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.update","title":"<code>update(rec, tar)</code>","text":"<p>Given a batch of reconstructed and target images, it updates the MSE and.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>Tensor</code> <p>Batch of reconstructed images (B, H, W).</p> required <code>tar</code> <code>Tensor</code> <p>Batch of target images (B, H, W).</p> required Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n    \"\"\"\n    Given a batch of reconstructed and target images, it updates the MSE and.\n\n    Parameters\n    ----------\n    rec: torch.Tensor\n        Batch of reconstructed images (B, H, W).\n    tar: torch.Tensor\n        Batch of target images (B, H, W).\n    \"\"\"\n    ins_max = torch.max(tar).item()\n    ins_min = torch.min(tar).item()\n    if self.max is None:\n        assert self.min is None\n        self.max = ins_max\n        self.min = ins_min\n    else:\n        self.max = max(self.max, ins_max)\n        self.min = min(self.min, ins_min)\n\n    mse = (rec - tar) ** 2\n    elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n    self.mse_sum += torch.nansum(elementwise_mse)\n    self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.PSNR","title":"<code>PSNR(gt, pred, range_=None)</code>","text":"<p>Compute PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <p>Ground truth image.</p> required <code>pred</code> <p>Predicted image.</p> required Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef PSNR(gt, pred, range_=None):\n    \"\"\"\n    Compute PSNR.\n\n    Parameters\n    ----------\n    gt: array\n        Ground truth image.\n    pred: array\n        Predicted image.\n    \"\"\"\n    assert len(gt.shape) == 3, \"Images must be in shape: (batch,H,W)\"\n\n    gt = gt.view(len(gt), -1)\n    pred = pred.view(len(gt), -1)\n    return _PSNR_internal(gt, pred, range_=range_)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RangeInvariantPsnr","title":"<code>RangeInvariantPsnr(gt, pred)</code>","text":"<p>NOTE: Works only for grayscale images. Adapted from https://github.com/juglab/ScaleInvPSNR/blob/master/psnr.py It rescales the prediction to ensure that the prediction has the same range as the ground truth.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef RangeInvariantPsnr(gt: torch.Tensor, pred: torch.Tensor):\n    \"\"\"\n    NOTE: Works only for grayscale images.\n    Adapted from https://github.com/juglab/ScaleInvPSNR/blob/master/psnr.py\n    It rescales the prediction to ensure that the prediction has the same range as the ground truth.\n    \"\"\"\n    assert len(gt.shape) == 3, \"Images must be in shape: (batch,H,W)\"\n    gt = gt.view(len(gt), -1)\n    pred = pred.view(len(gt), -1)\n    ra = (torch.max(gt, dim=1).values - torch.min(gt, dim=1).values) / torch.std(\n        gt, dim=1\n    )\n    gt_ = zero_mean(gt) / torch.std(gt, dim=1, keepdim=True)\n    return _PSNR_internal(zero_mean(gt_), fix(gt_, pred), ra)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.compute_multiscale_ssim","title":"<code>compute_multiscale_ssim(gt_, pred_, range_invariant=True)</code>","text":"<p>Computes multiscale ssim for each channel. Args: gt_: ground truth image with shape (N, H, W, C) pred_: predicted image with shape (N, H, W, C) range_invariant: whether to use range invariant multiscale ssim</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def compute_multiscale_ssim(gt_, pred_, range_invariant=True):\n    \"\"\"\n    Computes multiscale ssim for each channel.\n    Args:\n    gt_: ground truth image with shape (N, H, W, C)\n    pred_: predicted image with shape (N, H, W, C)\n    range_invariant: whether to use range invariant multiscale ssim\n    \"\"\"\n    ms_ssim_values = {i: None for i in range(gt_.shape[-1])}\n    for ch_idx in range(gt_.shape[-1]):\n        tar_tmp = gt_[..., ch_idx]\n        pred_tmp = pred_[..., ch_idx]\n        if range_invariant:\n            ms_ssim_values[ch_idx] = range_invariant_multiscale_ssim(tar_tmp, pred_tmp)\n        else:\n            ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n                data_range=tar_tmp.max() - tar_tmp.min()\n            )\n            ms_ssim_values[ch_idx] = ms_ssim(\n                torch.Tensor(pred_tmp[:, None]), torch.Tensor(tar_tmp[:, None])\n            ).item()\n\n    output = [ms_ssim_values[i] for i in range(gt_.shape[-1])]\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.range_invariant_multiscale_ssim","title":"<code>range_invariant_multiscale_ssim(gt_, pred_)</code>","text":"<p>Computes range invariant multiscale ssim for one channel. This has the benefit that it is invariant to scalar multiplications in the prediction.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef range_invariant_multiscale_ssim(gt_, pred_):\n    \"\"\"\n    Computes range invariant multiscale ssim for one channel.\n    This has the benefit that it is invariant to scalar multiplications in the prediction.\n    \"\"\"\n    shape = gt_.shape\n    gt_ = torch.Tensor(gt_.reshape((shape[0], -1)))\n    pred_ = torch.Tensor(pred_.reshape((shape[0], -1)))\n    gt_ = zero_mean(gt_)\n    pred_ = zero_mean(pred_)\n    pred_ = fix(gt_, pred_)\n    pred_ = pred_.reshape(shape)\n    gt_ = gt_.reshape(shape)\n\n    ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n        data_range=gt_.max() - gt_.min()\n    )\n    return ms_ssim(torch.Tensor(pred_[:, None]), torch.Tensor(gt_[:, None])).item()\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_lvae/","title":"train_lvae","text":"<p>This script is meant to load data, initialize the model, and provide the logic for training it.</p>"},{"location":"reference/careamics/lvae_training/train_lvae/#careamics.lvae_training.train_lvae.get_mean_std_dict_for_model","title":"<code>get_mean_std_dict_for_model(config, train_dset)</code>","text":"<p>Computes the mean and std for the model. This will be subsequently passed to the model.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_mean_std_dict_for_model(config, train_dset):\n    \"\"\"\n    Computes the mean and std for the model. This will be subsequently passed to the model.\n    \"\"\"\n    mean_dict, std_dict = train_dset.get_mean_std()\n\n    return deepcopy(mean_dict), deepcopy(std_dict)\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_lvae/#careamics.lvae_training.train_lvae.get_new_model_version","title":"<code>get_new_model_version(model_dir)</code>","text":"<p>A model will have multiple runs. Each run will have a different version.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_new_model_version(model_dir: str) -&gt; str:\n    \"\"\"\n    A model will have multiple runs. Each run will have a different version.\n    \"\"\"\n    versions = []\n    for version_dir in os.listdir(model_dir):\n        try:\n            versions.append(int(version_dir))\n        except:\n            print(\n                f\"Invalid subdirectory:{model_dir}/{version_dir}. Only integer versions are allowed\"\n            )\n            exit()\n    if len(versions) == 0:\n        return \"0\"\n    return f\"{max(versions) + 1}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_utils/","title":"train_utils","text":"<p>This script contains the utility functions for training the LVAE model. These functions are mainly used in <code>train.py</code> script.</p>"},{"location":"reference/careamics/lvae_training/train_utils/#careamics.lvae_training.train_utils.get_mean_std_dict_for_model","title":"<code>get_mean_std_dict_for_model(config, train_dset)</code>","text":"<p>Computes the mean and std for the model. This will be subsequently passed to the model.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_mean_std_dict_for_model(config, train_dset):\n    \"\"\"\n    Computes the mean and std for the model. This will be subsequently passed to the model.\n    \"\"\"\n    mean_dict, std_dict = train_dset.get_mean_std()\n\n    return deepcopy(mean_dict), deepcopy(std_dict)\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_utils/#careamics.lvae_training.train_utils.get_new_model_version","title":"<code>get_new_model_version(model_dir)</code>","text":"<p>A model will have multiple runs. Each run will have a different version.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_new_model_version(model_dir: str) -&gt; str:\n    \"\"\"\n    A model will have multiple runs. Each run will have a different version.\n    \"\"\"\n    versions = []\n    for version_dir in os.listdir(model_dir):\n        try:\n            versions.append(int(version_dir))\n        except:\n            print(\n                f\"Invalid subdirectory:{model_dir}/{version_dir}. Only integer versions are allowed\"\n            )\n            exit()\n    if len(versions) == 0:\n        return \"0\"\n    return f\"{max(versions) + 1}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/config/","title":"config","text":""},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig","title":"<code>MicroSplitDataConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/careamics/lvae_training/dataset/config.py</code> <pre><code>class MicroSplitDataConfig(BaseModel):\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n\n    data_type: Union[DataType, str] | None  # TODO remove or refactor!!\n    \"\"\"Type of the dataset, should be one of DataType\"\"\"\n\n    depth3D: int | None = 1\n    \"\"\"Number of slices in 3D. If data is 2D depth3D is equal to 1\"\"\"\n\n    datasplit_type: DataSplitType | None = None\n    \"\"\"Whether to return training, validation or test split, should be one of\n    DataSplitType\"\"\"\n\n    num_channels: int | None = 2\n    \"\"\"Number of channels in the input\"\"\"\n\n    # TODO: remove ch*_fname parameters, should be parsed automatically from a name list\n    ch1_fname: str | None = None\n    ch2_fname: str | None = None\n    ch_input_fname: str | None = None\n\n    input_is_sum: bool | None = False\n    \"\"\"Whether the input is the sum or average of channels\"\"\"\n\n    input_idx: int | None = None\n    \"\"\"Index of the channel where the input is stored in the data\"\"\"\n\n    target_idx_list: list[int] | None = None\n    \"\"\"Indices of the channels where the targets are stored in the data\"\"\"\n\n    # TODO: where are there used?\n    start_alpha: Any | None = None\n    end_alpha: Any | None = None\n\n    image_size: tuple  # TODO: revisit, new model_config uses tuple\n    \"\"\"Size of one patch of data\"\"\"\n\n    grid_size: Union[int, tuple[int, int, int]] | None = None\n    \"\"\"Frame is divided into square grids of this size. A patch centered on a grid \n    having size `image_size` is returned. Grid size not used in training,\n    used only during val / test, grid size controls the overlap of the patches\"\"\"\n\n    empty_patch_replacement_enabled: bool | None = False\n    \"\"\"Whether to replace the content of one of the channels\n    with background with given probability\"\"\"\n    empty_patch_replacement_channel_idx: Any | None = None\n    empty_patch_replacement_probab: Any | None = None\n    empty_patch_max_val_threshold: Any | None = None\n\n    uncorrelated_channels: bool | None = False\n    \"\"\"Replace the content in one of the channels with given probability to make\n    channel content 'uncorrelated'\"\"\"\n    uncorrelated_channel_probab: float | None = 0.5\n\n    poisson_noise_factor: float | None = -1\n    \"\"\"The added poisson noise factor\"\"\"\n\n    synthetic_gaussian_scale: float | None = 0.1\n\n    # TODO: set to True in training code, recheck\n    input_has_dependant_noise: bool | None = False\n\n    # TODO: sometimes max_val differs between runs with fixed seeds with noise enabled\n    enable_gaussian_noise: bool | None = False\n    \"\"\"Whether to enable gaussian noise\"\"\"\n\n    # TODO: is this parameter used?\n    allow_generation: bool = False\n\n    # TODO: both used in IndexSwitcher, insure correct passing\n    training_validtarget_fraction: Any = None\n    deterministic_grid: Any = None\n\n    # TODO: why is this not used?\n    enable_rotation_aug: bool | None = False\n\n    max_val: Union[float, tuple] | None = None\n    \"\"\"Maximum data in the dataset. Is calculated for train split, and should be\n    externally set for val and test splits.\"\"\"\n\n    overlapping_padding_kwargs: Any = None\n    \"\"\"Parameters for np.pad method\"\"\"\n\n    # TODO: remove this parameter, controls debug print\n    print_vars: bool | None = False\n\n    # Hard-coded parameters (used to be in the config file)\n    normalized_input: bool = True\n    \"\"\"If this is set to true, then one mean and stdev is used\n                for both channels. Otherwise, two different mean and stdev are used.\"\"\"\n    use_one_mu_std: bool | None = True\n\n    # TODO: is this parameter used?\n    train_aug_rotate: bool | None = False\n    enable_random_cropping: bool | None = True\n\n    multiscale_lowres_count: int | None = None\n    \"\"\"Number of LC scales\"\"\"\n\n    tiling_mode: TilingMode | None = TilingMode.ShiftBoundary\n\n    target_separate_normalization: bool | None = True\n\n    mode_3D: bool | None = False\n    \"\"\"If training in 3D mode or not\"\"\"\n\n    trainig_datausage_fraction: float | None = 1.0\n\n    validtarget_random_fraction: float | None = None\n\n    validation_datausage_fraction: float | None = 1.0\n\n    random_flip_z_3D: bool | None = False\n\n    padding_kwargs: dict = {\"mode\": \"reflect\"}  # TODO remove !!\n\n    def __init__(self, **data):\n        # Convert string data_type to enum if needed\n        if \"data_type\" in data and isinstance(data[\"data_type\"], str):\n            try:\n                data[\"data_type\"] = DataType[data[\"data_type\"]]\n            except KeyError:\n                # Keep original value to let validation handle the error\n                pass\n        super().__init__(**data)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of the dataset, should be one of DataType</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.datasplit_type","title":"<code>datasplit_type = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to return training, validation or test split, should be one of DataSplitType</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.depth3D","title":"<code>depth3D = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of slices in 3D. If data is 2D depth3D is equal to 1</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.empty_patch_replacement_enabled","title":"<code>empty_patch_replacement_enabled = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to replace the content of one of the channels with background with given probability</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.enable_gaussian_noise","title":"<code>enable_gaussian_noise = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to enable gaussian noise</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.grid_size","title":"<code>grid_size = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frame is divided into square grids of this size. A patch centered on a grid  having size <code>image_size</code> is returned. Grid size not used in training, used only during val / test, grid size controls the overlap of the patches</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.image_size","title":"<code>image_size</code>  <code>instance-attribute</code>","text":"<p>Size of one patch of data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.input_idx","title":"<code>input_idx = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the channel where the input is stored in the data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.input_is_sum","title":"<code>input_is_sum = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the input is the sum or average of channels</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.max_val","title":"<code>max_val = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum data in the dataset. Is calculated for train split, and should be externally set for val and test splits.</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.mode_3D","title":"<code>mode_3D = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If training in 3D mode or not</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.multiscale_lowres_count","title":"<code>multiscale_lowres_count = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of LC scales</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.normalized_input","title":"<code>normalized_input = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If this is set to true, then one mean and stdev is used for both channels. Otherwise, two different mean and stdev are used.</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.num_channels","title":"<code>num_channels = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels in the input</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.overlapping_padding_kwargs","title":"<code>overlapping_padding_kwargs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters for np.pad method</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.poisson_noise_factor","title":"<code>poisson_noise_factor = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The added poisson noise factor</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.target_idx_list","title":"<code>target_idx_list = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indices of the channels where the targets are stored in the data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.MicroSplitDataConfig.uncorrelated_channels","title":"<code>uncorrelated_channels = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Replace the content in one of the channels with given probability to make channel content 'uncorrelated'</p>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/","title":"lc_dataset","text":"<p>A place for Datasets and Dataloaders.</p>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/#careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader","title":"<code>LCMultiChDloader</code>","text":"<p>               Bases: <code>MultiChDloader</code></p> <p>Multi-channel dataset loader for LC-style datasets.</p> Source code in <code>src/careamics/lvae_training/dataset/lc_dataset.py</code> <pre><code>class LCMultiChDloader(MultiChDloader):\n    \"\"\"Multi-channel dataset loader for LC-style datasets.\"\"\"\n\n    def __init__(\n        self,\n        data_config: MicroSplitDataConfig,\n        datapath: Union[str, Path],\n        load_data_fn: Optional[Callable] = None,\n        val_fraction: float = 0.1,\n        test_fraction: float = 0.1,\n        allow_generation: bool = False,\n    ):\n        self._padding_kwargs = (\n            data_config.padding_kwargs  # mode=padding_mode, constant_values=constant_value\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n\n        super().__init__(\n            data_config,\n            datapath,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n        )\n\n        if data_config.overlapping_padding_kwargs is not None:\n            assert (\n                self._padding_kwargs == data_config.overlapping_padding_kwargs\n            ), \"During evaluation, overlapping_padding_kwargs should be same as padding_args. \\\n                It should be so since we just use overlapping_padding_kwargs when it is not None\"\n\n        else:\n            self._overlapping_padding_kwargs = data_config.padding_kwargs\n\n        self.multiscale_lowres_count = data_config.multiscale_lowres_count\n        assert self.multiscale_lowres_count is not None\n        self._scaled_data = [self._data]\n        self._scaled_noise_data = [self._noise_data]\n\n        assert (\n            isinstance(self.multiscale_lowres_count, int)\n            and self.multiscale_lowres_count &gt;= 1\n        )\n        assert isinstance(self._padding_kwargs, dict)\n        assert \"mode\" in self._padding_kwargs\n\n        for _ in range(1, self.multiscale_lowres_count):\n            shape = self._scaled_data[-1].shape\n            assert len(shape) == 4\n            new_shape = (shape[0], shape[1] // 2, shape[2] // 2, shape[3])\n            ds_data = resize(\n                self._scaled_data[-1].astype(np.float32), new_shape\n            ).astype(self._scaled_data[-1].dtype)\n            # NOTE: These asserts are important. the resize method expects np.float32. otherwise, one gets weird results.\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &lt; 5\n            ), \"Downsampled image should not have very different values\"\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &gt; 0.2\n            ), \"Downsampled image should not have very different values\"\n\n            self._scaled_data.append(ds_data)\n            # do the same for noise\n            if self._noise_data is not None:\n                noise_data = resize(self._scaled_noise_data[-1], new_shape)\n                self._scaled_noise_data.append(noise_data)\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert t_list is not None\n        assert h_start is None\n        assert h_end is None\n        assert w_start is None\n        assert w_end is None\n\n        self._data = self._data[t_list].copy()\n        self._scaled_data = [\n            self._scaled_data[i][t_list].copy() for i in range(len(self._scaled_data))\n        ]\n\n        if self._noise_data is not None:\n            self._noise_data = self._noise_data[t_list].copy()\n            self._scaled_noise_data = [\n                self._scaled_noise_data[i][t_list].copy()\n                for i in range(len(self._scaled_noise_data))\n            ]\n\n        self.N = len(t_list)\n        # TODO where tf is self._img_sz defined?\n        self.set_img_sz([self._img_sz, self._img_sz], self._grid_sz)\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data shape: {self._data.shape}\"\n        )\n\n    def _init_msg(self):\n        msg = super()._init_msg()\n        msg += f\" Pad:{self._padding_kwargs}\"\n        if self._uncorrelated_channels:\n            msg += f\" UncorrChProbab:{self._uncorrelated_channel_probab}\"\n        return msg\n\n    def _load_scaled_img(\n        self, scaled_index, index: Union[int, tuple[int, int]]\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        if isinstance(index, int):\n            idx = index\n        else:\n            idx, _ = index\n\n        # tidx = self.idx_manager.get_t(idx)\n        patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n        nidx = patch_loc_list[0]\n\n        imgs = self._scaled_data[scaled_index][nidx]\n        imgs = tuple([imgs[None, ..., i] for i in range(imgs.shape[-1])])\n        if self._noise_data is not None:\n            noisedata = self._scaled_noise_data[scaled_index][nidx]\n            noise = tuple([noisedata[None, ..., i] for i in range(noisedata.shape[-1])])\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            imgs = tuple([img + noise[0] * factor for img in imgs])\n        return imgs\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: tuple):\n        \"\"\"\n        Here, h_start, w_start could be negative. That simply means we need to pick the content from 0. So,\n        the cropped image will be smaller than self._img_sz * self._img_sz\n        \"\"\"\n        max_len_vals = list(self.idx_manager.data_shape[1:-1])\n        max_len_vals[-2:] = img.shape[-2:]\n        return self._crop_img_with_padding(\n            img, patch_start_loc, max_len_vals=max_len_vals\n        )\n\n    def _get_img(self, index: int):\n        \"\"\"\n        Returns the primary patch along with low resolution patches centered on the primary patch.\n        \"\"\"\n        # Noise_tuples is populated when there is synthetic noise in training\n        # Should have similar type of noise with the noise model\n        # Starting with microsplit, dump the noise, use it instead as an augmentation if nessesary\n        img_tuples, noise_tuples = self._load_img(index)\n        assert self._img_sz is not None\n        h, w = img_tuples[0].shape[-2:]\n        if self._enable_random_cropping:\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._5Ddata:\n                patch_start_loc = (\n                    np.random.choice(img_tuples[0].shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            patch_start_loc = self._get_deterministic_loc(index)\n\n        # LC logic is located here, the function crops the image of the highest resolution\n        cropped_img_tuples = [\n            self._crop_flip_img(img, patch_start_loc, False, False)\n            for img in img_tuples\n        ]\n        cropped_noise_tuples = [\n            self._crop_flip_img(noise, patch_start_loc, False, False)\n            for noise in noise_tuples\n        ]\n        patch_start_loc = list(patch_start_loc)\n        h_start, w_start = patch_start_loc[-2], patch_start_loc[-1]\n        h_center = h_start + self._img_sz // 2\n        w_center = w_start + self._img_sz // 2\n        allres_versions = {\n            i: [cropped_img_tuples[i]] for i in range(len(cropped_img_tuples))\n        }\n        for scale_idx in range(1, self.multiscale_lowres_count):\n            # Returning the image of the lower resolution\n            scaled_img_tuples = self._load_scaled_img(scale_idx, index)\n\n            h_center = h_center // 2\n            w_center = w_center // 2\n\n            h_start = h_center - self._img_sz // 2\n            w_start = w_center - self._img_sz // 2\n            patch_start_loc[-2:] = [h_start, w_start]\n            scaled_cropped_img_tuples = [\n                self._crop_flip_img(img, patch_start_loc, False, False)\n                for img in scaled_img_tuples\n            ]\n            for ch_idx in range(len(img_tuples)):\n                allres_versions[ch_idx].append(scaled_cropped_img_tuples[ch_idx])\n\n        output_img_tuples = tuple(\n            [\n                np.concatenate(allres_versions[ch_idx])\n                for ch_idx in range(len(img_tuples))\n            ]\n        )\n        return output_img_tuples, cropped_noise_tuples\n\n    def __getitem__(self, index: Union[int, tuple[int, int]]):\n        img_tuples, noise_tuples = self._get_img(index)\n        if self._uncorrelated_channels:\n            assert (\n                self._input_idx is None\n            ), \"Uncorrelated channels is not implemented when there is a separate input channel.\"\n            if np.random.rand() &lt; self._uncorrelated_channel_probab:\n                img_tuples_new = [None] * len(img_tuples)\n                img_tuples_new[0] = img_tuples[0]\n                for i in range(1, len(img_tuples)):\n                    new_index = np.random.randint(len(self))\n                    img_tuples_tmp, _ = self._get_img(new_index)\n                    img_tuples_new[i] = img_tuples_tmp[i]\n                img_tuples = img_tuples_new\n\n        if self._is_train:\n            if self._empty_patch_replacement_enabled:\n                if np.random.rand() &lt; self._empty_patch_replacement_probab:\n                    img_tuples = self.replace_with_empty_patch(img_tuples)\n\n        if self._enable_rotation:\n            img_tuples, noise_tuples = self._rotate(img_tuples, noise_tuples)\n\n        # add noise to input, if noise is present combine it with the image\n        # factor is for the compute input not to have too much noise because the average of two gaussians\n        if len(noise_tuples) &gt; 0:\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            input_tuples = []\n            for x in img_tuples:\n                x = (\n                    x.copy()\n                )  # to avoid changing the original image since it is later used for target\n                # NOTE: other LC levels already have noise added. So, we just need to add noise to the highest resolution.\n                x[0] = x[0] + noise_tuples[0] * factor\n                input_tuples.append(x)\n        else:\n            input_tuples = img_tuples\n\n        # Compute the input by sum / average the channels\n        # Alpha is an amount of weight which is applied to the channels when combining them\n        # How to sample alpha is still under research\n        inp, alpha = self._compute_input(input_tuples)\n        target_tuples = [img[:1] for img in img_tuples]\n        # add noise to target.\n        if len(noise_tuples) &gt;= 1:\n            target_tuples = [\n                x + noise for x, noise in zip(target_tuples, noise_tuples[1:])\n            ]\n\n        target = self._compute_target(target_tuples, alpha)\n\n        norm_target = self.normalize_target(target)\n\n        output = [inp, norm_target]\n\n        if self._return_alpha:\n            output.append(alpha)\n\n        if isinstance(index, int):\n            return tuple(output)\n\n        _, grid_size = index\n        output.append(grid_size)\n        return tuple(output)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/","title":"ms_dataset_ref","text":"<p>A place for Datasets and Dataloaders.</p>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.LCMultiChDloaderRef","title":"<code>LCMultiChDloaderRef</code>","text":"<p>               Bases: <code>MultiChDloaderRef</code></p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>class LCMultiChDloaderRef(MultiChDloaderRef):\n    def __init__(\n        self,\n        data_config: MicroSplitDataConfig,\n        fpath: str,\n        load_data_fn: Callable,\n        val_fraction=None,\n        test_fraction=None,\n    ):\n        self._padding_kwargs = (\n            data_config.padding_kwargs  # mode=padding_mode, constant_values=constant_value\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n\n        super().__init__(\n            data_config,\n            fpath,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n        )\n\n        if data_config.overlapping_padding_kwargs is not None:\n            assert (\n                self._padding_kwargs == data_config.overlapping_padding_kwargs\n            ), \"During evaluation, overlapping_padding_kwargs should be same as padding_args. \\\n                It should be so since we just use overlapping_padding_kwargs when it is not None\"\n\n        else:\n            self._overlapping_padding_kwargs = data_config.padding_kwargs\n\n        self.multiscale_lowres_count = data_config.multiscale_lowres_count\n        assert self.multiscale_lowres_count is not None\n        self._scaled_data = [self._data]\n        self._scaled_noise_data = [self._noise_data]\n\n        assert (\n            isinstance(self.multiscale_lowres_count, int)\n            and self.multiscale_lowres_count &gt;= 1\n        )\n        assert isinstance(self._padding_kwargs, dict)\n        assert \"mode\" in self._padding_kwargs\n\n        for _ in range(1, self.multiscale_lowres_count):\n            shape = self._scaled_data[-1].shape\n            assert len(shape) == 4\n            new_shape = (shape[0], shape[1] // 2, shape[2] // 2, shape[3])\n            ds_data = resize(\n                self._scaled_data[-1].astype(np.float32), new_shape\n            ).astype(self._scaled_data[-1].dtype)\n            # NOTE: These asserts are important. the resize method expects np.float32. otherwise, one gets weird results.\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &lt; 5\n            ), \"Downsampled image should not have very different values\"\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &gt; 0.2\n            ), \"Downsampled image should not have very different values\"\n\n            self._scaled_data.append(ds_data)\n            # do the same for noise\n            if self._noise_data is not None:\n                noise_data = resize(self._scaled_noise_data[-1], new_shape)\n                self._scaled_noise_data.append(noise_data)\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert t_list is not None\n        assert h_start is None\n        assert h_end is None\n        assert w_start is None\n        assert w_end is None\n\n        self._data = self._data[t_list].copy()\n        self._scaled_data = [\n            self._scaled_data[i][t_list].copy() for i in range(len(self._scaled_data))\n        ]\n\n        if self._noise_data is not None:\n            self._noise_data = self._noise_data[t_list].copy()\n            self._scaled_noise_data = [\n                self._scaled_noise_data[i][t_list].copy()\n                for i in range(len(self._scaled_noise_data))\n            ]\n\n        self.N = len(t_list)\n        # TODO where tf is self._img_sz defined?\n        self.set_img_sz([self._img_sz, self._img_sz], self._grid_sz)\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data shape: {self._data.shape}\"\n        )\n\n    def _init_msg(self):\n        msg = super()._init_msg()\n        msg += f\" Pad:{self._padding_kwargs}\"\n        if self._uncorrelated_channels:\n            msg += f\" UncorrChProbab:{self._uncorrelated_channel_probab}\"\n        return msg\n\n    def _load_scaled_img(\n        self, scaled_index, index: Union[int, tuple[int, int]]\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        if isinstance(index, int):\n            idx = index\n        else:\n            idx, _ = index\n\n        # tidx = self.idx_manager.get_t(idx)\n        patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n        nidx = patch_loc_list[0]\n\n        imgs = self._scaled_data[scaled_index][nidx]\n        imgs = tuple([imgs[None, ..., i] for i in range(imgs.shape[-1])])\n        if self._noise_data is not None:\n            noisedata = self._scaled_noise_data[scaled_index][nidx]\n            noise = tuple([noisedata[None, ..., i] for i in range(noisedata.shape[-1])])\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            imgs = tuple([img + noise[0] * factor for img in imgs])\n        return imgs\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: tuple):\n        \"\"\"\n        Here, h_start, w_start could be negative. That simply means we need to pick the content from 0. So,\n        the cropped image will be smaller than self._img_sz * self._img_sz\n        \"\"\"\n        max_len_vals = list(self.idx_manager.data_shape[1:-1])\n        max_len_vals[-2:] = img.shape[-2:]\n        return self._crop_img_with_padding(\n            img, patch_start_loc, max_len_vals=max_len_vals\n        )\n\n    def _get_img(self, index: int):\n        \"\"\"\n        Returns the primary patch along with low resolution patches centered on the primary patch.\n        \"\"\"\n        # Noise_tuples is populated when there is synthetic noise in training\n        # Should have similar type of noise with the noise model\n        # Starting with microsplit, dump the noise, use it instead as an augmentation if nessesary\n        img_tuples, noise_tuples = self._load_img(index)\n        assert self._img_sz is not None\n        h, w = img_tuples[0].shape[-2:]\n        if self._enable_random_cropping:\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._3Ddata:\n                patch_start_loc = (\n                    np.random.choice(img_tuples[0].shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            patch_start_loc = self._get_deterministic_loc(index)\n\n        # LC logic is located here, the function crops the image of the highest resolution\n        cropped_img_tuples = [\n            self._crop_flip_img(img, patch_start_loc, False, False)\n            for img in img_tuples\n        ]\n        cropped_noise_tuples = [\n            self._crop_flip_img(noise, patch_start_loc, False, False)\n            for noise in noise_tuples\n        ]\n        patch_start_loc = list(patch_start_loc)\n        h_start, w_start = patch_start_loc[-2], patch_start_loc[-1]\n        h_center = h_start + self._img_sz // 2\n        w_center = w_start + self._img_sz // 2\n        allres_versions = {\n            i: [cropped_img_tuples[i]] for i in range(len(cropped_img_tuples))\n        }\n        for scale_idx in range(1, self.multiscale_lowres_count):\n            # Returning the image of the lower resolution\n            scaled_img_tuples = self._load_scaled_img(scale_idx, index)\n\n            h_center = h_center // 2\n            w_center = w_center // 2\n\n            h_start = h_center - self._img_sz // 2\n            w_start = w_center - self._img_sz // 2\n            patch_start_loc[-2:] = [h_start, w_start]\n            scaled_cropped_img_tuples = [\n                self._crop_flip_img(img, patch_start_loc, False, False)\n                for img in scaled_img_tuples\n            ]\n            for ch_idx in range(len(img_tuples)):\n                allres_versions[ch_idx].append(scaled_cropped_img_tuples[ch_idx])\n\n        output_img_tuples = tuple(\n            [\n                np.concatenate(allres_versions[ch_idx])\n                for ch_idx in range(len(img_tuples))\n            ]\n        )\n        return output_img_tuples, cropped_noise_tuples\n\n    def __getitem__(self, index: Union[int, tuple[int, int]]):\n        img_tuples, noise_tuples = self._get_img(index)\n        if self._uncorrelated_channels:\n            assert (\n                self._input_idx is None\n            ), \"Uncorrelated channels is not implemented when there is a separate input channel.\"\n            if np.random.rand() &lt; self._uncorrelated_channel_probab:\n                img_tuples_new = [None] * len(img_tuples)\n                img_tuples_new[0] = img_tuples[0]\n                for i in range(1, len(img_tuples)):\n                    new_index = np.random.randint(len(self))\n                    img_tuples_tmp, _ = self._get_img(new_index)\n                    img_tuples_new[i] = img_tuples_tmp[i]\n                img_tuples = img_tuples_new\n\n        if self._is_train:\n            if self._empty_patch_replacement_enabled:\n                if np.random.rand() &lt; self._empty_patch_replacement_probab:\n                    img_tuples = self.replace_with_empty_patch(img_tuples)\n\n        if self._enable_rotation:\n            img_tuples, noise_tuples = self._rotate(img_tuples, noise_tuples)\n\n        # add noise to input, if noise is present combine it with the image\n        # factor is for the compute input not to have too much noise because the average of two gaussians\n        if len(noise_tuples) &gt; 0:\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            input_tuples = []\n            for x in img_tuples:\n                x = (\n                    x.copy()\n                )  # to avoid changing the original image since it is later used for target\n                # NOTE: other LC levels already have noise added. So, we just need to add noise to the highest resolution.\n                x[0] = x[0] + noise_tuples[0] * factor\n                input_tuples.append(x)\n        else:\n            input_tuples = img_tuples\n\n        # Compute the input by sum / average the channels\n        # Alpha is an amount of weight which is applied to the channels when combining them\n        # How to sample alpha is still under research\n        inp, alpha = self._compute_input(input_tuples)\n        target_tuples = [img[:1] for img in img_tuples]\n        # add noise to target.\n        if len(noise_tuples) &gt;= 1:\n            target_tuples = [\n                x + noise for x, noise in zip(target_tuples, noise_tuples[1:])\n            ]\n\n        target = self._compute_target(target_tuples, alpha)\n\n        norm_target = self.normalize_target(target)\n\n        output = [inp, norm_target]\n\n        if self._return_alpha:\n            output.append(alpha)\n\n        if isinstance(index, int):\n            return tuple(output)\n\n        _, grid_size = index\n        output.append(grid_size)\n        return tuple(output)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef","title":"<code>MultiChDloaderRef</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>class MultiChDloaderRef:\n    def __init__(\n        self,\n        data_config: MicroSplitDataConfig,\n        fpath: str,\n        load_data_fn: Callable,\n        val_fraction: float = None,\n        test_fraction: float = None,\n    ):\n        \"\"\" \"\"\"\n        self._data_type = data_config.data_type\n        self._fpath = Path(fpath)\n        self._data = None\n        self._3Ddata = False  # TODO wtf it was 5D\n        self._tiling_mode = data_config.tiling_mode\n        # by default, if the noise is present, add it to the input and target.\n        self._depth3D = data_config.depth3D\n        self._mode_3D = data_config.mode_3D\n        # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n        self._input_is_sum = data_config.input_is_sum\n        self._num_channels = data_config.num_channels\n        self._input_idx = data_config.input_idx\n        self._tar_idx_list = data_config.target_idx_list\n\n        self.load_data(\n            data_config,\n            data_config.datasplit_type,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=data_config.allow_generation,\n        )\n\n        self._data_shapes = self.get_data_shapes()\n        self._normalized_input = data_config.normalized_input\n        self._quantile = 1.0\n        self._channelwise_quantile = False\n        self._background_quantile = 0.0\n        self._clip_background_noise_to_zero = False\n        self._skip_normalization_using_mean = False\n        self._empty_patch_replacement_enabled = False\n\n        self._background_values = None\n\n        self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            if (\n                self._overlapping_padding_kwargs is None\n                or data_config.multiscale_lowres_count is not None\n            ):\n                # raise warning\n                print(\"Padding is not used with this alignement style\")\n        else:\n            assert (\n                self._overlapping_padding_kwargs is not None\n            ), \"When not trimming boudnary, padding is needed.\"\n\n        self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n        # input = alpha * ch1 + (1-alpha)*ch2.\n        # alpha is sampled randomly between these two extremes\n        self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n        self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n        # changed set_img_sz because \"grid_size\" in data_config returns false\n        try:\n            grid_size = data_config.grid_size\n        except AttributeError:\n            grid_size = data_config.image_size\n\n        if self._is_train:\n            self._start_alpha_arr = data_config.start_alpha  # TODO why only for train?\n            self._end_alpha_arr = data_config.end_alpha\n\n        self.set_img_sz(data_config.image_size, grid_size)\n\n        self._empty_patch_replacement_enabled = (\n            data_config.empty_patch_replacement_enabled and self._is_train\n        )\n        if self._empty_patch_replacement_enabled:\n            self._empty_patch_replacement_channel_idx = (\n                data_config.empty_patch_replacement_channel_idx\n            )\n            self._empty_patch_replacement_probab = (\n                data_config.empty_patch_replacement_probab\n            )\n            data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n            # NOTE: This is on the raw data. So, it must be called before removing the background.\n            self._empty_patch_fetcher = EmptyPatchFetcher(\n                self.idx_manager,\n                self._img_sz,\n                data_frames,\n                max_val_threshold=data_config.empty_patch_max_val_threshold,\n            )\n\n        self.rm_bkground_set_max_val_and_upperclip_data(\n            data_config.max_val, data_config.datasplit_type\n        )\n\n        # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n        self._mean = None\n        self._std = None\n        self._use_one_mu_std = data_config.use_one_mu_std\n\n        self._target_separate_normalization = data_config.target_separate_normalization\n\n        self._enable_rotation = data_config.enable_rotation_aug\n        flipz_3D = data_config.random_flip_z_3D\n        self._flipz_3D = flipz_3D and self._enable_rotation\n\n        self._enable_random_cropping = data_config.enable_random_cropping\n        self._uncorrelated_channels = (\n            data_config.uncorrelated_channels and self._is_train\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n        assert self._is_train or self._uncorrelated_channels is False\n        assert (\n            self._enable_random_cropping is True or self._uncorrelated_channels is False\n        )\n        # Randomly rotate [-90,90]\n\n        self._rotation_transform = None\n        if self._enable_rotation:\n            # TODO: fix this import\n            import albumentations as A\n\n            self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n\n        # TODO: remove print log messages\n        # if print_vars:\n        #     msg = self._init_msg()\n        #     print(msg)\n\n    def get_data_shapes(self):\n        if self._3Ddata:  # TODO we assume images don't have a channel dimension\n            [\n                [\n                    im.shape if len(im.shape) == 4 else (1, *im.shape)\n                    for im in self._data[ch]\n                ]\n                for ch in range(len(self._data))\n            ]\n        else:\n            return [\n                [\n                    im.shape if len(im.shape) == 3 else (1, *im.shape)\n                    for im in self._data[ch]\n                ]\n                for ch in range(len(self._data))\n            ]\n\n    def load_data(\n        self,\n        data_config: MicroSplitDataConfig,\n        datasplit_type: DataSplitType,\n        load_data_fn: Callable,\n        val_fraction=None,\n        test_fraction=None,\n        allow_generation=None,\n    ):\n        self._data = load_data_fn(\n            data_config,\n            self._fpath,\n            datasplit_type,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=allow_generation,\n        )\n\n    # TODO check for 2D/3D data consistency with config\n    # TODO check number of channels consistency with config\n\n    def save_background(self, channel_idx, frame_idx, background_value):\n        self._background_values[frame_idx, channel_idx] = background_value\n\n    def get_background(self, channel_idx, frame_idx):\n        return self._background_values[frame_idx, channel_idx]\n\n    def rm_bkground_set_max_val_and_upperclip_data(self, max_val, datasplit_type):\n        # self.remove_background() # TODO revisit\n        self.set_max_val(max_val, datasplit_type)\n        self.upperclip_data()\n\n    def upperclip_data(self):\n        for ch_idx, data in enumerate(self._data):\n            if self.max_val[ch_idx] is not None:\n                for idx in range(len(data)):\n                    data[idx][data[idx] &gt; self.max_val[ch_idx]] = self.max_val[ch_idx]\n\n    def compute_max_val(self):\n        # TODO add channelwise quantile ?\n        return [\n            max([np.quantile(im, self._quantile) for im in ch]) for ch in self._data\n        ]\n\n    def set_max_val(self, max_val, datasplit_type):\n        if max_val is None:\n            assert datasplit_type in [DataSplitType.Train, DataSplitType.All]\n            self.max_val = self.compute_max_val()\n        else:\n            assert max_val is not None\n            self.max_val = max_val\n\n    def get_max_val(self):\n        return self.max_val\n\n    def get_img_sz(self):\n        return self._img_sz\n\n    def get_num_frames(self):\n        \"\"\"Returns the number of the longest channel.\"\"\"\n        return max(self.idx_manager.total_grid_count()[0])\n\n    def reduce_data(\n        self,\n        t_list=None,\n        z_start=None,\n        z_end=None,\n        h_start=None,\n        h_end=None,\n        w_start=None,\n        w_end=None,\n    ):\n        raise NotImplementedError(\"Not implemented\")\n\n    def get_idx_manager_shapes(\n        self, patch_size: int, grid_size: Union[int, tuple[int, int, int]]\n    ):\n        numC = len(self._data_shapes)\n        if self._3Ddata:\n            patch_shape = (1, self._depth3D, patch_size, patch_size)\n            if isinstance(grid_size, int):\n                grid_shape = (1, 1, grid_size, grid_size)\n            else:\n                assert len(grid_size) == 3\n                assert all(\n                    [g &lt;= p for g, p in zip(grid_size, patch_shape[1:-1])]\n                ), f\"Grid size {grid_size} must be less than patch size {patch_shape[1:-1]}\"\n                grid_shape = (1, grid_size[0], grid_size[1], grid_size[2])\n        else:\n            assert isinstance(grid_size, int)\n            grid_shape = (1, grid_size, grid_size)\n            patch_shape = (1, patch_size, patch_size)\n\n        return patch_shape, grid_shape\n\n    def set_img_sz(self, image_size, grid_size: Union[int, tuple[int, int, int]]):\n        \"\"\"\n        If one wants to change the image size on the go, then this can be used.\n        Args:\n            image_size: size of one patch\n            grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n        \"\"\"\n        # hacky way to deal with image shape from new conf\n        self._img_sz = image_size[-1]  # TODO revisit!\n        self._grid_sz = grid_size\n        shapes = self._data_shapes\n\n        patch_shape, grid_shape = self.get_idx_manager_shapes(\n            self._img_sz, self._grid_sz\n        )\n        self.idx_manager = GridIndexManagerRef(\n            shapes, grid_shape, patch_shape, self._tiling_mode\n        )\n\n    def __len__(self):\n        # If channel length is not equal, return the longest\n        return max(self.idx_manager.total_grid_count()[0])\n\n    def _init_msg(\n        self,\n    ):\n        msg = (\n            f\"[{self.__class__.__name__}] Train:{int(self._is_train)} Sz:{self._img_sz}\"\n        )\n        dim_sizes = [\n            self.idx_manager.get_individual_dim_grid_count(dim)\n            for dim in range(len(self._data.shape))\n        ]\n        dim_sizes = \",\".join([str(x) for x in dim_sizes])\n        msg += f\" N:{self.N} NumPatchPerN:{self._repeat_factor}\"\n        msg += f\"{self.idx_manager.total_grid_count()} DimSz:({dim_sizes})\"\n        msg += f\" TrimB:{self._tiling_mode}\"\n        # msg += f' NormInp:{self._normalized_input}'\n        # msg += f' SingleNorm:{self._use_one_mu_std}'\n        msg += f\" Rot:{self._enable_rotation}\"\n        if self._flipz_3D:\n            msg += f\" FlipZ:{self._flipz_3D}\"\n\n        msg += f\" RandCrop:{self._enable_random_cropping}\"\n        msg += f\" Channel:{self._num_channels}\"\n        # msg += f' Q:{self._quantile}'\n        if self._input_is_sum:\n            msg += f\" SummedInput:{self._input_is_sum}\"\n\n        if self._empty_patch_replacement_enabled:\n            msg += f\" ReplaceWithRandSample:{self._empty_patch_replacement_enabled}\"\n        if self._uncorrelated_channels:\n            msg += f\" Uncorr:{self._uncorrelated_channels}\"\n        if self._empty_patch_replacement_enabled:\n            msg += f\"-{self._empty_patch_replacement_channel_idx}-{self._empty_patch_replacement_probab}\"\n        if self._background_quantile &gt; 0.0:\n            msg += f\" BckQ:{self._background_quantile}\"\n\n        if self._start_alpha_arr is not None:\n            msg += f\" Alpha:[{self._start_alpha_arr},{self._end_alpha_arr}]\"\n        return msg\n\n    def _crop_imgs(self, ch_idx: int, patch_idx: int, img: np.ndarray):\n        h, w = img.shape[-2:]\n        if self._img_sz is None:\n            return (\n                img,\n                {\"h\": [0, h], \"w\": [0, w], \"hflip\": False, \"wflip\": False},\n            )\n\n        if self._enable_random_cropping:\n            # this parameter is ambiguous. It toggles between random/deterministic patching\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._3Ddata:\n                patch_start_loc = (\n                    np.random.choice(1 + img.shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            # Patch coordinates are calculated by the index manager.\n            patch_start_loc = self._get_deterministic_loc(ch_idx, patch_idx)\n        cropped_img = self._crop_flip_img(img, patch_start_loc, False, False)\n\n        return cropped_img\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: tuple):\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            # In training, this is used.\n            # NOTE: It is my opinion that if I just use self._crop_img_with_padding, it will work perfectly fine.\n            # The only benefit this if else loop provides is that it makes it easier to see what happens during training.\n            patch_end_loc = (\n                np.array(patch_start_loc, dtype=np.int32)\n                + self.idx_manager.patch_shape[1:-1]\n            )\n            if self._3Ddata:\n                z_start, h_start, w_start = patch_start_loc\n                z_end, h_end, w_end = patch_end_loc\n                new_img = img[..., z_start:z_end, h_start:h_end, w_start:w_end]\n            else:\n                h_start, w_start = patch_start_loc\n                h_end, w_end = patch_end_loc\n                new_img = img[..., h_start:h_end, w_start:w_end]\n\n            return new_img\n        else:\n            # During evaluation, this is used. In this situation, we can have negative h_start, w_start. Or h_start +self._img_sz can be larger than frame\n            # In these situations, we need some sort of padding. This is not needed  in the LeftTop alignement.\n            return self._crop_img_with_padding(img, patch_start_loc)\n\n    def get_begin_end_padding(self, start_pos, end_pos, max_len):\n        \"\"\"\n        The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n        padding on all four sides so that the final patch size is self._img_sz.\n        \"\"\"\n        pad_start = 0\n        pad_end = 0\n        if start_pos &lt; 0:\n            pad_start = -1 * start_pos\n\n        pad_end = max(0, end_pos - max_len)\n\n        return pad_start, pad_end\n\n    def _crop_img_with_padding(\n        self, img: np.ndarray, patch_start_loc, max_len_vals=None\n    ):\n        if max_len_vals is None:\n            max_len_vals = self.idx_manager.data_shape[1:-1]\n        patch_end_loc = np.array(patch_start_loc, dtype=int) + np.array(\n            self.idx_manager.patch_shape[1:-1], dtype=int\n        )\n        boundary_crossed = []\n        valid_slice = []\n        padding = [[0, 0]]\n        for start_idx, end_idx, max_len in zip(\n            patch_start_loc, patch_end_loc, max_len_vals\n        ):\n            boundary_crossed.append(end_idx &gt; max_len or start_idx &lt; 0)\n            valid_slice.append((max(0, start_idx), min(max_len, end_idx)))\n            pad = [0, 0]\n            if boundary_crossed[-1]:\n                pad = self.get_begin_end_padding(start_idx, end_idx, max_len)\n            padding.append(pad)\n        # max() is needed since h_start could be negative.\n        if self._3Ddata:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n                valid_slice[2][0] : valid_slice[2][1],\n            ]\n        else:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n            ]\n\n        # print(np.array(padding).shape, img.shape, new_img.shape)\n        # print(padding)\n        if not np.all(padding == 0):\n            new_img = np.pad(new_img, padding, **self._overlapping_padding_kwargs)\n\n        return new_img\n\n    def _crop_flip_img(\n        self, img: np.ndarray, patch_start_loc: tuple, h_flip: bool, w_flip: bool\n    ):\n        new_img = self._crop_img(img, patch_start_loc)\n        if h_flip:\n            new_img = new_img[..., ::-1, :]\n        if w_flip:\n            new_img = new_img[..., :, ::-1]\n\n        return new_img.astype(np.float32)\n\n    def _load_img(self, ch_idx: int, patch_idx: int) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the channels and also the respective noise channels.\n        \"\"\"\n        patch_loc_list = self.idx_manager.get_patch_location_from_patch_idx(\n            ch_idx, patch_idx\n        )\n        # TODO we should be adding channel dim here probably\n        img = self._data[ch_idx][patch_loc_list[0]]\n        return img\n\n    def get_mean_std(self):\n        return self._mean, self._std\n\n    def set_mean_std(self, mean_val, std_val):\n        self._mean = mean_val\n        self._std = std_val\n\n    def normalize_target(self, target):\n        mean_dict, std_dict = self.get_mean_std()\n        mean_ = mean_dict[\"target\"]  # .squeeze(0)\n        std_ = std_dict[\"target\"]  # .squeeze(0)\n        return (target - mean_) / std_\n\n    def get_grid_size(self):\n        return self._grid_sz\n\n    def get_idx_manager(self):\n        return self.idx_manager\n\n    def per_side_overlap_pixelcount(self):\n        return (self._img_sz - self._grid_sz) // 2\n\n    def _get_deterministic_loc(self, ch_idx: int, patch_idx: int):\n        \"\"\"\n        It returns the top-left corner of the patch corresponding to index.\n        \"\"\"\n        loc_list = self.idx_manager.get_patch_location_from_patch_idx(ch_idx, patch_idx)\n        # last dim is channel. we need to take the third and the second last element.\n        return loc_list[2:]\n\n    @cache\n    def crop_probablities(self, ch_idx):\n        sizes = np.array([np.prod(x.shape) for x in self._data[ch_idx]])\n        return sizes / sizes.sum()\n\n    def sample_crop(self, ch_idx):\n        idx = None\n        count = 0\n        while idx is None:\n            count += 1\n            idx = np.random.choice(\n                len(self._data[ch_idx]), p=self.crop_probablities(ch_idx)\n            )\n            data = self._data[ch_idx][idx]  # TODO no channel and S dim ?\n            # changed for ndim\n            if all(\n                d &gt;= self._img_sz for d in data.shape[-2:]\n            ):  # TODO dims were hardcoded\n                h = np.random.randint(0, data.shape[-2] - self._img_sz)\n                w = np.random.randint(0, data.shape[-1] - self._img_sz)\n\n                if len(data.shape) &gt; 2 and not self._3Ddata:\n                    s = np.random.randint(0, data.shape[0] - 1)\n                    return data[s, h : h + self._img_sz, w : w + self._img_sz]\n                else:\n                    return data[h : h + self._img_sz, w : w + self._img_sz]\n\n            elif count &gt; 100:\n                raise ValueError(\"Cannot find a valid crop\")\n            else:\n                idx = None\n\n        return None\n\n    def _l2(self, x):\n        return np.sqrt(np.mean(np.array(x) ** 2))\n\n    def compute_mean_std(self, allow_for_validation_data=False):\n        \"\"\"\n        Note that we must compute this only for training data.\n        \"\"\"\n        if self._3Ddata:\n            raise NotImplementedError(\"Not implemented for 3D data\")\n\n        if self._input_is_sum:\n            mean_tar_dict = defaultdict(list)\n            std_tar_dict = defaultdict(list)\n            mean_inp = []\n            std_inp = []\n            for _ in range(30000):\n                crops = []\n                for ch_idx in range(len(self._data)):\n                    crop = self.sample_crop(ch_idx)\n                    mean_tar_dict[ch_idx].append(np.mean(crop))\n                    std_tar_dict[ch_idx].append(np.std(crop))\n                    crops.append(crop)\n\n                inp = 0\n                for img in crops:\n                    inp += img\n\n                mean_inp.append(np.mean(inp))\n                std_inp.append(np.std(inp))\n\n            output_mean = defaultdict(list)\n            output_std = defaultdict(list)\n\n            NC = len(self._data)\n            for ch_idx in range(NC):\n                output_mean[\"target\"].append(np.mean(mean_tar_dict[ch_idx]))\n                output_std[\"target\"].append(self._l2(std_tar_dict[ch_idx]))\n\n            output_mean[\"target\"] = np.array(output_mean[\"target\"]).reshape(NC, 1, 1)\n            output_std[\"target\"] = np.array(output_std[\"target\"]).reshape(NC, 1, 1)\n\n            output_mean[\"input\"] = np.array([np.mean(mean_inp)]).reshape(1, 1, 1)\n            output_std[\"input\"] = np.array([self._l2(std_inp)]).reshape(1, 1, 1)\n        else:\n            raise NotImplementedError(\"Not implemented for non-summed input\")\n\n        return dict(output_mean), dict(output_std)\n\n    def set_mean_std(self, mean_dict, std_dict):\n        self._data_mean = mean_dict\n        self._data_std = std_dict\n\n    def get_mean_std(self):\n        return self._data_mean, self._data_std\n\n    def _get_random_hw(self, h: int, w: int):\n        \"\"\"\n        Random starting position for the crop for the img with index `index`.\n        \"\"\"\n        if h != self._img_sz:\n            h_start = np.random.choice(h - self._img_sz)\n            w_start = np.random.choice(w - self._img_sz)\n        else:\n            h_start = 0\n            w_start = 0\n        return h_start, w_start\n\n    def replace_with_empty_patch(self, img_tuples):\n        \"\"\"\n        Replaces the content of one of the channels with background\n        \"\"\"\n        empty_index = self._empty_patch_fetcher.sample()\n        empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n        assert (\n            len(empty_img_noise_tuples) == 0\n        ), \"Noise is not supported with empty patch replacement\"\n        final_img_tuples = []\n        for tuple_idx in range(len(img_tuples)):\n            if tuple_idx == self._empty_patch_replacement_channel_idx:\n                final_img_tuples.append(empty_img_tuples[tuple_idx])\n            else:\n                final_img_tuples.append(img_tuples[tuple_idx])\n        return tuple(final_img_tuples)\n\n    def get_mean_std_for_input(self):\n        mean, std = self.get_mean_std()\n        return mean[\"input\"], std[\"input\"]\n\n    def _compute_target(self, img_tuples, alpha):\n        if self._tar_idx_list is not None and isinstance(self._tar_idx_list, int):\n            target = img_tuples[self._tar_idx_list]\n        else:\n            if self._tar_idx_list is not None:\n                assert isinstance(self._tar_idx_list, list) or isinstance(\n                    self._tar_idx_list, tuple\n                )\n                img_tuples = [img_tuples[i] for i in self._tar_idx_list]\n\n            target = np.stack(img_tuples, axis=0)\n        return target\n\n    def _compute_input_with_alpha(self, img_tuples, alpha_list):\n        # assert self._normalized_input is True, \"normalization should happen here\"\n        if self._input_idx is not None:\n            inp = img_tuples[self._input_idx]\n        else:\n            inp = 0\n            for alpha, img in zip(alpha_list, img_tuples):\n                inp += img * alpha\n\n            if self._normalized_input is False:\n                return inp.astype(np.float32)\n\n        mean, std = self.get_mean_std_for_input()\n        mean = mean.squeeze()\n        std = std.squeeze()\n        if mean.size == 1:\n            mean = mean.reshape(\n                1,\n            )\n            std = std.reshape(\n                1,\n            )\n\n        for i in range(len(mean)):\n            assert mean[0] == mean[i]\n            assert std[0] == std[i]\n\n        inp = (inp - mean[0]) / std[0]\n        return inp.astype(np.float32)\n\n    def _sample_alpha(self):\n        alpha_arr = []\n        for i in range(self._num_channels):\n            alpha_pos = np.random.rand()\n            alpha = self._start_alpha_arr[i] + alpha_pos * (\n                self._end_alpha_arr[i] - self._start_alpha_arr[i]\n            )\n            alpha_arr.append(alpha)\n        return alpha_arr\n\n    def _compute_input(self, img_tuples):\n        alpha = [1 / len(img_tuples) for _ in range(len(img_tuples))]\n        if self._start_alpha_arr is not None:\n            alpha = self._sample_alpha()\n\n        inp = self._compute_input_with_alpha(img_tuples, alpha)\n        if self._input_is_sum:\n            inp = len(img_tuples) * inp\n\n        # TODO instead we add channel here\n        if len(inp.shape) == 2 or (len(inp.shape) == 3 and self._3Ddata):\n            inp = inp[None, ...]\n\n        return inp, alpha\n\n    def _get_index_from_valid_target_logic(self, index):\n        if self._validtarget_rand_fract is not None:\n            if np.random.rand() &lt; self._validtarget_rand_fract:\n                index = self._train_index_switcher.get_valid_target_index()\n            else:\n                index = self._train_index_switcher.get_invalid_target_index()\n        return index\n\n    def _rotate2D(self, img_tuples):\n        img_kwargs = {}\n        for i, img in enumerate(img_tuples):\n            for k in range(len(img)):\n                img_kwargs[f\"img{i}_{k}\"] = img[k]\n\n        keys = list(img_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(image=img_tuples[0][0], **img_kwargs)\n\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(rot_dic[f\"img{i}_0\"][None])\n            else:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [rot_dic[f\"img{i}_{k}\"][None] for k in range(len(img))], axis=0\n                    )\n                )\n\n        return rotated_img_tuples\n\n    def _rotate3D(self, img_tuples):\n        img_kwargs = {}\n        # random flip in z direction\n        flip_z = self._flipz_3D and np.random.rand() &lt; 0.5\n        for i, img in enumerate(img_tuples):\n            for j in range(self._depth3D):\n                for k in range(len(img)):\n                    if flip_z:\n                        z_idx = self._depth3D - 1 - j\n                    else:\n                        z_idx = j\n                    img_kwargs[f\"img{i}_{z_idx}_{k}\"] = img[k, j]\n\n        keys = list(img_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(image=img_tuples[0][0][0], **img_kwargs)\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [\n                            rot_dic[f\"img{i}_{j}_0\"][None, None]\n                            for j in range(self._depth3D)\n                        ],\n                        axis=1,\n                    )\n                )\n            else:\n                temp_arr = []\n                for k in range(len(img)):\n                    temp_arr.append(\n                        np.concatenate(\n                            [\n                                rot_dic[f\"img{i}_{j}_{k}\"][None, None]\n                                for j in range(self._depth3D)\n                            ],\n                            axis=1,\n                        )\n                    )\n                rotated_img_tuples.append(np.concatenate(temp_arr, axis=0))\n\n        return rotated_img_tuples\n\n    def _rotate(self, img_tuples, noise_tuples):\n\n        if self._3Ddata:\n            return self._rotate3D(img_tuples, noise_tuples)\n        else:\n            return self._rotate2D(img_tuples, noise_tuples)\n\n    def _get_img(self, ch_idx: int, patch_idx: int):\n        \"\"\"\n        Loads an image.\n        Crops the image such that cropped image has content.\n        \"\"\"\n        img = self._load_img(ch_idx, patch_idx)\n        cropped_img = self._crop_imgs(ch_idx, patch_idx, img)\n        return cropped_img\n\n    def get_uncorrelated_img_tuples(self, index):\n        \"\"\"\n        Content of channels like actin and nuclei is \"correlated\" in its\n        respective location, this function allows to pick channels' content\n        from different patches of the image to make it \"uncorrelated\".\n        \"\"\"\n        img_tuples = []\n        for ch_idx in range(len(self._data)):\n            if ch_idx == 0:\n                # dataset index becomes sample index because all channels have the same\n                # length\n                img_tuples.append(self._get_img(0, index))\n            else:\n                # get a random index from corresponding channel\n                sample_index = np.random.randint(\n                    self.idx_manager.total_grid_count()[0][ch_idx]\n                )\n                img_tuples.append(self._get_img(ch_idx, sample_index))\n        return img_tuples\n\n    def __getitem__(\n        self, index: Union[int, tuple[int, int]]\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n\n        # Uncorrelated channels means crops to create the input are taken from different\n        # spatial locations of the image.\n        if (\n            self._uncorrelated_channels\n            and np.random.rand() &lt; self._uncorrelated_channel_probab\n        ):\n            input_tuples = self.get_uncorrelated_img_tuples(index)\n        else:\n            # 0 is the channel index, because in this case locations are the same for\n            # all channels\n            # tuple for compatibility with _compute_input. #TODO check\n            input_tuples = (self._get_img(0, index),)\n\n        if self._enable_rotation:\n            input_tuples = self._rotate(input_tuples)\n\n        # Weight the individual channels, typically alpha is fixed\n        inp, alpha = self._compute_input(input_tuples)\n\n        target = self._compute_target(input_tuples, alpha)\n        norm_target = self.normalize_target(target)\n\n        return inp, norm_target\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.__init__","title":"<code>__init__(data_config, fpath, load_data_fn, val_fraction=None, test_fraction=None)</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def __init__(\n    self,\n    data_config: MicroSplitDataConfig,\n    fpath: str,\n    load_data_fn: Callable,\n    val_fraction: float = None,\n    test_fraction: float = None,\n):\n    \"\"\" \"\"\"\n    self._data_type = data_config.data_type\n    self._fpath = Path(fpath)\n    self._data = None\n    self._3Ddata = False  # TODO wtf it was 5D\n    self._tiling_mode = data_config.tiling_mode\n    # by default, if the noise is present, add it to the input and target.\n    self._depth3D = data_config.depth3D\n    self._mode_3D = data_config.mode_3D\n    # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n    self._input_is_sum = data_config.input_is_sum\n    self._num_channels = data_config.num_channels\n    self._input_idx = data_config.input_idx\n    self._tar_idx_list = data_config.target_idx_list\n\n    self.load_data(\n        data_config,\n        data_config.datasplit_type,\n        load_data_fn=load_data_fn,\n        val_fraction=val_fraction,\n        test_fraction=test_fraction,\n        allow_generation=data_config.allow_generation,\n    )\n\n    self._data_shapes = self.get_data_shapes()\n    self._normalized_input = data_config.normalized_input\n    self._quantile = 1.0\n    self._channelwise_quantile = False\n    self._background_quantile = 0.0\n    self._clip_background_noise_to_zero = False\n    self._skip_normalization_using_mean = False\n    self._empty_patch_replacement_enabled = False\n\n    self._background_values = None\n\n    self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n    if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n        if (\n            self._overlapping_padding_kwargs is None\n            or data_config.multiscale_lowres_count is not None\n        ):\n            # raise warning\n            print(\"Padding is not used with this alignement style\")\n    else:\n        assert (\n            self._overlapping_padding_kwargs is not None\n        ), \"When not trimming boudnary, padding is needed.\"\n\n    self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n    # input = alpha * ch1 + (1-alpha)*ch2.\n    # alpha is sampled randomly between these two extremes\n    self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n    self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n    # changed set_img_sz because \"grid_size\" in data_config returns false\n    try:\n        grid_size = data_config.grid_size\n    except AttributeError:\n        grid_size = data_config.image_size\n\n    if self._is_train:\n        self._start_alpha_arr = data_config.start_alpha  # TODO why only for train?\n        self._end_alpha_arr = data_config.end_alpha\n\n    self.set_img_sz(data_config.image_size, grid_size)\n\n    self._empty_patch_replacement_enabled = (\n        data_config.empty_patch_replacement_enabled and self._is_train\n    )\n    if self._empty_patch_replacement_enabled:\n        self._empty_patch_replacement_channel_idx = (\n            data_config.empty_patch_replacement_channel_idx\n        )\n        self._empty_patch_replacement_probab = (\n            data_config.empty_patch_replacement_probab\n        )\n        data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n        # NOTE: This is on the raw data. So, it must be called before removing the background.\n        self._empty_patch_fetcher = EmptyPatchFetcher(\n            self.idx_manager,\n            self._img_sz,\n            data_frames,\n            max_val_threshold=data_config.empty_patch_max_val_threshold,\n        )\n\n    self.rm_bkground_set_max_val_and_upperclip_data(\n        data_config.max_val, data_config.datasplit_type\n    )\n\n    # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n    self._mean = None\n    self._std = None\n    self._use_one_mu_std = data_config.use_one_mu_std\n\n    self._target_separate_normalization = data_config.target_separate_normalization\n\n    self._enable_rotation = data_config.enable_rotation_aug\n    flipz_3D = data_config.random_flip_z_3D\n    self._flipz_3D = flipz_3D and self._enable_rotation\n\n    self._enable_random_cropping = data_config.enable_random_cropping\n    self._uncorrelated_channels = (\n        data_config.uncorrelated_channels and self._is_train\n    )\n    self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n    assert self._is_train or self._uncorrelated_channels is False\n    assert (\n        self._enable_random_cropping is True or self._uncorrelated_channels is False\n    )\n    # Randomly rotate [-90,90]\n\n    self._rotation_transform = None\n    if self._enable_rotation:\n        # TODO: fix this import\n        import albumentations as A\n\n        self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.compute_mean_std","title":"<code>compute_mean_std(allow_for_validation_data=False)</code>","text":"<p>Note that we must compute this only for training data.</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def compute_mean_std(self, allow_for_validation_data=False):\n    \"\"\"\n    Note that we must compute this only for training data.\n    \"\"\"\n    if self._3Ddata:\n        raise NotImplementedError(\"Not implemented for 3D data\")\n\n    if self._input_is_sum:\n        mean_tar_dict = defaultdict(list)\n        std_tar_dict = defaultdict(list)\n        mean_inp = []\n        std_inp = []\n        for _ in range(30000):\n            crops = []\n            for ch_idx in range(len(self._data)):\n                crop = self.sample_crop(ch_idx)\n                mean_tar_dict[ch_idx].append(np.mean(crop))\n                std_tar_dict[ch_idx].append(np.std(crop))\n                crops.append(crop)\n\n            inp = 0\n            for img in crops:\n                inp += img\n\n            mean_inp.append(np.mean(inp))\n            std_inp.append(np.std(inp))\n\n        output_mean = defaultdict(list)\n        output_std = defaultdict(list)\n\n        NC = len(self._data)\n        for ch_idx in range(NC):\n            output_mean[\"target\"].append(np.mean(mean_tar_dict[ch_idx]))\n            output_std[\"target\"].append(self._l2(std_tar_dict[ch_idx]))\n\n        output_mean[\"target\"] = np.array(output_mean[\"target\"]).reshape(NC, 1, 1)\n        output_std[\"target\"] = np.array(output_std[\"target\"]).reshape(NC, 1, 1)\n\n        output_mean[\"input\"] = np.array([np.mean(mean_inp)]).reshape(1, 1, 1)\n        output_std[\"input\"] = np.array([self._l2(std_inp)]).reshape(1, 1, 1)\n    else:\n        raise NotImplementedError(\"Not implemented for non-summed input\")\n\n    return dict(output_mean), dict(output_std)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.get_begin_end_padding","title":"<code>get_begin_end_padding(start_pos, end_pos, max_len)</code>","text":"<p>The effect is that the image with size self._grid_sz is in the center of the patch with sufficient padding on all four sides so that the final patch size is self._img_sz.</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def get_begin_end_padding(self, start_pos, end_pos, max_len):\n    \"\"\"\n    The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n    padding on all four sides so that the final patch size is self._img_sz.\n    \"\"\"\n    pad_start = 0\n    pad_end = 0\n    if start_pos &lt; 0:\n        pad_start = -1 * start_pos\n\n    pad_end = max(0, end_pos - max_len)\n\n    return pad_start, pad_end\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.get_num_frames","title":"<code>get_num_frames()</code>","text":"<p>Returns the number of the longest channel.</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def get_num_frames(self):\n    \"\"\"Returns the number of the longest channel.\"\"\"\n    return max(self.idx_manager.total_grid_count()[0])\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.get_uncorrelated_img_tuples","title":"<code>get_uncorrelated_img_tuples(index)</code>","text":"<p>Content of channels like actin and nuclei is \"correlated\" in its respective location, this function allows to pick channels' content from different patches of the image to make it \"uncorrelated\".</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def get_uncorrelated_img_tuples(self, index):\n    \"\"\"\n    Content of channels like actin and nuclei is \"correlated\" in its\n    respective location, this function allows to pick channels' content\n    from different patches of the image to make it \"uncorrelated\".\n    \"\"\"\n    img_tuples = []\n    for ch_idx in range(len(self._data)):\n        if ch_idx == 0:\n            # dataset index becomes sample index because all channels have the same\n            # length\n            img_tuples.append(self._get_img(0, index))\n        else:\n            # get a random index from corresponding channel\n            sample_index = np.random.randint(\n                self.idx_manager.total_grid_count()[0][ch_idx]\n            )\n            img_tuples.append(self._get_img(ch_idx, sample_index))\n    return img_tuples\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.replace_with_empty_patch","title":"<code>replace_with_empty_patch(img_tuples)</code>","text":"<p>Replaces the content of one of the channels with background</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def replace_with_empty_patch(self, img_tuples):\n    \"\"\"\n    Replaces the content of one of the channels with background\n    \"\"\"\n    empty_index = self._empty_patch_fetcher.sample()\n    empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n    assert (\n        len(empty_img_noise_tuples) == 0\n    ), \"Noise is not supported with empty patch replacement\"\n    final_img_tuples = []\n    for tuple_idx in range(len(img_tuples)):\n        if tuple_idx == self._empty_patch_replacement_channel_idx:\n            final_img_tuples.append(empty_img_tuples[tuple_idx])\n        else:\n            final_img_tuples.append(img_tuples[tuple_idx])\n    return tuple(final_img_tuples)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/ms_dataset_ref/#careamics.lvae_training.dataset.ms_dataset_ref.MultiChDloaderRef.set_img_sz","title":"<code>set_img_sz(image_size, grid_size)</code>","text":"<p>If one wants to change the image size on the go, then this can be used. Args:     image_size: size of one patch     grid_size: frame is divided into square grids of this size. A patch centered on a grid having size <code>image_size</code> is returned.</p> Source code in <code>src/careamics/lvae_training/dataset/ms_dataset_ref.py</code> <pre><code>def set_img_sz(self, image_size, grid_size: Union[int, tuple[int, int, int]]):\n    \"\"\"\n    If one wants to change the image size on the go, then this can be used.\n    Args:\n        image_size: size of one patch\n        grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n    \"\"\"\n    # hacky way to deal with image shape from new conf\n    self._img_sz = image_size[-1]  # TODO revisit!\n    self._grid_sz = grid_size\n    shapes = self._data_shapes\n\n    patch_shape, grid_shape = self.get_idx_manager_shapes(\n        self._img_sz, self._grid_sz\n    )\n    self.idx_manager = GridIndexManagerRef(\n        shapes, grid_shape, patch_shape, self._tiling_mode\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/","title":"multich_dataset","text":"<p>A place for Datasets and Dataloaders.</p>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader","title":"<code>MultiChDloader</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Multi-channel dataset loader.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>class MultiChDloader(Dataset):\n    \"\"\"Multi-channel dataset loader.\"\"\"\n\n    def __init__(\n        self,\n        data_config: MicroSplitDataConfig,\n        datapath: Union[str, Path],\n        load_data_fn: Optional[Callable] = None,\n        val_fraction: float = 0.1,\n        test_fraction: float = 0.1,\n        allow_generation: bool = False,\n    ):\n        \"\"\" \"\"\"\n        self._data_type = data_config.data_type\n        self._fpath = datapath\n        self._data = self._noise_data = None\n        self.Z = 1\n        self._5Ddata = False\n        self._tiling_mode = data_config.tiling_mode\n        # by default, if the noise is present, add it to the input and target.\n        self._disable_noise = False  # to add synthetic noise\n        self._poisson_noise_factor = None\n        self._train_index_switcher = None\n        self._depth3D = data_config.depth3D\n        self._mode_3D = data_config.mode_3D\n        # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n        self._input_is_sum = data_config.input_is_sum\n        self._num_channels = data_config.num_channels\n        self._input_idx = data_config.input_idx\n        self._tar_idx_list = data_config.target_idx_list\n\n        if data_config.datasplit_type == DataSplitType.Train:\n            self._datausage_fraction = data_config.trainig_datausage_fraction\n            # assert self._datausage_fraction == 1.0, 'Not supported. Use validtarget_random_fraction and training_validtarget_fraction to get the same effect'\n            self._validtarget_rand_fract = data_config.validtarget_random_fraction\n            # self._validtarget_random_fraction_final = data_config.get('validtarget_random_fraction_final', None)\n            # self._validtarget_random_fraction_stepepoch = data_config.get('validtarget_random_fraction_stepepoch', None)\n            # self._idx_count = 0\n        elif data_config.datasplit_type == DataSplitType.Val:\n            self._datausage_fraction = data_config.validation_datausage_fraction\n        else:\n            self._datausage_fraction = 1.0\n\n        self.load_data(\n            data_config,\n            data_config.datasplit_type,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=data_config.allow_generation,\n        )\n        self._normalized_input = data_config.normalized_input\n        self._quantile = 1.0\n        self._channelwise_quantile = False\n        self._background_quantile = 0.0\n        self._clip_background_noise_to_zero = False\n        self._skip_normalization_using_mean = False\n        self._empty_patch_replacement_enabled = False\n\n        self._background_values = None\n\n        self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            if (\n                self._overlapping_padding_kwargs is None\n                or data_config.multiscale_lowres_count is not None\n            ):\n                # raise warning\n                print(\"Padding is not used with this alignement style\")\n        else:\n            assert (\n                self._overlapping_padding_kwargs is not None\n            ), \"When not trimming boudnary, padding is needed.\"\n\n        self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n        # input = alpha * ch1 + (1-alpha)*ch2.\n        # alpha is sampled randomly between these two extremes\n        self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n        self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n        # changed set_img_sz because \"grid_size\" in data_config returns false\n        try:\n            grid_size = data_config.grid_size\n        except AttributeError:\n            grid_size = data_config.image_size\n\n        if self._is_train:\n            self._start_alpha_arr = data_config.start_alpha\n            self._end_alpha_arr = data_config.end_alpha\n\n            self.set_img_sz(data_config.image_size, grid_size)\n\n            if self._validtarget_rand_fract is not None:\n                self._train_index_switcher = IndexSwitcher(\n                    self.idx_manager, data_config, self._img_sz\n                )\n\n        else:\n            self.set_img_sz(data_config.image_size, grid_size)\n\n        self._return_alpha = False\n        self._return_index = False\n\n        self._empty_patch_replacement_enabled = (\n            data_config.empty_patch_replacement_enabled and self._is_train\n        )\n        if self._empty_patch_replacement_enabled:\n            self._empty_patch_replacement_channel_idx = (\n                data_config.empty_patch_replacement_channel_idx\n            )\n            self._empty_patch_replacement_probab = (\n                data_config.empty_patch_replacement_probab\n            )\n            data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n            # NOTE: This is on the raw data. So, it must be called before removing the background.\n            self._empty_patch_fetcher = EmptyPatchFetcher(\n                self.idx_manager,\n                self._img_sz,\n                data_frames,\n                max_val_threshold=data_config.empty_patch_max_val_threshold,\n            )\n\n        self.rm_bkground_set_max_val_and_upperclip_data(\n            data_config.max_val, data_config.datasplit_type\n        )\n\n        # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n        self._mean = None\n        self._std = None\n        self._use_one_mu_std = data_config.use_one_mu_std\n\n        self._target_separate_normalization = data_config.target_separate_normalization\n\n        self._enable_rotation = data_config.enable_rotation_aug\n        flipz_3D = data_config.random_flip_z_3D\n        self._flipz_3D = flipz_3D and self._enable_rotation\n\n        self._enable_random_cropping = data_config.enable_random_cropping\n        self._uncorrelated_channels = (\n            data_config.uncorrelated_channels and self._is_train\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n        assert self._is_train or self._uncorrelated_channels is False\n        assert (\n            self._enable_random_cropping is True or self._uncorrelated_channels is False\n        )\n        # Randomly rotate [-90,90]\n\n        self._rotation_transform = None\n        if self._enable_rotation:\n            # TODO: fix this import\n            import albumentations as A\n\n            self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n\n        # TODO: remove print log messages\n        # if print_vars:\n        #     msg = self._init_msg()\n        #     print(msg)\n\n    def disable_noise(self):\n        assert (\n            self._poisson_noise_factor is None\n        ), \"This is not supported. Poisson noise is added to the data itself and so the noise cannot be disabled.\"\n        self._disable_noise = True\n\n    def enable_noise(self):\n        self._disable_noise = False\n\n    def get_data_shape(self):\n        return self._data.shape\n\n    def load_data(\n        self,\n        data_config,\n        datasplit_type,\n        load_data_fn: Callable,\n        val_fraction=None,\n        test_fraction=None,\n        allow_generation=None,\n    ):\n        self._data = load_data_fn(\n            data_config,\n            self._fpath,\n            datasplit_type,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=allow_generation,\n        )\n        self._loaded_data_preprocessing(data_config)\n\n    def _loaded_data_preprocessing(self, data_config):\n        old_shape = self._data.shape\n        if self._datausage_fraction &lt; 1.0:\n            framepixelcount = np.prod(self._data.shape[1:3])\n            pixelcount = int(\n                len(self._data) * framepixelcount * self._datausage_fraction\n            )\n            frame_count = int(np.ceil(pixelcount / framepixelcount))\n            last_frame_reduced_size, _ = IndexSwitcher.get_reduced_frame_size(\n                self._data.shape[:3], self._datausage_fraction\n            )\n            self._data = self._data[:frame_count].copy()\n            if frame_count == 1:\n                self._data = self._data[\n                    :, :last_frame_reduced_size, :last_frame_reduced_size\n                ].copy()\n            print(\n                f\"[{self.__class__.__name__}] New data shape: {self._data.shape} Old: {old_shape}\"\n            )\n\n        msg = \"\"\n        if data_config.poisson_noise_factor &gt; 0:\n            self._poisson_noise_factor = data_config.poisson_noise_factor\n            msg += f\"Adding Poisson noise with factor {self._poisson_noise_factor}.\\t\"\n            self._data = np.random.poisson(self._data / self._poisson_noise_factor)\n\n        if data_config.enable_gaussian_noise:\n            synthetic_scale = data_config.synthetic_gaussian_scale\n            msg += f\"Adding Gaussian noise with scale {synthetic_scale}\"\n            # 0 =&gt; noise for input. 1: =&gt; noise for all targets.\n            shape = self._data.shape\n            self._noise_data = np.random.normal(\n                0, synthetic_scale, (*shape[:-1], shape[-1] + 1)\n            )\n            if data_config.input_has_dependant_noise:\n                msg += \". Moreover, input has dependent noise\"\n                self._noise_data[..., 0] = np.mean(self._noise_data[..., 1:], axis=-1)\n        print(msg)\n\n        if len(self._data.shape) == 5:\n            if self._mode_3D:\n                self._5Ddata = True\n            else:\n                assert self._depth3D == 1, \"Depth3D must be 1 for 2D training\"\n                self._data = self._data.reshape(-1, *self._data.shape[2:])\n\n        if self._5Ddata:\n            self.Z = self._data.shape[1]\n\n        if self._depth3D &gt; 1:\n            assert self._5Ddata, \"Data must be 5D:NxZxHxWxC for 3D data\"\n\n        assert (\n            self._data.shape[-1] == self._num_channels\n        ), \"Number of channels in data and config do not match.\"\n\n    def save_background(self, channel_idx, frame_idx, background_value):\n        self._background_values[frame_idx, channel_idx] = background_value\n\n    def get_background(self, channel_idx, frame_idx):\n        return self._background_values[frame_idx, channel_idx]\n\n    def remove_background(self):\n\n        self._background_values = np.zeros((self._data.shape[0], self._data.shape[-1]))\n\n        if self._background_quantile == 0.0:\n            assert (\n                self._clip_background_noise_to_zero is False\n            ), \"This operation currently happens later in this function.\"\n            return\n\n        if self._data.dtype in [np.uint16]:\n            # unsigned integer creates havoc\n            self._data = self._data.astype(np.int32)\n\n        for ch in range(self._data.shape[-1]):\n            for idx in range(self._data.shape[0]):\n                qval = np.quantile(self._data[idx, ..., ch], self._background_quantile)\n                assert (\n                    np.abs(qval) &gt; 20\n                ), \"We are truncating the qval to an integer which will only make sense if it is large enough\"\n                # NOTE: Here, there can be an issue if you work with normalized data\n                qval = int(qval)\n                self.save_background(ch, idx, qval)\n                self._data[idx, ..., ch] -= qval\n\n        if self._clip_background_noise_to_zero:\n            self._data[self._data &lt; 0] = 0\n\n    def rm_bkground_set_max_val_and_upperclip_data(self, max_val, datasplit_type):\n        self.remove_background()\n        self.set_max_val(max_val, datasplit_type)\n        self.upperclip_data()\n\n    def upperclip_data(self):\n        if isinstance(self.max_val, list):\n            chN = self._data.shape[-1]\n            assert chN == len(self.max_val)\n            for ch in range(chN):\n                ch_data = self._data[..., ch]\n                ch_q = self.max_val[ch]\n                ch_data[ch_data &gt; ch_q] = ch_q\n                self._data[..., ch] = ch_data\n        else:\n            self._data[self._data &gt; self.max_val] = self.max_val\n\n    def compute_max_val(self):\n        if self._channelwise_quantile:\n            max_val_arr = [\n                np.quantile(self._data[..., i], self._quantile)\n                for i in range(self._data.shape[-1])\n            ]\n            return max_val_arr\n        else:\n            return np.quantile(self._data, self._quantile)\n\n    def set_max_val(self, max_val, datasplit_type):\n\n        if max_val is None:\n            assert datasplit_type == DataSplitType.Train\n            self.max_val = self.compute_max_val()\n        else:\n            assert max_val is not None\n            self.max_val = max_val\n\n    def get_max_val(self):\n        return self.max_val\n\n    def get_img_sz(self):\n        return self._img_sz\n\n    def get_num_frames(self):\n        return self._data.shape[0]\n\n    def reduce_data(\n        self,\n        t_list=None,\n        z_start=None,\n        z_end=None,\n        h_start=None,\n        h_end=None,\n        w_start=None,\n        w_end=None,\n    ):\n        if self._5Ddata:\n            if t_list is None:\n                t_list = list(range(self._data.shape[0]))\n            if z_start is None:\n                z_start = 0\n            if z_end is None:\n                z_end = self._data.shape[1]\n            if h_start is None:\n                h_start = 0\n            if h_end is None:\n                h_end = self._data.shape[2]\n            if w_start is None:\n                w_start = 0\n            if w_end is None:\n                w_end = self._data.shape[3]\n            self._data = self._data[\n                t_list, z_start:z_end, h_start:h_end, w_start:w_end, :\n            ].copy()\n            if self._noise_data is not None:\n                self._noise_data = self._noise_data[\n                    t_list, z_start:z_end, h_start:h_end, w_start:w_end, :\n                ].copy()\n        else:\n            if t_list is None:\n                t_list = list(range(self._data.shape[0]))\n            if h_start is None:\n                h_start = 0\n            if h_end is None:\n                h_end = self._data.shape[1]\n            if w_start is None:\n                w_start = 0\n            if w_end is None:\n                w_end = self._data.shape[2]\n\n            self._data = self._data[t_list, h_start:h_end, w_start:w_end, :].copy()\n            if self._noise_data is not None:\n                self._noise_data = self._noise_data[\n                    t_list, h_start:h_end, w_start:w_end, :\n                ].copy()\n        # TODO where tf is self._img_sz defined?\n        self.set_img_sz([self._img_sz, self._img_sz], self._grid_sz)\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data shape: {self._data.shape}\"\n        )\n\n    def get_idx_manager_shapes(\n        self, patch_size: int, grid_size: Union[int, tuple[int, int, int]]\n    ):\n        numC = self._data.shape[-1]\n        if self._5Ddata:\n            patch_shape = (1, self._depth3D, patch_size, patch_size, numC)\n            if isinstance(grid_size, int):\n                grid_shape = (1, 1, grid_size, grid_size, numC)\n            else:\n                assert len(grid_size) == 3\n                assert all(\n                    [g &lt;= p for g, p in zip(grid_size, patch_shape[1:-1])]\n                ), f\"Grid size {grid_size} must be less than patch size {patch_shape[1:-1]}\"\n                grid_shape = (1, grid_size[0], grid_size[1], grid_size[2], numC)\n        else:\n            assert isinstance(grid_size, int)\n            grid_shape = (1, grid_size, grid_size, numC)\n            patch_shape = (1, patch_size, patch_size, numC)\n\n        return patch_shape, grid_shape\n\n    def set_img_sz(self, image_size, grid_size: Union[int, tuple[int, int, int]]):\n        \"\"\"\n        If one wants to change the image size on the go, then this can be used.\n        Args:\n            image_size: size of one patch\n            grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n        \"\"\"\n        # hacky way to deal with image shape from new conf\n        self._img_sz = image_size[-1]  # TODO revisit!\n        self._grid_sz = grid_size\n        shape = self._data.shape\n\n        patch_shape, grid_shape = self.get_idx_manager_shapes(\n            self._img_sz, self._grid_sz\n        )\n        self.idx_manager = GridIndexManager(\n            shape, grid_shape, patch_shape, self._tiling_mode\n        )\n        # self.set_repeat_factor()\n\n    def __len__(self):\n        # Vera: N is the number of frames in Z stack\n        # Repeat factor is n_rows * n_cols\n        return self.idx_manager.total_grid_count()\n\n    def set_repeat_factor(self):\n        if self._grid_sz &gt; 1:\n            self._repeat_factor = self.idx_manager.grid_rows(\n                self._grid_sz\n            ) * self.idx_manager.grid_cols(self._grid_sz)\n        else:\n            self._repeat_factor = self.idx_manager.grid_rows(\n                self._img_sz\n            ) * self.idx_manager.grid_cols(self._img_sz)\n\n    def _init_msg(\n        self,\n    ):\n        msg = (\n            f\"[{self.__class__.__name__}] Train:{int(self._is_train)} Sz:{self._img_sz}\"\n        )\n        dim_sizes = [\n            self.idx_manager.get_individual_dim_grid_count(dim)\n            for dim in range(len(self._data.shape))\n        ]\n        dim_sizes = \",\".join([str(x) for x in dim_sizes])\n        msg += f\" N:{self.N} NumPatchPerN:{self._repeat_factor}\"\n        msg += f\"{self.idx_manager.total_grid_count()} DimSz:({dim_sizes})\"\n        msg += f\" TrimB:{self._tiling_mode}\"\n        # msg += f' NormInp:{self._normalized_input}'\n        # msg += f' SingleNorm:{self._use_one_mu_std}'\n        msg += f\" Rot:{self._enable_rotation}\"\n        if self._flipz_3D:\n            msg += f\" FlipZ:{self._flipz_3D}\"\n\n        msg += f\" RandCrop:{self._enable_random_cropping}\"\n        msg += f\" Channel:{self._num_channels}\"\n        # msg += f' Q:{self._quantile}'\n        if self._input_is_sum:\n            msg += f\" SummedInput:{self._input_is_sum}\"\n\n        if self._empty_patch_replacement_enabled:\n            msg += f\" ReplaceWithRandSample:{self._empty_patch_replacement_enabled}\"\n        if self._uncorrelated_channels:\n            msg += f\" Uncorr:{self._uncorrelated_channels}\"\n        if self._empty_patch_replacement_enabled:\n            msg += f\"-{self._empty_patch_replacement_channel_idx}-{self._empty_patch_replacement_probab}\"\n        if self._background_quantile &gt; 0.0:\n            msg += f\" BckQ:{self._background_quantile}\"\n\n        if self._start_alpha_arr is not None:\n            msg += f\" Alpha:[{self._start_alpha_arr},{self._end_alpha_arr}]\"\n        return msg\n\n    def _crop_imgs(self, index, *img_tuples: np.ndarray):\n        h, w = img_tuples[0].shape[-2:]\n        if self._img_sz is None:\n            return (\n                *img_tuples,\n                {\"h\": [0, h], \"w\": [0, w], \"hflip\": False, \"wflip\": False},\n            )\n\n        if self._enable_random_cropping:\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._5Ddata:\n                patch_start_loc = (\n                    np.random.choice(1 + img_tuples[0].shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            patch_start_loc = self._get_deterministic_loc(index)\n\n        cropped_imgs = []\n        for img in img_tuples:\n            img = self._crop_flip_img(img, patch_start_loc, False, False)\n            cropped_imgs.append(img)\n\n        return (\n            *tuple(cropped_imgs),\n            {\n                \"hflip\": False,\n                \"wflip\": False,\n            },\n        )\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: tuple):\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            # In training, this is used.\n            # NOTE: It is my opinion that if I just use self._crop_img_with_padding, it will work perfectly fine.\n            # The only benefit this if else loop provides is that it makes it easier to see what happens during training.\n            patch_end_loc = (\n                np.array(patch_start_loc, dtype=np.int32)\n                + self.idx_manager.patch_shape[1:-1]\n            )\n            if self._5Ddata:\n                z_start, h_start, w_start = patch_start_loc\n                z_end, h_end, w_end = patch_end_loc\n                new_img = img[..., z_start:z_end, h_start:h_end, w_start:w_end]\n            else:\n                h_start, w_start = patch_start_loc\n                h_end, w_end = patch_end_loc\n                new_img = img[..., h_start:h_end, w_start:w_end]\n\n            return new_img\n        else:\n            # During evaluation, this is used. In this situation, we can have negative h_start, w_start. Or h_start +self._img_sz can be larger than frame\n            # In these situations, we need some sort of padding. This is not needed  in the LeftTop alignement.\n            return self._crop_img_with_padding(img, patch_start_loc)\n\n    def get_begin_end_padding(self, start_pos, end_pos, max_len):\n        \"\"\"\n        The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n        padding on all four sides so that the final patch size is self._img_sz.\n        \"\"\"\n        pad_start = 0\n        pad_end = 0\n        if start_pos &lt; 0:\n            pad_start = -1 * start_pos\n\n        pad_end = max(0, end_pos - max_len)\n\n        return pad_start, pad_end\n\n    def _crop_img_with_padding(\n        self, img: np.ndarray, patch_start_loc, max_len_vals=None\n    ):\n        if max_len_vals is None:\n            max_len_vals = self.idx_manager.data_shape[1:-1]\n        patch_end_loc = np.array(patch_start_loc, dtype=int) + np.array(\n            self.idx_manager.patch_shape[1:-1], dtype=int\n        )\n        boundary_crossed = []\n        valid_slice = []\n        padding = [[0, 0]]\n        for start_idx, end_idx, max_len in zip(\n            patch_start_loc, patch_end_loc, max_len_vals\n        ):\n            boundary_crossed.append(end_idx &gt; max_len or start_idx &lt; 0)\n            valid_slice.append((max(0, start_idx), min(max_len, end_idx)))\n            pad = [0, 0]\n            if boundary_crossed[-1]:\n                pad = self.get_begin_end_padding(start_idx, end_idx, max_len)\n            padding.append(pad)\n        # max() is needed since h_start could be negative.\n        if self._5Ddata:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n                valid_slice[2][0] : valid_slice[2][1],\n            ]\n        else:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n            ]\n\n        # print(np.array(padding).shape, img.shape, new_img.shape)\n        # print(padding)\n        if not np.all(padding == 0):\n            new_img = np.pad(new_img, padding, **self._overlapping_padding_kwargs)\n\n        return new_img\n\n    def _crop_flip_img(\n        self, img: np.ndarray, patch_start_loc: tuple, h_flip: bool, w_flip: bool\n    ):\n        new_img = self._crop_img(img, patch_start_loc)\n        if h_flip:\n            new_img = new_img[..., ::-1, :]\n        if w_flip:\n            new_img = new_img[..., :, ::-1]\n\n        return new_img.astype(np.float32)\n\n    def _load_img(\n        self, index: Union[int, tuple[int, int]]\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the channels and also the respective noise channels.\n        \"\"\"\n        if isinstance(index, int) or isinstance(index, np.int64):\n            idx = index\n        else:\n            idx = index[0]\n\n        patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n        imgs = self._data[patch_loc_list[0]]\n        # if self._5Ddata:\n        #     assert self._noise_data is None, 'Noise is not supported for 5D data'\n        #     n_loc, z_loc = patch_loc_list[:2]\n        #     z_loc_interval = range(z_loc, z_loc + self._depth3D)\n        #     imgs = self._data[n_loc, z_loc_interval]\n        # else:\n        #     imgs = self._data[patch_loc_list[0]]\n\n        loaded_imgs = [imgs[None, ..., i] for i in range(imgs.shape[-1])]\n        noise = []\n        if self._noise_data is not None and not self._disable_noise:\n            noise = [\n                self._noise_data[patch_loc_list[0]][None, ..., i]\n                for i in range(self._noise_data.shape[-1])\n            ]\n        return tuple(loaded_imgs), tuple(noise)\n\n    def get_mean_std(self):\n        return self._mean, self._std\n\n    def set_mean_std(self, mean_val, std_val):\n        self._mean = mean_val\n        self._std = std_val\n\n    def normalize_img(self, *img_tuples):\n        mean, std = self.get_mean_std()\n        mean = mean[\"target\"]\n        std = std[\"target\"]\n        mean = mean.squeeze()\n        std = std.squeeze()\n        normalized_imgs = []\n        for i, img in enumerate(img_tuples):\n            img = (img - mean[i]) / std[i]\n            normalized_imgs.append(img)\n        return tuple(normalized_imgs)\n\n    def normalize_input(self, x):\n        mean_dict, std_dict = self.get_mean_std()\n        mean_ = mean_dict[\"input\"].mean()\n        std_ = std_dict[\"input\"].mean()\n        return (x - mean_) / std_\n\n    def normalize_target(self, target):\n        mean_dict, std_dict = self.get_mean_std()\n        mean_ = mean_dict[\"target\"].squeeze(0)\n        std_ = std_dict[\"target\"].squeeze(0)\n        return (target - mean_) / std_\n\n    def get_grid_size(self):\n        return self._grid_sz\n\n    def get_idx_manager(self):\n        return self.idx_manager\n\n    def per_side_overlap_pixelcount(self):\n        return (self._img_sz - self._grid_sz) // 2\n\n    # def on_boundary(self, cur_loc, frame_size):\n    #     return cur_loc + self._img_sz &gt; frame_size or cur_loc &lt; 0\n\n    def _get_deterministic_loc(self, index: int):\n        \"\"\"\n        It returns the top-left corner of the patch corresponding to index.\n        \"\"\"\n        loc_list = self.idx_manager.get_patch_location_from_dataset_idx(index)\n        # last dim is channel. we need to take the third and the second last element.\n        return loc_list[1:-1]\n\n    def compute_individual_mean_std(self):\n        # numpy 1.19.2 has issues in computing for large arrays. https://github.com/numpy/numpy/issues/8869\n        # mean = np.mean(self._data, axis=(0, 1, 2))\n        # std = np.std(self._data, axis=(0, 1, 2))\n        mean_arr = []\n        std_arr = []\n        for ch_idx in range(self._data.shape[-1]):\n            mean_ = (\n                0.0\n                if self._skip_normalization_using_mean\n                else self._data[..., ch_idx].mean()\n            )\n            if self._noise_data is not None:\n                std_ = (\n                    self._data[..., ch_idx] + self._noise_data[..., ch_idx + 1]\n                ).std()\n            else:\n                std_ = self._data[..., ch_idx].std()\n\n            mean_arr.append(mean_)\n            std_arr.append(std_)\n\n        mean = np.array(mean_arr)\n        std = np.array(std_arr)\n        if (\n            self._5Ddata\n        ):  # NOTE: IDEALLY this should be only when the model expects 3D data.\n            return mean[None, :, None, None, None], std[None, :, None, None, None]\n\n        return mean[None, :, None, None], std[None, :, None, None]\n\n    def compute_mean_std(self, allow_for_validation_data=False):\n        \"\"\"\n        Note that we must compute this only for training data.\n        \"\"\"\n        assert (\n            self._is_train is True or allow_for_validation_data\n        ), \"This is just allowed for training data\"\n        assert self._use_one_mu_std is True, \"This is the only supported case\"\n\n        if self._input_idx is not None:\n            assert (\n                self._tar_idx_list is not None\n            ), \"tar_idx_list must be set if input_idx is set.\"\n            assert self._noise_data is None, \"This is not supported with noise\"\n            assert (\n                self._target_separate_normalization is True\n            ), \"This is not supported with target_separate_normalization=False\"\n\n            mean, std = self.compute_individual_mean_std()\n            mean_dict = {\n                \"input\": mean[:, self._input_idx : self._input_idx + 1],\n                \"target\": mean[:, self._tar_idx_list],\n            }\n            std_dict = {\n                \"input\": std[:, self._input_idx : self._input_idx + 1],\n                \"target\": std[:, self._tar_idx_list],\n            }\n            return mean_dict, std_dict\n\n        if self._input_is_sum:\n            assert self._noise_data is None, \"This is not supported with noise\"\n            mean = [\n                np.mean(self._data[..., k : k + 1], keepdims=True)\n                for k in range(self._num_channels)\n            ]\n            mean = np.sum(mean, keepdims=True)[0]\n            std = np.linalg.norm(\n                [\n                    np.std(self._data[..., k : k + 1], keepdims=True)\n                    for k in range(self._num_channels)\n                ],\n                keepdims=True,\n            )[0]\n        else:\n            mean = np.mean(self._data, keepdims=True).reshape(1, 1, 1, 1)\n            if self._noise_data is not None:\n                std = np.std(\n                    self._data + self._noise_data[..., 1:], keepdims=True\n                ).reshape(1, 1, 1, 1)\n            else:\n                std = np.std(self._data, keepdims=True).reshape(1, 1, 1, 1)\n\n        mean = np.repeat(mean, self._num_channels, axis=1)\n        std = np.repeat(std, self._num_channels, axis=1)\n\n        if self._skip_normalization_using_mean:\n            mean = np.zeros_like(mean)\n\n        if self._5Ddata:\n            mean = mean[:, :, None]\n            std = std[:, :, None]\n\n        mean_dict = {\"input\": mean}  # , 'target':mean}\n        std_dict = {\"input\": std}  # , 'target':std}\n\n        if self._target_separate_normalization:\n            mean, std = self.compute_individual_mean_std()\n\n        mean_dict[\"target\"] = mean\n        std_dict[\"target\"] = std\n        return mean_dict, std_dict\n\n    def _get_random_hw(self, h: int, w: int):\n        \"\"\"\n        Random starting position for the crop for the img with index `index`.\n        \"\"\"\n        if h != self._img_sz:\n            h_start = np.random.choice(h - self._img_sz)\n            w_start = np.random.choice(w - self._img_sz)\n        else:\n            h_start = 0\n            w_start = 0\n        return h_start, w_start\n\n    def _get_img(self, index: Union[int, tuple[int, int]]):\n        \"\"\"\n        Loads an image.\n        Crops the image such that cropped image has content.\n        \"\"\"\n        img_tuples, noise_tuples = self._load_img(index)\n        cropped_img_tuples = self._crop_imgs(index, *img_tuples, *noise_tuples)[:-1]\n        cropped_noise_tuples = cropped_img_tuples[len(img_tuples) :]\n        cropped_img_tuples = cropped_img_tuples[: len(img_tuples)]\n        return cropped_img_tuples, cropped_noise_tuples\n\n    def replace_with_empty_patch(self, img_tuples):\n        \"\"\"\n        Replaces the content of one of the channels with background\n        \"\"\"\n        empty_index = self._empty_patch_fetcher.sample()\n        empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n        assert (\n            len(empty_img_noise_tuples) == 0\n        ), \"Noise is not supported with empty patch replacement\"\n        final_img_tuples = []\n        for tuple_idx in range(len(img_tuples)):\n            if tuple_idx == self._empty_patch_replacement_channel_idx:\n                final_img_tuples.append(empty_img_tuples[tuple_idx])\n            else:\n                final_img_tuples.append(img_tuples[tuple_idx])\n        return tuple(final_img_tuples)\n\n    def get_mean_std_for_input(self):\n        mean, std = self.get_mean_std()\n        return mean[\"input\"], std[\"input\"]\n\n    def _compute_target(self, img_tuples, alpha):\n        if self._tar_idx_list is not None and isinstance(self._tar_idx_list, int):\n            target = img_tuples[self._tar_idx_list]\n        else:\n            if self._tar_idx_list is not None:\n                assert isinstance(self._tar_idx_list, list) or isinstance(\n                    self._tar_idx_list, tuple\n                )\n                img_tuples = [img_tuples[i] for i in self._tar_idx_list]\n\n            target = np.concatenate(img_tuples, axis=0)\n        return target\n\n    def _compute_input_with_alpha(self, img_tuples, alpha_list):\n        # assert self._normalized_input is True, \"normalization should happen here\"\n        if self._input_idx is not None:\n            inp = img_tuples[self._input_idx]\n        else:\n            inp = 0\n            for alpha, img in zip(alpha_list, img_tuples):\n                inp += img * alpha\n\n            if self._normalized_input is False:\n                return inp.astype(np.float32)\n\n        mean, std = self.get_mean_std_for_input()\n        mean = mean.squeeze()\n        std = std.squeeze()\n        if mean.size == 1:\n            mean = mean.reshape(\n                1,\n            )\n            std = std.reshape(\n                1,\n            )\n\n        for i in range(len(mean)):\n            assert mean[0] == mean[i]\n            assert std[0] == std[i]\n\n        inp = (inp - mean[0]) / std[0]\n        return inp.astype(np.float32)\n\n    def _sample_alpha(self):\n        alpha_arr = []\n        for i in range(self._num_channels):\n            alpha_pos = np.random.rand()\n            alpha = self._start_alpha_arr[i] + alpha_pos * (\n                self._end_alpha_arr[i] - self._start_alpha_arr[i]\n            )\n            alpha_arr.append(alpha)\n        return alpha_arr\n\n    def _compute_input(self, img_tuples):\n        alpha = [1 / len(img_tuples) for _ in range(len(img_tuples))]\n        if self._start_alpha_arr is not None:\n            alpha = self._sample_alpha()\n\n        inp = self._compute_input_with_alpha(img_tuples, alpha)\n        if self._input_is_sum:\n            inp = len(img_tuples) * inp\n        return inp, alpha\n\n    def _get_index_from_valid_target_logic(self, index):\n        if self._validtarget_rand_fract is not None:\n            if np.random.rand() &lt; self._validtarget_rand_fract:\n                index = self._train_index_switcher.get_valid_target_index()\n            else:\n                index = self._train_index_switcher.get_invalid_target_index()\n        return index\n\n    def _rotate2D(self, img_tuples, noise_tuples):\n        img_kwargs = {}\n        for i, img in enumerate(img_tuples):\n            for k in range(len(img)):\n                img_kwargs[f\"img{i}_{k}\"] = img[k]\n\n        noise_kwargs = {}\n        for i, nimg in enumerate(noise_tuples):\n            for k in range(len(nimg)):\n                noise_kwargs[f\"noise{i}_{k}\"] = nimg[k]\n\n        keys = list(img_kwargs.keys()) + list(noise_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(\n            image=img_tuples[0][0], **img_kwargs, **noise_kwargs\n        )\n\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(rot_dic[f\"img{i}_0\"][None])\n            else:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [rot_dic[f\"img{i}_{k}\"][None] for k in range(len(img))], axis=0\n                    )\n                )\n\n        rotated_noise_tuples = []\n        for i, nimg in enumerate(noise_tuples):\n            if len(nimg) == 1:\n                rotated_noise_tuples.append(rot_dic[f\"noise{i}_0\"][None])\n            else:\n                rotated_noise_tuples.append(\n                    np.concatenate(\n                        [rot_dic[f\"noise{i}_{k}\"][None] for k in range(len(nimg))],\n                        axis=0,\n                    )\n                )\n\n        return rotated_img_tuples, rotated_noise_tuples\n\n    def _rotate(self, img_tuples, noise_tuples):\n\n        if self._5Ddata:\n            return self._rotate3D(img_tuples, noise_tuples)\n        else:\n            return self._rotate2D(img_tuples, noise_tuples)\n\n    def _rotate3D(self, img_tuples, noise_tuples):\n        img_kwargs = {}\n        # random flip in z direction\n        flip_z = self._flipz_3D and np.random.rand() &lt; 0.5\n        for i, img in enumerate(img_tuples):\n            for j in range(self._depth3D):\n                for k in range(len(img)):\n                    if flip_z:\n                        z_idx = self._depth3D - 1 - j\n                    else:\n                        z_idx = j\n                    img_kwargs[f\"img{i}_{z_idx}_{k}\"] = img[k, j]\n\n        noise_kwargs = {}\n        for i, nimg in enumerate(noise_tuples):\n            for j in range(self._depth3D):\n                for k in range(len(nimg)):\n                    if flip_z:\n                        z_idx = self._depth3D - 1 - j\n                    else:\n                        z_idx = j\n                    noise_kwargs[f\"noise{i}_{z_idx}_{k}\"] = nimg[k, j]\n\n        keys = list(img_kwargs.keys()) + list(noise_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(\n            image=img_tuples[0][0][0], **img_kwargs, **noise_kwargs\n        )\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [\n                            rot_dic[f\"img{i}_{j}_0\"][None, None]\n                            for j in range(self._depth3D)\n                        ],\n                        axis=1,\n                    )\n                )\n            else:\n                temp_arr = []\n                for k in range(len(img)):\n                    temp_arr.append(\n                        np.concatenate(\n                            [\n                                rot_dic[f\"img{i}_{j}_{k}\"][None, None]\n                                for j in range(self._depth3D)\n                            ],\n                            axis=1,\n                        )\n                    )\n                rotated_img_tuples.append(np.concatenate(temp_arr, axis=0))\n\n        rotated_noise_tuples = []\n        for i, nimg in enumerate(noise_tuples):\n            if len(nimg) == 1:\n                rotated_noise_tuples.append(\n                    np.concatenate(\n                        [\n                            rot_dic[f\"noise{i}_{j}_0\"][None, None]\n                            for j in range(self._depth3D)\n                        ],\n                        axis=1,\n                    )\n                )\n            else:\n                temp_arr = []\n                for k in range(len(nimg)):\n                    temp_arr.append(\n                        np.concatenate(\n                            [\n                                rot_dic[f\"noise{i}_{j}_{k}\"][None, None]\n                                for j in range(self._depth3D)\n                            ],\n                            axis=1,\n                        )\n                    )\n                rotated_noise_tuples.append(np.concatenate(temp_arr, axis=0))\n\n        return rotated_img_tuples, rotated_noise_tuples\n\n    def get_uncorrelated_img_tuples(self, index):\n        \"\"\"\n        Content of channels like actin and nuclei is \"correlated\" in its\n        respective location, this function allows to pick channels' content\n        from different patches of the image to make it \"uncorrelated\".\n        \"\"\"\n        img_tuples, noise_tuples = self._get_img(index)\n        assert len(noise_tuples) == 0\n        img_tuples = [img_tuples[0]]\n        for ch_idx in range(1, len(img_tuples)):\n            new_index = np.random.randint(len(self))\n            other_img_tuples, _ = self._get_img(new_index)\n            img_tuples.append(other_img_tuples[ch_idx])\n        return img_tuples, noise_tuples\n\n    def __getitem__(\n        self, index: Union[int, tuple[int, int]]\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        # Vera: input can be both real microscopic image and two separate channels that are summed in the code\n\n        if self._train_index_switcher is not None:\n            index = self._get_index_from_valid_target_logic(index)\n\n        if (\n            self._uncorrelated_channels\n            and np.random.rand() &lt; self._uncorrelated_channel_probab\n        ):\n            img_tuples, noise_tuples = self.get_uncorrelated_img_tuples(index)\n        else:\n            img_tuples, noise_tuples = self._get_img(index)\n\n        assert (\n            self._empty_patch_replacement_enabled != True\n        ), \"This is not supported with noise\"\n\n        # Replace the content of one of the channels\n        # with background with given probability\n        if self._empty_patch_replacement_enabled:\n            if np.random.rand() &lt; self._empty_patch_replacement_probab:\n                img_tuples = self.replace_with_empty_patch(img_tuples)\n\n        # Noise tuples are not needed for the paper\n        # the image tuples are noisy by default\n        # TODO: remove noise tuples completely?\n        if self._enable_rotation:\n            img_tuples, noise_tuples = self._rotate(img_tuples, noise_tuples)\n\n        # Add noise tuples with image tuples to create the input\n        if len(noise_tuples) &gt; 0:\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            input_tuples = [x + noise_tuples[0] * factor for x in img_tuples]\n        else:\n            input_tuples = img_tuples\n\n        # Weight the individual channels, typically alpha is fixed\n        inp, alpha = self._compute_input(input_tuples)\n\n        # Add noise tuples to the image tuples to create the target\n        if len(noise_tuples) &gt;= 1:\n            img_tuples = [x + noise for x, noise in zip(img_tuples, noise_tuples[1:])]\n\n        target = self._compute_target(img_tuples, alpha)\n        norm_target = self.normalize_target(target)\n\n        output = [inp, norm_target]\n\n        if self._return_alpha:\n            output.append(alpha)\n\n        if self._return_index:\n            output.append(index)\n\n        return tuple(output)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.__init__","title":"<code>__init__(data_config, datapath, load_data_fn=None, val_fraction=0.1, test_fraction=0.1, allow_generation=False)</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: MicroSplitDataConfig,\n    datapath: Union[str, Path],\n    load_data_fn: Optional[Callable] = None,\n    val_fraction: float = 0.1,\n    test_fraction: float = 0.1,\n    allow_generation: bool = False,\n):\n    \"\"\" \"\"\"\n    self._data_type = data_config.data_type\n    self._fpath = datapath\n    self._data = self._noise_data = None\n    self.Z = 1\n    self._5Ddata = False\n    self._tiling_mode = data_config.tiling_mode\n    # by default, if the noise is present, add it to the input and target.\n    self._disable_noise = False  # to add synthetic noise\n    self._poisson_noise_factor = None\n    self._train_index_switcher = None\n    self._depth3D = data_config.depth3D\n    self._mode_3D = data_config.mode_3D\n    # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n    self._input_is_sum = data_config.input_is_sum\n    self._num_channels = data_config.num_channels\n    self._input_idx = data_config.input_idx\n    self._tar_idx_list = data_config.target_idx_list\n\n    if data_config.datasplit_type == DataSplitType.Train:\n        self._datausage_fraction = data_config.trainig_datausage_fraction\n        # assert self._datausage_fraction == 1.0, 'Not supported. Use validtarget_random_fraction and training_validtarget_fraction to get the same effect'\n        self._validtarget_rand_fract = data_config.validtarget_random_fraction\n        # self._validtarget_random_fraction_final = data_config.get('validtarget_random_fraction_final', None)\n        # self._validtarget_random_fraction_stepepoch = data_config.get('validtarget_random_fraction_stepepoch', None)\n        # self._idx_count = 0\n    elif data_config.datasplit_type == DataSplitType.Val:\n        self._datausage_fraction = data_config.validation_datausage_fraction\n    else:\n        self._datausage_fraction = 1.0\n\n    self.load_data(\n        data_config,\n        data_config.datasplit_type,\n        load_data_fn=load_data_fn,\n        val_fraction=val_fraction,\n        test_fraction=test_fraction,\n        allow_generation=data_config.allow_generation,\n    )\n    self._normalized_input = data_config.normalized_input\n    self._quantile = 1.0\n    self._channelwise_quantile = False\n    self._background_quantile = 0.0\n    self._clip_background_noise_to_zero = False\n    self._skip_normalization_using_mean = False\n    self._empty_patch_replacement_enabled = False\n\n    self._background_values = None\n\n    self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n    if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n        if (\n            self._overlapping_padding_kwargs is None\n            or data_config.multiscale_lowres_count is not None\n        ):\n            # raise warning\n            print(\"Padding is not used with this alignement style\")\n    else:\n        assert (\n            self._overlapping_padding_kwargs is not None\n        ), \"When not trimming boudnary, padding is needed.\"\n\n    self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n    # input = alpha * ch1 + (1-alpha)*ch2.\n    # alpha is sampled randomly between these two extremes\n    self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n    self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n    # changed set_img_sz because \"grid_size\" in data_config returns false\n    try:\n        grid_size = data_config.grid_size\n    except AttributeError:\n        grid_size = data_config.image_size\n\n    if self._is_train:\n        self._start_alpha_arr = data_config.start_alpha\n        self._end_alpha_arr = data_config.end_alpha\n\n        self.set_img_sz(data_config.image_size, grid_size)\n\n        if self._validtarget_rand_fract is not None:\n            self._train_index_switcher = IndexSwitcher(\n                self.idx_manager, data_config, self._img_sz\n            )\n\n    else:\n        self.set_img_sz(data_config.image_size, grid_size)\n\n    self._return_alpha = False\n    self._return_index = False\n\n    self._empty_patch_replacement_enabled = (\n        data_config.empty_patch_replacement_enabled and self._is_train\n    )\n    if self._empty_patch_replacement_enabled:\n        self._empty_patch_replacement_channel_idx = (\n            data_config.empty_patch_replacement_channel_idx\n        )\n        self._empty_patch_replacement_probab = (\n            data_config.empty_patch_replacement_probab\n        )\n        data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n        # NOTE: This is on the raw data. So, it must be called before removing the background.\n        self._empty_patch_fetcher = EmptyPatchFetcher(\n            self.idx_manager,\n            self._img_sz,\n            data_frames,\n            max_val_threshold=data_config.empty_patch_max_val_threshold,\n        )\n\n    self.rm_bkground_set_max_val_and_upperclip_data(\n        data_config.max_val, data_config.datasplit_type\n    )\n\n    # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n    self._mean = None\n    self._std = None\n    self._use_one_mu_std = data_config.use_one_mu_std\n\n    self._target_separate_normalization = data_config.target_separate_normalization\n\n    self._enable_rotation = data_config.enable_rotation_aug\n    flipz_3D = data_config.random_flip_z_3D\n    self._flipz_3D = flipz_3D and self._enable_rotation\n\n    self._enable_random_cropping = data_config.enable_random_cropping\n    self._uncorrelated_channels = (\n        data_config.uncorrelated_channels and self._is_train\n    )\n    self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n    assert self._is_train or self._uncorrelated_channels is False\n    assert (\n        self._enable_random_cropping is True or self._uncorrelated_channels is False\n    )\n    # Randomly rotate [-90,90]\n\n    self._rotation_transform = None\n    if self._enable_rotation:\n        # TODO: fix this import\n        import albumentations as A\n\n        self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.compute_mean_std","title":"<code>compute_mean_std(allow_for_validation_data=False)</code>","text":"<p>Note that we must compute this only for training data.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def compute_mean_std(self, allow_for_validation_data=False):\n    \"\"\"\n    Note that we must compute this only for training data.\n    \"\"\"\n    assert (\n        self._is_train is True or allow_for_validation_data\n    ), \"This is just allowed for training data\"\n    assert self._use_one_mu_std is True, \"This is the only supported case\"\n\n    if self._input_idx is not None:\n        assert (\n            self._tar_idx_list is not None\n        ), \"tar_idx_list must be set if input_idx is set.\"\n        assert self._noise_data is None, \"This is not supported with noise\"\n        assert (\n            self._target_separate_normalization is True\n        ), \"This is not supported with target_separate_normalization=False\"\n\n        mean, std = self.compute_individual_mean_std()\n        mean_dict = {\n            \"input\": mean[:, self._input_idx : self._input_idx + 1],\n            \"target\": mean[:, self._tar_idx_list],\n        }\n        std_dict = {\n            \"input\": std[:, self._input_idx : self._input_idx + 1],\n            \"target\": std[:, self._tar_idx_list],\n        }\n        return mean_dict, std_dict\n\n    if self._input_is_sum:\n        assert self._noise_data is None, \"This is not supported with noise\"\n        mean = [\n            np.mean(self._data[..., k : k + 1], keepdims=True)\n            for k in range(self._num_channels)\n        ]\n        mean = np.sum(mean, keepdims=True)[0]\n        std = np.linalg.norm(\n            [\n                np.std(self._data[..., k : k + 1], keepdims=True)\n                for k in range(self._num_channels)\n            ],\n            keepdims=True,\n        )[0]\n    else:\n        mean = np.mean(self._data, keepdims=True).reshape(1, 1, 1, 1)\n        if self._noise_data is not None:\n            std = np.std(\n                self._data + self._noise_data[..., 1:], keepdims=True\n            ).reshape(1, 1, 1, 1)\n        else:\n            std = np.std(self._data, keepdims=True).reshape(1, 1, 1, 1)\n\n    mean = np.repeat(mean, self._num_channels, axis=1)\n    std = np.repeat(std, self._num_channels, axis=1)\n\n    if self._skip_normalization_using_mean:\n        mean = np.zeros_like(mean)\n\n    if self._5Ddata:\n        mean = mean[:, :, None]\n        std = std[:, :, None]\n\n    mean_dict = {\"input\": mean}  # , 'target':mean}\n    std_dict = {\"input\": std}  # , 'target':std}\n\n    if self._target_separate_normalization:\n        mean, std = self.compute_individual_mean_std()\n\n    mean_dict[\"target\"] = mean\n    std_dict[\"target\"] = std\n    return mean_dict, std_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.get_begin_end_padding","title":"<code>get_begin_end_padding(start_pos, end_pos, max_len)</code>","text":"<p>The effect is that the image with size self._grid_sz is in the center of the patch with sufficient padding on all four sides so that the final patch size is self._img_sz.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def get_begin_end_padding(self, start_pos, end_pos, max_len):\n    \"\"\"\n    The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n    padding on all four sides so that the final patch size is self._img_sz.\n    \"\"\"\n    pad_start = 0\n    pad_end = 0\n    if start_pos &lt; 0:\n        pad_start = -1 * start_pos\n\n    pad_end = max(0, end_pos - max_len)\n\n    return pad_start, pad_end\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.get_uncorrelated_img_tuples","title":"<code>get_uncorrelated_img_tuples(index)</code>","text":"<p>Content of channels like actin and nuclei is \"correlated\" in its respective location, this function allows to pick channels' content from different patches of the image to make it \"uncorrelated\".</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def get_uncorrelated_img_tuples(self, index):\n    \"\"\"\n    Content of channels like actin and nuclei is \"correlated\" in its\n    respective location, this function allows to pick channels' content\n    from different patches of the image to make it \"uncorrelated\".\n    \"\"\"\n    img_tuples, noise_tuples = self._get_img(index)\n    assert len(noise_tuples) == 0\n    img_tuples = [img_tuples[0]]\n    for ch_idx in range(1, len(img_tuples)):\n        new_index = np.random.randint(len(self))\n        other_img_tuples, _ = self._get_img(new_index)\n        img_tuples.append(other_img_tuples[ch_idx])\n    return img_tuples, noise_tuples\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.replace_with_empty_patch","title":"<code>replace_with_empty_patch(img_tuples)</code>","text":"<p>Replaces the content of one of the channels with background</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def replace_with_empty_patch(self, img_tuples):\n    \"\"\"\n    Replaces the content of one of the channels with background\n    \"\"\"\n    empty_index = self._empty_patch_fetcher.sample()\n    empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n    assert (\n        len(empty_img_noise_tuples) == 0\n    ), \"Noise is not supported with empty patch replacement\"\n    final_img_tuples = []\n    for tuple_idx in range(len(img_tuples)):\n        if tuple_idx == self._empty_patch_replacement_channel_idx:\n            final_img_tuples.append(empty_img_tuples[tuple_idx])\n        else:\n            final_img_tuples.append(img_tuples[tuple_idx])\n    return tuple(final_img_tuples)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.set_img_sz","title":"<code>set_img_sz(image_size, grid_size)</code>","text":"<p>If one wants to change the image size on the go, then this can be used. Args:     image_size: size of one patch     grid_size: frame is divided into square grids of this size. A patch centered on a grid having size <code>image_size</code> is returned.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def set_img_sz(self, image_size, grid_size: Union[int, tuple[int, int, int]]):\n    \"\"\"\n    If one wants to change the image size on the go, then this can be used.\n    Args:\n        image_size: size of one patch\n        grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n    \"\"\"\n    # hacky way to deal with image shape from new conf\n    self._img_sz = image_size[-1]  # TODO revisit!\n    self._grid_sz = grid_size\n    shape = self._data.shape\n\n    patch_shape, grid_shape = self.get_idx_manager_shapes(\n        self._img_sz, self._grid_sz\n    )\n    self.idx_manager = GridIndexManager(\n        shape, grid_shape, patch_shape, self._tiling_mode\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multicrop_dset/","title":"multicrop_dset","text":"<p>Here, we have multiple folders, each containing images of a single channel.</p>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/","title":"multifile_dataset","text":""},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.MultiChannelData","title":"<code>MultiChannelData</code>","text":"<p>               Bases: <code>Sequence</code></p> <p>each element in data_arr should be a NHW array</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class MultiChannelData(Sequence):\n    \"\"\"\n    each element in data_arr should be a N*H*W array\n    \"\"\"\n\n    def __init__(self, data_arr, paths=None):\n        self.paths = paths\n\n        self._data = data_arr\n\n    def __len__(self):\n        n = 0\n        for x in self._data:\n            n += x.shape[0]\n        return n\n\n    def __getitem__(self, idx):\n        n = 0\n        for dataidx, x in enumerate(self._data):\n            if idx &lt; n + x.shape[0]:\n                if self.paths is None:\n                    return x[idx - n], None\n                else:\n                    return x[idx - n], (self.paths[dataidx])\n            n += x.shape[0]\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.MultiFileDset","title":"<code>MultiFileDset</code>","text":"<p>Here, we handle dataset having multiple files. Each file can have a different spatial dimension and number of frames (Z stack).</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class MultiFileDset:\n    \"\"\"\n    Here, we handle dataset having multiple files. Each file can have a different spatial dimension and number of frames (Z stack).\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: MicroSplitDataConfig,\n        fpath: str,\n        load_data_fn: Callable[..., Union[TwoChannelData, MultiChannelData]],\n        val_fraction=None,\n        test_fraction=None,\n    ):\n        self._fpath = fpath\n        data: Union[TwoChannelData, MultiChannelData] = load_data_fn(\n            data_config,\n            self._fpath,\n            data_config.datasplit_type,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n        )\n        self.dsets = []\n\n        for i in range(len(data)):\n            prefetched_data, fpath_tuple = data[i]\n            if (\n                data_config.multiscale_lowres_count is not None\n                and data_config.multiscale_lowres_count &gt; 1\n            ):\n\n                self.dsets.append(\n                    SingleFileLCDset(\n                        prefetched_data[None],\n                        data_config,\n                        fpath_tuple,\n                        load_data_fn,\n                        val_fraction=val_fraction,\n                        test_fraction=test_fraction,\n                    )\n                )\n\n            else:\n                self.dsets.append(\n                    SingleFileDset(\n                        prefetched_data[None],\n                        data_config,\n                        fpath_tuple,\n                        load_data_fn,\n                        val_fraction=val_fraction,\n                        test_fraction=test_fraction,\n                    )\n                )\n\n        self.rm_bkground_set_max_val_and_upperclip_data(\n            data_config.max_val, data_config.datasplit_type\n        )\n        count = 0\n        avg_height = 0\n        avg_width = 0\n        for dset in self.dsets:\n            shape = dset.get_data_shape()\n            avg_height += shape[1]\n            avg_width += shape[2]\n            count += shape[0]\n\n        avg_height = int(avg_height / len(self.dsets))\n        avg_width = int(avg_width / len(self.dsets))\n        print(\n            f\"{self.__class__.__name__} avg height: {avg_height}, avg width: {avg_width}, count: {count}\"\n        )\n\n    def rm_bkground_set_max_val_and_upperclip_data(self, max_val, datasplit_type):\n        self.set_max_val(max_val, datasplit_type)\n        self.upperclip_data()\n\n    def set_mean_std(self, mean_val, std_val):\n        for dset in self.dsets:\n            dset.set_mean_std(mean_val, std_val)\n\n    def get_mean_std(self):\n        return self.dsets[0].get_mean_std()\n\n    def compute_max_val(self):\n        max_val_arr = []\n        for dset in self.dsets:\n            max_val_arr.append(dset.compute_max_val())\n        return np.max(max_val_arr)\n\n    def set_max_val(self, max_val, datasplit_type):\n        if datasplit_type == DataSplitType.Train:\n            assert max_val is None\n            max_val = self.compute_max_val()\n        for dset in self.dsets:\n            dset.set_max_val(max_val, datasplit_type)\n\n    def upperclip_data(self):\n        for dset in self.dsets:\n            dset.upperclip_data()\n\n    def get_max_val(self):\n        return self.dsets[0].get_max_val()\n\n    def get_img_sz(self):\n        return self.dsets[0].get_img_sz()\n\n    def set_img_sz(self, image_size, grid_size):\n        for dset in self.dsets:\n            dset.set_img_sz(image_size, grid_size)\n\n    def compute_mean_std(self):\n        cur_mean = {\"target\": 0, \"input\": 0}\n        cur_std = {\"target\": 0, \"input\": 0}\n        for dset in self.dsets:\n            mean, std = dset.compute_mean_std()\n            cur_mean[\"target\"] += mean[\"target\"]\n            cur_mean[\"input\"] += mean[\"input\"]\n\n            cur_std[\"target\"] += std[\"target\"]\n            cur_std[\"input\"] += std[\"input\"]\n\n        cur_mean[\"target\"] /= len(self.dsets)\n        cur_mean[\"input\"] /= len(self.dsets)\n        cur_std[\"target\"] /= len(self.dsets)\n        cur_std[\"input\"] /= len(self.dsets)\n        return cur_mean, cur_std\n\n    def compute_individual_mean_std(self):\n        cum_mean = 0\n        cum_std = 0\n        for dset in self.dsets:\n            mean, std = dset.compute_individual_mean_std()\n            cum_mean += mean\n            cum_std += std\n        return cum_mean / len(self.dsets), cum_std / len(self.dsets)\n\n    def get_num_frames(self):\n        return len(self.dsets)\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert h_start is None\n        assert h_end is None\n        assert w_start is None\n        assert w_end is None\n        self.dsets = [self.dsets[t] for t in t_list]\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data count: {len(self.dsets)}\"\n        )\n\n    def __len__(self):\n        out = 0\n        for dset in self.dsets:\n            out += len(dset)\n        return out\n\n    def __getitem__(self, idx):\n        cum_len = 0\n        for dset in self.dsets:\n            cum_len += len(dset)\n            if idx &lt; cum_len:\n                rel_idx = idx - (cum_len - len(dset))\n                return dset[rel_idx]\n\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.TwoChannelData","title":"<code>TwoChannelData</code>","text":"<p>               Bases: <code>Sequence</code></p> <p>each element in data_arr should be a NHW array</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class TwoChannelData(Sequence):\n    \"\"\"\n    each element in data_arr should be a N*H*W array\n    \"\"\"\n\n    def __init__(self, data_arr1, data_arr2, paths_data1=None, paths_data2=None):\n        assert len(data_arr1) == len(data_arr2)\n        self.paths1 = paths_data1\n        self.paths2 = paths_data2\n\n        self._data = []\n        for i in range(len(data_arr1)):\n            assert data_arr1[i].shape == data_arr2[i].shape\n            assert (\n                len(data_arr1[i].shape) == 3\n            ), f\"Each element in data arrays should be a N*H*W, but {data_arr1[i].shape}\"\n            self._data.append(\n                np.concatenate(\n                    [data_arr1[i][..., None], data_arr2[i][..., None]], axis=-1\n                )\n            )\n\n    def __len__(self):\n        n = 0\n        for x in self._data:\n            n += x.shape[0]\n        return n\n\n    def __getitem__(self, idx):\n        n = 0\n        for dataidx, x in enumerate(self._data):\n            if idx &lt; n + x.shape[0]:\n                if self.paths1 is None:\n                    return x[idx - n], None\n                else:\n                    return x[idx - n], (self.paths1[dataidx], self.paths2[dataidx])\n            n += x.shape[0]\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/types/","title":"types","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/","title":"data_utils","text":"<p>Utility functions needed by dataloader &amp; co.</p>"},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/#careamics.lvae_training.dataset.utils.data_utils.adjust_for_imbalance_in_fraction_value","title":"<code>adjust_for_imbalance_in_fraction_value(val, test, val_fraction, test_fraction, total_size)</code>","text":"<p>here, val and test are divided almost equally. Here, we need to take into account their respective fractions and pick elements rendomly from one array and put in the other array.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/data_utils.py</code> <pre><code>def adjust_for_imbalance_in_fraction_value(\n    val: List[int],\n    test: List[int],\n    val_fraction: float,\n    test_fraction: float,\n    total_size: int,\n):\n    \"\"\"\n    here, val and test are divided almost equally. Here, we need to take into account their respective fractions\n    and pick elements rendomly from one array and put in the other array.\n    \"\"\"\n    if val_fraction == 0:\n        test += val\n        val = []\n    elif test_fraction == 0:\n        val += test\n        test = []\n    else:\n        diff_fraction = test_fraction - val_fraction\n        if diff_fraction &gt; 0:\n            imb_count = int(diff_fraction * total_size / 2)\n            val = list(np.random.RandomState(seed=955).permutation(val))\n            test += val[:imb_count]\n            val = val[imb_count:]\n        elif diff_fraction &lt; 0:\n            imb_count = int(-1 * diff_fraction * total_size / 2)\n            test = list(np.random.RandomState(seed=955).permutation(test))\n            val += test[:imb_count]\n            test = test[imb_count:]\n    return val, test\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/#careamics.lvae_training.dataset.utils.data_utils.load_tiff","title":"<code>load_tiff(path)</code>","text":"<p>Returns a 4d numpy array: num_imgshw*num_channels</p> Source code in <code>src/careamics/lvae_training/dataset/utils/data_utils.py</code> <pre><code>def load_tiff(path):\n    \"\"\"\n    Returns a 4d numpy array: num_imgs*h*w*num_channels\n    \"\"\"\n    data = imread(path, plugin=\"tifffile\")\n    return data\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/","title":"empty_patch_fetcher","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/#careamics.lvae_training.dataset.utils.empty_patch_fetcher.EmptyPatchFetcher","title":"<code>EmptyPatchFetcher</code>","text":"<p>The idea is to fetch empty patches so that real content can be replaced with this.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/empty_patch_fetcher.py</code> <pre><code>class EmptyPatchFetcher:\n    \"\"\"\n    The idea is to fetch empty patches so that real content can be replaced with this.\n    \"\"\"\n\n    def __init__(self, idx_manager, patch_size, data_frames, max_val_threshold=None):\n        self._frames = data_frames\n        self._idx_manager = idx_manager\n        self._max_val_threshold = max_val_threshold\n        self._idx_list = []\n        self._patch_size = patch_size\n        self._grid_size = 1\n        self.set_empty_idx()\n\n        print(f\"[{self.__class__.__name__}] MaxVal:{self._max_val_threshold}\")\n\n    def compute_max(self, window):\n        \"\"\"\n        Rolling compute.\n        \"\"\"\n        N, H, W = self._frames.shape\n        randnum = -954321\n        assert self._grid_size == 1\n        max_data = np.zeros((N, H - window, W - window)) * randnum\n\n        for h in tqdm(range(H - window)):\n            for w in range(W - window):\n                max_data[:, h, w] = self._frames[:, h : h + window, w : w + window].max(\n                    axis=(1, 2)\n                )\n\n        assert (max_data != 954321).any()\n        return max_data\n\n    def set_empty_idx(self):\n        max_data = self.compute_max(self._patch_size)\n        empty_loc = np.where(\n            np.logical_and(max_data &gt;= 0, max_data &lt; self._max_val_threshold)\n        )\n        # print(max_data.shape, len(empty_loc))\n        self._idx_list = []\n        for idx in range(len(empty_loc[0])):\n            n_idx = empty_loc[0][idx]\n            h_start = empty_loc[1][idx]\n            w_start = empty_loc[2][idx]\n            # print(n_idx,h_start,w_start)\n            # channel_idx = self._idx_manager.get_location_from_dataset_idx(0)[-1]\n            loc = (n_idx, h_start, w_start, 0)\n            idx = self._idx_manager.get_dataset_idx_from_location(loc)\n            t, h, w, _ = self._idx_manager.get_location_from_dataset_idx(idx)\n            assert h == h_start, f\"{h} != {h_start}\"\n            assert w == w_start, f\"{w} != {w_start}\"\n            assert t == n_idx, f\"{t} != {n_idx}\"\n            self._idx_list.append(idx)\n\n        self._idx_list = np.array(self._idx_list)\n\n        assert len(self._idx_list) &gt; 0\n\n    def sample(self):\n        return (np.random.choice(self._idx_list), self._grid_size)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/#careamics.lvae_training.dataset.utils.empty_patch_fetcher.EmptyPatchFetcher.compute_max","title":"<code>compute_max(window)</code>","text":"<p>Rolling compute.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/empty_patch_fetcher.py</code> <pre><code>def compute_max(self, window):\n    \"\"\"\n    Rolling compute.\n    \"\"\"\n    N, H, W = self._frames.shape\n    randnum = -954321\n    assert self._grid_size == 1\n    max_data = np.zeros((N, H - window, W - window)) * randnum\n\n    for h in tqdm(range(H - window)):\n        for w in range(W - window):\n            max_data[:, h, w] = self._frames[:, h : h + window, w : w + window].max(\n                axis=(1, 2)\n            )\n\n    assert (max_data != 954321).any()\n    return max_data\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/","title":"index_manager","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager","title":"<code>GridIndexManager</code>  <code>dataclass</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>@dataclass\nclass GridIndexManager:\n    data_shape: tuple\n    grid_shape: tuple\n    patch_shape: tuple\n    tiling_mode: TilingMode\n\n    # Patch is centered on index in the grid, grid size not used in training,\n    # used only during val / test, grid size controls the overlap of the patches\n    # in training you only get random patches every time\n    # For borders - just cropped the data, so it perfectly divisible\n\n    def __post_init__(self):\n        assert len(self.data_shape) == len(\n            self.grid_shape\n        ), f\"Data shape:{self.data_shape} and grid size:{self.grid_shape} must have the same dimension\"\n        assert len(self.data_shape) == len(\n            self.patch_shape\n        ), f\"Data shape:{self.data_shape} and patch shape:{self.patch_shape} must have the same dimension\"\n        innerpad = np.array(self.patch_shape) - np.array(self.grid_shape)\n        for dim, pad in enumerate(innerpad):\n            if pad &lt; 0:\n                raise ValueError(\n                    f\"Patch shape:{self.patch_shape} must be greater than or equal to grid shape:{self.grid_shape} in dimension {dim}\"\n                )\n            if pad % 2 != 0:\n                raise ValueError(\n                    f\"Patch shape:{self.patch_shape} must have even padding in dimension {dim}\"\n                )\n\n    def patch_offset(self):\n        return (np.array(self.patch_shape) - np.array(self.grid_shape)) // 2\n\n    def get_individual_dim_grid_count(self, dim: int):\n        \"\"\"\n        Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return self.data_shape[dim]\n        elif self.tiling_mode == TilingMode.PadBoundary:\n            return int(np.ceil(self.data_shape[dim] / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(\n                np.ceil((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n            )\n        else:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(\n                np.floor((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n            )\n\n    def total_grid_count(self):\n        \"\"\"\n        Returns the total number of grids in the dataset.\n        \"\"\"\n        return self.grid_count(0) * self.get_individual_dim_grid_count(0)\n\n    def grid_count(self, dim: int):\n        \"\"\"\n        Returns the total number of grids for one value in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        if dim == len(self.data_shape) - 1:\n            return 1\n\n        return self.get_individual_dim_grid_count(dim + 1) * self.grid_count(dim + 1)\n\n    def get_grid_index(self, dim: int, coordinate: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        assert (\n            coordinate &lt; self.data_shape[dim]\n        ), f\"Coordinate {coordinate} is out of bounds for data shape {self.data_shape}\"\n\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return coordinate\n        elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n            return np.floor(coordinate / self.grid_shape[dim])\n        elif self.tiling_mode == TilingMode.TrimBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            if coordinate + self.grid_shape[dim] + excess_size == self.data_shape[dim]:\n                return self.get_individual_dim_grid_count(dim) - 1\n            else:\n                # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n                return max(\n                    0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n                )\n\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def dataset_idx_from_grid_idx(self, grid_idx: tuple):\n        \"\"\"\n        Returns the index of the grid in the dataset.\n        \"\"\"\n        assert len(grid_idx) == len(\n            self.data_shape\n        ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {self.data_shape}\"\n        index = 0\n        for dim in range(len(grid_idx)):\n            index += grid_idx[dim] * self.grid_count(dim)\n        return index\n\n    def get_patch_location_from_dataset_idx(self, dataset_idx: int):\n        \"\"\"\n        Returns the patch location of the grid in the dataset.\n        \"\"\"\n        grid_location = self.get_location_from_dataset_idx(dataset_idx)\n        offset = self.patch_offset()\n        return tuple(np.array(grid_location) - np.array(offset))\n\n    def get_dataset_idx_from_grid_location(self, location: tuple):\n        assert len(location) == len(\n            self.data_shape\n        ), f\"Location {location} must have the same dimension as data shape {self.data_shape}\"\n        grid_idx = [\n            self.get_grid_index(dim, location[dim]) for dim in range(len(location))\n        ]\n        return self.dataset_idx_from_grid_idx(tuple(grid_idx))\n\n    def get_gridstart_location_from_dim_index(self, dim: int, dim_index: int):\n        \"\"\"\n        Returns the grid-start coordinate of the grid in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        # assert dim_index &lt; self.get_individual_dim_grid_count(\n        #     dim\n        # ), f\"Dimension index {dim_index} is out of bounds for data shape {self.data_shape}\"\n        # TODO comented out this shit cuz I have no interest to dig why it's failing at this point !\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return dim_index\n        elif self.tiling_mode == TilingMode.PadBoundary:\n            return dim_index * self.grid_shape[dim]\n        elif self.tiling_mode == TilingMode.TrimBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            return dim_index * self.grid_shape[dim] + excess_size\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n                return dim_index * self.grid_shape[dim] + excess_size\n            else:\n                # on boundary. grid should be placed such that the patch covers the entire data.\n                return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def get_location_from_dataset_idx(self, dataset_idx: int):\n        \"\"\"\n        Returns the start location of the grid in the dataset.\n        \"\"\"\n        grid_idx = []\n        for dim in range(len(self.data_shape)):\n            grid_idx.append(dataset_idx // self.grid_count(dim))\n            dataset_idx = dataset_idx % self.grid_count(dim)\n        location = [\n            self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n            for dim in range(len(self.data_shape))\n        ]\n        return tuple(location)\n\n    def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n        \"\"\"\n        Returns True if the grid is on the boundary in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n        if dim &gt; 0:\n            dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n        dim_index = dataset_idx // self.grid_count(dim)\n        if only_end:\n            return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n        return (\n            dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n        )\n\n    def next_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx + self.grid_count(dim)\n        if new_idx &gt;= self.total_grid_count():\n            return None\n        return new_idx\n\n    def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx - self.grid_count(dim)\n        if new_idx &lt; 0:\n            return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.dataset_idx_from_grid_idx","title":"<code>dataset_idx_from_grid_idx(grid_idx)</code>","text":"<p>Returns the index of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def dataset_idx_from_grid_idx(self, grid_idx: tuple):\n    \"\"\"\n    Returns the index of the grid in the dataset.\n    \"\"\"\n    assert len(grid_idx) == len(\n        self.data_shape\n    ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {self.data_shape}\"\n    index = 0\n    for dim in range(len(grid_idx)):\n        index += grid_idx[dim] * self.grid_count(dim)\n    return index\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_grid_index","title":"<code>get_grid_index(dim, coordinate)</code>","text":"<p>Returns the index of the grid in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_grid_index(self, dim: int, coordinate: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    assert (\n        coordinate &lt; self.data_shape[dim]\n    ), f\"Coordinate {coordinate} is out of bounds for data shape {self.data_shape}\"\n\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return coordinate\n    elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n        return np.floor(coordinate / self.grid_shape[dim])\n    elif self.tiling_mode == TilingMode.TrimBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n        return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        if coordinate + self.grid_shape[dim] + excess_size == self.data_shape[dim]:\n            return self.get_individual_dim_grid_count(dim) - 1\n        else:\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(\n                0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n            )\n\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_gridstart_location_from_dim_index","title":"<code>get_gridstart_location_from_dim_index(dim, dim_index)</code>","text":"<p>Returns the grid-start coordinate of the grid in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_gridstart_location_from_dim_index(self, dim: int, dim_index: int):\n    \"\"\"\n    Returns the grid-start coordinate of the grid in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    # assert dim_index &lt; self.get_individual_dim_grid_count(\n    #     dim\n    # ), f\"Dimension index {dim_index} is out of bounds for data shape {self.data_shape}\"\n    # TODO comented out this shit cuz I have no interest to dig why it's failing at this point !\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return dim_index\n    elif self.tiling_mode == TilingMode.PadBoundary:\n        return dim_index * self.grid_shape[dim]\n    elif self.tiling_mode == TilingMode.TrimBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        return dim_index * self.grid_shape[dim] + excess_size\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n            return dim_index * self.grid_shape[dim] + excess_size\n        else:\n            # on boundary. grid should be placed such that the patch covers the entire data.\n            return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_individual_dim_grid_count","title":"<code>get_individual_dim_grid_count(dim)</code>","text":"<p>Returns the number of the grid in the specified dimension, ignoring all other dimensions.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_individual_dim_grid_count(self, dim: int):\n    \"\"\"\n    Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return self.data_shape[dim]\n    elif self.tiling_mode == TilingMode.PadBoundary:\n        return int(np.ceil(self.data_shape[dim] / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(\n            np.ceil((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n        )\n    else:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(\n            np.floor((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n        )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_location_from_dataset_idx","title":"<code>get_location_from_dataset_idx(dataset_idx)</code>","text":"<p>Returns the start location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_location_from_dataset_idx(self, dataset_idx: int):\n    \"\"\"\n    Returns the start location of the grid in the dataset.\n    \"\"\"\n    grid_idx = []\n    for dim in range(len(self.data_shape)):\n        grid_idx.append(dataset_idx // self.grid_count(dim))\n        dataset_idx = dataset_idx % self.grid_count(dim)\n    location = [\n        self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n        for dim in range(len(self.data_shape))\n    ]\n    return tuple(location)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_patch_location_from_dataset_idx","title":"<code>get_patch_location_from_dataset_idx(dataset_idx)</code>","text":"<p>Returns the patch location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_patch_location_from_dataset_idx(self, dataset_idx: int):\n    \"\"\"\n    Returns the patch location of the grid in the dataset.\n    \"\"\"\n    grid_location = self.get_location_from_dataset_idx(dataset_idx)\n    offset = self.patch_offset()\n    return tuple(np.array(grid_location) - np.array(offset))\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.grid_count","title":"<code>grid_count(dim)</code>","text":"<p>Returns the total number of grids for one value in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def grid_count(self, dim: int):\n    \"\"\"\n    Returns the total number of grids for one value in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    if dim == len(self.data_shape) - 1:\n        return 1\n\n    return self.get_individual_dim_grid_count(dim + 1) * self.grid_count(dim + 1)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.next_grid_along_dim","title":"<code>next_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def next_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx + self.grid_count(dim)\n    if new_idx &gt;= self.total_grid_count():\n        return None\n    return new_idx\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.on_boundary","title":"<code>on_boundary(dataset_idx, dim, only_end=False)</code>","text":"<p>Returns True if the grid is on the boundary in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n    \"\"\"\n    Returns True if the grid is on the boundary in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n    if dim &gt; 0:\n        dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n    dim_index = dataset_idx // self.grid_count(dim)\n    if only_end:\n        return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n    return (\n        dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.prev_grid_along_dim","title":"<code>prev_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx - self.grid_count(dim)\n    if new_idx &lt; 0:\n        return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.total_grid_count","title":"<code>total_grid_count()</code>","text":"<p>Returns the total number of grids in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def total_grid_count(self):\n    \"\"\"\n    Returns the total number of grids in the dataset.\n    \"\"\"\n    return self.grid_count(0) * self.get_individual_dim_grid_count(0)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef","title":"<code>GridIndexManagerRef</code>  <code>dataclass</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>@dataclass\nclass GridIndexManagerRef:\n    data_shapes: tuple\n    grid_shape: tuple\n    patch_shape: tuple\n    tiling_mode: TilingMode\n\n    # This class is used to calculate and store information about patches, and calculate\n    # the total length of the dataset in patches.\n    # It introduces a concept of a grid, to which input images are split.\n    # The grid is defined by the grid_shape and patch_shape, with former controlling the\n    # overlap.\n    # In this reimplementation it can accept multiple channels with different lengths,\n    # and every image can have different shape.\n\n    def __post_init__(self):\n        if len(self.data_shapes) &gt; 1:\n            assert {len(ds) for ds in self.data_shapes[0]}.pop() == {\n                len(ds) for ds in self.data_shapes[1]\n            }.pop(), \"Data shape for all channels must be the same\"  # TODO better way to assert this\n        assert {len(ds) for ds in self.data_shapes[0]}.pop() == len(\n            self.grid_shape\n        ), \"Data shape and grid size must have the same dimension\"\n        assert {len(ds) for ds in self.data_shapes[0]}.pop() == len(\n            self.patch_shape\n        ), \"Data shape and patch shape must have the same dimension\"\n        innerpad = np.array(self.patch_shape) - np.array(self.grid_shape)\n        for dim, pad in enumerate(innerpad):\n            if pad &lt; 0:\n                raise ValueError(\n                    f\"Patch shape must be greater than or equal to grid shape in dimension {dim}\"\n                )\n            if pad % 2 != 0:\n                raise ValueError(\n                    f\"Patch shape must have even padding in dimension {dim}\"\n                )\n        self.num_patches_per_channel = self.total_grid_count()[1]\n\n    def patch_offset(self):\n        return (np.array(self.patch_shape) - np.array(self.grid_shape)) // 2\n\n    def get_individual_dim_grid_count(self, shape: tuple, dim: int):\n        \"\"\"\n        Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n        \"\"\"\n        # assert that dim is less than the number of dimensions in data shape\n\n        # if dim &gt; len()\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return shape[dim]\n        elif self.tiling_mode == TilingMode.PadBoundary:\n            return int(np.ceil(shape[dim] / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(np.ceil((shape[dim] - excess_size) / self.grid_shape[dim]))\n            # if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n            #         return dim_index * self.grid_shape[dim] + excess_size\n            # on boundary. grid should be placed such that the patch covers the entire data.\n            # return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n        else:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(np.floor((shape[dim] - excess_size) / self.grid_shape[dim]))\n\n    def total_grid_count(self):\n        \"\"\"Returns the total number of patches in the dataset.\"\"\"\n        len_per_channel = []\n        num_patches_per_sample = []\n        for channel_data in self.data_shapes:\n            num_patches = []\n            for file_shape in channel_data:\n                num_patches.append(np.prod(self.grid_count_per_sample(file_shape)))\n            len_per_channel.append(np.sum(num_patches))\n            num_patches_per_sample.append(num_patches)\n\n        return len_per_channel, num_patches_per_sample\n\n    def grid_count_per_sample(self, shape: tuple):\n        \"\"\"Returns the total number of patches for one dimension.\"\"\"\n        grid_count = []\n        for dim in range(len(shape)):\n            grid_count.append(self.get_individual_dim_grid_count(shape, dim))\n        return grid_count\n\n    def get_grid_index(self, shape, dim: int, coordinate: int):\n        \"\"\"Returns the index of the patch in the specified dimension.\"\"\"\n        assert dim &lt; len(\n            shape\n        ), f\"Dimension {dim} is out of bounds for data shape {shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        assert (\n            coordinate &lt; shape[dim]\n        ), f\"Coordinate {coordinate} is out of bounds for data shape {shape}\"\n\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return coordinate\n        elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n            return np.floor(coordinate / self.grid_shape[dim])\n        elif self.tiling_mode == TilingMode.TrimBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            if coordinate + self.grid_shape[dim] + excess_size == self.data_shapes[dim]:\n                return self.get_individual_dim_grid_count(shape, dim) - 1\n            else:\n                # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n                return max(\n                    0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n                )\n\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def patch_idx_from_grid_idx(self, shape: tuple, grid_idx: tuple):\n        \"\"\"Returns the index of the patch in the dataset.\"\"\"\n        assert len(grid_idx) == len(\n            shape\n        ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {shape}\"\n        index = 0\n        for dim in range(len(grid_idx)):\n            index += grid_idx[dim] * self.grid_count(shape, dim)\n        return index\n\n    def get_patch_location_from_patch_idx(self, ch_idx: int, patch_idx: int):\n        \"\"\"Returns the patch location of the grid in the dataset.\"\"\"\n        grid_location = self.get_location_from_patch_idx(ch_idx, patch_idx)\n        offset = self.patch_offset()\n        return tuple(np.array(grid_location) - np.concatenate((np.array((0,)), offset)))\n\n    def get_patch_idx_from_grid_location(self, shape, location: tuple):\n        assert len(location) == len(\n            shape\n        ), f\"Location {location} must have the same dimension as data shape {shape}\"\n        grid_idx = [\n            self.get_grid_index(dim, location[dim]) for dim in range(len(location))\n        ]\n        return self.patch_idx_from_grid_idx(tuple(grid_idx))\n\n    def get_gridstart_location_from_dim_index(\n        self, shape: tuple, dim_idx: int, dim: int\n    ):\n        \"\"\"Returns the grid-start coordinate of the grid in the specified dimension.\n\n        dim_idx: int\n            Index of the dimension in the data shape.\n        dim: int\n            Value of the dimension in the grid (relative to num patches in dimension).\n        \"\"\"\n        if self.grid_shape[dim_idx] == 1 and self.patch_shape[dim_idx] == 1:\n            return dim_idx\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim_idx] - self.grid_shape[dim_idx]) // 2\n            if dim &lt; self.get_individual_dim_grid_count(shape, dim_idx) - 1:\n                return dim * self.grid_shape[dim_idx] + excess_size\n            else:\n                # on boundary. grid should be placed such that the patch covers the entire data.\n                return shape[dim_idx] - self.grid_shape[dim_idx] - excess_size\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def get_location_from_patch_idx(self, channel_idx: int, patch_idx: int):\n        \"\"\"\n        Returns the start location of the grid in the dataset. Per channel!.\n\n        Parameters\n        ----------\n        patch_idx : int\n            The index of the patch in a list of samples within a channel. Channels can\n            be different in length.\n        \"\"\"\n        # TODO assert patch_idx &lt;= num of patches in the channel\n        # create cumulative sum of the grid counts for each channel\n        cumulative_indices = np.cumsum(self.total_grid_count()[1][channel_idx])\n        # find the channel index\n        sample_idx = np.searchsorted(cumulative_indices, patch_idx, side=\"right\")\n        sample_shape = self.data_shapes[channel_idx][sample_idx]\n        # TODO duplicated runs, revisit\n        # ingoring the channel dimension because we index it explicitly\n        grid_count = self.grid_count_per_sample(sample_shape)[1:]\n\n        grid_idx = []\n        for i in range(len(grid_count) - 1, -1, -1):\n            stride = np.prod(grid_count[:i]) if i &gt; 0 else 1\n            grid_idx.insert(0, patch_idx // stride)\n            patch_idx %= stride\n        # TODO check for 3D !\n        # adding channel index\n        grid_idx = [channel_idx] + grid_idx\n        location = [\n            sample_idx,\n        ] + [\n            self.get_gridstart_location_from_dim_index(\n                shape=sample_shape, dim_idx=dim_idx, dim=grid_idx[dim_idx]\n            )\n            for dim_idx in range(len(grid_idx))\n        ]\n        return tuple(location)\n\n    def get_location_from_patch_idx_o(self, dataset_idx: int):\n        \"\"\"\n        Returns the start location of the grid in the dataset.\n        \"\"\"\n        grid_idx = []\n        for dim in range(len(self.data_shape)):\n            grid_idx.append(dataset_idx // self.grid_count(dim))\n            dataset_idx = dataset_idx % self.grid_count(dim)\n        location = [\n            self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n            for dim in range(len(self.data_shape))\n        ]\n        return tuple(location)\n\n    def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n        \"\"\"\n        Returns True if the grid is on the boundary in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shapes\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n        if dim &gt; 0:\n            dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n        dim_index = dataset_idx // self.grid_count(dim)\n        if only_end:\n            return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n        return (\n            dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n        )\n\n    def next_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shapes\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx + self.grid_count(dim)\n        if new_idx &gt;= self.total_grid_count():\n            return None\n        return new_idx\n\n    def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shapes\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx - self.grid_count(dim)\n        if new_idx &lt; 0:\n            return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_grid_index","title":"<code>get_grid_index(shape, dim, coordinate)</code>","text":"<p>Returns the index of the patch in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_grid_index(self, shape, dim: int, coordinate: int):\n    \"\"\"Returns the index of the patch in the specified dimension.\"\"\"\n    assert dim &lt; len(\n        shape\n    ), f\"Dimension {dim} is out of bounds for data shape {shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    assert (\n        coordinate &lt; shape[dim]\n    ), f\"Coordinate {coordinate} is out of bounds for data shape {shape}\"\n\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return coordinate\n    elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n        return np.floor(coordinate / self.grid_shape[dim])\n    elif self.tiling_mode == TilingMode.TrimBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n        return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        if coordinate + self.grid_shape[dim] + excess_size == self.data_shapes[dim]:\n            return self.get_individual_dim_grid_count(shape, dim) - 1\n        else:\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(\n                0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n            )\n\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_gridstart_location_from_dim_index","title":"<code>get_gridstart_location_from_dim_index(shape, dim_idx, dim)</code>","text":"<p>Returns the grid-start coordinate of the grid in the specified dimension.</p> <p>dim_idx: int     Index of the dimension in the data shape. dim: int     Value of the dimension in the grid (relative to num patches in dimension).</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_gridstart_location_from_dim_index(\n    self, shape: tuple, dim_idx: int, dim: int\n):\n    \"\"\"Returns the grid-start coordinate of the grid in the specified dimension.\n\n    dim_idx: int\n        Index of the dimension in the data shape.\n    dim: int\n        Value of the dimension in the grid (relative to num patches in dimension).\n    \"\"\"\n    if self.grid_shape[dim_idx] == 1 and self.patch_shape[dim_idx] == 1:\n        return dim_idx\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim_idx] - self.grid_shape[dim_idx]) // 2\n        if dim &lt; self.get_individual_dim_grid_count(shape, dim_idx) - 1:\n            return dim * self.grid_shape[dim_idx] + excess_size\n        else:\n            # on boundary. grid should be placed such that the patch covers the entire data.\n            return shape[dim_idx] - self.grid_shape[dim_idx] - excess_size\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_individual_dim_grid_count","title":"<code>get_individual_dim_grid_count(shape, dim)</code>","text":"<p>Returns the number of the grid in the specified dimension, ignoring all other dimensions.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_individual_dim_grid_count(self, shape: tuple, dim: int):\n    \"\"\"\n    Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n    \"\"\"\n    # assert that dim is less than the number of dimensions in data shape\n\n    # if dim &gt; len()\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return shape[dim]\n    elif self.tiling_mode == TilingMode.PadBoundary:\n        return int(np.ceil(shape[dim] / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(np.ceil((shape[dim] - excess_size) / self.grid_shape[dim]))\n        # if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n        #         return dim_index * self.grid_shape[dim] + excess_size\n        # on boundary. grid should be placed such that the patch covers the entire data.\n        # return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n    else:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(np.floor((shape[dim] - excess_size) / self.grid_shape[dim]))\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_location_from_patch_idx","title":"<code>get_location_from_patch_idx(channel_idx, patch_idx)</code>","text":"<p>Returns the start location of the grid in the dataset. Per channel!.</p> <p>Parameters:</p> Name Type Description Default <code>patch_idx</code> <code>int</code> <p>The index of the patch in a list of samples within a channel. Channels can be different in length.</p> required Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_location_from_patch_idx(self, channel_idx: int, patch_idx: int):\n    \"\"\"\n    Returns the start location of the grid in the dataset. Per channel!.\n\n    Parameters\n    ----------\n    patch_idx : int\n        The index of the patch in a list of samples within a channel. Channels can\n        be different in length.\n    \"\"\"\n    # TODO assert patch_idx &lt;= num of patches in the channel\n    # create cumulative sum of the grid counts for each channel\n    cumulative_indices = np.cumsum(self.total_grid_count()[1][channel_idx])\n    # find the channel index\n    sample_idx = np.searchsorted(cumulative_indices, patch_idx, side=\"right\")\n    sample_shape = self.data_shapes[channel_idx][sample_idx]\n    # TODO duplicated runs, revisit\n    # ingoring the channel dimension because we index it explicitly\n    grid_count = self.grid_count_per_sample(sample_shape)[1:]\n\n    grid_idx = []\n    for i in range(len(grid_count) - 1, -1, -1):\n        stride = np.prod(grid_count[:i]) if i &gt; 0 else 1\n        grid_idx.insert(0, patch_idx // stride)\n        patch_idx %= stride\n    # TODO check for 3D !\n    # adding channel index\n    grid_idx = [channel_idx] + grid_idx\n    location = [\n        sample_idx,\n    ] + [\n        self.get_gridstart_location_from_dim_index(\n            shape=sample_shape, dim_idx=dim_idx, dim=grid_idx[dim_idx]\n        )\n        for dim_idx in range(len(grid_idx))\n    ]\n    return tuple(location)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_location_from_patch_idx_o","title":"<code>get_location_from_patch_idx_o(dataset_idx)</code>","text":"<p>Returns the start location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_location_from_patch_idx_o(self, dataset_idx: int):\n    \"\"\"\n    Returns the start location of the grid in the dataset.\n    \"\"\"\n    grid_idx = []\n    for dim in range(len(self.data_shape)):\n        grid_idx.append(dataset_idx // self.grid_count(dim))\n        dataset_idx = dataset_idx % self.grid_count(dim)\n    location = [\n        self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n        for dim in range(len(self.data_shape))\n    ]\n    return tuple(location)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.get_patch_location_from_patch_idx","title":"<code>get_patch_location_from_patch_idx(ch_idx, patch_idx)</code>","text":"<p>Returns the patch location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_patch_location_from_patch_idx(self, ch_idx: int, patch_idx: int):\n    \"\"\"Returns the patch location of the grid in the dataset.\"\"\"\n    grid_location = self.get_location_from_patch_idx(ch_idx, patch_idx)\n    offset = self.patch_offset()\n    return tuple(np.array(grid_location) - np.concatenate((np.array((0,)), offset)))\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.grid_count_per_sample","title":"<code>grid_count_per_sample(shape)</code>","text":"<p>Returns the total number of patches for one dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def grid_count_per_sample(self, shape: tuple):\n    \"\"\"Returns the total number of patches for one dimension.\"\"\"\n    grid_count = []\n    for dim in range(len(shape)):\n        grid_count.append(self.get_individual_dim_grid_count(shape, dim))\n    return grid_count\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.next_grid_along_dim","title":"<code>next_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def next_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shapes\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx + self.grid_count(dim)\n    if new_idx &gt;= self.total_grid_count():\n        return None\n    return new_idx\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.on_boundary","title":"<code>on_boundary(dataset_idx, dim, only_end=False)</code>","text":"<p>Returns True if the grid is on the boundary in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n    \"\"\"\n    Returns True if the grid is on the boundary in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shapes\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n    if dim &gt; 0:\n        dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n    dim_index = dataset_idx // self.grid_count(dim)\n    if only_end:\n        return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n    return (\n        dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.patch_idx_from_grid_idx","title":"<code>patch_idx_from_grid_idx(shape, grid_idx)</code>","text":"<p>Returns the index of the patch in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def patch_idx_from_grid_idx(self, shape: tuple, grid_idx: tuple):\n    \"\"\"Returns the index of the patch in the dataset.\"\"\"\n    assert len(grid_idx) == len(\n        shape\n    ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {shape}\"\n    index = 0\n    for dim in range(len(grid_idx)):\n        index += grid_idx[dim] * self.grid_count(shape, dim)\n    return index\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.prev_grid_along_dim","title":"<code>prev_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shapes\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shapes}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx - self.grid_count(dim)\n    if new_idx &lt; 0:\n        return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManagerRef.total_grid_count","title":"<code>total_grid_count()</code>","text":"<p>Returns the total number of patches in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def total_grid_count(self):\n    \"\"\"Returns the total number of patches in the dataset.\"\"\"\n    len_per_channel = []\n    num_patches_per_sample = []\n    for channel_data in self.data_shapes:\n        num_patches = []\n        for file_shape in channel_data:\n            num_patches.append(np.prod(self.grid_count_per_sample(file_shape)))\n        len_per_channel.append(np.sum(num_patches))\n        num_patches_per_sample.append(num_patches)\n\n    return len_per_channel, num_patches_per_sample\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/","title":"index_switcher","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher","title":"<code>IndexSwitcher</code>","text":"<p>The idea is to switch from valid indices for target to invalid indices for target. If index in invalid for the target, then we return all zero vector as target. This combines both logic: 1. Using less amount of total data. 2. Using less amount of target data but using full data.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>class IndexSwitcher:\n    \"\"\"\n    The idea is to switch from valid indices for target to invalid indices for target.\n    If index in invalid for the target, then we return all zero vector as target.\n    This combines both logic:\n    1. Using less amount of total data.\n    2. Using less amount of target data but using full data.\n    \"\"\"\n\n    def __init__(self, idx_manager, data_config, patch_size) -&gt; None:\n        self.idx_manager = idx_manager\n        self._data_shape = self.idx_manager.get_data_shape()\n        self._training_validtarget_fraction = data_config.get(\n            \"training_validtarget_fraction\", 1.0\n        )\n        self._validtarget_ceilT = int(\n            np.ceil(self._data_shape[0] * self._training_validtarget_fraction)\n        )\n        self._patch_size = patch_size\n        assert (\n            data_config.deterministic_grid is True\n        ), \"This only works when the dataset has deterministic grid. Needed randomness comes from this class.\"\n        assert (\n            \"grid_size\" in data_config and data_config.grid_size == 1\n        ), \"We need a one to one mapping between index and h, w, t\"\n\n        self._h_validmax, self._w_validmax = self.get_reduced_frame_size(\n            self._data_shape[:3], self._training_validtarget_fraction\n        )\n        if self._h_validmax &lt; self._patch_size or self._w_validmax &lt; self._patch_size:\n            print(\n                \"WARNING: The valid target size is smaller than the patch size. This will result in all zero target. so, we are ignoring this frame for target.\"\n            )\n            self._h_validmax = 0\n            self._w_validmax = 0\n\n        print(\n            f\"[{self.__class__.__name__}] Target Indices: [0,{self._validtarget_ceilT - 1}]. Index={self._validtarget_ceilT - 1} has shape [:{self._h_validmax},:{self._w_validmax}].  Available data: {self._data_shape[0]}\"\n        )\n\n    def get_valid_target_index(self):\n        \"\"\"\n        Returns an index which corresponds to a frame which is expected to have a target.\n        \"\"\"\n        _, h, w, _ = self._data_shape\n        framepixelcount = h * w\n        targetpixels = np.array(\n            [framepixelcount] * (self._validtarget_ceilT - 1)\n            + [self._h_validmax * self._w_validmax]\n        )\n        targetpixels = targetpixels / np.sum(targetpixels)\n        t = np.random.choice(self._validtarget_ceilT, p=targetpixels)\n        # t = np.random.randint(0, self._validtarget_ceilT) if self._validtarget_ceilT &gt;= 1 else 0\n        h, w = self.get_valid_target_hw(t)\n        index = self.idx_manager.idx_from_hwt(h, w, t)\n        # print('Valid', index, h,w,t)\n        return index\n\n    def get_invalid_target_index(self):\n        # if self._validtarget_ceilT == 0:\n        # TODO: There may not be enough data for this to work. The better way is to skip using 0 for invalid target.\n        # t = np.random.randint(1, self._data_shape[0])\n        # elif self._validtarget_ceilT &lt; self._data_shape[0]:\n        #     t = np.random.randint(self._validtarget_ceilT, self._data_shape[0])\n        # else:\n        #     t = self._validtarget_ceilT - 1\n        # 5\n        # 1.2 =&gt; 2\n        total_t, h, w, _ = self._data_shape\n        framepixelcount = h * w\n        available_h = h - self._h_validmax\n        if available_h &lt; self._patch_size:\n            available_h = 0\n        available_w = w - self._w_validmax\n        if available_w &lt; self._patch_size:\n            available_w = 0\n\n        targetpixels = np.array(\n            [available_h * available_w]\n            + [framepixelcount] * (total_t - self._validtarget_ceilT)\n        )\n        t_probab = targetpixels / np.sum(targetpixels)\n        t = np.random.choice(\n            np.arange(self._validtarget_ceilT - 1, total_t), p=t_probab\n        )\n\n        h, w = self.get_invalid_target_hw(t)\n        index = self.idx_manager.idx_from_hwt(h, w, t)\n        # print('Invalid', index, h,w,t)\n        return index\n\n    def get_valid_target_hw(self, t):\n        \"\"\"\n        This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target.\n        This is only valid for single frame setup.\n        \"\"\"\n        if t == self._validtarget_ceilT - 1:\n            h = np.random.randint(0, self._h_validmax - self._patch_size)\n            w = np.random.randint(0, self._w_validmax - self._patch_size)\n        else:\n            h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n            w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n        return h, w\n\n    def get_invalid_target_hw(self, t):\n        \"\"\"\n        This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target.\n        This is only valid for single frame setup.\n        \"\"\"\n        if t == self._validtarget_ceilT - 1:\n            h = np.random.randint(\n                self._h_validmax, self._data_shape[1] - self._patch_size\n            )\n            w = np.random.randint(\n                self._w_validmax, self._data_shape[2] - self._patch_size\n            )\n        else:\n            h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n            w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n        return h, w\n\n    def _get_tidx(self, index):\n        if isinstance(index, int) or isinstance(index, np.int64):\n            idx = index\n        else:\n            idx = index[0]\n        return self.idx_manager.get_t(idx)\n\n    def index_should_have_target(self, index):\n        tidx = self._get_tidx(index)\n        if tidx &lt; self._validtarget_ceilT - 1:\n            return True\n        elif tidx &gt; self._validtarget_ceilT - 1:\n            return False\n        else:\n            h, w, _ = self.idx_manager.hwt_from_idx(index)\n            return (\n                h + self._patch_size &lt; self._h_validmax\n                and w + self._patch_size &lt; self._w_validmax\n            )\n\n    @staticmethod\n    def get_reduced_frame_size(data_shape_nhw, fraction):\n        n, h, w = data_shape_nhw\n\n        framepixelcount = h * w\n        targetpixelcount = int(n * framepixelcount * fraction)\n\n        # We are currently supporting this only when there is just one frame.\n        # if np.ceil(pixelcount / framepixelcount) &gt; 1:\n        #     return None, None\n\n        lastframepixelcount = targetpixelcount % framepixelcount\n        assert data_shape_nhw[1] == data_shape_nhw[2]\n        if lastframepixelcount &gt; 0:\n            new_size = int(np.sqrt(lastframepixelcount))\n            return new_size, new_size\n        else:\n            assert (\n                targetpixelcount / framepixelcount &gt;= 1\n            ), \"This is not possible in euclidean space :D (so this is a bug)\"\n            return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_invalid_target_hw","title":"<code>get_invalid_target_hw(t)</code>","text":"<p>This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target. This is only valid for single frame setup.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_invalid_target_hw(self, t):\n    \"\"\"\n    This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target.\n    This is only valid for single frame setup.\n    \"\"\"\n    if t == self._validtarget_ceilT - 1:\n        h = np.random.randint(\n            self._h_validmax, self._data_shape[1] - self._patch_size\n        )\n        w = np.random.randint(\n            self._w_validmax, self._data_shape[2] - self._patch_size\n        )\n    else:\n        h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n        w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n    return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_valid_target_hw","title":"<code>get_valid_target_hw(t)</code>","text":"<p>This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target. This is only valid for single frame setup.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_valid_target_hw(self, t):\n    \"\"\"\n    This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target.\n    This is only valid for single frame setup.\n    \"\"\"\n    if t == self._validtarget_ceilT - 1:\n        h = np.random.randint(0, self._h_validmax - self._patch_size)\n        w = np.random.randint(0, self._w_validmax - self._patch_size)\n    else:\n        h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n        w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n    return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_valid_target_index","title":"<code>get_valid_target_index()</code>","text":"<p>Returns an index which corresponds to a frame which is expected to have a target.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_valid_target_index(self):\n    \"\"\"\n    Returns an index which corresponds to a frame which is expected to have a target.\n    \"\"\"\n    _, h, w, _ = self._data_shape\n    framepixelcount = h * w\n    targetpixels = np.array(\n        [framepixelcount] * (self._validtarget_ceilT - 1)\n        + [self._h_validmax * self._w_validmax]\n    )\n    targetpixels = targetpixels / np.sum(targetpixels)\n    t = np.random.choice(self._validtarget_ceilT, p=targetpixels)\n    # t = np.random.randint(0, self._validtarget_ceilT) if self._validtarget_ceilT &gt;= 1 else 0\n    h, w = self.get_valid_target_hw(t)\n    index = self.idx_manager.idx_from_hwt(h, w, t)\n    # print('Valid', index, h,w,t)\n    return index\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/","title":"bmz_io","text":"<p>Function to export to the BioImage Model Zoo format.</p>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io.export_to_bmz","title":"<code>export_to_bmz(model, config, path_to_archive, model_name, general_description, data_description, authors, input_array, output_array, covers=None, channel_names=None, model_version='0.1.0')</code>","text":"<p>Export the model to BioImage Model Zoo format.</p> <p>Arrays are expected to be SC(Z)YX with singleton dimensions allowed for S and C.</p> <p><code>model_name</code> should consist of letters, numbers, dashes, underscores and parentheses only.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CAREamicsModule</code> <p>CAREamics model to export.</p> required <code>config</code> <code>Configuration</code> <p>Model configuration.</p> required <code>path_to_archive</code> <code>Union[Path, str]</code> <p>Path to the output file.</p> required <code>model_name</code> <code>str</code> <p>Model name.</p> required <code>general_description</code> <code>str</code> <p>General description of the model.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>authors</code> <code>list[dict]</code> <p>Authors of the model.</p> required <code>input_array</code> <code>ndarray</code> <p>Input array, should not have been normalized.</p> required <code>output_array</code> <code>ndarray</code> <p>Output array, should have been denormalized.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to the cover images.</p> <code>None</code> <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Model version.</p> <code>\"0.1.0\"</code> Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def export_to_bmz(\n    model: Union[FCNModule, VAEModule],\n    config: Configuration,\n    path_to_archive: Union[Path, str],\n    model_name: str,\n    general_description: str,\n    data_description: str,\n    authors: list[dict],\n    input_array: np.ndarray,\n    output_array: np.ndarray,\n    covers: list[Union[Path, str]] | None = None,\n    channel_names: list[str] | None = None,\n    model_version: str = \"0.1.0\",\n) -&gt; None:\n    \"\"\"Export the model to BioImage Model Zoo format.\n\n    Arrays are expected to be SC(Z)YX with singleton dimensions allowed for S and C.\n\n    `model_name` should consist of letters, numbers, dashes, underscores and parentheses\n    only.\n\n    Parameters\n    ----------\n    model : CAREamicsModule\n        CAREamics model to export.\n    config : Configuration\n        Model configuration.\n    path_to_archive : Union[Path, str]\n        Path to the output file.\n    model_name : str\n        Model name.\n    general_description : str\n        General description of the model.\n    data_description : str\n        Description of the data the model was trained on.\n    authors : list[dict]\n        Authors of the model.\n    input_array : np.ndarray\n        Input array, should not have been normalized.\n    output_array : np.ndarray\n        Output array, should have been denormalized.\n    covers : list of pathlib.Path or str, default=None\n        Paths to the cover images.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n    model_version : str, default=\"0.1.0\"\n        Model version.\n    \"\"\"\n    path_to_archive = Path(path_to_archive)\n\n    if path_to_archive.suffix != \".zip\":\n        raise ValueError(\n            f\"Path to archive must point to a zip file, got {path_to_archive}.\"\n        )\n\n    if not path_to_archive.parent.exists():\n        path_to_archive.parent.mkdir(parents=True, exist_ok=True)\n\n    # versions\n    careamics_version = get_careamics_version()\n\n    # save files in temporary folder\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        temp_path = Path(tmpdirname)\n\n        # create environment file\n        # TODO move in bioimage module\n        env_path = temp_path / \"environment.yml\"\n        env_path.write_text(create_env_text(PYTORCH_VERSION, TORCHVISION_VERSION))\n\n        # export input and ouputs\n        inputs = temp_path / \"inputs.npy\"\n        np.save(inputs, input_array)\n        outputs = temp_path / \"outputs.npy\"\n        np.save(outputs, output_array)\n\n        # export configuration\n        config_path = save_configuration(config, temp_path / \"careamics.yaml\")\n\n        # export model state dictionary\n        weight_path = _export_state_dict(model, temp_path / \"weights.pth\")\n\n        # export cover if necesary\n        if covers is None:\n            covers = [create_cover(temp_path, input_array, output_array)]\n\n        # create model description\n        model_description = create_model_description(\n            config=config,\n            name=model_name,\n            general_description=general_description,\n            data_description=data_description,\n            authors=authors,\n            inputs=inputs,\n            outputs=outputs,\n            weights_path=weight_path,\n            torch_version=PYTORCH_VERSION,\n            careamics_version=careamics_version,\n            config_path=config_path,\n            env_path=env_path,\n            covers=covers,\n            channel_names=channel_names,\n            model_version=model_version,\n        )\n\n        # test model description\n        test_kwargs = {}\n        if hasattr(model_description, \"config\") and isinstance(\n            model_description.config, dict\n        ):\n            bioimageio_config = model_description.config.get(\"bioimageio\", {})\n            test_kwargs = bioimageio_config.get(\"test_kwargs\", {}).get(\n                \"pytorch_state_dict\", {}\n            )\n\n        summary: ValidationSummary = test_model(model_description, **test_kwargs)\n        if summary.status == \"failed\":\n            raise ValueError(f\"Model description test failed: {summary}\")\n\n        # save bmz model\n        save_bioimageio_package(model_description, output_path=path_to_archive)\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io.load_from_bmz","title":"<code>load_from_bmz(path)</code>","text":"<p>Load a model from a BioImage Model Zoo archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>(Path, str or HttpUrl)</code> <p>Path to the BioImage Model Zoo archive. A Http URL must point to a downloadable location.</p> required <p>Returns:</p> Type Description <code>FCNModel or VAEModel</code> <p>The loaded CAREamics model.</p> <code>Configuration</code> <p>The loaded CAREamics configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path is not a zip file.</p> Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def load_from_bmz(\n    path: Union[Path, str, HttpUrl],\n) -&gt; tuple[Union[FCNModule, VAEModule], Configuration]:\n    \"\"\"Load a model from a BioImage Model Zoo archive.\n\n    Parameters\n    ----------\n    path : Path, str or HttpUrl\n        Path to the BioImage Model Zoo archive. A Http URL must point to a downloadable\n        location.\n\n    Returns\n    -------\n    FCNModel or VAEModel\n        The loaded CAREamics model.\n    Configuration\n        The loaded CAREamics configuration.\n\n    Raises\n    ------\n    ValueError\n        If the path is not a zip file.\n    \"\"\"\n    # load description, this creates an unzipped folder next to the archive\n    model_desc = load_model_description(path)\n\n    # extract paths\n    weights_path, config_path = extract_model_path(model_desc)\n\n    # load configuration\n    config = load_configuration(config_path)\n\n    # create careamics lightning module\n    if config.algorithm_config.model.architecture == SupportedArchitecture.UNET:\n        model = FCNModule(algorithm_config=config.algorithm_config)\n    elif config.algorithm_config.model.architecture == SupportedArchitecture.LVAE:\n        model = VAEModule(algorithm_config=config.algorithm_config)\n    else:\n        raise ValueError(\n            f\"Unsupported architecture {config.algorithm_config.model.architecture}\"\n        )  # TODO ugly ?\n\n    # load model state dictionary\n    _load_state_dict(model, weights_path)\n\n    return model, config\n</code></pre>"},{"location":"reference/careamics/model_io/model_io_utils/","title":"model_io_utils","text":"<p>Utility functions to load pretrained models.</p>"},{"location":"reference/careamics/model_io/model_io_utils/#careamics.model_io.model_io_utils.load_pretrained","title":"<code>load_pretrained(path)</code>","text":"<p>Load a pretrained model from a checkpoint or a BioImage Model Zoo model.</p> <p>Expected formats are .ckpt or .zip files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path to the pretrained model.</p> required <p>Returns:</p> Type Description <code>tuple[CAREamicsKiln, Configuration]</code> <p>tuple of CAREamics model and its configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model format is not supported.</p> Source code in <code>src/careamics/model_io/model_io_utils.py</code> <pre><code>def load_pretrained(\n    path: Union[Path, str],\n) -&gt; tuple[Union[FCNModule, VAEModule], Configuration]:\n    \"\"\"\n    Load a pretrained model from a checkpoint or a BioImage Model Zoo model.\n\n    Expected formats are .ckpt or .zip files.\n\n    Parameters\n    ----------\n    path : Union[Path, str]\n        Path to the pretrained model.\n\n    Returns\n    -------\n    tuple[CAREamicsKiln, Configuration]\n        tuple of CAREamics model and its configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model format is not supported.\n    \"\"\"\n    path = check_path_exists(path)\n\n    if path.suffix == \".ckpt\":\n        return _load_checkpoint(path)\n    elif path.suffix == \".zip\":\n        return load_from_bmz(path)\n    else:\n        raise ValueError(\n            f\"Invalid model format. Expected .ckpt or .zip, got {path.suffix}.\"\n        )\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/_readme_factory/","title":"_readme_factory","text":"<p>Functions used to create a README.md file for BMZ export.</p>"},{"location":"reference/careamics/model_io/bioimage/_readme_factory/#careamics.model_io.bioimage._readme_factory.readme_factory","title":"<code>readme_factory(config, careamics_version, data_description)</code>","text":"<p>Create a README file for the model.</p> <p><code>data_description</code> can be used to add more information about the content of the data the model was trained on.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration.</p> required <code>careamics_version</code> <code>str</code> <p>CAREamics version.</p> required <code>data_description</code> <code>str</code> <p>Description of the data.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the README file.</p> Source code in <code>src/careamics/model_io/bioimage/_readme_factory.py</code> <pre><code>def readme_factory(\n    config: Configuration,\n    careamics_version: str,\n    data_description: str,\n) -&gt; Path:\n    \"\"\"Create a README file for the model.\n\n    `data_description` can be used to add more information about the content of the\n    data the model was trained on.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration.\n    careamics_version : str\n        CAREamics version.\n    data_description : str\n        Description of the data.\n\n    Returns\n    -------\n    Path\n        Path to the README file.\n    \"\"\"\n    # create file\n    # TODO use tempfile as in the bmz_io module\n    with cwd(get_careamics_home()):\n        readme = Path(\"README.md\")\n        readme.touch()\n\n        # algorithm pretty name\n        algorithm_flavour = config.get_algorithm_friendly_name()\n        algorithm_pretty_name = algorithm_flavour + \" - CAREamics\"\n\n        description = [f\"# {algorithm_pretty_name}\\n\\n\"]\n\n        # data description\n        description.append(\"## Data description\\n\\n\")\n        description.append(data_description)\n        description.append(\"\\n\\n\")\n\n        # algorithm description\n        description.append(\"## Algorithm description:\\n\\n\")\n        description.append(config.get_algorithm_description())\n        description.append(\"\\n\\n\")\n\n        # configuration description\n        description.append(\"## Configuration\\n\\n\")\n\n        description.append(\n            f\"{algorithm_flavour} was trained using CAREamics (version \"\n            f\"{careamics_version}) using the following configuration:\\n\\n\"\n        )\n\n        description.append(_yaml_block(yaml.dump(config.model_dump(exclude_none=True))))\n        description.append(\"\\n\\n\")\n\n        # validation\n        description.append(\"# Validation\\n\\n\")\n\n        description.append(\n            \"In order to validate the model, we encourage users to acquire a \"\n            \"test dataset with ground-truth data. Comparing the ground-truth data \"\n            \"with the prediction allows unbiased evaluation of the model performances. \"\n            \"This can be done for instance by using metrics such as PSNR, SSIM, or\"\n            \"MicroSSIM. In the absence of ground-truth, inspecting the residual image \"\n            \"(difference between input and predicted image) can be helpful to identify \"\n            \"whether real signal is removed from the input image.\\n\\n\"\n        )\n\n        # references\n        reference = config.get_algorithm_references()\n        if reference != \"\":\n            description.append(\"## References\\n\\n\")\n            description.append(reference)\n            description.append(\"\\n\\n\")\n\n        # links\n        description.append(\n            \"# Links\\n\\n\"\n            \"- [CAREamics repository](https://github.com/CAREamics/careamics)\\n\"\n            \"- [CAREamics documentation](https://careamics.github.io/)\\n\"\n        )\n\n        readme.write_text(\"\".join(description))\n\n        return readme.absolute()\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/","title":"bioimage_utils","text":"<p>Bioimage.io utils.</p>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/#careamics.model_io.bioimage.bioimage_utils.create_env_text","title":"<code>create_env_text(pytorch_version, torchvision_version)</code>","text":"<p>Create environment yaml content for the bioimage model.</p> <p>This installs an environment with the specified pytorch version and the latest changes to careamics.</p> <p>Parameters:</p> Name Type Description Default <code>pytorch_version</code> <code>str</code> <p>Pytorch version.</p> required <code>torchvision_version</code> <code>str</code> <p>Torchvision version.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Environment text.</p> Source code in <code>src/careamics/model_io/bioimage/bioimage_utils.py</code> <pre><code>def create_env_text(pytorch_version: str, torchvision_version: str) -&gt; str:\n    \"\"\"Create environment yaml content for the bioimage model.\n\n    This installs an environment with the specified pytorch version and the latest\n    changes to careamics.\n\n    Parameters\n    ----------\n    pytorch_version : str\n        Pytorch version.\n    torchvision_version : str\n        Torchvision version.\n\n    Returns\n    -------\n    str\n        Environment text.\n    \"\"\"\n    env = (\n        f\"name: careamics\\n\"\n        f\"dependencies:\\n\"\n        f\"  - python=3.12\\n\"\n        f\"  - pip\\n\"\n        f\"  - pip:\\n\"\n        f\"    - torch=={pytorch_version}\\n\"\n        f\"    - torchvision=={torchvision_version}\\n\"\n        f\"    - careamics=={get_careamics_version()}\"\n    )\n\n    return env\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/#careamics.model_io.bioimage.bioimage_utils.get_unzip_path","title":"<code>get_unzip_path(zip_path)</code>","text":"<p>Generate unzipped folder path from the bioimage.io model path.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>Path</code> <p>Path to the bioimage.io model.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the unzipped folder.</p> Source code in <code>src/careamics/model_io/bioimage/bioimage_utils.py</code> <pre><code>def get_unzip_path(zip_path: Union[Path, str]) -&gt; Path:\n    \"\"\"Generate unzipped folder path from the bioimage.io model path.\n\n    Parameters\n    ----------\n    zip_path : Path\n        Path to the bioimage.io model.\n\n    Returns\n    -------\n    Path\n        Path to the unzipped folder.\n    \"\"\"\n    zip_path = Path(zip_path)\n\n    return zip_path.parent / (str(zip_path.name) + \".unzip\")\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/","title":"cover_factory","text":"<p>Convenience function to create covers for the BMZ.</p>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/#careamics.model_io.bioimage.cover_factory.create_cover","title":"<code>create_cover(directory, array_in, array_out)</code>","text":"<p>Create a cover image from input and output arrays.</p> <p>Input and output arrays are expected to be SC(Z)YX. For images with a Z dimension, the middle slice is taken.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory in which to save the cover.</p> required <code>array_in</code> <code>ndarray</code> <p>Array from which to create the cover image.</p> required <code>array_out</code> <code>ndarray</code> <p>Array from which to create the cover image.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved cover image.</p> Source code in <code>src/careamics/model_io/bioimage/cover_factory.py</code> <pre><code>def create_cover(directory: Path, array_in: NDArray, array_out: NDArray) -&gt; Path:\n    \"\"\"Create a cover image from input and output arrays.\n\n    Input and output arrays are expected to be SC(Z)YX. For images with a Z\n    dimension, the middle slice is taken.\n\n    Parameters\n    ----------\n    directory : Path\n        Directory in which to save the cover.\n    array_in : numpy.ndarray\n        Array from which to create the cover image.\n    array_out : numpy.ndarray\n        Array from which to create the cover image.\n\n    Returns\n    -------\n    Path\n        Path to the saved cover image.\n    \"\"\"\n    # extract slice and normalize arrays\n    slice_in = _get_norm_slice(array_in)\n    slice_out = _get_norm_slice(array_out)\n\n    horizontal_split = slice_in.shape[-1] == slice_out.shape[-1]\n    if not horizontal_split:\n        if slice_in.shape[-2] != slice_out.shape[-2]:\n            raise ValueError(\"Input and output arrays have different shapes.\")\n\n    # convert to Image\n    image_in = _convert_to_image(array_in.shape, slice_in)\n    image_out = _convert_to_image(array_out.shape, slice_out)\n\n    # split horizontally or vertically\n    if horizontal_split:\n        width = image_in.width // 2\n\n        cover = Image.new(\"RGB\", (image_in.width, image_in.height))\n        cover.paste(image_in.crop((0, 0, width, image_in.height)), (0, 0))\n        cover.paste(\n            image_out.crop(\n                (image_in.width - width, 0, image_in.width, image_in.height)\n            ),\n            (width, 0),\n        )\n    else:\n        height = image_in.height // 2\n\n        cover = Image.new(\"RGB\", (image_in.width, image_in.height))\n        cover.paste(image_in.crop((0, 0, image_in.width, height)), (0, 0))\n        cover.paste(\n            image_out.crop(\n                (0, image_in.height - height, image_in.width, image_in.height)\n            ),\n            (0, height),\n        )\n\n    # save\n    cover_path = directory / \"cover.png\"\n    cover.save(cover_path)\n\n    return cover_path\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/","title":"model_description","text":"<p>Module use to build BMZ model description.</p>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description.create_model_description","title":"<code>create_model_description(config, name, general_description, data_description, authors, inputs, outputs, weights_path, torch_version, careamics_version, config_path, env_path, covers, channel_names=None, model_version='0.1.0')</code>","text":"<p>Create model description.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration.</p> required <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>general_description</code> <code>str</code> <p>General description of the model.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>authors</code> <code>list[Author]</code> <p>Authors of the model.</p> required <code>inputs</code> <code>Union[Path, str]</code> <p>Path to input .npy file.</p> required <code>outputs</code> <code>Union[Path, str]</code> <p>Path to output .npy file.</p> required <code>weights_path</code> <code>Union[Path, str]</code> <p>Path to model weights.</p> required <code>torch_version</code> <code>str</code> <p>Pytorch version.</p> required <code>careamics_version</code> <code>str</code> <p>CAREamics version.</p> required <code>config_path</code> <code>Union[Path, str]</code> <p>Path to model configuration.</p> required <code>env_path</code> <code>Union[Path, str]</code> <p>Path to environment file.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to cover images.</p> required <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Model version.</p> <code>\"0.1.0\"</code> <p>Returns:</p> Type Description <code>ModelDescr</code> <p>Model description.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def create_model_description(\n    config: Configuration,\n    name: str,\n    general_description: str,\n    data_description: str,\n    authors: list[Author],\n    inputs: Union[Path, str],\n    outputs: Union[Path, str],\n    weights_path: Union[Path, str],\n    torch_version: str,\n    careamics_version: str,\n    config_path: Union[Path, str],\n    env_path: Union[Path, str],\n    covers: list[Union[Path, str]],\n    channel_names: list[str] | None = None,\n    model_version: str = \"0.1.0\",\n) -&gt; ModelDescr:\n    \"\"\"Create model description.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration.\n    name : str\n        Name of the model.\n    general_description : str\n        General description of the model.\n    data_description : str\n        Description of the data the model was trained on.\n    authors : list[Author]\n        Authors of the model.\n    inputs : Union[Path, str]\n        Path to input .npy file.\n    outputs : Union[Path, str]\n        Path to output .npy file.\n    weights_path : Union[Path, str]\n        Path to model weights.\n    torch_version : str\n        Pytorch version.\n    careamics_version : str\n        CAREamics version.\n    config_path : Union[Path, str]\n        Path to model configuration.\n    env_path : Union[Path, str]\n        Path to environment file.\n    covers : list of pathlib.Path or str\n        Paths to cover images.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n    model_version : str, default \"0.1.0\"\n        Model version.\n\n    Returns\n    -------\n    ModelDescr\n        Model description.\n    \"\"\"\n    # documentation\n    doc = readme_factory(\n        config,\n        careamics_version=careamics_version,\n        data_description=data_description,\n    )\n\n    # inputs, outputs\n    input_descr, output_descr = _create_inputs_ouputs(\n        input_array=np.load(inputs),\n        output_array=np.load(outputs),\n        data_config=config.data_config,\n        input_path=inputs,\n        output_path=outputs,\n        channel_names=channel_names,\n    )\n\n    # weights description\n    architecture_descr = ArchitectureFromLibraryDescr(\n        import_from=\"careamics.models.unet\",\n        callable=f\"{config.algorithm_config.model.architecture}\",\n        kwargs=config.algorithm_config.model.model_dump(),\n    )\n\n    weights_descr = WeightsDescr(\n        pytorch_state_dict=PytorchStateDictWeightsDescr(\n            source=weights_path,\n            architecture=architecture_descr,\n            pytorch_version=Version(torch_version),\n            dependencies=FileDescr(source=Path(env_path)),\n        ),\n    )\n\n    # overall model description\n    model = ModelDescr(\n        name=name,\n        authors=authors,\n        description=general_description,\n        documentation=doc,\n        inputs=[input_descr],\n        outputs=[output_descr],\n        tags=config.get_algorithm_keywords(),\n        links=[\n            \"https://github.com/CAREamics/careamics\",\n            \"https://careamics.github.io/latest/\",\n        ],\n        license=\"BSD-3-Clause\",\n        config={\n            \"bioimageio\": {\n                \"test_kwargs\": {\n                    \"pytorch_state_dict\": {\n                        \"absolute_tolerance\": 1e-2,\n                        \"relative_tolerance\": 1e-2,\n                    }\n                }\n            }\n        },\n        version=model_version,\n        weights=weights_descr,\n        attachments=[FileDescr(source=config_path)],\n        cite=config.get_algorithm_citations(),\n        covers=covers,\n    )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description.extract_model_path","title":"<code>extract_model_path(model_desc)</code>","text":"<p>Return the relative path to the weights and configuration files.</p> <p>Parameters:</p> Name Type Description Default <code>model_desc</code> <code>ModelDescr</code> <p>Model description.</p> required <p>Returns:</p> Type Description <code>tuple of (path, path)</code> <p>Weights and configuration paths.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def extract_model_path(model_desc: ModelDescr) -&gt; tuple[Path, Path]:\n    \"\"\"Return the relative path to the weights and configuration files.\n\n    Parameters\n    ----------\n    model_desc : ModelDescr\n        Model description.\n\n    Returns\n    -------\n    tuple of (path, path)\n        Weights and configuration paths.\n    \"\"\"\n    if model_desc.weights.pytorch_state_dict is None:\n        raise ValueError(\"No model weights found in model description.\")\n\n    # extract the zip model and return the directory\n    model_dir = extract(model_desc.root)\n\n    weights_path = model_dir.joinpath(model_desc.weights.pytorch_state_dict.source.path)\n\n    for file in model_desc.attachments:\n        file_path = file.source if isinstance(file.source, Path) else file.source.path\n        if file_path is None:\n            continue\n        file_path = Path(file_path)\n        if file_path.name == \"careamics.yaml\":\n            config_path = model_dir.joinpath(file.source.path)\n            break\n    else:\n        raise ValueError(\"Configuration file not found.\")\n\n    return weights_path, config_path\n</code></pre>"},{"location":"reference/careamics/models/activation/","title":"activation","text":"<p>Activations for CAREamics models.</p>"},{"location":"reference/careamics/models/activation/#careamics.models.activation.get_activation","title":"<code>get_activation(activation)</code>","text":"<p>Get activation function.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Activation function name.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Activation function.</p> Source code in <code>src/careamics/models/activation.py</code> <pre><code>def get_activation(activation: Union[SupportedActivation, str]) -&gt; Callable:\n    \"\"\"\n    Get activation function.\n\n    Parameters\n    ----------\n    activation : str\n        Activation function name.\n\n    Returns\n    -------\n    Callable\n        Activation function.\n    \"\"\"\n    if activation == SupportedActivation.RELU:\n        return nn.ReLU()\n    elif activation == SupportedActivation.ELU:\n        return nn.ELU()\n    elif activation == SupportedActivation.LEAKYRELU:\n        return nn.LeakyReLU()\n    elif activation == SupportedActivation.TANH:\n        return nn.Tanh()\n    elif activation == SupportedActivation.SIGMOID:\n        return nn.Sigmoid()\n    elif activation == SupportedActivation.SOFTMAX:\n        return nn.Softmax(dim=1)\n    elif activation == SupportedActivation.NONE:\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported.\")\n</code></pre>"},{"location":"reference/careamics/models/layers/","title":"layers","text":"<p>Layer module.</p> <p>This submodule contains layers used in the CAREamics models.</p>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block","title":"<code>Conv_Block</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolution block used in UNets.</p> <p>Convolution block consist of two convolution layers with optional batch norm, dropout and with a final activation function.</p> <p>The parameters are directly mapped to PyTorch Conv2D and Conv3d parameters, see PyTorch torch.nn.Conv2d and torch.nn.Conv3d for more information.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolutions, 2 or 3.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>intermediate_channel_multiplier</code> <code>int</code> <p>Multiplied for the number of output channels, by default 1.</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride of the convolutions, by default 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding of the convolutions, by default 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of the convolutions, by default True.</p> <code>True</code> <code>groups</code> <code>int</code> <p>Controls the connections between inputs and outputs, by default 1.</p> <code>1</code> <code>activation</code> <code>str</code> <p>Activation function, by default \"ReLU\".</p> <code>'ReLU'</code> <code>dropout_perc</code> <code>float</code> <p>Dropout percentage, by default 0.</p> <code>0</code> <code>use_batch_norm</code> <code>bool</code> <p>Use batch norm, by default False.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>class Conv_Block(nn.Module):\n    \"\"\"\n    Convolution block used in UNets.\n\n    Convolution block consist of two convolution layers with optional batch norm,\n    dropout and with a final activation function.\n\n    The parameters are directly mapped to PyTorch Conv2D and Conv3d parameters, see\n    PyTorch torch.nn.Conv2d and torch.nn.Conv3d for more information.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolutions, 2 or 3.\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    intermediate_channel_multiplier : int, optional\n        Multiplied for the number of output channels, by default 1.\n    stride : int, optional\n        Stride of the convolutions, by default 1.\n    padding : int, optional\n        Padding of the convolutions, by default 1.\n    bias : bool, optional\n        Bias of the convolutions, by default True.\n    groups : int, optional\n        Controls the connections between inputs and outputs, by default 1.\n    activation : str, optional\n        Activation function, by default \"ReLU\".\n    dropout_perc : float, optional\n        Dropout percentage, by default 0.\n    use_batch_norm : bool, optional\n        Use batch norm, by default False.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        in_channels: int,\n        out_channels: int,\n        intermediate_channel_multiplier: int = 1,\n        stride: int = 1,\n        padding: int = 1,\n        bias: bool = True,\n        groups: int = 1,\n        activation: str = \"ReLU\",\n        dropout_perc: float = 0,\n        use_batch_norm: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolutions, 2 or 3.\n        in_channels : int\n            Number of input channels.\n        out_channels : int\n            Number of output channels.\n        intermediate_channel_multiplier : int, optional\n            Multiplied for the number of output channels, by default 1.\n        stride : int, optional\n            Stride of the convolutions, by default 1.\n        padding : int, optional\n            Padding of the convolutions, by default 1.\n        bias : bool, optional\n            Bias of the convolutions, by default True.\n        groups : int, optional\n            Controls the connections between inputs and outputs, by default 1.\n        activation : str, optional\n            Activation function, by default \"ReLU\".\n        dropout_perc : float, optional\n            Dropout percentage, by default 0.\n        use_batch_norm : bool, optional\n            Use batch norm, by default False.\n        \"\"\"\n        super().__init__()\n        self.use_batch_norm = use_batch_norm\n        self.conv1 = getattr(nn, f\"Conv{conv_dim}d\")(\n            in_channels,\n            out_channels * intermediate_channel_multiplier,\n            kernel_size=3,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n\n        self.conv2 = getattr(nn, f\"Conv{conv_dim}d\")(\n            out_channels * intermediate_channel_multiplier,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n\n        self.batch_norm1 = getattr(nn, f\"BatchNorm{conv_dim}d\")(\n            out_channels * intermediate_channel_multiplier\n        )\n        self.batch_norm2 = getattr(nn, f\"BatchNorm{conv_dim}d\")(out_channels)\n\n        self.dropout = (\n            getattr(nn, f\"Dropout{conv_dim}d\")(dropout_perc)\n            if dropout_perc &gt; 0\n            else None\n        )\n        self.activation = (\n            getattr(nn, f\"{activation}\")() if activation is not None else nn.Identity()\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor.\n        \"\"\"\n        if self.use_batch_norm:\n            x = self.conv1(x)\n            x = self.batch_norm1(x)\n            x = self.activation(x)\n            x = self.conv2(x)\n            x = self.batch_norm2(x)\n            x = self.activation(x)\n        else:\n            x = self.conv1(x)\n            x = self.activation(x)\n            x = self.conv2(x)\n            x = self.activation(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block.__init__","title":"<code>__init__(conv_dim, in_channels, out_channels, intermediate_channel_multiplier=1, stride=1, padding=1, bias=True, groups=1, activation='ReLU', dropout_perc=0, use_batch_norm=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolutions, 2 or 3.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>intermediate_channel_multiplier</code> <code>int</code> <p>Multiplied for the number of output channels, by default 1.</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride of the convolutions, by default 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding of the convolutions, by default 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of the convolutions, by default True.</p> <code>True</code> <code>groups</code> <code>int</code> <p>Controls the connections between inputs and outputs, by default 1.</p> <code>1</code> <code>activation</code> <code>str</code> <p>Activation function, by default \"ReLU\".</p> <code>'ReLU'</code> <code>dropout_perc</code> <code>float</code> <p>Dropout percentage, by default 0.</p> <code>0</code> <code>use_batch_norm</code> <code>bool</code> <p>Use batch norm, by default False.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    in_channels: int,\n    out_channels: int,\n    intermediate_channel_multiplier: int = 1,\n    stride: int = 1,\n    padding: int = 1,\n    bias: bool = True,\n    groups: int = 1,\n    activation: str = \"ReLU\",\n    dropout_perc: float = 0,\n    use_batch_norm: bool = False,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolutions, 2 or 3.\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    intermediate_channel_multiplier : int, optional\n        Multiplied for the number of output channels, by default 1.\n    stride : int, optional\n        Stride of the convolutions, by default 1.\n    padding : int, optional\n        Padding of the convolutions, by default 1.\n    bias : bool, optional\n        Bias of the convolutions, by default True.\n    groups : int, optional\n        Controls the connections between inputs and outputs, by default 1.\n    activation : str, optional\n        Activation function, by default \"ReLU\".\n    dropout_perc : float, optional\n        Dropout percentage, by default 0.\n    use_batch_norm : bool, optional\n        Use batch norm, by default False.\n    \"\"\"\n    super().__init__()\n    self.use_batch_norm = use_batch_norm\n    self.conv1 = getattr(nn, f\"Conv{conv_dim}d\")(\n        in_channels,\n        out_channels * intermediate_channel_multiplier,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n\n    self.conv2 = getattr(nn, f\"Conv{conv_dim}d\")(\n        out_channels * intermediate_channel_multiplier,\n        out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n\n    self.batch_norm1 = getattr(nn, f\"BatchNorm{conv_dim}d\")(\n        out_channels * intermediate_channel_multiplier\n    )\n    self.batch_norm2 = getattr(nn, f\"BatchNorm{conv_dim}d\")(out_channels)\n\n    self.dropout = (\n        getattr(nn, f\"Dropout{conv_dim}d\")(dropout_perc)\n        if dropout_perc &gt; 0\n        else None\n    )\n    self.activation = (\n        getattr(nn, f\"{activation}\")() if activation is not None else nn.Identity()\n    )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    if self.use_batch_norm:\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        x = self.activation(x)\n    else:\n        x = self.conv1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        x = self.activation(x)\n    if self.dropout is not None:\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool","title":"<code>MaxBlurPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute pools and blurs and downsample a given feature map.</p> <p>Inspired by Kornia MaxBlurPool implementation. Equivalent to <code>nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))</code></p>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool--parameters","title":"Parameters","text":"<p>dim : int     Toggles between 2D and 3D. kernel_size : Union[tuple[int, int], int]     Kernel size for max pooling. stride : int     Stride for pooling. max_pool_size : int     Max kernel size for max pooling. ceil_mode : bool     Ceil mode, by default False. Set to True to match output size of conv2d.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>class MaxBlurPool(nn.Module):\n    \"\"\"Compute pools and blurs and downsample a given feature map.\n\n    Inspired by Kornia MaxBlurPool implementation. Equivalent to\n    ```nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))```\n\n    Parameters\n    ----------\n    dim : int\n        Toggles between 2D and 3D.\n    kernel_size : Union[tuple[int, int], int]\n        Kernel size for max pooling.\n    stride : int\n        Stride for pooling.\n    max_pool_size : int\n        Max kernel size for max pooling.\n    ceil_mode : bool\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        kernel_size: Union[tuple[int, int], int],\n        stride: int = 2,\n        max_pool_size: int = 2,\n        ceil_mode: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        dim : int\n            Dimension of the convolution.\n        kernel_size : Union[tuple[int, int], int]\n            Kernel size for max pooling.\n        stride : int, optional\n            Stride, by default 2.\n        max_pool_size : int, optional\n            Maximum pool size, by default 2.\n        ceil_mode : bool, optional\n            Ceil mode, by default False. Set to True to match output size of conv2d.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.max_pool_size = max_pool_size\n        self.ceil_mode = ceil_mode\n        kernel = _get_pascal_kernel_nd(kernel_size, norm=True, dim=self.dim)\n        self.register_buffer(\"kernel\", kernel, persistent=False)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the function.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor.\n        \"\"\"\n        kernel = self.kernel.to(dtype=x.dtype)\n        num_channels = int(x.size(1))\n        if self.dim == 2:\n            return _max_blur_pool_by_kernel2d(\n                x,\n                kernel.repeat((num_channels, 1, 1, 1)),\n                self.stride,\n                self.max_pool_size,\n                self.ceil_mode,\n            )\n        else:\n            return _max_blur_pool_by_kernel3d(\n                x,\n                kernel.repeat((num_channels, 1, 1, 1, 1)),\n                self.stride,\n                self.max_pool_size,\n                self.ceil_mode,\n            )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool.__init__","title":"<code>__init__(dim, kernel_size, stride=2, max_pool_size=2, ceil_mode=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of the convolution.</p> required <code>kernel_size</code> <code>Union[tuple[int, int], int]</code> <p>Kernel size for max pooling.</p> required <code>stride</code> <code>int</code> <p>Stride, by default 2.</p> <code>2</code> <code>max_pool_size</code> <code>int</code> <p>Maximum pool size, by default 2.</p> <code>2</code> <code>ceil_mode</code> <code>bool</code> <p>Ceil mode, by default False. Set to True to match output size of conv2d.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    kernel_size: Union[tuple[int, int], int],\n    stride: int = 2,\n    max_pool_size: int = 2,\n    ceil_mode: bool = False,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension of the convolution.\n    kernel_size : Union[tuple[int, int], int]\n        Kernel size for max pooling.\n    stride : int, optional\n        Stride, by default 2.\n    max_pool_size : int, optional\n        Maximum pool size, by default 2.\n    ceil_mode : bool, optional\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n    \"\"\"\n    super().__init__()\n    self.dim = dim\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.max_pool_size = max_pool_size\n    self.ceil_mode = ceil_mode\n    kernel = _get_pascal_kernel_nd(kernel_size, norm=True, dim=self.dim)\n    self.register_buffer(\"kernel\", kernel, persistent=False)\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the function.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    kernel = self.kernel.to(dtype=x.dtype)\n    num_channels = int(x.size(1))\n    if self.dim == 2:\n        return _max_blur_pool_by_kernel2d(\n            x,\n            kernel.repeat((num_channels, 1, 1, 1)),\n            self.stride,\n            self.max_pool_size,\n            self.ceil_mode,\n        )\n    else:\n        return _max_blur_pool_by_kernel3d(\n            x,\n            kernel.repeat((num_channels, 1, 1, 1, 1)),\n            self.stride,\n            self.max_pool_size,\n            self.ceil_mode,\n        )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.get_pascal_kernel_1d","title":"<code>get_pascal_kernel_1d(kernel_size, norm=False, *, device=None, dtype=None)</code>","text":"<p>Generate Yang Hui triangle (Pascal's triangle) for a given number.</p> <p>Inspired by Kornia implementation. TODO link</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int</code> <p>Kernel size.</p> required <code>norm</code> <code>bool</code> <p>Normalize the kernel, by default False.</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>Device of the tensor, by default None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>Data type of the tensor, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Pascal kernel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_pascal_kernel_1d(1)\ntensor([1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(2)\ntensor([1., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(3)\ntensor([1., 2., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(4)\ntensor([1., 3., 3., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(5)\ntensor([1., 4., 6., 4., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(6)\ntensor([ 1.,  5., 10., 10.,  5.,  1.])\n</code></pre> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def get_pascal_kernel_1d(\n    kernel_size: int,\n    norm: bool = False,\n    *,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Generate Yang Hui triangle (Pascal's triangle) for a given number.\n\n    Inspired by Kornia implementation. TODO link\n\n    Parameters\n    ----------\n    kernel_size : int\n        Kernel size.\n    norm : bool\n        Normalize the kernel, by default False.\n    device : Optional[torch.device]\n        Device of the tensor, by default None.\n    dtype : Optional[torch.dtype]\n        Data type of the tensor, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        Pascal kernel.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_pascal_kernel_1d(1)\n    tensor([1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(2)\n    tensor([1., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(3)\n    tensor([1., 2., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(4)\n    tensor([1., 3., 3., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(5)\n    tensor([1., 4., 6., 4., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(6)\n    tensor([ 1.,  5., 10., 10.,  5.,  1.])\n    \"\"\"\n    pre: list[float] = []\n    cur: list[float] = []\n    for i in range(kernel_size):\n        cur = [1.0] * (i + 1)\n\n        for j in range(1, i // 2 + 1):\n            value = pre[j - 1] + pre[j]\n            cur[j] = value\n            if i != 2 * j:\n                cur[-j - 1] = value\n        pre = cur\n\n    out = torch.tensor(cur, device=device, dtype=dtype)\n\n    if norm:\n        out = out / out.sum()\n\n    return out\n</code></pre>"},{"location":"reference/careamics/models/model_factory/","title":"model_factory","text":"<p>Model creation factory functions.</p>"},{"location":"reference/careamics/models/model_factory/#careamics.models.model_factory.model_factory","title":"<code>model_factory(model_configuration)</code>","text":"<p>Deep learning model factory.</p> <p>Supported models are defined in careamics.config.SupportedArchitecture.</p> <p>Parameters:</p> Name Type Description Default <code>model_configuration</code> <code>Union[UNetModel, VAEModel]</code> <p>Model configuration.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>Model class.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the requested architecture is not implemented.</p> Source code in <code>src/careamics/models/model_factory.py</code> <pre><code>def model_factory(\n    model_configuration: Union[UNetModel, LVAEModel],\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Deep learning model factory.\n\n    Supported models are defined in careamics.config.SupportedArchitecture.\n\n    Parameters\n    ----------\n    model_configuration : Union[UNetModel, VAEModel]\n        Model configuration.\n\n    Returns\n    -------\n    torch.nn.Module\n        Model class.\n\n    Raises\n    ------\n    NotImplementedError\n        If the requested architecture is not implemented.\n    \"\"\"\n    if model_configuration.architecture == SupportedArchitecture.UNET:\n        return UNet(**model_configuration.model_dump())\n    elif model_configuration.architecture == SupportedArchitecture.LVAE:\n        return LVAE(**model_configuration.model_dump())\n    else:\n        raise NotImplementedError(\n            f\"Model {model_configuration.architecture} is not implemented or unknown.\"\n        )\n</code></pre>"},{"location":"reference/careamics/models/unet/","title":"unet","text":"<p>UNet model.</p> <p>A UNet encoder, decoder and complete model.</p>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNet model.</p> <p>Adapted for PyTorch from: https://github.com/juglab/n2v/blob/main/n2v/nets/unet_blocks.py.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dims</code> <code>int</code> <p>Number of dimensions of the convolution layers (2 or 3).</p> required <code>num_classes</code> <code>int</code> <p>Number of classes to predict, by default 1.</p> <code>1</code> <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of downsamplings, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of filters in the first convolution layer, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size of the pooling layers, by default 2.</p> <code>2</code> <code>final_activation</code> <code>Optional[Callable]</code> <p>Activation function to use for the last layer, by default None.</p> <code>NONE</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train the channels independently, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UNet(nn.Module):\n    \"\"\"\n    UNet model.\n\n    Adapted for PyTorch from:\n    https://github.com/juglab/n2v/blob/main/n2v/nets/unet_blocks.py.\n\n    Parameters\n    ----------\n    conv_dims : int\n        Number of dimensions of the convolution layers (2 or 3).\n    num_classes : int, optional\n        Number of classes to predict, by default 1.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of downsamplings, by default 3.\n    num_channels_init : int, optional\n        Number of filters in the first convolution layer, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size of the pooling layers, by default 2.\n    final_activation : Optional[Callable], optional\n        Activation function to use for the last layer, by default None.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    independent_channels : bool\n        Whether to train the channels independently, by default True.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dims: int,\n        num_classes: int = 1,\n        in_channels: int = 1,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        pool_kernel: int = 2,\n        final_activation: Union[SupportedActivation, str] = SupportedActivation.NONE,\n        n2v2: bool = False,\n        independent_channels: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dims : int\n            Number of dimensions of the convolution layers (2 or 3).\n        num_classes : int, optional\n            Number of classes to predict, by default 1.\n        in_channels : int, optional\n            Number of input channels, by default 1.\n        depth : int, optional\n            Number of downsamplings, by default 3.\n        num_channels_init : int, optional\n            Number of filters in the first convolution layer, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        pool_kernel : int, optional\n            Kernel size of the pooling layers, by default 2.\n        final_activation : Optional[Callable], optional\n            Activation function to use for the last layer, by default None.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        independent_channels : bool\n            Whether to train parallel independent networks for each channel, by\n            default True.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n        \"\"\"\n        super().__init__()\n\n        groups = in_channels if independent_channels else 1\n\n        self.encoder = UnetEncoder(\n            conv_dims,\n            in_channels=in_channels,\n            depth=depth,\n            num_channels_init=num_channels_init,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            pool_kernel=pool_kernel,\n            n2v2=n2v2,\n            groups=groups,\n        )\n\n        self.decoder = UnetDecoder(\n            conv_dims,\n            depth=depth,\n            num_channels_init=num_channels_init,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            n2v2=n2v2,\n            groups=groups,\n        )\n        self.final_conv = getattr(nn, f\"Conv{conv_dims}d\")(\n            in_channels=num_channels_init * groups,\n            out_channels=num_classes,\n            kernel_size=1,\n            groups=groups,\n        )\n        self.final_activation = get_activation(final_activation)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x :  torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output of the model.\n        \"\"\"\n        encoder_features = self.encoder(x)\n        x = self.decoder(*encoder_features)\n        x = self.final_conv(x)\n        x = self.final_activation(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet.__init__","title":"<code>__init__(conv_dims, num_classes=1, in_channels=1, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, pool_kernel=2, final_activation=SupportedActivation.NONE, n2v2=False, independent_channels=True, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dims</code> <code>int</code> <p>Number of dimensions of the convolution layers (2 or 3).</p> required <code>num_classes</code> <code>int</code> <p>Number of classes to predict, by default 1.</p> <code>1</code> <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of downsamplings, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of filters in the first convolution layer, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size of the pooling layers, by default 2.</p> <code>2</code> <code>final_activation</code> <code>Optional[Callable]</code> <p>Activation function to use for the last layer, by default None.</p> <code>NONE</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train parallel independent networks for each channel, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dims: int,\n    num_classes: int = 1,\n    in_channels: int = 1,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    pool_kernel: int = 2,\n    final_activation: Union[SupportedActivation, str] = SupportedActivation.NONE,\n    n2v2: bool = False,\n    independent_channels: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dims : int\n        Number of dimensions of the convolution layers (2 or 3).\n    num_classes : int, optional\n        Number of classes to predict, by default 1.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of downsamplings, by default 3.\n    num_channels_init : int, optional\n        Number of filters in the first convolution layer, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size of the pooling layers, by default 2.\n    final_activation : Optional[Callable], optional\n        Activation function to use for the last layer, by default None.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    independent_channels : bool\n        Whether to train parallel independent networks for each channel, by\n        default True.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n    super().__init__()\n\n    groups = in_channels if independent_channels else 1\n\n    self.encoder = UnetEncoder(\n        conv_dims,\n        in_channels=in_channels,\n        depth=depth,\n        num_channels_init=num_channels_init,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        pool_kernel=pool_kernel,\n        n2v2=n2v2,\n        groups=groups,\n    )\n\n    self.decoder = UnetDecoder(\n        conv_dims,\n        depth=depth,\n        num_channels_init=num_channels_init,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        n2v2=n2v2,\n        groups=groups,\n    )\n    self.final_conv = getattr(nn, f\"Conv{conv_dims}d\")(\n        in_channels=num_channels_init * groups,\n        out_channels=num_classes,\n        kernel_size=1,\n        groups=groups,\n    )\n    self.final_activation = get_activation(final_activation)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code> torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the model.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x :  torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output of the model.\n    \"\"\"\n    encoder_features = self.encoder(x)\n    x = self.decoder(*encoder_features)\n    x = self.final_conv(x)\n    x = self.final_activation(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder","title":"<code>UnetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unet decoder pathway.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>depth</code> <code>int</code> <p>Number of decoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UnetDecoder(nn.Module):\n    \"\"\"\n    Unet decoder pathway.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    depth : int, optional\n        Number of decoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        n2v2: bool = False,\n        groups: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n        depth : int, optional\n            Number of decoder blocks, by default 3.\n        num_channels_init : int, optional\n            Number of channels in the first encoder block, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        groups : int, optional\n            Number of blocked connections from input channels to output\n            channels, by default 1.\n        \"\"\"\n        super().__init__()\n\n        upsampling = nn.Upsample(\n            scale_factor=2, mode=\"bilinear\" if conv_dim == 2 else \"trilinear\"\n        )\n        in_channels = out_channels = num_channels_init * groups * (2 ** (depth - 1))\n\n        self.n2v2 = n2v2\n        self.groups = groups\n\n        self.bottleneck = Conv_Block(\n            conv_dim,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            intermediate_channel_multiplier=2,\n            use_batch_norm=use_batch_norm,\n            dropout_perc=dropout,\n            groups=self.groups,\n        )\n\n        decoder_blocks: list[nn.Module] = []\n        for n in range(depth):\n            decoder_blocks.append(upsampling)\n\n            in_channels = (num_channels_init * 2 ** (depth - n - 1)) * groups\n            # final decoder block has the same number in and out features\n            out_channels = in_channels // 2 if n != depth - 1 else in_channels\n            if not (n2v2 and (n == depth - 1)):\n                in_channels = in_channels * 2  # accounting for skip connection concat\n\n            decoder_blocks.append(\n                Conv_Block(\n                    conv_dim,\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    # TODO: Tensorflow n2v implementation has intermediate channel\n                    #   multiplication for skip_skipone=True but not skip_skipone=False\n                    #   this needs to be benchmarked.\n                    # final decoder block doesn't multiply the intermediate features\n                    intermediate_channel_multiplier=2 if n != depth - 1 else 1,\n                    dropout_perc=dropout,\n                    activation=\"ReLU\",\n                    use_batch_norm=use_batch_norm,\n                    groups=groups,\n                )\n            )\n\n        self.decoder_blocks = nn.ModuleList(decoder_blocks)\n\n    def forward(self, *features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        *features :  list[torch.Tensor]\n            List containing the output of each encoder block(skip connections) and final\n            output of the encoder.\n\n        Returns\n        -------\n        torch.Tensor\n            Output of the decoder.\n        \"\"\"\n        x: torch.Tensor = features[0]\n        skip_connections: tuple[torch.Tensor, ...] = features[-1:0:-1]\n        depth = len(skip_connections)\n\n        x = self.bottleneck(x)\n\n        for i, module in enumerate(self.decoder_blocks):\n            x = module(x)\n            if isinstance(module, nn.Upsample):\n                # divide index by 2 because of upsampling layers\n                skip_connection: torch.Tensor = skip_connections[i // 2]\n                # top level skip connection not added for n2v2\n                if (not self.n2v2) or (self.n2v2 and (i // 2 &lt; depth - 1)):\n                    x = self._interleave(x, skip_connection, self.groups)\n        return x\n\n    @staticmethod\n    def _interleave(A: torch.Tensor, B: torch.Tensor, groups: int) -&gt; torch.Tensor:\n        \"\"\"Interleave two tensors.\n\n        Splits the tensors `A` and `B` into equally sized groups along the channel\n        axis (axis=1); then concatenates the groups in alternating order along the\n        channel axis, starting with the first group from tensor A.\n\n        Parameters\n        ----------\n        A : torch.Tensor\n            First tensor.\n        B : torch.Tensor\n            Second tensor.\n        groups : int\n            The number of groups.\n\n        Returns\n        -------\n        torch.Tensor\n            Interleaved tensor.\n\n        Raises\n        ------\n        ValueError:\n            If either of `A` or `B`'s channel axis is not divisible by `groups`.\n        \"\"\"\n        if (A.shape[1] % groups != 0) or (B.shape[1] % groups != 0):\n            raise ValueError(f\"Number of channels not divisible by {groups} groups.\")\n\n        m = A.shape[1] // groups\n        n = B.shape[1] // groups\n\n        A_groups: list[torch.Tensor] = [\n            A[:, i * m : (i + 1) * m] for i in range(groups)\n        ]\n        B_groups: list[torch.Tensor] = [\n            B[:, i * n : (i + 1) * n] for i in range(groups)\n        ]\n\n        interleaved = torch.cat(\n            [\n                tensor_list[i]\n                for i in range(groups)\n                for tensor_list in [A_groups, B_groups]\n            ],\n            dim=1,\n        )\n\n        return interleaved\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder.__init__","title":"<code>__init__(conv_dim, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, n2v2=False, groups=1)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>depth</code> <code>int</code> <p>Number of decoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    n2v2: bool = False,\n    groups: int = 1,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    depth : int, optional\n        Number of decoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n    super().__init__()\n\n    upsampling = nn.Upsample(\n        scale_factor=2, mode=\"bilinear\" if conv_dim == 2 else \"trilinear\"\n    )\n    in_channels = out_channels = num_channels_init * groups * (2 ** (depth - 1))\n\n    self.n2v2 = n2v2\n    self.groups = groups\n\n    self.bottleneck = Conv_Block(\n        conv_dim,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        intermediate_channel_multiplier=2,\n        use_batch_norm=use_batch_norm,\n        dropout_perc=dropout,\n        groups=self.groups,\n    )\n\n    decoder_blocks: list[nn.Module] = []\n    for n in range(depth):\n        decoder_blocks.append(upsampling)\n\n        in_channels = (num_channels_init * 2 ** (depth - n - 1)) * groups\n        # final decoder block has the same number in and out features\n        out_channels = in_channels // 2 if n != depth - 1 else in_channels\n        if not (n2v2 and (n == depth - 1)):\n            in_channels = in_channels * 2  # accounting for skip connection concat\n\n        decoder_blocks.append(\n            Conv_Block(\n                conv_dim,\n                in_channels=in_channels,\n                out_channels=out_channels,\n                # TODO: Tensorflow n2v implementation has intermediate channel\n                #   multiplication for skip_skipone=True but not skip_skipone=False\n                #   this needs to be benchmarked.\n                # final decoder block doesn't multiply the intermediate features\n                intermediate_channel_multiplier=2 if n != depth - 1 else 1,\n                dropout_perc=dropout,\n                activation=\"ReLU\",\n                use_batch_norm=use_batch_norm,\n                groups=groups,\n            )\n        )\n\n    self.decoder_blocks = nn.ModuleList(decoder_blocks)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder.forward","title":"<code>forward(*features)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>*features</code> <code> list[torch.Tensor]</code> <p>List containing the output of each encoder block(skip connections) and final output of the encoder.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the decoder.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, *features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    *features :  list[torch.Tensor]\n        List containing the output of each encoder block(skip connections) and final\n        output of the encoder.\n\n    Returns\n    -------\n    torch.Tensor\n        Output of the decoder.\n    \"\"\"\n    x: torch.Tensor = features[0]\n    skip_connections: tuple[torch.Tensor, ...] = features[-1:0:-1]\n    depth = len(skip_connections)\n\n    x = self.bottleneck(x)\n\n    for i, module in enumerate(self.decoder_blocks):\n        x = module(x)\n        if isinstance(module, nn.Upsample):\n            # divide index by 2 because of upsampling layers\n            skip_connection: torch.Tensor = skip_connections[i // 2]\n            # top level skip connection not added for n2v2\n            if (not self.n2v2) or (self.n2v2 and (i // 2 &lt; depth - 1)):\n                x = self._interleave(x, skip_connection, self.groups)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder","title":"<code>UnetEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unet encoder pathway.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of encoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size for the max pooling layers, by default 2.</p> <code>2</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UnetEncoder(nn.Module):\n    \"\"\"\n    Unet encoder pathway.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of encoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size for the max pooling layers, by default 2.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        in_channels: int = 1,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        pool_kernel: int = 2,\n        n2v2: bool = False,\n        groups: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n        in_channels : int, optional\n            Number of input channels, by default 1.\n        depth : int, optional\n            Number of encoder blocks, by default 3.\n        num_channels_init : int, optional\n            Number of channels in the first encoder block, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        pool_kernel : int, optional\n            Kernel size for the max pooling layers, by default 2.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        groups : int, optional\n            Number of blocked connections from input channels to output\n            channels, by default 1.\n        \"\"\"\n        super().__init__()\n\n        self.pooling = (\n            getattr(nn, f\"MaxPool{conv_dim}d\")(kernel_size=pool_kernel)\n            if not n2v2\n            else MaxBlurPool(dim=conv_dim, kernel_size=3, max_pool_size=pool_kernel)\n        )\n\n        encoder_blocks = []\n\n        for n in range(depth):\n            out_channels = num_channels_init * (2**n) * groups\n            in_channels = in_channels if n == 0 else out_channels // 2\n            encoder_blocks.append(\n                Conv_Block(\n                    conv_dim,\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout_perc=dropout,\n                    use_batch_norm=use_batch_norm,\n                    groups=groups,\n                )\n            )\n            encoder_blocks.append(self.pooling)\n        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            Output of each encoder block (skip connections) and final output of the\n            encoder.\n        \"\"\"\n        encoder_features = []\n        for module in self.encoder_blocks:\n            x = module(x)\n            if isinstance(module, Conv_Block):\n                encoder_features.append(x)\n        features = [x, *encoder_features]\n        return features\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder.__init__","title":"<code>__init__(conv_dim, in_channels=1, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, pool_kernel=2, n2v2=False, groups=1)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of encoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size for the max pooling layers, by default 2.</p> <code>2</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    in_channels: int = 1,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    pool_kernel: int = 2,\n    n2v2: bool = False,\n    groups: int = 1,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of encoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size for the max pooling layers, by default 2.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n    super().__init__()\n\n    self.pooling = (\n        getattr(nn, f\"MaxPool{conv_dim}d\")(kernel_size=pool_kernel)\n        if not n2v2\n        else MaxBlurPool(dim=conv_dim, kernel_size=3, max_pool_size=pool_kernel)\n    )\n\n    encoder_blocks = []\n\n    for n in range(depth):\n        out_channels = num_channels_init * (2**n) * groups\n        in_channels = in_channels if n == 0 else out_channels // 2\n        encoder_blocks.append(\n            Conv_Block(\n                conv_dim,\n                in_channels=in_channels,\n                out_channels=out_channels,\n                dropout_perc=dropout,\n                use_batch_norm=use_batch_norm,\n                groups=groups,\n            )\n        )\n        encoder_blocks.append(self.pooling)\n    self.encoder_blocks = nn.ModuleList(encoder_blocks)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>Output of each encoder block (skip connections) and final output of the encoder.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        Output of each encoder block (skip connections) and final output of the\n        encoder.\n    \"\"\"\n    encoder_features = []\n    for module in self.encoder_blocks:\n        x = module(x)\n        if isinstance(module, Conv_Block):\n            encoder_features.append(x)\n    features = [x, *encoder_features]\n    return features\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/","title":"layers","text":"<p>Script containing the common basic blocks (nn.Module) reused by the LadderVAE.</p>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpDeterministicResBlock","title":"<code>BottomUpDeterministicResBlock</code>","text":"<p>               Bases: <code>ResBlockWithResampling</code></p> <p>Resnet block for bottom-up deterministic layers.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class BottomUpDeterministicResBlock(ResBlockWithResampling):\n    \"\"\"Resnet block for bottom-up deterministic layers.\"\"\"\n\n    def __init__(self, *args, downsample: bool = False, **kwargs):\n        kwargs[\"resample\"] = downsample\n        super().__init__(\"bottom-up\", *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer","title":"<code>BottomUpLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Bottom-up deterministic layer.</p> <p>It consists of one or a stack of <code>BottomUpDeterministicResBlock</code>'s. The outputs are the so-called <code>bu_values</code> that are later used in the Decoder to update the generative distributions.</p> <p>NOTE: When Lateral Contextualization is Enabled (i.e., <code>enable_multiscale=True</code>), the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB) (without downsampling), and then merged to the latent tensor produced by the primary flow of the <code>BottomUpLayer</code> through the <code>MergeLowRes</code> layer. It is meaningful to remark that the BUDB that takes care of encoding the low-res input can be either shared with the primary flow (and in that case it is the \"same_size\" BUDB (or stack of BUDBs) -&gt; see <code>self.net</code>), or can be a deep-copy of the primary flow's BUDB. This behaviour is controlled by <code>lowres_separate_branch</code> parameter.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class BottomUpLayer(nn.Module):\n    \"\"\"\n    Bottom-up deterministic layer.\n\n    It consists of one or a stack of `BottomUpDeterministicResBlock`'s.\n    The outputs are the so-called `bu_values` that are later used in the Decoder to update the\n    generative distributions.\n\n    NOTE: When Lateral Contextualization is Enabled (i.e., `enable_multiscale=True`),\n    the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB)\n    (without downsampling), and then merged to the latent tensor produced by the primary flow\n    of the `BottomUpLayer` through the `MergeLowRes` layer. It is meaningful to remark that\n    the BUDB that takes care of encoding the low-res input can be either shared with the\n    primary flow (and in that case it is the \"same_size\" BUDB (or stack of BUDBs) -&gt; see `self.net`),\n    or can be a deep-copy of the primary flow's BUDB.\n    This behaviour is controlled by `lowres_separate_branch` parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_res_blocks: int,\n        n_filters: int,\n        conv_strides: tuple[int] = (2, 2),\n        downsampling_steps: int = 0,\n        nonlin: Optional[Callable] = None,\n        batchnorm: bool = True,\n        dropout: Optional[float] = None,\n        res_block_type: Optional[str] = None,\n        res_block_kernel: Optional[int] = None,\n        gated: Optional[bool] = None,\n        enable_multiscale: bool = False,\n        multiscale_lowres_size_factor: Optional[int] = None,\n        lowres_separate_branch: bool = False,\n        multiscale_retain_spatial_dims: bool = False,\n        decoder_retain_spatial_dims: bool = False,\n        output_expected_shape: Optional[Iterable[int]] = None,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        n_res_blocks: int\n            Number of `BottomUpDeterministicResBlock` modules stacked in this layer.\n        n_filters: int\n            Number of channels present through out the layers of this block.\n        downsampling_steps: int, optional\n            Number of downsampling steps that has to be done in this layer (typically 1).\n            Default is 0.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `None`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        enable_multiscale: bool, optional\n            Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.\n        multiscale_lowres_size_factor: int, optional\n            A factor the expresses the relative size of the primary flow tensor with respect to the\n            lower-resolution lateral input tensor. Default in `None`.\n        lowres_separate_branch: bool, optional\n            Whether the residual block(s) encoding the low-res input should be shared (`False`) or\n            not (`True`) with the primary flow \"same-size\" residual block(s). Default is `False`.\n        multiscale_retain_spatial_dims: bool, optional\n            Whether to pad the latent tensor resulting from the bottom-up layer's primary flow\n            to match the size of the low-res input. Default is `False`.\n        decoder_retain_spatial_dims: bool, optional\n            Whether in the corresponding top-down layer the shape of tensor is retained between\n            input and output. Default is `False`.\n        output_expected_shape: Iterable[int], optional\n            The expected shape of the layer output (only used if `enable_multiscale == True`).\n            Default is `None`.\n        \"\"\"\n        super().__init__()\n\n        # Define attributes for Lateral Contextualization\n        self.enable_multiscale = enable_multiscale\n        self.lowres_separate_branch = lowres_separate_branch\n        self.multiscale_retain_spatial_dims = multiscale_retain_spatial_dims\n        self.multiscale_lowres_size_factor = multiscale_lowres_size_factor\n        self.decoder_retain_spatial_dims = decoder_retain_spatial_dims\n        self.output_expected_shape = output_expected_shape\n        assert self.output_expected_shape is None or self.enable_multiscale is True\n\n        bu_blocks_downsized = []\n        bu_blocks_samesize = []\n        for _ in range(n_res_blocks):\n            do_resample = False\n            if downsampling_steps &gt; 0:\n                do_resample = True\n                downsampling_steps -= 1\n            block = BottomUpDeterministicResBlock(\n                conv_strides=conv_strides,\n                c_in=n_filters,\n                c_out=n_filters,\n                nonlin=nonlin,\n                downsample=do_resample,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                gated=gated,\n            )\n            if do_resample:\n                bu_blocks_downsized.append(block)\n            else:\n                bu_blocks_samesize.append(block)\n\n        self.net_downsized = nn.Sequential(*bu_blocks_downsized)\n        self.net = nn.Sequential(*bu_blocks_samesize)\n\n        # Using the same net for the low resolution (and larger sized image)\n        self.lowres_net = self.lowres_merge = None\n        if self.enable_multiscale:\n            self._init_multiscale(\n                n_filters=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n            )\n\n        # msg = f'[{self.__class__.__name__}] McEnabled:{int(enable_multiscale)} '\n        # if enable_multiscale:\n        #     msg += f'McParallelBeam:{int(multiscale_retain_spatial_dims)} McFactor{multiscale_lowres_size_factor}'\n        # print(msg)\n\n    def _init_multiscale(\n        self,\n        nonlin: Callable = None,\n        n_filters: int = None,\n        conv_strides: tuple[int] = (2, 2),\n        batchnorm: bool = None,\n        dropout: float = None,\n        res_block_type: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Bottom-up layer's method that initializes the LC modules.\n\n        Defines the modules responsible of merging compressed lateral inputs to the\n        outputs of the primary flow at different hierarchical levels in the\n        multiresolution approach (LC). Specifically, the method initializes `lowres_net`\n        , which is a stack of `BottomUpDeterministicBlock`'s (w/out downsampling) that\n        takes care of additionally processing the low-res input, and `lowres_merge`,\n        which is the module responsible of merging the compressed lateral input to the\n        main flow.\n\n        NOTE: The merge modality is set by default to \"residual\", meaning that the\n        merge layer performs concatenation on dim=1, followed by 1x1 convolution and\n        a Residual Gated block.\n\n        Parameters\n        ----------\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `None`.\n        n_filters: int\n            Number of channels present through out the layers of this block.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        \"\"\"\n        self.lowres_net = self.net\n        if self.lowres_separate_branch:\n            self.lowres_net = deepcopy(self.net)\n\n        self.lowres_merge = MergeLowRes(\n            channels=n_filters,\n            conv_strides=conv_strides,\n            merge_type=\"residual\",\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n            multiscale_lowres_size_factor=self.multiscale_lowres_size_factor,\n        )\n\n    def forward(\n        self, x: torch.Tensor, lowres_x: Union[torch.Tensor, None] = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input of the `BottomUpLayer`, i.e., the input image or the output of the\n            previous layer.\n        lowres_x: torch.Tensor, optional\n            The low-res input used for Lateral Contextualization (LC). Default is `None`.\n\n        NOTE: first returned tensor is used as input for the next BU layer, while the second\n        tensor is the bu_value passed to the top-down layer.\n        \"\"\"\n        # The input is fed through the residual downsampling block(s)\n        primary_flow = self.net_downsized(x)\n        # The downsampling output is fed through additional residual block(s)\n        primary_flow = self.net(primary_flow)\n\n        # If LC is not used, simply return output of primary-flow\n        if self.enable_multiscale is False:\n            assert lowres_x is None\n            return primary_flow, primary_flow\n\n        if lowres_x is not None:\n            # First encode the low-res lateral input\n            lowres_flow = self.lowres_net(lowres_x)\n            # Then pass the result through the MergeLowRes layer\n            merged = self.lowres_merge(primary_flow, lowres_flow)\n        else:\n            merged = primary_flow\n\n        # NOTE: Explanation of possible cases for the conditionals:\n        # - if both are `True` -&gt; `merged` has the same spatial dims as the input (`x`) since\n        #   spatial dims are retained by padding `primary_flow` in `MergeLowRes`. This is\n        #   OK for the corresp TopDown layer, as it also retains spatial dims.\n        # - if both are `False` -&gt; `merged`'s spatial dims are equal to `self.net_downsized(x)`,\n        #   since no padding is done in `MergeLowRes` and, instead, the lowres input is cropped.\n        #   This is OK for the corresp TopDown layer, as it also halves the spatial dims.\n        # - if 1st is `False` and 2nd is `True` -&gt; not a concern, it cannot happen\n        #   (see lvae.py, line 111, intialization of `multiscale_decoder_retain_spatial_dims`).\n        if (\n            self.multiscale_retain_spatial_dims is False\n            or self.decoder_retain_spatial_dims is True\n        ):\n            return merged, merged\n\n        # NOTE: if we reach here, it means that `multiscale_retain_spatial_dims` is `True`,\n        # but `decoder_retain_spatial_dims` is `False`, meaning that merging LC preserves\n        # the spatial dimensions, but at the same time we don't want to retain the spatial\n        # dims in the corresponding top-down layer. Therefore, we need to crop the tensor.\n        if self.output_expected_shape is not None:\n            expected_shape = self.output_expected_shape\n        else:\n            fac = self.multiscale_lowres_size_factor\n            expected_shape = (merged.shape[-2] // fac, merged.shape[-1] // fac)\n            assert merged.shape[-2:] != expected_shape\n\n        # Crop the resulting tensor so that it matches with the Decoder\n        value_to_use_in_topdown = crop_img_tensor(merged, expected_shape)\n        return merged, value_to_use_in_topdown\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer.__init__","title":"<code>__init__(n_res_blocks, n_filters, conv_strides=(2, 2), downsampling_steps=0, nonlin=None, batchnorm=True, dropout=None, res_block_type=None, res_block_kernel=None, gated=None, enable_multiscale=False, multiscale_lowres_size_factor=None, lowres_separate_branch=False, multiscale_retain_spatial_dims=False, decoder_retain_spatial_dims=False, output_expected_shape=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n_res_blocks</code> <code>int</code> <p>Number of <code>BottomUpDeterministicResBlock</code> modules stacked in this layer.</p> required <code>n_filters</code> <code>int</code> <p>Number of channels present through out the layers of this block.</p> required <code>downsampling_steps</code> <code>int</code> <p>Number of downsampling steps that has to be done in this layer (typically 1). Default is 0.</p> <code>0</code> <code>nonlin</code> <code>Optional[Callable]</code> <p>The non-linearity function used in the block. Default is <code>None</code>.</p> <code>None</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Optional[float]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>res_block_type</code> <code>Optional[str]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Optional[int]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>Optional[bool]</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>enable_multiscale</code> <code>bool</code> <p>Whether to enable multiscale (Lateral Contextualization) or not. Default is <code>False</code>.</p> <code>False</code> <code>multiscale_lowres_size_factor</code> <code>Optional[int]</code> <p>A factor the expresses the relative size of the primary flow tensor with respect to the lower-resolution lateral input tensor. Default in <code>None</code>.</p> <code>None</code> <code>lowres_separate_branch</code> <code>bool</code> <p>Whether the residual block(s) encoding the low-res input should be shared (<code>False</code>) or not (<code>True</code>) with the primary flow \"same-size\" residual block(s). Default is <code>False</code>.</p> <code>False</code> <code>multiscale_retain_spatial_dims</code> <code>bool</code> <p>Whether to pad the latent tensor resulting from the bottom-up layer's primary flow to match the size of the low-res input. Default is <code>False</code>.</p> <code>False</code> <code>decoder_retain_spatial_dims</code> <code>bool</code> <p>Whether in the corresponding top-down layer the shape of tensor is retained between input and output. Default is <code>False</code>.</p> <code>False</code> <code>output_expected_shape</code> <code>Optional[Iterable[int]]</code> <p>The expected shape of the layer output (only used if <code>enable_multiscale == True</code>). Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    n_res_blocks: int,\n    n_filters: int,\n    conv_strides: tuple[int] = (2, 2),\n    downsampling_steps: int = 0,\n    nonlin: Optional[Callable] = None,\n    batchnorm: bool = True,\n    dropout: Optional[float] = None,\n    res_block_type: Optional[str] = None,\n    res_block_kernel: Optional[int] = None,\n    gated: Optional[bool] = None,\n    enable_multiscale: bool = False,\n    multiscale_lowres_size_factor: Optional[int] = None,\n    lowres_separate_branch: bool = False,\n    multiscale_retain_spatial_dims: bool = False,\n    decoder_retain_spatial_dims: bool = False,\n    output_expected_shape: Optional[Iterable[int]] = None,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    n_res_blocks: int\n        Number of `BottomUpDeterministicResBlock` modules stacked in this layer.\n    n_filters: int\n        Number of channels present through out the layers of this block.\n    downsampling_steps: int, optional\n        Number of downsampling steps that has to be done in this layer (typically 1).\n        Default is 0.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `None`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    enable_multiscale: bool, optional\n        Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.\n    multiscale_lowres_size_factor: int, optional\n        A factor the expresses the relative size of the primary flow tensor with respect to the\n        lower-resolution lateral input tensor. Default in `None`.\n    lowres_separate_branch: bool, optional\n        Whether the residual block(s) encoding the low-res input should be shared (`False`) or\n        not (`True`) with the primary flow \"same-size\" residual block(s). Default is `False`.\n    multiscale_retain_spatial_dims: bool, optional\n        Whether to pad the latent tensor resulting from the bottom-up layer's primary flow\n        to match the size of the low-res input. Default is `False`.\n    decoder_retain_spatial_dims: bool, optional\n        Whether in the corresponding top-down layer the shape of tensor is retained between\n        input and output. Default is `False`.\n    output_expected_shape: Iterable[int], optional\n        The expected shape of the layer output (only used if `enable_multiscale == True`).\n        Default is `None`.\n    \"\"\"\n    super().__init__()\n\n    # Define attributes for Lateral Contextualization\n    self.enable_multiscale = enable_multiscale\n    self.lowres_separate_branch = lowres_separate_branch\n    self.multiscale_retain_spatial_dims = multiscale_retain_spatial_dims\n    self.multiscale_lowres_size_factor = multiscale_lowres_size_factor\n    self.decoder_retain_spatial_dims = decoder_retain_spatial_dims\n    self.output_expected_shape = output_expected_shape\n    assert self.output_expected_shape is None or self.enable_multiscale is True\n\n    bu_blocks_downsized = []\n    bu_blocks_samesize = []\n    for _ in range(n_res_blocks):\n        do_resample = False\n        if downsampling_steps &gt; 0:\n            do_resample = True\n            downsampling_steps -= 1\n        block = BottomUpDeterministicResBlock(\n            conv_strides=conv_strides,\n            c_in=n_filters,\n            c_out=n_filters,\n            nonlin=nonlin,\n            downsample=do_resample,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            gated=gated,\n        )\n        if do_resample:\n            bu_blocks_downsized.append(block)\n        else:\n            bu_blocks_samesize.append(block)\n\n    self.net_downsized = nn.Sequential(*bu_blocks_downsized)\n    self.net = nn.Sequential(*bu_blocks_samesize)\n\n    # Using the same net for the low resolution (and larger sized image)\n    self.lowres_net = self.lowres_merge = None\n    if self.enable_multiscale:\n        self._init_multiscale(\n            n_filters=n_filters,\n            conv_strides=conv_strides,\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer.forward","title":"<code>forward(x, lowres_x=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input of the <code>BottomUpLayer</code>, i.e., the input image or the output of the previous layer.</p> required <code>lowres_x</code> <code>Union[Tensor, None]</code> <p>The low-res input used for Lateral Contextualization (LC). Default is <code>None</code>.</p> <code>None</code> <code>NOTE</code> required <code>tensor</code> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, lowres_x: Union[torch.Tensor, None] = None\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input of the `BottomUpLayer`, i.e., the input image or the output of the\n        previous layer.\n    lowres_x: torch.Tensor, optional\n        The low-res input used for Lateral Contextualization (LC). Default is `None`.\n\n    NOTE: first returned tensor is used as input for the next BU layer, while the second\n    tensor is the bu_value passed to the top-down layer.\n    \"\"\"\n    # The input is fed through the residual downsampling block(s)\n    primary_flow = self.net_downsized(x)\n    # The downsampling output is fed through additional residual block(s)\n    primary_flow = self.net(primary_flow)\n\n    # If LC is not used, simply return output of primary-flow\n    if self.enable_multiscale is False:\n        assert lowres_x is None\n        return primary_flow, primary_flow\n\n    if lowres_x is not None:\n        # First encode the low-res lateral input\n        lowres_flow = self.lowres_net(lowres_x)\n        # Then pass the result through the MergeLowRes layer\n        merged = self.lowres_merge(primary_flow, lowres_flow)\n    else:\n        merged = primary_flow\n\n    # NOTE: Explanation of possible cases for the conditionals:\n    # - if both are `True` -&gt; `merged` has the same spatial dims as the input (`x`) since\n    #   spatial dims are retained by padding `primary_flow` in `MergeLowRes`. This is\n    #   OK for the corresp TopDown layer, as it also retains spatial dims.\n    # - if both are `False` -&gt; `merged`'s spatial dims are equal to `self.net_downsized(x)`,\n    #   since no padding is done in `MergeLowRes` and, instead, the lowres input is cropped.\n    #   This is OK for the corresp TopDown layer, as it also halves the spatial dims.\n    # - if 1st is `False` and 2nd is `True` -&gt; not a concern, it cannot happen\n    #   (see lvae.py, line 111, intialization of `multiscale_decoder_retain_spatial_dims`).\n    if (\n        self.multiscale_retain_spatial_dims is False\n        or self.decoder_retain_spatial_dims is True\n    ):\n        return merged, merged\n\n    # NOTE: if we reach here, it means that `multiscale_retain_spatial_dims` is `True`,\n    # but `decoder_retain_spatial_dims` is `False`, meaning that merging LC preserves\n    # the spatial dimensions, but at the same time we don't want to retain the spatial\n    # dims in the corresponding top-down layer. Therefore, we need to crop the tensor.\n    if self.output_expected_shape is not None:\n        expected_shape = self.output_expected_shape\n    else:\n        fac = self.multiscale_lowres_size_factor\n        expected_shape = (merged.shape[-2] // fac, merged.shape[-1] // fac)\n        assert merged.shape[-2:] != expected_shape\n\n    # Crop the resulting tensor so that it matches with the Decoder\n    value_to_use_in_topdown = crop_img_tensor(merged, expected_shape)\n    return merged, value_to_use_in_topdown\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.GateLayer","title":"<code>GateLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Layer class that implements a gating mechanism.</p> <p>Double the number of channels through a convolutional layer, then use half the channels as gate for the other half.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class GateLayer(nn.Module):\n    \"\"\"\n    Layer class that implements a gating mechanism.\n\n    Double the number of channels through a convolutional layer, then use\n    half the channels as gate for the other half.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        conv_strides: tuple[int] = (2, 2),\n        kernel_size: int = 3,\n        nonlin: Callable = nn.LeakyReLU(),\n    ):\n        super().__init__()\n        assert kernel_size % 2 == 1\n        pad = kernel_size // 2\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        self.conv = conv_layer(channels, 2 * channels, kernel_size, padding=pad)\n        self.nonlin = nonlin\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output # TODO add shape\n        \"\"\"\n        x = self.conv(x)\n        x, gate = torch.chunk(x, 2, dim=1)\n        x = self.nonlin(x)  # TODO remove this?\n        gate = torch.sigmoid(gate)\n        return x * gate\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.GateLayer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output # TODO add shape\n    \"\"\"\n    x = self.conv(x)\n    x, gate = torch.chunk(x, 2, dim=1)\n    x = self.nonlin(x)  # TODO remove this?\n    gate = torch.sigmoid(gate)\n    return x * gate\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLayer","title":"<code>MergeLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Layer class that merges two or more input tensors.</p> <p>Merges two or more (B, C, [Z], Y, X) input tensors by concatenating them along dim=1 and passes the result through: a) a convolutional 1x1 layer (<code>merge_type == \"linear\"</code>), or b) a convolutional 1x1 layer and then a gated residual block (<code>merge_type == \"residual\"</code>), or c) a convolutional 1x1 layer and then an ungated residual block (<code>merge_type == \"residual_ungated\"</code>).</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class MergeLayer(nn.Module):\n    \"\"\"\n    Layer class that merges two or more input tensors.\n\n    Merges two or more (B, C, [Z], Y, X) input tensors by concatenating\n    them along dim=1 and passes the result through:\n    a) a convolutional 1x1 layer (`merge_type == \"linear\"`), or\n    b) a convolutional 1x1 layer and then a gated residual block (`merge_type == \"residual\"`), or\n    c) a convolutional 1x1 layer and then an ungated residual block (`merge_type == \"residual_ungated\"`).\n    \"\"\"\n\n    def __init__(\n        self,\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"],\n        channels: Union[int, Iterable[int]],\n        conv_strides: tuple[int] = (2, 2),\n        nonlin: Callable = nn.LeakyReLU(),\n        batchnorm: bool = True,\n        dropout: Optional[float] = None,\n        res_block_type: Optional[str] = None,\n        res_block_kernel: Optional[int] = None,\n        conv2d_bias: Optional[bool] = True,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n            The type of merge done in the layer. It can be chosen between \"linear\",\n            \"residual\", and \"residual_ungated\". Check the class docstring for more\n            information about the behaviour of different merge modalities.\n        channels: Union[int, Iterable[int]]\n            The number of channels used in the convolutional blocks of this layer.\n            If it is an `int`:\n                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n                - (Optional) ResBlock: in_channels=channels, out_channels=channels\n            If it is an Iterable (must have `len(channels)==3`):\n                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),\n                out_channels=channels[-1]\n                - (Optional) ResBlock: in_channels=channels[-1],\n                out_channels=channels[-1]\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared\n            kernel.\n            Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n        try:\n            iter(channels)\n        except TypeError:  # it is not iterable\n            channels = [channels] * 3\n        else:  # it is iterable\n            if len(channels) == 1:\n                channels = [channels[0]] * 3\n\n        self.conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n\n        if merge_type == \"linear\":\n            self.layer = self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, bias=conv2d_bias\n            )\n        elif merge_type == \"residual\":\n            self.layer = nn.Sequential(\n                self.conv_layer(\n                    sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n                ),\n                ResidualGatedBlock(\n                    conv_strides=conv_strides,\n                    channels=channels[-1],\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    block_type=res_block_type,\n                    kernel=res_block_kernel,\n                    conv2d_bias=conv2d_bias,\n                ),\n            )\n        elif merge_type == \"residual_ungated\":\n            self.layer = nn.Sequential(\n                self.conv_layer(\n                    sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n                ),\n                ResidualBlock(\n                    conv_strides=conv_strides,\n                    channels=channels[-1],\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    block_type=res_block_type,\n                    kernel=res_block_kernel,\n                    conv2d_bias=conv2d_bias,\n                ),\n            )\n\n    def forward(self, *args) -&gt; torch.Tensor:\n\n        # Concatenate the input tensors along dim=1\n        x = torch.cat(args, dim=1)\n\n        # Pass the concatenated tensor through the conv layer\n        x = self.layer(x)\n\n        return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLayer.__init__","title":"<code>__init__(merge_type, channels, conv_strides=(2, 2), nonlin=nn.LeakyReLU(), batchnorm=True, dropout=None, res_block_type=None, res_block_kernel=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>merge_type</code> <code>Literal['linear', 'residual', 'residual_ungated']</code> <p>The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\". Check the class docstring for more information about the behaviour of different merge modalities.</p> required <code>channels</code> <code>Union[int, Iterable[int]]</code> <p>The number of channels used in the convolutional blocks of this layer. If it is an <code>int</code>:     - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels     - (Optional) ResBlock: in_channels=channels, out_channels=channels If it is an Iterable (must have <code>len(channels)==3</code>):     - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),     out_channels=channels[-1]     - (Optional) ResBlock: in_channels=channels[-1],     out_channels=channels[-1]</p> required <code>conv_strides</code> <code>tuple[int]</code> <p>The strides used in the convolutions. Default is <code>(2, 2)</code>.</p> <code>(2, 2)</code> <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> <code>LeakyReLU()</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Optional[float]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>res_block_type</code> <code>Optional[str]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Optional[int]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>Optional[bool]</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"],\n    channels: Union[int, Iterable[int]],\n    conv_strides: tuple[int] = (2, 2),\n    nonlin: Callable = nn.LeakyReLU(),\n    batchnorm: bool = True,\n    dropout: Optional[float] = None,\n    res_block_type: Optional[str] = None,\n    res_block_kernel: Optional[int] = None,\n    conv2d_bias: Optional[bool] = True,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n        The type of merge done in the layer. It can be chosen between \"linear\",\n        \"residual\", and \"residual_ungated\". Check the class docstring for more\n        information about the behaviour of different merge modalities.\n    channels: Union[int, Iterable[int]]\n        The number of channels used in the convolutional blocks of this layer.\n        If it is an `int`:\n            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n            - (Optional) ResBlock: in_channels=channels, out_channels=channels\n        If it is an Iterable (must have `len(channels)==3`):\n            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),\n            out_channels=channels[-1]\n            - (Optional) ResBlock: in_channels=channels[-1],\n            out_channels=channels[-1]\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared\n        kernel.\n        Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n    try:\n        iter(channels)\n    except TypeError:  # it is not iterable\n        channels = [channels] * 3\n    else:  # it is iterable\n        if len(channels) == 1:\n            channels = [channels[0]] * 3\n\n    self.conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n\n    if merge_type == \"linear\":\n        self.layer = self.conv_layer(\n            sum(channels[:-1]), channels[-1], 1, bias=conv2d_bias\n        )\n    elif merge_type == \"residual\":\n        self.layer = nn.Sequential(\n            self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n            ),\n            ResidualGatedBlock(\n                conv_strides=conv_strides,\n                channels=channels[-1],\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                block_type=res_block_type,\n                kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            ),\n        )\n    elif merge_type == \"residual_ungated\":\n        self.layer = nn.Sequential(\n            self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n            ),\n            ResidualBlock(\n                conv_strides=conv_strides,\n                channels=channels[-1],\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                block_type=res_block_type,\n                kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            ),\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLowRes","title":"<code>MergeLowRes</code>","text":"<p>               Bases: <code>MergeLayer</code></p> <p>Child class of <code>MergeLayer</code>.</p> <p>Specifically designed to merge the low-resolution patches that are used in Lateral Contextualization approach.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class MergeLowRes(MergeLayer):\n    \"\"\"\n    Child class of `MergeLayer`.\n\n    Specifically designed to merge the low-resolution patches\n    that are used in Lateral Contextualization approach.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.retain_spatial_dims = kwargs.pop(\"multiscale_retain_spatial_dims\")\n        self.multiscale_lowres_size_factor = kwargs.pop(\"multiscale_lowres_size_factor\")\n        super().__init__(*args, **kwargs)\n\n    def forward(self, latent: torch.Tensor, lowres: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        latent: torch.Tensor\n            The output latent tensor from previous layer in the LVAE hierarchy.\n        lowres: torch.Tensor\n            The low-res patch image to be merged to increase the context.\n        \"\"\"\n        # TODO: treat (X, Y) and Z differently (e.g., line 762)\n        if self.retain_spatial_dims:\n            # Pad latent tensor to match lowres tensor's shape\n            # Output.shape == Lowres.shape (== Input.shape),\n            # where Input is the input to the BU layer\n            latent = pad_img_tensor(latent, lowres.shape[2:])\n        else:\n            # Crop lowres tensor to match latent tensor's shape\n            lz, ly, lx = lowres.shape[2:]\n            z = lz // self.multiscale_lowres_size_factor\n            y = ly // self.multiscale_lowres_size_factor\n            x = lx // self.multiscale_lowres_size_factor\n            z_pad = (lz - z) // 2\n            y_pad = (ly - y) // 2\n            x_pad = (lx - x) // 2\n            lowres = lowres[:, :, z_pad:-z_pad, y_pad:-y_pad, x_pad:-x_pad]\n\n        return super().forward(latent, lowres)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLowRes.forward","title":"<code>forward(latent, lowres)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>latent</code> <code>Tensor</code> <p>The output latent tensor from previous layer in the LVAE hierarchy.</p> required <code>lowres</code> <code>Tensor</code> <p>The low-res patch image to be merged to increase the context.</p> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, latent: torch.Tensor, lowres: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    latent: torch.Tensor\n        The output latent tensor from previous layer in the LVAE hierarchy.\n    lowres: torch.Tensor\n        The low-res patch image to be merged to increase the context.\n    \"\"\"\n    # TODO: treat (X, Y) and Z differently (e.g., line 762)\n    if self.retain_spatial_dims:\n        # Pad latent tensor to match lowres tensor's shape\n        # Output.shape == Lowres.shape (== Input.shape),\n        # where Input is the input to the BU layer\n        latent = pad_img_tensor(latent, lowres.shape[2:])\n    else:\n        # Crop lowres tensor to match latent tensor's shape\n        lz, ly, lx = lowres.shape[2:]\n        z = lz // self.multiscale_lowres_size_factor\n        y = ly // self.multiscale_lowres_size_factor\n        x = lx // self.multiscale_lowres_size_factor\n        z_pad = (lz - z) // 2\n        y_pad = (ly - y) // 2\n        x_pad = (lx - x) // 2\n        lowres = lowres[:, :, z_pad:-z_pad, y_pad:-y_pad, x_pad:-x_pad]\n\n    return super().forward(latent, lowres)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling","title":"<code>ResBlockWithResampling</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual block with resampling.</p> <p>Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2). It is structured as follows:     1. <code>pre_conv</code>: a downsampling or upsampling strided convolutional layer in case of resampling, or         a 1x1 convolutional layer that maps the number of channels of the input to <code>inner_channels</code>.     2. <code>ResidualBlock</code>     3. <code>post_conv</code>: a 1x1 convolutional layer that maps the number of channels to <code>c_out</code>.</p> <p>Some implementation notes: - Resampling is performed through a strided convolution layer at the beginning of the block. - The strided convolution block has fixed kernel size of 3x3 and 1 layer of padding with zeros. - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers. - The number of internal channels is by default the same as the number of output channels, but   min_inner_channels can override the behaviour.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResBlockWithResampling(nn.Module):\n    \"\"\"\n    Residual block with resampling.\n\n    Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2).\n    It is structured as follows:\n        1. `pre_conv`: a downsampling or upsampling strided convolutional layer in case of resampling, or\n            a 1x1 convolutional layer that maps the number of channels of the input to `inner_channels`.\n        2. `ResidualBlock`\n        3. `post_conv`: a 1x1 convolutional layer that maps the number of channels to `c_out`.\n\n    Some implementation notes:\n    - Resampling is performed through a strided convolution layer at the beginning of the block.\n    - The strided convolution block has fixed kernel size of 3x3 and 1 layer of padding with zeros.\n    - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers.\n    - The number of internal channels is by default the same as the number of output channels, but\n      min_inner_channels can override the behaviour.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: Literal[\"top-down\", \"bottom-up\"],\n        c_in: int,\n        c_out: int,\n        conv_strides: tuple[int],\n        min_inner_channels: Union[int, None] = None,\n        nonlin: Callable = nn.LeakyReLU(),\n        resample: bool = False,\n        res_block_kernel: Optional[Union[int, Iterable[int]]] = None,\n        groups: int = 1,\n        batchnorm: bool = True,\n        res_block_type: Union[str, None] = None,\n        dropout: Union[float, None] = None,\n        gated: Union[bool, None] = None,\n        conv2d_bias: bool = True,\n        # lowres_input: bool = False,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        mode: Literal[\"top-down\", \"bottom-up\"]\n            The type of resampling performed in the initial strided convolution of the block.\n            If \"bottom-up\" downsampling of a factor 2 is done.\n            If \"top-down\" upsampling of a factor 2 is done.\n        c_in: int\n            The number of input channels.\n        c_out: int\n            The number of output channels.\n        min_inner_channels: int, optional\n            The number of channels used in the inner layer of this module.\n            Default is `None`, meaning that the number of inner channels is set to `c_out`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        resample: bool, optional\n            Whether to perform resampling in the first convolutional layer.\n            If `False`, the first convolutional layer just maps the input to a tensor with\n            `inner_channels` channels through 1x1 convolution. Default is `False`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n        assert mode in [\"top-down\", \"bottom-up\"]\n\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        transp_conv_layer: ConvType = getattr(nn, f\"ConvTranspose{len(conv_strides)}d\")\n\n        if min_inner_channels is None:\n            min_inner_channels = 0\n        # inner_channels is the number of channels used in the inner layers\n        # of ResBlockWithResampling\n        inner_channels = max(c_out, min_inner_channels)\n\n        # Define first conv layer to change num channels and/or up/downsample\n        if resample:\n            if mode == \"bottom-up\":  # downsample\n                self.pre_conv = conv_layer(\n                    in_channels=c_in,\n                    out_channels=inner_channels,\n                    kernel_size=3,\n                    padding=1,\n                    stride=conv_strides,\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n            elif mode == \"top-down\":  # upsample\n                self.pre_conv = transp_conv_layer(\n                    in_channels=c_in,\n                    kernel_size=3,\n                    out_channels=inner_channels,\n                    padding=1,  # TODO maybe don't hardcode this?\n                    stride=conv_strides,\n                    groups=groups,\n                    output_padding=1 if len(conv_strides) == 2 else (0, 1, 1),\n                    bias=conv2d_bias,\n                )\n        elif c_in != inner_channels:\n            self.pre_conv = conv_layer(\n                c_in, inner_channels, 1, groups=groups, bias=conv2d_bias\n            )\n        else:\n            self.pre_conv = None\n\n        # Residual block\n        self.res = ResidualBlock(\n            channels=inner_channels,\n            conv_strides=conv_strides,\n            nonlin=nonlin,\n            kernel=res_block_kernel,\n            groups=groups,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            gated=gated,\n            block_type=res_block_type,\n            conv2d_bias=conv2d_bias,\n        )\n\n        # Define last conv layer to get correct num output channels\n        if inner_channels != c_out:\n            self.post_conv = conv_layer(\n                inner_channels, c_out, 1, groups=groups, bias=conv2d_bias\n            )\n        else:\n            self.post_conv = None\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output # TODO add shape\n        \"\"\"\n        if self.pre_conv is not None:\n            x = self.pre_conv(x)\n\n        x = self.res(x)\n\n        if self.post_conv is not None:\n            x = self.post_conv(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling.__init__","title":"<code>__init__(mode, c_in, c_out, conv_strides, min_inner_channels=None, nonlin=nn.LeakyReLU(), resample=False, res_block_kernel=None, groups=1, batchnorm=True, res_block_type=None, dropout=None, gated=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['top-down', 'bottom-up']</code> <p>The type of resampling performed in the initial strided convolution of the block. If \"bottom-up\" downsampling of a factor 2 is done. If \"top-down\" upsampling of a factor 2 is done.</p> required <code>c_in</code> <code>int</code> <p>The number of input channels.</p> required <code>c_out</code> <code>int</code> <p>The number of output channels.</p> required <code>min_inner_channels</code> <code>Union[int, None]</code> <p>The number of channels used in the inner layer of this module. Default is <code>None</code>, meaning that the number of inner channels is set to <code>c_out</code>.</p> <code>None</code> <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> <code>LeakyReLU()</code> <code>resample</code> <code>bool</code> <p>Whether to perform resampling in the first convolutional layer. If <code>False</code>, the first convolutional layer just maps the input to a tensor with <code>inner_channels</code> channels through 1x1 convolution. Default is <code>False</code>.</p> <code>False</code> <code>res_block_kernel</code> <code>Optional[Union[int, Iterable[int]]]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>res_block_type</code> <code>Union[str, None]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>dropout</code> <code>Union[float, None]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>Union[bool, None]</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    mode: Literal[\"top-down\", \"bottom-up\"],\n    c_in: int,\n    c_out: int,\n    conv_strides: tuple[int],\n    min_inner_channels: Union[int, None] = None,\n    nonlin: Callable = nn.LeakyReLU(),\n    resample: bool = False,\n    res_block_kernel: Optional[Union[int, Iterable[int]]] = None,\n    groups: int = 1,\n    batchnorm: bool = True,\n    res_block_type: Union[str, None] = None,\n    dropout: Union[float, None] = None,\n    gated: Union[bool, None] = None,\n    conv2d_bias: bool = True,\n    # lowres_input: bool = False,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    mode: Literal[\"top-down\", \"bottom-up\"]\n        The type of resampling performed in the initial strided convolution of the block.\n        If \"bottom-up\" downsampling of a factor 2 is done.\n        If \"top-down\" upsampling of a factor 2 is done.\n    c_in: int\n        The number of input channels.\n    c_out: int\n        The number of output channels.\n    min_inner_channels: int, optional\n        The number of channels used in the inner layer of this module.\n        Default is `None`, meaning that the number of inner channels is set to `c_out`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    resample: bool, optional\n        Whether to perform resampling in the first convolutional layer.\n        If `False`, the first convolutional layer just maps the input to a tensor with\n        `inner_channels` channels through 1x1 convolution. Default is `False`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n    assert mode in [\"top-down\", \"bottom-up\"]\n\n    conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n    transp_conv_layer: ConvType = getattr(nn, f\"ConvTranspose{len(conv_strides)}d\")\n\n    if min_inner_channels is None:\n        min_inner_channels = 0\n    # inner_channels is the number of channels used in the inner layers\n    # of ResBlockWithResampling\n    inner_channels = max(c_out, min_inner_channels)\n\n    # Define first conv layer to change num channels and/or up/downsample\n    if resample:\n        if mode == \"bottom-up\":  # downsample\n            self.pre_conv = conv_layer(\n                in_channels=c_in,\n                out_channels=inner_channels,\n                kernel_size=3,\n                padding=1,\n                stride=conv_strides,\n                groups=groups,\n                bias=conv2d_bias,\n            )\n        elif mode == \"top-down\":  # upsample\n            self.pre_conv = transp_conv_layer(\n                in_channels=c_in,\n                kernel_size=3,\n                out_channels=inner_channels,\n                padding=1,  # TODO maybe don't hardcode this?\n                stride=conv_strides,\n                groups=groups,\n                output_padding=1 if len(conv_strides) == 2 else (0, 1, 1),\n                bias=conv2d_bias,\n            )\n    elif c_in != inner_channels:\n        self.pre_conv = conv_layer(\n            c_in, inner_channels, 1, groups=groups, bias=conv2d_bias\n        )\n    else:\n        self.pre_conv = None\n\n    # Residual block\n    self.res = ResidualBlock(\n        channels=inner_channels,\n        conv_strides=conv_strides,\n        nonlin=nonlin,\n        kernel=res_block_kernel,\n        groups=groups,\n        batchnorm=batchnorm,\n        dropout=dropout,\n        gated=gated,\n        block_type=res_block_type,\n        conv2d_bias=conv2d_bias,\n    )\n\n    # Define last conv layer to get correct num output channels\n    if inner_channels != c_out:\n        self.post_conv = conv_layer(\n            inner_channels, c_out, 1, groups=groups, bias=conv2d_bias\n        )\n    else:\n        self.post_conv = None\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output # TODO add shape\n    \"\"\"\n    if self.pre_conv is not None:\n        x = self.pre_conv(x)\n\n    x = self.res(x)\n\n    if self.post_conv is not None:\n        x = self.post_conv(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock","title":"<code>ResidualBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual block with 2 convolutional layers.</p> <p>Some architectural notes:     - The number of input, intermediate, and output channels is the same,     - Padding is always 'same',     - The 2 convolutional layers have the same groups,     - No stride allowed,     - Kernel sizes must be odd.</p> <p>The output isgiven by: <code>out = gate(f(x)) + x</code>. The presence of the gating mechanism is optional, and f(x) has different structures depending on the <code>block_type</code> argument. Specifically, <code>block_type</code> is a string specifying the block's structure, with:     a = activation     b = batch norm     c = conv layer     d = dropout. For example, \"bacdbacd\" defines a block with 2x[batchnorm, activation, conv, dropout].</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResidualBlock(nn.Module):\n    \"\"\"\n    Residual block with 2 convolutional layers.\n\n    Some architectural notes:\n        - The number of input, intermediate, and output channels is the same,\n        - Padding is always 'same',\n        - The 2 convolutional layers have the same groups,\n        - No stride allowed,\n        - Kernel sizes must be odd.\n\n    The output isgiven by: `out = gate(f(x)) + x`.\n    The presence of the gating mechanism is optional, and f(x) has different\n    structures depending on the `block_type` argument.\n    Specifically, `block_type` is a string specifying the block's structure, with:\n        a = activation\n        b = batch norm\n        c = conv layer\n        d = dropout.\n    For example, \"bacdbacd\" defines a block with 2x[batchnorm, activation, conv, dropout].\n    \"\"\"\n\n    default_kernel_size = (3, 3)\n\n    def __init__(\n        self,\n        channels: int,\n        nonlin: Callable,\n        conv_strides: tuple[int] = (2, 2),\n        kernel: Union[int, Iterable[int], None] = None,\n        groups: int = 1,\n        batchnorm: bool = True,\n        block_type: str = None,\n        dropout: float = None,\n        gated: bool = None,\n        conv2d_bias: bool = True,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        channels: int\n            The number of input and output channels (they are the same).\n        nonlin: Callable\n            The non-linearity function used in the block (e.g., `nn.ReLU`).\n        kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        block_type: str, optional\n            A string specifying the block structure, check class docstring for more info.\n            Default is `None`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n\n        # Set kernel size &amp; padding\n        if kernel is None:\n            kernel = self.default_kernel_size\n        elif isinstance(kernel, int):\n            kernel = (kernel, kernel)\n        elif len(kernel) != 2:\n            raise ValueError(\"kernel has to be None, int, or an iterable of length 2\")\n        assert all(k % 2 == 1 for k in kernel), \"kernel sizes have to be odd\"\n        kernel = list(kernel)\n\n        # Define modules\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        norm_layer: NormType = getattr(nn, f\"BatchNorm{len(conv_strides)}d\")\n        dropout_layer: DropoutType = getattr(nn, f\"Dropout{len(conv_strides)}d\")\n        # TODO: same comment as in lvae.py, would be more readable to have `conv_dims`\n\n        modules = []\n        if block_type == \"cabdcabd\":\n            for i in range(2):\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                modules.append(nonlin)\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                if dropout is not None:\n                    modules.append(dropout_layer(dropout))\n        elif block_type == \"bacdbac\":\n            for i in range(2):\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                modules.append(nonlin)\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                if dropout is not None and i == 0:\n                    modules.append(dropout_layer(dropout))\n        elif block_type == \"bacdbacd\":\n            for i in range(2):\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                modules.append(nonlin)\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                modules.append(dropout_layer(dropout))\n\n        else:\n            raise ValueError(f\"unrecognized block type '{block_type}'\")\n\n        self.gated = gated\n        if gated:\n            modules.append(\n                GateLayer(\n                    channels=channels,\n                    conv_strides=conv_strides,\n                    kernel_size=1,\n                    nonlin=nonlin,\n                )\n            )\n\n        self.block = nn.Sequential(*modules)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input tensor # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output tensor # TODO add shape\n        \"\"\"\n        out = self.block(x)\n        assert (\n            out.shape == x.shape\n        ), f\"output shape: {out.shape} != input shape: {x.shape}\"\n        return out + x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock.__init__","title":"<code>__init__(channels, nonlin, conv_strides=(2, 2), kernel=None, groups=1, batchnorm=True, block_type=None, dropout=None, gated=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>The number of input and output channels (they are the same).</p> required <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>).</p> required <code>kernel</code> <code>Union[int, Iterable[int], None]</code> <p>The kernel size used in the convolutions of the block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>block_type</code> <code>str</code> <p>A string specifying the block structure, check class docstring for more info. Default is <code>None</code>.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>bool</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    channels: int,\n    nonlin: Callable,\n    conv_strides: tuple[int] = (2, 2),\n    kernel: Union[int, Iterable[int], None] = None,\n    groups: int = 1,\n    batchnorm: bool = True,\n    block_type: str = None,\n    dropout: float = None,\n    gated: bool = None,\n    conv2d_bias: bool = True,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    channels: int\n        The number of input and output channels (they are the same).\n    nonlin: Callable\n        The non-linearity function used in the block (e.g., `nn.ReLU`).\n    kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    block_type: str, optional\n        A string specifying the block structure, check class docstring for more info.\n        Default is `None`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n\n    # Set kernel size &amp; padding\n    if kernel is None:\n        kernel = self.default_kernel_size\n    elif isinstance(kernel, int):\n        kernel = (kernel, kernel)\n    elif len(kernel) != 2:\n        raise ValueError(\"kernel has to be None, int, or an iterable of length 2\")\n    assert all(k % 2 == 1 for k in kernel), \"kernel sizes have to be odd\"\n    kernel = list(kernel)\n\n    # Define modules\n    conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n    norm_layer: NormType = getattr(nn, f\"BatchNorm{len(conv_strides)}d\")\n    dropout_layer: DropoutType = getattr(nn, f\"Dropout{len(conv_strides)}d\")\n    # TODO: same comment as in lvae.py, would be more readable to have `conv_dims`\n\n    modules = []\n    if block_type == \"cabdcabd\":\n        for i in range(2):\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            modules.append(nonlin)\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            if dropout is not None:\n                modules.append(dropout_layer(dropout))\n    elif block_type == \"bacdbac\":\n        for i in range(2):\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            modules.append(nonlin)\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            if dropout is not None and i == 0:\n                modules.append(dropout_layer(dropout))\n    elif block_type == \"bacdbacd\":\n        for i in range(2):\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            modules.append(nonlin)\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            modules.append(dropout_layer(dropout))\n\n    else:\n        raise ValueError(f\"unrecognized block type '{block_type}'\")\n\n    self.gated = gated\n    if gated:\n        modules.append(\n            GateLayer(\n                channels=channels,\n                conv_strides=conv_strides,\n                kernel_size=1,\n                nonlin=nonlin,\n            )\n        )\n\n    self.block = nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input tensor # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor # TODO add shape\n    \"\"\"\n    out = self.block(x)\n    assert (\n        out.shape == x.shape\n    ), f\"output shape: {out.shape} != input shape: {x.shape}\"\n    return out + x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualGatedBlock","title":"<code>ResidualGatedBlock</code>","text":"<p>               Bases: <code>ResidualBlock</code></p> <p>Layer class that implements a residual block with a gating mechanism.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResidualGatedBlock(ResidualBlock):\n    \"\"\"Layer class that implements a residual block with a gating mechanism.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, gated=True)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.SkipConnectionMerger","title":"<code>SkipConnectionMerger</code>","text":"<p>               Bases: <code>MergeLayer</code></p> <p>Specialized <code>MergeLayer</code> module, handles skip connections in the model.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class SkipConnectionMerger(MergeLayer):\n    \"\"\"Specialized `MergeLayer` module, handles skip connections in the model.\"\"\"\n\n    def __init__(\n        self,\n        nonlin: Callable,\n        channels: Union[int, Iterable[int]],\n        batchnorm: bool,\n        dropout: float,\n        res_block_type: str,\n        conv_strides: tuple[int] = (2, 2),\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"] = \"residual\",\n        conv2d_bias: bool = True,\n        res_block_kernel: Optional[int] = None,\n    ):\n        \"\"\"\n        Constructor.\n\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        channels: Union[int, Iterable[int]]\n            The number of channels used in the convolutional blocks of this layer.\n            If it is an `int`:\n                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n                - (Optional) ResBlock: in_channels=channels, out_channels=channels\n            If it is an Iterable (must have `len(channels)==3`):\n                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]\n                - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]\n        batchnorm: bool\n            Whether to use batchnorm layers.\n        dropout: float\n            The dropout probability in dropout layers. If `None` dropout is not used.\n        res_block_type: str\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n            The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".\n            Check the class docstring for more information about the behaviour of different merge modalities.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        \"\"\"\n        super().__init__(\n            conv_strides=conv_strides,\n            channels=channels,\n            nonlin=nonlin,\n            merge_type=merge_type,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            conv2d_bias=conv2d_bias,\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.SkipConnectionMerger.__init__","title":"<code>__init__(nonlin, channels, batchnorm, dropout, res_block_type, conv_strides=(2, 2), merge_type='residual', conv2d_bias=True, res_block_kernel=None)</code>","text":"<p>Constructor.</p> <p>nonlin: Callable, optional     The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>. channels: Union[int, Iterable[int]]     The number of channels used in the convolutional blocks of this layer.     If it is an <code>int</code>:         - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels         - (Optional) ResBlock: in_channels=channels, out_channels=channels     If it is an Iterable (must have <code>len(channels)==3</code>):         - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]         - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1] batchnorm: bool     Whether to use batchnorm layers. dropout: float     The dropout probability in dropout layers. If <code>None</code> dropout is not used. res_block_type: str     A string specifying the structure of residual block.     Check <code>ResidualBlock</code> doscstring for more information. conv_strides: tuple, optional     The strides used in the convolutions. Default is <code>(2, 2)</code>. merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]     The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".     Check the class docstring for more information about the behaviour of different merge modalities. conv2d_bias: bool, optional     Whether to use bias term in convolutions. Default is <code>True</code>. res_block_kernel: Union[int, Iterable[int]], optional     The kernel size used in the convolutions of the residual block.     It can be either a single integer or a pair of integers defining the squared kernel.     Default is <code>None</code>.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    nonlin: Callable,\n    channels: Union[int, Iterable[int]],\n    batchnorm: bool,\n    dropout: float,\n    res_block_type: str,\n    conv_strides: tuple[int] = (2, 2),\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"] = \"residual\",\n    conv2d_bias: bool = True,\n    res_block_kernel: Optional[int] = None,\n):\n    \"\"\"\n    Constructor.\n\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    channels: Union[int, Iterable[int]]\n        The number of channels used in the convolutional blocks of this layer.\n        If it is an `int`:\n            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n            - (Optional) ResBlock: in_channels=channels, out_channels=channels\n        If it is an Iterable (must have `len(channels)==3`):\n            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]\n            - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]\n    batchnorm: bool\n        Whether to use batchnorm layers.\n    dropout: float\n        The dropout probability in dropout layers. If `None` dropout is not used.\n    res_block_type: str\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n        The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".\n        Check the class docstring for more information about the behaviour of different merge modalities.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    \"\"\"\n    super().__init__(\n        conv_strides=conv_strides,\n        channels=channels,\n        nonlin=nonlin,\n        merge_type=merge_type,\n        batchnorm=batchnorm,\n        dropout=dropout,\n        res_block_type=res_block_type,\n        res_block_kernel=res_block_kernel,\n        conv2d_bias=conv2d_bias,\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownDeterministicResBlock","title":"<code>TopDownDeterministicResBlock</code>","text":"<p>               Bases: <code>ResBlockWithResampling</code></p> <p>Resnet block for top-down deterministic layers.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class TopDownDeterministicResBlock(ResBlockWithResampling):\n    \"\"\"Resnet block for top-down deterministic layers.\"\"\"\n\n    def __init__(self, *args, upsample: bool = False, **kwargs):\n        kwargs[\"resample\"] = upsample\n        super().__init__(\"top-down\", *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer","title":"<code>TopDownLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Top-down inference layer.</p> <p>It includes:     - Stochastic sampling,     - Computation of KL divergence,     - A small deterministic ResNet that performs upsampling.</p> <p>NOTE 1:     The algorithm for generative inference approximately works as follows:         - p_params = output of top-down layer above         - bu = inferred bottom-up value at this layer         - q_params = merge(bu, p_params)         - z = stochastic_layer(q_params)         - (optional) get and merge skip connection from prev top-down layer         - top-down deterministic ResNet</p> <p>NOTE 2:     The Top-Down layer can work in two modes: inference and prediction/generative.     Depending on the particular mode, it follows distinct behaviours:     - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path,     by merging outcomes of bottom-up and top-down passes. The exception is the top layer,     in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer.     - On the contrary in predicition/generative mode, parameters of q(z_i|z_i+1) can be obtained     once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is     possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).</p> <p>NOTE 3:     When doing unconditional generation, bu_value is not available. Hence the     merge layer is not used, and z is sampled directly from p_params.</p> <p>NOTE 4:     If this is the top layer, at inference time, the uppermost bottom-up value     is used directly as q_params, and p_params are defined in this layer     (while they are usually taken from the previous layer), and can be learned.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class TopDownLayer(nn.Module):\n    \"\"\"Top-down inference layer.\n\n    It includes:\n        - Stochastic sampling,\n        - Computation of KL divergence,\n        - A small deterministic ResNet that performs upsampling.\n\n    NOTE 1:\n        The algorithm for generative inference approximately works as follows:\n            - p_params = output of top-down layer above\n            - bu = inferred bottom-up value at this layer\n            - q_params = merge(bu, p_params)\n            - z = stochastic_layer(q_params)\n            - (optional) get and merge skip connection from prev top-down layer\n            - top-down deterministic ResNet\n\n    NOTE 2:\n        The Top-Down layer can work in two modes: inference and prediction/generative.\n        Depending on the particular mode, it follows distinct behaviours:\n        - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path,\n        by merging outcomes of bottom-up and top-down passes. The exception is the top layer,\n        in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer.\n        - On the contrary in predicition/generative mode, parameters of q(z_i|z_i+1) can be obtained\n        once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is\n        possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).\n\n    NOTE 3:\n        When doing unconditional generation, bu_value is not available. Hence the\n        merge layer is not used, and z is sampled directly from p_params.\n\n    NOTE 4:\n        If this is the top layer, at inference time, the uppermost bottom-up value\n        is used directly as q_params, and p_params are defined in this layer\n        (while they are usually taken from the previous layer), and can be learned.\n    \"\"\"\n\n    def __init__(\n        self,\n        z_dim: int,\n        n_res_blocks: int,\n        n_filters: int,\n        conv_strides: tuple[int],\n        is_top_layer: bool = False,\n        upsampling_steps: Union[int, None] = None,\n        nonlin: Union[Callable, None] = None,\n        merge_type: Union[\n            Literal[\"linear\", \"residual\", \"residual_ungated\"], None\n        ] = None,\n        batchnorm: bool = True,\n        dropout: Union[float, None] = None,\n        stochastic_skip: bool = False,\n        res_block_type: Union[str, None] = None,\n        res_block_kernel: Union[int, None] = None,\n        groups: int = 1,\n        gated: Union[bool, None] = None,\n        learn_top_prior: bool = False,\n        top_prior_param_shape: Union[Iterable[int], None] = None,\n        analytical_kl: bool = False,\n        retain_spatial_dims: bool = False,\n        vanilla_latent_hw: Union[Iterable[int], None] = None,\n        input_image_shape: Union[tuple[int, int], None] = None,\n        normalize_latent_factor: float = 1.0,\n        conv2d_bias: bool = True,\n        stochastic_use_naive_exponential: bool = False,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        z_dim: int\n            The size of the latent space.\n        n_res_blocks: int\n            The number of TopDownDeterministicResBlock blocks\n        n_filters: int\n            The number of channels present through out the layers of this block.\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        is_top_layer: bool, optional\n            Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.\n        upsampling_steps: int, optional\n            The number of upsampling steps that has to be done in this layer (typically 1).\n            Default is `None`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block (e.g., `nn.ReLU`). Default is `None`.\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"], optional\n            The type of merge done in the layer. It can be chosen between \"linear\", \"residual\",\n            and \"residual_ungated\". Check the `MergeLayer` class docstring for more information\n            about the behaviour of different merging modalities. Default is `None`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        stochastic_skip: bool, optional\n            Whether to use skip connections between previous top-down layer's output and this layer's stochastic output.\n            Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical\n            level, hence facilitating the gradient flow during backpropagation. Default is `False`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` documentation for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        gated: bool, optional\n            Whether to use gated layer in `ResidualBlock`. Default is `None`.\n        learn_top_prior:\n            Whether to set the top prior as learnable.\n            If this is set to `False`, in the top-most layer the prior will be N(0,1).\n            Otherwise, we will still have a normal distribution whose parameters will be learnt.\n            Default is `False`.\n        top_prior_param_shape: Iterable[int], optional\n            The size of the tensor which expresses the mean and the variance\n            of the prior for the top most layer. Default is `None`.\n        analytical_kl: bool, optional\n            If True, KL divergence is calculated according to the analytical formula.\n            Otherwise, an MC approximation using sampled latents is calculated.\n            Default is `False`.\n        retain_spatial_dims: bool, optional\n            If `True`, the size of Encoder's latent space is kept to `input_image_shape` within the topdown layer.\n            This implies that the oput spatial size equals the input spatial size.\n            To achieve this, we centercrop the intermediate representation.\n            Default is `False`.\n        vanilla_latent_hw: Iterable[int], optional\n            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n            Default is `None`.\n        input_image_shape: Tuple[int, int], optionalut\n            The shape of the input image tensor.\n            When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer\n            output has the same shape as the input. Default is `None`.\n        normalize_latent_factor: float, optional\n            A factor used to normalize the latent tensors `q_params`.\n            Specifically, normalization is done by dividing the latent tensor by this factor.\n            Default is 1.0.\n        conv2d_bias: bool, optional\n            Whether to use bias term is the convolutional blocks of this layer.\n            Default is `True`.\n        stochastic_use_naive_exponential: bool, optional\n            If `False`, in the NormalStochasticBlock2d exponentials are computed according\n            to the alternative definition provided by `StableExponential` class.\n            This should improve numerical stability in the training process.\n            Default is `False`.\n        \"\"\"\n        super().__init__()\n\n        self.is_top_layer = is_top_layer\n        self.z_dim = z_dim\n        self.stochastic_skip = stochastic_skip\n        self.learn_top_prior = learn_top_prior\n        self.analytical_kl = analytical_kl\n        self.retain_spatial_dims = retain_spatial_dims\n        self.input_image_shape = (\n            input_image_shape if len(conv_strides) == 3 else input_image_shape[1:]\n        )\n        self.latent_shape = self.input_image_shape if self.retain_spatial_dims else None\n        self.normalize_latent_factor = normalize_latent_factor\n        self._vanilla_latent_hw = vanilla_latent_hw  # TODO: check this, it is not used\n\n        # Define top layer prior parameters, possibly learnable\n        if is_top_layer:\n            self.top_prior_params = nn.Parameter(\n                torch.zeros(top_prior_param_shape), requires_grad=learn_top_prior\n            )\n\n        # Upsampling steps left to do in this layer\n        ups_left = upsampling_steps\n\n        # Define deterministic top-down block, which is a sequence of deterministic\n        # residual blocks with (optional) upsampling.\n        block_list = []\n        for _ in range(n_res_blocks):\n            do_resample = False\n            if ups_left &gt; 0:\n                do_resample = True\n                ups_left -= 1\n            block_list.append(\n                TopDownDeterministicResBlock(\n                    c_in=n_filters,\n                    c_out=n_filters,\n                    conv_strides=conv_strides,\n                    nonlin=nonlin,\n                    upsample=do_resample,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    res_block_type=res_block_type,\n                    res_block_kernel=res_block_kernel,\n                    gated=gated,\n                    conv2d_bias=conv2d_bias,\n                    groups=groups,\n                )\n            )\n        self.deterministic_block = nn.Sequential(*block_list)\n\n        # Define stochastic block with convolutions\n\n        self.stochastic = NormalStochasticBlock(\n            c_in=n_filters,\n            c_vars=z_dim,\n            c_out=n_filters,\n            conv_dims=len(conv_strides),\n            transform_p_params=(not is_top_layer),\n            vanilla_latent_hw=vanilla_latent_hw,\n            use_naive_exponential=stochastic_use_naive_exponential,\n        )\n\n        if not is_top_layer:\n            # Merge layer: it combines bottom-up inference and top-down\n            # generative outcomes to give posterior parameters\n            self.merge = MergeLayer(\n                channels=n_filters,\n                conv_strides=conv_strides,\n                merge_type=merge_type,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            )\n\n            # Skip connection that goes around the stochastic top-down layer\n            if stochastic_skip:\n                self.skip_connection_merger = SkipConnectionMerger(\n                    channels=n_filters,\n                    conv_strides=conv_strides,\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    res_block_type=res_block_type,\n                    merge_type=merge_type,\n                    conv2d_bias=conv2d_bias,\n                    res_block_kernel=res_block_kernel,\n                )\n\n    def sample_from_q(\n        self,\n        input_: torch.Tensor,\n        bu_value: torch.Tensor,\n        var_clip_max: Optional[float] = None,\n        mask: torch.Tensor = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Method computes the latent inference distribution q(z_i|z_{i+1}).\n\n        Used for sampling a latent tensor from it.\n\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The input tensor to the layer, which is the output of the top-down layer.\n        bu_value: torch.Tensor\n            The tensor defining the parameters /mu_q and /sigma_q computed during the\n            bottom-up deterministic pass at the correspondent hierarchical layer.\n        var_clip_max: float, optional\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped. Default is `None`.\n        mask: Union[None, torch.Tensor], optional\n            A tensor that is used to mask the sampled latent tensor. Default is `None`.\n        \"\"\"\n        if self.is_top_layer:  # In top layer, we don't merge bu_value with p_params\n            q_params = bu_value\n        else:\n            # NOTE: Here the assumption is that the vampprior is only applied on the top layer.\n            n_img_prior = None\n            p_params = self.get_p_params(input_, n_img_prior)\n            q_params = self.merge(bu_value, p_params)\n\n        sample = self.stochastic.sample_from_q(q_params, var_clip_max)\n\n        if mask:\n            return sample[mask]\n\n        return sample\n\n    def get_p_params(\n        self,\n        input_: torch.Tensor,\n        n_img_prior: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the parameters of the prior distribution p(z_i|z_{i+1}).\n\n        The parameters depend on the hierarchical level of the layer:\n        - if it is the topmost level, parameters are the ones of the prior.\n        - else, the input from the layer above is the parameters itself.\n\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The input tensor to the layer, which is the output of the top-down layer above.\n        n_img_prior: int\n            The number of images to be generated from the unconditional prior distribution p(z_L).\n        \"\"\"\n        p_params = None\n\n        # If top layer, define p_params as the ones of the prior p(z_L)\n        if self.is_top_layer:\n            p_params = self.top_prior_params\n\n            # Sample specific number of images by expanding the prior\n            if n_img_prior is not None:\n                p_params = p_params.expand(n_img_prior, -1, -1, -1)\n\n        # Else the input from the layer above is p_params itself\n        else:\n            p_params = input_\n\n        return p_params\n\n    def forward(\n        self,\n        input_: Union[torch.Tensor, None] = None,\n        skip_connection_input: Union[torch.Tensor, None] = None,\n        inference_mode: bool = False,\n        bu_value: Union[torch.Tensor, None] = None,\n        n_img_prior: Union[int, None] = None,\n        forced_latent: Union[torch.Tensor, None] = None,\n        force_constant_output: bool = False,\n        mode_pred: bool = False,\n        use_uncond_mode: bool = False,\n        var_clip_max: Union[float, None] = None,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        input_: torch.Tensor, optional\n            The input tensor to the layer, which is the output of the top-down layer.\n            Default is `None`.\n        skip_connection_input: torch.Tensor, optional\n            The tensor brought by the skip connection between the current and the\n            previous top-down layer.\n            Default is `None`.\n        inference_mode: bool, optional\n            Whether the layer is in inference mode. See NOTE 2 in class description\n            for more info.\n            Default is `False`.\n        bu_value: torch.Tensor, optional\n            The tensor defining the parameters /mu_q and /sigma_q computed during the\n            bottom-up deterministic pass\n            at the correspondent hierarchical layer. Default is `None`.\n        n_img_prior: int, optional\n            The number of images to be generated from the unconditional prior\n            distribution p(z_L).\n            Default is `None`.\n        forced_latent: torch.Tensor, optional\n            A pre-defined latent tensor. If it is not `None`, than it is used as the\n            actual latent tensor and,\n            hence, sampling does not happen. Default is `None`.\n        force_constant_output: bool, optional\n            Whether to copy the first sample (and rel. distrib parameters) over the\n            whole batch.\n            This is used when doing experiment from the prior - q is not used.\n            Default is `False`.\n        mode_pred: bool, optional\n            Whether the model is in prediction mode. Default is `False`.\n        use_uncond_mode: bool, optional\n            Whether to use the uncoditional distribution p(z) to sample latents in\n            prediction mode.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        # Check consistency of arguments\n        inputs_none = input_ is None and skip_connection_input is None\n        if self.is_top_layer and not inputs_none:\n            raise ValueError(\"In top layer, inputs should be None\")\n\n        p_params = self.get_p_params(input_, n_img_prior)\n\n        # Get the parameters for the latent distribution to sample from\n        if inference_mode:  # TODO What's this ? reuse Fede's code?\n            if self.is_top_layer:\n                q_params = bu_value\n                if mode_pred is False:\n                    assert p_params.shape[2:] == bu_value.shape[2:], (\n                        \"Spatial dimensions of p_params and bu_value should match. \"\n                        f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                        f\"bu_value={bu_value.shape[2:]}.\"\n                    )\n            else:\n                if use_uncond_mode:\n                    q_params = p_params\n                else:\n                    assert p_params.shape[2:] == bu_value.shape[2:], (\n                        \"Spatial dimensions of p_params and bu_value should match. \"\n                        f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                        f\"bu_value={bu_value.shape[2:]}.\"\n                    )\n                    q_params = self.merge(bu_value, p_params)\n        else:  # generative mode, q is not used, we sample from p(z_i | z_{i+1})\n            q_params = None\n\n        # NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})\n        # depending on the mode (hence, in practice, by checking whether q_params is None).\n\n        # Normalization of latent space parameters for stablity.\n        # See Very deep VAEs generalize autoregressive models.\n        if self.normalize_latent_factor:\n            q_params = q_params / self.normalize_latent_factor\n\n        # Sample (and process) a latent tensor in the stochastic layer\n        x, data_stoch = self.stochastic(\n            p_params=p_params,\n            q_params=q_params,\n            forced_latent=forced_latent,\n            force_constant_output=force_constant_output,\n            analytical_kl=self.analytical_kl,\n            mode_pred=mode_pred,\n            use_uncond_mode=use_uncond_mode,\n            var_clip_max=var_clip_max,\n        )\n        # Merge skip connection from previous layer\n        if self.stochastic_skip and not self.is_top_layer:\n            x = self.skip_connection_merger(x, skip_connection_input)\n        if self.retain_spatial_dims:\n            # NOTE: we assume that one topdown layer will have exactly one upscaling layer.\n\n            # NOTE: in case, in the Bottom-Up layer, LC retains spatial dimensions,\n            # we have the following (see `MergeLowRes`):\n            # - the \"primary-flow\" tensor is padded to match the low-res patch size\n            #   (e.g., from 32x32 to 64x64)\n            # - padded tensor is then merged with the low-res patch (concatenation\n            #   along dim=1 + convolution)\n            # Therefore, we need to do the symmetric operation here, that is to\n            # crop `x` for the same amount we padded it in the correspondent BU layer.\n\n            # NOTE: cropping is done to retain the shape of the input in the output.\n            # Therefore we need it only in the case `x` is the same shape of the input,\n            # because that's the only case in which we need to retain the shape.\n            # Here, it must be strictly greater than half the input shape, which is\n            # the case if and only if `x.shape == self.latent_shape`.\n            rescale = (\n                np.array((1, 2, 2)) if len(self.latent_shape) == 3 else np.array((2, 2))\n            )  # TODO better way?\n            new_latent_shape = tuple(np.array(self.latent_shape) // rescale)\n            if x.shape[-1] &gt; new_latent_shape[-1]:\n                x = crop_img_tensor(x, new_latent_shape)\n        # TODO: `retain_spatial_dims` is the same for all the TD layers.\n        # How to handle the case in which we do not have LC for all layers?\n        # The answer is in `self.latent_shape`, which is equal to `input_image_shape`\n        # (e.g., (64, 64)) if `retain_spatial_dims` is `True`, else it is `None`.\n        # Last top-down block (sequence of residual blocks w\\ upsampling)\n        x = self.deterministic_block(x)\n        # Save some metrics that will be used in the loss computation\n        keys = [\n            \"z\",\n            \"kl_samplewise\",\n            \"kl_samplewise_restricted\",\n            \"kl_spatial\",\n            \"kl_channelwise\",\n            \"logprob_q\",\n            \"qvar_max\",\n        ]\n        data = {k: data_stoch.get(k, None) for k in keys}\n        data[\"q_mu\"] = None\n        data[\"q_lv\"] = None\n        if data_stoch[\"q_params\"] is not None:\n            q_mu, q_lv = data_stoch[\"q_params\"]\n            data[\"q_mu\"] = q_mu\n            data[\"q_lv\"] = q_lv\n        return x, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.__init__","title":"<code>__init__(z_dim, n_res_blocks, n_filters, conv_strides, is_top_layer=False, upsampling_steps=None, nonlin=None, merge_type=None, batchnorm=True, dropout=None, stochastic_skip=False, res_block_type=None, res_block_kernel=None, groups=1, gated=None, learn_top_prior=False, top_prior_param_shape=None, analytical_kl=False, retain_spatial_dims=False, vanilla_latent_hw=None, input_image_shape=None, normalize_latent_factor=1.0, conv2d_bias=True, stochastic_use_naive_exponential=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>z_dim</code> <code>int</code> <p>The size of the latent space.</p> required <code>n_res_blocks</code> <code>int</code> <p>The number of TopDownDeterministicResBlock blocks</p> required <code>n_filters</code> <code>int</code> <p>The number of channels present through out the layers of this block.</p> required <code>conv_strides</code> <code>tuple[int]</code> <p>The strides used in the convolutions. Default is <code>(2, 2)</code>.</p> required <code>is_top_layer</code> <code>bool</code> <p>Whether the current layer is at the top of the Decoder hierarchy. Default is <code>False</code>.</p> <code>False</code> <code>upsampling_steps</code> <code>Union[int, None]</code> <p>The number of upsampling steps that has to be done in this layer (typically 1). Default is <code>None</code>.</p> <code>None</code> <code>nonlin</code> <code>Union[Callable, None]</code> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>). Default is <code>None</code>.</p> <code>None</code> <code>merge_type</code> <code>Union[Literal['linear', 'residual', 'residual_ungated'], None]</code> <p>The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\". Check the <code>MergeLayer</code> class docstring for more information about the behaviour of different merging modalities. Default is <code>None</code>.</p> <code>None</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Union[float, None]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>stochastic_skip</code> <code>bool</code> <p>Whether to use skip connections between previous top-down layer's output and this layer's stochastic output. Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical level, hence facilitating the gradient flow during backpropagation. Default is <code>False</code>.</p> <code>False</code> <code>res_block_type</code> <code>Union[str, None]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> documentation for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Union[int, None]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>gated</code> <code>Union[bool, None]</code> <p>Whether to use gated layer in <code>ResidualBlock</code>. Default is <code>None</code>.</p> <code>None</code> <code>learn_top_prior</code> <code>bool</code> <p>Whether to set the top prior as learnable. If this is set to <code>False</code>, in the top-most layer the prior will be N(0,1). Otherwise, we will still have a normal distribution whose parameters will be learnt. Default is <code>False</code>.</p> <code>False</code> <code>top_prior_param_shape</code> <code>Union[Iterable[int], None]</code> <p>The size of the tensor which expresses the mean and the variance of the prior for the top most layer. Default is <code>None</code>.</p> <code>None</code> <code>analytical_kl</code> <code>bool</code> <p>If True, KL divergence is calculated according to the analytical formula. Otherwise, an MC approximation using sampled latents is calculated. Default is <code>False</code>.</p> <code>False</code> <code>retain_spatial_dims</code> <code>bool</code> <p>If <code>True</code>, the size of Encoder's latent space is kept to <code>input_image_shape</code> within the topdown layer. This implies that the oput spatial size equals the input spatial size. To achieve this, we centercrop the intermediate representation. Default is <code>False</code>.</p> <code>False</code> <code>vanilla_latent_hw</code> <code>Union[Iterable[int], None]</code> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> <code>None</code> <code>input_image_shape</code> <code>Union[tuple[int, int], None]</code> <p>The shape of the input image tensor. When <code>retain_spatial_dims</code> is set to <code>True</code>, this is used to ensure that the shape of this layer output has the same shape as the input. Default is <code>None</code>.</p> <code>None</code> <code>normalize_latent_factor</code> <code>float</code> <p>A factor used to normalize the latent tensors <code>q_params</code>. Specifically, normalization is done by dividing the latent tensor by this factor. Default is 1.0.</p> <code>1.0</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term is the convolutional blocks of this layer. Default is <code>True</code>.</p> <code>True</code> <code>stochastic_use_naive_exponential</code> <code>bool</code> <p>If <code>False</code>, in the NormalStochasticBlock2d exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> <code>False</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    z_dim: int,\n    n_res_blocks: int,\n    n_filters: int,\n    conv_strides: tuple[int],\n    is_top_layer: bool = False,\n    upsampling_steps: Union[int, None] = None,\n    nonlin: Union[Callable, None] = None,\n    merge_type: Union[\n        Literal[\"linear\", \"residual\", \"residual_ungated\"], None\n    ] = None,\n    batchnorm: bool = True,\n    dropout: Union[float, None] = None,\n    stochastic_skip: bool = False,\n    res_block_type: Union[str, None] = None,\n    res_block_kernel: Union[int, None] = None,\n    groups: int = 1,\n    gated: Union[bool, None] = None,\n    learn_top_prior: bool = False,\n    top_prior_param_shape: Union[Iterable[int], None] = None,\n    analytical_kl: bool = False,\n    retain_spatial_dims: bool = False,\n    vanilla_latent_hw: Union[Iterable[int], None] = None,\n    input_image_shape: Union[tuple[int, int], None] = None,\n    normalize_latent_factor: float = 1.0,\n    conv2d_bias: bool = True,\n    stochastic_use_naive_exponential: bool = False,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    z_dim: int\n        The size of the latent space.\n    n_res_blocks: int\n        The number of TopDownDeterministicResBlock blocks\n    n_filters: int\n        The number of channels present through out the layers of this block.\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    is_top_layer: bool, optional\n        Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.\n    upsampling_steps: int, optional\n        The number of upsampling steps that has to be done in this layer (typically 1).\n        Default is `None`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block (e.g., `nn.ReLU`). Default is `None`.\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"], optional\n        The type of merge done in the layer. It can be chosen between \"linear\", \"residual\",\n        and \"residual_ungated\". Check the `MergeLayer` class docstring for more information\n        about the behaviour of different merging modalities. Default is `None`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    stochastic_skip: bool, optional\n        Whether to use skip connections between previous top-down layer's output and this layer's stochastic output.\n        Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical\n        level, hence facilitating the gradient flow during backpropagation. Default is `False`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` documentation for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    gated: bool, optional\n        Whether to use gated layer in `ResidualBlock`. Default is `None`.\n    learn_top_prior:\n        Whether to set the top prior as learnable.\n        If this is set to `False`, in the top-most layer the prior will be N(0,1).\n        Otherwise, we will still have a normal distribution whose parameters will be learnt.\n        Default is `False`.\n    top_prior_param_shape: Iterable[int], optional\n        The size of the tensor which expresses the mean and the variance\n        of the prior for the top most layer. Default is `None`.\n    analytical_kl: bool, optional\n        If True, KL divergence is calculated according to the analytical formula.\n        Otherwise, an MC approximation using sampled latents is calculated.\n        Default is `False`.\n    retain_spatial_dims: bool, optional\n        If `True`, the size of Encoder's latent space is kept to `input_image_shape` within the topdown layer.\n        This implies that the oput spatial size equals the input spatial size.\n        To achieve this, we centercrop the intermediate representation.\n        Default is `False`.\n    vanilla_latent_hw: Iterable[int], optional\n        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n        Default is `None`.\n    input_image_shape: Tuple[int, int], optionalut\n        The shape of the input image tensor.\n        When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer\n        output has the same shape as the input. Default is `None`.\n    normalize_latent_factor: float, optional\n        A factor used to normalize the latent tensors `q_params`.\n        Specifically, normalization is done by dividing the latent tensor by this factor.\n        Default is 1.0.\n    conv2d_bias: bool, optional\n        Whether to use bias term is the convolutional blocks of this layer.\n        Default is `True`.\n    stochastic_use_naive_exponential: bool, optional\n        If `False`, in the NormalStochasticBlock2d exponentials are computed according\n        to the alternative definition provided by `StableExponential` class.\n        This should improve numerical stability in the training process.\n        Default is `False`.\n    \"\"\"\n    super().__init__()\n\n    self.is_top_layer = is_top_layer\n    self.z_dim = z_dim\n    self.stochastic_skip = stochastic_skip\n    self.learn_top_prior = learn_top_prior\n    self.analytical_kl = analytical_kl\n    self.retain_spatial_dims = retain_spatial_dims\n    self.input_image_shape = (\n        input_image_shape if len(conv_strides) == 3 else input_image_shape[1:]\n    )\n    self.latent_shape = self.input_image_shape if self.retain_spatial_dims else None\n    self.normalize_latent_factor = normalize_latent_factor\n    self._vanilla_latent_hw = vanilla_latent_hw  # TODO: check this, it is not used\n\n    # Define top layer prior parameters, possibly learnable\n    if is_top_layer:\n        self.top_prior_params = nn.Parameter(\n            torch.zeros(top_prior_param_shape), requires_grad=learn_top_prior\n        )\n\n    # Upsampling steps left to do in this layer\n    ups_left = upsampling_steps\n\n    # Define deterministic top-down block, which is a sequence of deterministic\n    # residual blocks with (optional) upsampling.\n    block_list = []\n    for _ in range(n_res_blocks):\n        do_resample = False\n        if ups_left &gt; 0:\n            do_resample = True\n            ups_left -= 1\n        block_list.append(\n            TopDownDeterministicResBlock(\n                c_in=n_filters,\n                c_out=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                upsample=do_resample,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                gated=gated,\n                conv2d_bias=conv2d_bias,\n                groups=groups,\n            )\n        )\n    self.deterministic_block = nn.Sequential(*block_list)\n\n    # Define stochastic block with convolutions\n\n    self.stochastic = NormalStochasticBlock(\n        c_in=n_filters,\n        c_vars=z_dim,\n        c_out=n_filters,\n        conv_dims=len(conv_strides),\n        transform_p_params=(not is_top_layer),\n        vanilla_latent_hw=vanilla_latent_hw,\n        use_naive_exponential=stochastic_use_naive_exponential,\n    )\n\n    if not is_top_layer:\n        # Merge layer: it combines bottom-up inference and top-down\n        # generative outcomes to give posterior parameters\n        self.merge = MergeLayer(\n            channels=n_filters,\n            conv_strides=conv_strides,\n            merge_type=merge_type,\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            conv2d_bias=conv2d_bias,\n        )\n\n        # Skip connection that goes around the stochastic top-down layer\n        if stochastic_skip:\n            self.skip_connection_merger = SkipConnectionMerger(\n                channels=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                merge_type=merge_type,\n                conv2d_bias=conv2d_bias,\n                res_block_kernel=res_block_kernel,\n            )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.forward","title":"<code>forward(input_=None, skip_connection_input=None, inference_mode=False, bu_value=None, n_img_prior=None, forced_latent=None, force_constant_output=False, mode_pred=False, use_uncond_mode=False, var_clip_max=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Union[Tensor, None]</code> <p>The input tensor to the layer, which is the output of the top-down layer. Default is <code>None</code>.</p> <code>None</code> <code>skip_connection_input</code> <code>Union[Tensor, None]</code> <p>The tensor brought by the skip connection between the current and the previous top-down layer. Default is <code>None</code>.</p> <code>None</code> <code>inference_mode</code> <code>bool</code> <p>Whether the layer is in inference mode. See NOTE 2 in class description for more info. Default is <code>False</code>.</p> <code>False</code> <code>bu_value</code> <code>Union[Tensor, None]</code> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer. Default is <code>None</code>.</p> <code>None</code> <code>n_img_prior</code> <code>Union[int, None]</code> <p>The number of images to be generated from the unconditional prior distribution p(z_L). Default is <code>None</code>.</p> <code>None</code> <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> <code>None</code> <code>force_constant_output</code> <code>bool</code> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> <code>False</code> <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> <code>False</code> <code>var_clip_max</code> <code>Union[float, None]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(\n    self,\n    input_: Union[torch.Tensor, None] = None,\n    skip_connection_input: Union[torch.Tensor, None] = None,\n    inference_mode: bool = False,\n    bu_value: Union[torch.Tensor, None] = None,\n    n_img_prior: Union[int, None] = None,\n    forced_latent: Union[torch.Tensor, None] = None,\n    force_constant_output: bool = False,\n    mode_pred: bool = False,\n    use_uncond_mode: bool = False,\n    var_clip_max: Union[float, None] = None,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    input_: torch.Tensor, optional\n        The input tensor to the layer, which is the output of the top-down layer.\n        Default is `None`.\n    skip_connection_input: torch.Tensor, optional\n        The tensor brought by the skip connection between the current and the\n        previous top-down layer.\n        Default is `None`.\n    inference_mode: bool, optional\n        Whether the layer is in inference mode. See NOTE 2 in class description\n        for more info.\n        Default is `False`.\n    bu_value: torch.Tensor, optional\n        The tensor defining the parameters /mu_q and /sigma_q computed during the\n        bottom-up deterministic pass\n        at the correspondent hierarchical layer. Default is `None`.\n    n_img_prior: int, optional\n        The number of images to be generated from the unconditional prior\n        distribution p(z_L).\n        Default is `None`.\n    forced_latent: torch.Tensor, optional\n        A pre-defined latent tensor. If it is not `None`, than it is used as the\n        actual latent tensor and,\n        hence, sampling does not happen. Default is `None`.\n    force_constant_output: bool, optional\n        Whether to copy the first sample (and rel. distrib parameters) over the\n        whole batch.\n        This is used when doing experiment from the prior - q is not used.\n        Default is `False`.\n    mode_pred: bool, optional\n        Whether the model is in prediction mode. Default is `False`.\n    use_uncond_mode: bool, optional\n        Whether to use the uncoditional distribution p(z) to sample latents in\n        prediction mode.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    # Check consistency of arguments\n    inputs_none = input_ is None and skip_connection_input is None\n    if self.is_top_layer and not inputs_none:\n        raise ValueError(\"In top layer, inputs should be None\")\n\n    p_params = self.get_p_params(input_, n_img_prior)\n\n    # Get the parameters for the latent distribution to sample from\n    if inference_mode:  # TODO What's this ? reuse Fede's code?\n        if self.is_top_layer:\n            q_params = bu_value\n            if mode_pred is False:\n                assert p_params.shape[2:] == bu_value.shape[2:], (\n                    \"Spatial dimensions of p_params and bu_value should match. \"\n                    f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                    f\"bu_value={bu_value.shape[2:]}.\"\n                )\n        else:\n            if use_uncond_mode:\n                q_params = p_params\n            else:\n                assert p_params.shape[2:] == bu_value.shape[2:], (\n                    \"Spatial dimensions of p_params and bu_value should match. \"\n                    f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                    f\"bu_value={bu_value.shape[2:]}.\"\n                )\n                q_params = self.merge(bu_value, p_params)\n    else:  # generative mode, q is not used, we sample from p(z_i | z_{i+1})\n        q_params = None\n\n    # NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})\n    # depending on the mode (hence, in practice, by checking whether q_params is None).\n\n    # Normalization of latent space parameters for stablity.\n    # See Very deep VAEs generalize autoregressive models.\n    if self.normalize_latent_factor:\n        q_params = q_params / self.normalize_latent_factor\n\n    # Sample (and process) a latent tensor in the stochastic layer\n    x, data_stoch = self.stochastic(\n        p_params=p_params,\n        q_params=q_params,\n        forced_latent=forced_latent,\n        force_constant_output=force_constant_output,\n        analytical_kl=self.analytical_kl,\n        mode_pred=mode_pred,\n        use_uncond_mode=use_uncond_mode,\n        var_clip_max=var_clip_max,\n    )\n    # Merge skip connection from previous layer\n    if self.stochastic_skip and not self.is_top_layer:\n        x = self.skip_connection_merger(x, skip_connection_input)\n    if self.retain_spatial_dims:\n        # NOTE: we assume that one topdown layer will have exactly one upscaling layer.\n\n        # NOTE: in case, in the Bottom-Up layer, LC retains spatial dimensions,\n        # we have the following (see `MergeLowRes`):\n        # - the \"primary-flow\" tensor is padded to match the low-res patch size\n        #   (e.g., from 32x32 to 64x64)\n        # - padded tensor is then merged with the low-res patch (concatenation\n        #   along dim=1 + convolution)\n        # Therefore, we need to do the symmetric operation here, that is to\n        # crop `x` for the same amount we padded it in the correspondent BU layer.\n\n        # NOTE: cropping is done to retain the shape of the input in the output.\n        # Therefore we need it only in the case `x` is the same shape of the input,\n        # because that's the only case in which we need to retain the shape.\n        # Here, it must be strictly greater than half the input shape, which is\n        # the case if and only if `x.shape == self.latent_shape`.\n        rescale = (\n            np.array((1, 2, 2)) if len(self.latent_shape) == 3 else np.array((2, 2))\n        )  # TODO better way?\n        new_latent_shape = tuple(np.array(self.latent_shape) // rescale)\n        if x.shape[-1] &gt; new_latent_shape[-1]:\n            x = crop_img_tensor(x, new_latent_shape)\n    # TODO: `retain_spatial_dims` is the same for all the TD layers.\n    # How to handle the case in which we do not have LC for all layers?\n    # The answer is in `self.latent_shape`, which is equal to `input_image_shape`\n    # (e.g., (64, 64)) if `retain_spatial_dims` is `True`, else it is `None`.\n    # Last top-down block (sequence of residual blocks w\\ upsampling)\n    x = self.deterministic_block(x)\n    # Save some metrics that will be used in the loss computation\n    keys = [\n        \"z\",\n        \"kl_samplewise\",\n        \"kl_samplewise_restricted\",\n        \"kl_spatial\",\n        \"kl_channelwise\",\n        \"logprob_q\",\n        \"qvar_max\",\n    ]\n    data = {k: data_stoch.get(k, None) for k in keys}\n    data[\"q_mu\"] = None\n    data[\"q_lv\"] = None\n    if data_stoch[\"q_params\"] is not None:\n        q_mu, q_lv = data_stoch[\"q_params\"]\n        data[\"q_mu\"] = q_mu\n        data[\"q_lv\"] = q_lv\n    return x, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.get_p_params","title":"<code>get_p_params(input_, n_img_prior)</code>","text":"<p>Return the parameters of the prior distribution p(z_i|z_{i+1}).</p> <p>The parameters depend on the hierarchical level of the layer: - if it is the topmost level, parameters are the ones of the prior. - else, the input from the layer above is the parameters itself.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input tensor to the layer, which is the output of the top-down layer above.</p> required <code>n_img_prior</code> <code>int</code> <p>The number of images to be generated from the unconditional prior distribution p(z_L).</p> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def get_p_params(\n    self,\n    input_: torch.Tensor,\n    n_img_prior: int,\n) -&gt; torch.Tensor:\n    \"\"\"Return the parameters of the prior distribution p(z_i|z_{i+1}).\n\n    The parameters depend on the hierarchical level of the layer:\n    - if it is the topmost level, parameters are the ones of the prior.\n    - else, the input from the layer above is the parameters itself.\n\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The input tensor to the layer, which is the output of the top-down layer above.\n    n_img_prior: int\n        The number of images to be generated from the unconditional prior distribution p(z_L).\n    \"\"\"\n    p_params = None\n\n    # If top layer, define p_params as the ones of the prior p(z_L)\n    if self.is_top_layer:\n        p_params = self.top_prior_params\n\n        # Sample specific number of images by expanding the prior\n        if n_img_prior is not None:\n            p_params = p_params.expand(n_img_prior, -1, -1, -1)\n\n    # Else the input from the layer above is p_params itself\n    else:\n        p_params = input_\n\n    return p_params\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.sample_from_q","title":"<code>sample_from_q(input_, bu_value, var_clip_max=None, mask=None)</code>","text":"<p>Method computes the latent inference distribution q(z_i|z_{i+1}).</p> <p>Used for sampling a latent tensor from it.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input tensor to the layer, which is the output of the top-down layer.</p> required <code>bu_value</code> <code>Tensor</code> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer.</p> required <code>var_clip_max</code> <code>Optional[float]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> <code>None</code> <code>mask</code> <code>Tensor</code> <p>A tensor that is used to mask the sampled latent tensor. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def sample_from_q(\n    self,\n    input_: torch.Tensor,\n    bu_value: torch.Tensor,\n    var_clip_max: Optional[float] = None,\n    mask: torch.Tensor = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Method computes the latent inference distribution q(z_i|z_{i+1}).\n\n    Used for sampling a latent tensor from it.\n\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The input tensor to the layer, which is the output of the top-down layer.\n    bu_value: torch.Tensor\n        The tensor defining the parameters /mu_q and /sigma_q computed during the\n        bottom-up deterministic pass at the correspondent hierarchical layer.\n    var_clip_max: float, optional\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped. Default is `None`.\n    mask: Union[None, torch.Tensor], optional\n        A tensor that is used to mask the sampled latent tensor. Default is `None`.\n    \"\"\"\n    if self.is_top_layer:  # In top layer, we don't merge bu_value with p_params\n        q_params = bu_value\n    else:\n        # NOTE: Here the assumption is that the vampprior is only applied on the top layer.\n        n_img_prior = None\n        p_params = self.get_p_params(input_, n_img_prior)\n        q_params = self.merge(bu_value, p_params)\n\n    sample = self.stochastic.sample_from_q(q_params, var_clip_max)\n\n    if mask:\n        return sample[mask]\n\n    return sample\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/","title":"likelihoods","text":"<p>Script containing modules for defining different likelihood functions (as nn.Module).</p>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood","title":"<code>GaussianLikelihood</code>","text":"<p>               Bases: <code>LikelihoodModule</code></p> <p>A specialized <code>LikelihoodModule</code> for Gaussian likelihood.</p> <p>Specifically, in the LVAE model, the likelihood is defined as:     p(x|z_1) = N(x|\\mu_{p,1}, \\sigma_{p,1}^2)</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class GaussianLikelihood(LikelihoodModule):\n    r\"\"\"A specialized `LikelihoodModule` for Gaussian likelihood.\n\n    Specifically, in the LVAE model, the likelihood is defined as:\n        p(x|z_1) = N(x|\\mu_{p,1}, \\sigma_{p,1}^2)\n    \"\"\"\n\n    def __init__(\n        self,\n        predict_logvar: Union[Literal[\"pixelwise\"], None] = None,\n        logvar_lowerbound: Union[float, None] = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        predict_logvar: Union[Literal[\"pixelwise\"], None], optional\n            If `pixelwise`, log-variance is computed for each pixel, else log-variance\n            is not computed. Default is `None`.\n        logvar_lowerbound: float, optional\n            The lowerbound value for log-variance. Default is `None`.\n        \"\"\"\n        super().__init__()\n\n        self.predict_logvar = predict_logvar\n        self.logvar_lowerbound = logvar_lowerbound\n        assert self.predict_logvar in [None, \"pixelwise\"]\n\n        print(\n            f\"[{self.__class__.__name__}] PredLVar:{self.predict_logvar} LowBLVar:{self.logvar_lowerbound}\"\n        )\n\n    def get_mean_lv(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Given the output of the top-down pass, compute the mean and log-variance of the\n        Gaussian distribution defining the likelihood.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor to the likelihood module, i.e., the output of the top-down\n            pass.\n\n        Returns\n        -------\n        tuple of (torch.tensor, optional torch.tensor)\n            The first element of the tuple is the mean, the second element is the\n            log-variance. If the attribute `predict_logvar` is `None` then the second\n            element will be `None`.\n        \"\"\"\n        # if LadderVAE.predict_logvar is None, dim 1 of `x`` has no. of target channels\n        if self.predict_logvar is None:\n            return x, None\n\n        # Get pixel-wise mean and logvar\n        # if LadderVAE.predict_logvar is not None,\n        #   dim 1 has double no. of target channels\n        mean, lv = x.chunk(2, dim=1)\n\n        # Optionally, clip log-var to a lower bound\n        if self.logvar_lowerbound is not None:\n            lv = torch.clip(lv, min=self.logvar_lowerbound)\n\n        return mean, lv\n\n    def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor to the likelihood module, i.e., the output\n            the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case\n            `predict_logvar` is not None, or (B, C, [Z], Y, X) otherwise.\n        \"\"\"\n        mean, lv = self.get_mean_lv(x)\n        params = {\n            \"mean\": mean,\n            \"logvar\": lv,\n        }\n        return params\n\n    @staticmethod\n    def mean(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def mode(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def sample(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        # p = Normal(params['mean'], (params['logvar'] / 2).exp())\n        # return p.rsample()\n        return params[\"mean\"]\n\n    @staticmethod\n    def logvar(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"logvar\"]\n\n    def log_likelihood(\n        self, x: torch.Tensor, params: dict[str, Union[torch.Tensor, None]]\n    ):\n        \"\"\"Compute Gaussian log-likelihood\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The target tensor. Shape is (B, C, [Z], Y, X).\n        params: dict[str, Union[torch.Tensor, None]]\n            The tensors obtained by chunking the output of the top-down pass,\n            here used as parameters of the Gaussian distribution.\n\n        Returns\n        -------\n        torch.Tensor\n            The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n        \"\"\"\n        if self.predict_logvar is not None:\n            logprob = log_normal(x, params[\"mean\"], params[\"logvar\"])\n        else:\n            logprob = -0.5 * (params[\"mean\"] - x) ** 2\n        return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.__init__","title":"<code>__init__(predict_logvar=None, logvar_lowerbound=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>predict_logvar</code> <code>Union[Literal['pixelwise'], None]</code> <p>If <code>pixelwise</code>, log-variance is computed for each pixel, else log-variance is not computed. Default is <code>None</code>.</p> <code>None</code> <code>logvar_lowerbound</code> <code>Union[float, None]</code> <p>The lowerbound value for log-variance. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def __init__(\n    self,\n    predict_logvar: Union[Literal[\"pixelwise\"], None] = None,\n    logvar_lowerbound: Union[float, None] = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    predict_logvar: Union[Literal[\"pixelwise\"], None], optional\n        If `pixelwise`, log-variance is computed for each pixel, else log-variance\n        is not computed. Default is `None`.\n    logvar_lowerbound: float, optional\n        The lowerbound value for log-variance. Default is `None`.\n    \"\"\"\n    super().__init__()\n\n    self.predict_logvar = predict_logvar\n    self.logvar_lowerbound = logvar_lowerbound\n    assert self.predict_logvar in [None, \"pixelwise\"]\n\n    print(\n        f\"[{self.__class__.__name__}] PredLVar:{self.predict_logvar} LowBLVar:{self.logvar_lowerbound}\"\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.distr_params","title":"<code>distr_params(x)</code>","text":"<p>Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the likelihood module, i.e., the output the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case <code>predict_logvar</code> is not None, or (B, C, [Z], Y, X) otherwise.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor to the likelihood module, i.e., the output\n        the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case\n        `predict_logvar` is not None, or (B, C, [Z], Y, X) otherwise.\n    \"\"\"\n    mean, lv = self.get_mean_lv(x)\n    params = {\n        \"mean\": mean,\n        \"logvar\": lv,\n    }\n    return params\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.get_mean_lv","title":"<code>get_mean_lv(x)</code>","text":"<p>Given the output of the top-down pass, compute the mean and log-variance of the Gaussian distribution defining the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the likelihood module, i.e., the output of the top-down pass.</p> required <p>Returns:</p> Type Description <code>tuple of (torch.tensor, optional torch.tensor)</code> <p>The first element of the tuple is the mean, the second element is the log-variance. If the attribute <code>predict_logvar</code> is <code>None</code> then the second element will be <code>None</code>.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def get_mean_lv(\n    self, x: torch.Tensor\n) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Given the output of the top-down pass, compute the mean and log-variance of the\n    Gaussian distribution defining the likelihood.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor to the likelihood module, i.e., the output of the top-down\n        pass.\n\n    Returns\n    -------\n    tuple of (torch.tensor, optional torch.tensor)\n        The first element of the tuple is the mean, the second element is the\n        log-variance. If the attribute `predict_logvar` is `None` then the second\n        element will be `None`.\n    \"\"\"\n    # if LadderVAE.predict_logvar is None, dim 1 of `x`` has no. of target channels\n    if self.predict_logvar is None:\n        return x, None\n\n    # Get pixel-wise mean and logvar\n    # if LadderVAE.predict_logvar is not None,\n    #   dim 1 has double no. of target channels\n    mean, lv = x.chunk(2, dim=1)\n\n    # Optionally, clip log-var to a lower bound\n    if self.logvar_lowerbound is not None:\n        lv = torch.clip(lv, min=self.logvar_lowerbound)\n\n    return mean, lv\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.log_likelihood","title":"<code>log_likelihood(x, params)</code>","text":"<p>Compute Gaussian log-likelihood</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The target tensor. Shape is (B, C, [Z], Y, X).</p> required <code>params</code> <code>dict[str, Union[Tensor, None]]</code> <p>The tensors obtained by chunking the output of the top-down pass, here used as parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The log-likelihood tensor. Shape is (B, C, [Z], Y, X).</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_likelihood(\n    self, x: torch.Tensor, params: dict[str, Union[torch.Tensor, None]]\n):\n    \"\"\"Compute Gaussian log-likelihood\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The target tensor. Shape is (B, C, [Z], Y, X).\n    params: dict[str, Union[torch.Tensor, None]]\n        The tensors obtained by chunking the output of the top-down pass,\n        here used as parameters of the Gaussian distribution.\n\n    Returns\n    -------\n    torch.Tensor\n        The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n    \"\"\"\n    if self.predict_logvar is not None:\n        logprob = log_normal(x, params[\"mean\"], params[\"logvar\"])\n    else:\n        logprob = -0.5 * (params[\"mean\"] - x) ** 2\n    return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.LikelihoodModule","title":"<code>LikelihoodModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>The base class for all likelihood modules. It defines the fundamental structure and methods for specialized likelihood models.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class LikelihoodModule(nn.Module):\n    \"\"\"\n    The base class for all likelihood modules.\n    It defines the fundamental structure and methods for specialized likelihood models.\n    \"\"\"\n\n    def distr_params(self, x: Any) -&gt; None:\n        return None\n\n    def set_params_to_same_device_as(self, correct_device_tensor: Any) -&gt; None:\n        pass\n\n    @staticmethod\n    def logvar(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def mean(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def mode(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def sample(params: Any) -&gt; None:\n        return None\n\n    def log_likelihood(self, x: Any, params: Any) -&gt; None:\n        return None\n\n    def get_mean_lv(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]: ...\n\n    def forward(\n        self, input_: torch.Tensor, x: Union[torch.Tensor, None]\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The output of the top-down pass (e.g., reconstructed image in HDN,\n            or the unmixed images in 'Split' models).\n        x: Union[torch.Tensor, None]\n            The target tensor. If None, the log-likelihood is not computed.\n        \"\"\"\n        distr_params = self.distr_params(input_)\n        mean = self.mean(distr_params)\n        mode = self.mode(distr_params)\n        sample = self.sample(distr_params)\n        logvar = self.logvar(distr_params)\n\n        if x is None:\n            ll = None\n        else:\n            ll = self.log_likelihood(x, distr_params)\n\n        dct = {\n            \"mean\": mean,\n            \"mode\": mode,\n            \"sample\": sample,\n            \"params\": distr_params,\n            \"logvar\": logvar,\n        }\n\n        return ll, dct\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.LikelihoodModule.forward","title":"<code>forward(input_, x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The output of the top-down pass (e.g., reconstructed image in HDN, or the unmixed images in 'Split' models).</p> required <code>x</code> <code>Union[Tensor, None]</code> <p>The target tensor. If None, the log-likelihood is not computed.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def forward(\n    self, input_: torch.Tensor, x: Union[torch.Tensor, None]\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The output of the top-down pass (e.g., reconstructed image in HDN,\n        or the unmixed images in 'Split' models).\n    x: Union[torch.Tensor, None]\n        The target tensor. If None, the log-likelihood is not computed.\n    \"\"\"\n    distr_params = self.distr_params(input_)\n    mean = self.mean(distr_params)\n    mode = self.mode(distr_params)\n    sample = self.sample(distr_params)\n    logvar = self.logvar(distr_params)\n\n    if x is None:\n        ll = None\n    else:\n        ll = self.log_likelihood(x, distr_params)\n\n    dct = {\n        \"mean\": mean,\n        \"mode\": mode,\n        \"sample\": sample,\n        \"params\": distr_params,\n        \"logvar\": logvar,\n    }\n\n    return ll, dct\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood","title":"<code>NoiseModelLikelihood</code>","text":"<p>               Bases: <code>LikelihoodModule</code></p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class NoiseModelLikelihood(LikelihoodModule):\n\n    def __init__(\n        self,\n        noise_model: NoiseModel,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        noiseModel: NoiseModel\n            The noise model instance used to compute the likelihood.\n        \"\"\"\n        super().__init__()\n        self.data_mean = None\n        self.data_std = None\n        self.noiseModel = noise_model\n\n    def set_data_stats(\n        self,\n        data_mean: Union[np.ndarray, torch.Tensor],\n        data_std: Union[np.ndarray, torch.Tensor],\n    ) -&gt; None:\n        \"\"\"Set the data mean and std for denormalization.\n        # TODO check this !!\n        Parameters\n        ----------\n        data_mean : Union[np.ndarray, torch.Tensor]\n            Mean values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.\n        data_std : Union[np.ndarray, torch.Tensor]\n            Standard deviation values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.\n        \"\"\"\n        # Convert to tensor if needed\n        self.data_mean = torch.as_tensor(data_mean, dtype=torch.float32)\n        self.data_std = torch.as_tensor(data_std, dtype=torch.float32)\n\n        # TODO add extra dim for 3D ?\n\n    def _set_params_to_same_device_as(\n        self, correct_device_tensor: torch.Tensor\n    ) -&gt; None:\n        \"\"\"Set the parameters to the same device as the input tensor.\n\n        Parameters\n        ----------\n        correct_device_tensor: torch.Tensor\n            The tensor whose device is used to set the parameters.\n        \"\"\"\n        if (\n            self.data_mean is not None\n            and self.data_mean.device != correct_device_tensor.device\n        ):\n            self.data_mean = self.data_mean.to(correct_device_tensor.device)\n            self.data_std = self.data_std.to(correct_device_tensor.device)\n        if correct_device_tensor.device != self.noiseModel.device:\n            self.noiseModel.to_device(correct_device_tensor.device)\n\n    def get_mean_lv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, None]:\n        return x, None\n\n    def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        mean, lv = self.get_mean_lv(x)\n        params = {\n            \"mean\": mean,\n            \"logvar\": lv,\n        }\n        return params\n\n    @staticmethod\n    def mean(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def mode(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def sample(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    def log_likelihood(self, x: torch.Tensor, params: dict[str, torch.Tensor]):\n        \"\"\"Compute the log-likelihood given the parameters `params` obtained\n        from the reconstruction tensor and the target tensor `x`.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The target tensor. Shape is (B, C, [Z], Y, X).\n        params: dict[str, Union[torch.Tensor, None]]\n            The tensors obtained from output of the top-down pass.\n            Here, \"mean\" correspond to the whole output, while logvar is `None`.\n\n        Returns\n        -------\n        torch.Tensor\n            The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n        \"\"\"\n        if self.data_mean is None or self.data_std is None:\n            raise RuntimeError(\n                \"NoiseModelLikelihood: data_mean and data_std must be set before\"\n                \"callinglog_likelihood.\"\n            )\n        self._set_params_to_same_device_as(x)\n        predicted_s_denormalized = params[\"mean\"] * self.data_std + self.data_mean\n        x_denormalized = x * self.data_std + self.data_mean\n        likelihoods = self.noiseModel.likelihood(\n            x_denormalized, predicted_s_denormalized\n        )\n        logprob = torch.log(likelihoods)\n        return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.__init__","title":"<code>__init__(noise_model)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>noiseModel</code> <p>The noise model instance used to compute the likelihood.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def __init__(\n    self,\n    noise_model: NoiseModel,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    noiseModel: NoiseModel\n        The noise model instance used to compute the likelihood.\n    \"\"\"\n    super().__init__()\n    self.data_mean = None\n    self.data_std = None\n    self.noiseModel = noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.log_likelihood","title":"<code>log_likelihood(x, params)</code>","text":"<p>Compute the log-likelihood given the parameters <code>params</code> obtained from the reconstruction tensor and the target tensor <code>x</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The target tensor. Shape is (B, C, [Z], Y, X).</p> required <code>params</code> <code>dict[str, Tensor]</code> <p>The tensors obtained from output of the top-down pass. Here, \"mean\" correspond to the whole output, while logvar is <code>None</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The log-likelihood tensor. Shape is (B, C, [Z], Y, X).</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_likelihood(self, x: torch.Tensor, params: dict[str, torch.Tensor]):\n    \"\"\"Compute the log-likelihood given the parameters `params` obtained\n    from the reconstruction tensor and the target tensor `x`.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The target tensor. Shape is (B, C, [Z], Y, X).\n    params: dict[str, Union[torch.Tensor, None]]\n        The tensors obtained from output of the top-down pass.\n        Here, \"mean\" correspond to the whole output, while logvar is `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n    \"\"\"\n    if self.data_mean is None or self.data_std is None:\n        raise RuntimeError(\n            \"NoiseModelLikelihood: data_mean and data_std must be set before\"\n            \"callinglog_likelihood.\"\n        )\n    self._set_params_to_same_device_as(x)\n    predicted_s_denormalized = params[\"mean\"] * self.data_std + self.data_mean\n    x_denormalized = x * self.data_std + self.data_mean\n    likelihoods = self.noiseModel.likelihood(\n        x_denormalized, predicted_s_denormalized\n    )\n    logprob = torch.log(likelihoods)\n    return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.set_data_stats","title":"<code>set_data_stats(data_mean, data_std)</code>","text":"<p>Set the data mean and std for denormalization.</p>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.set_data_stats--todo-check-this","title":"TODO check this !!","text":"<p>Parameters:</p> Name Type Description Default <code>data_mean</code> <code>Union[ndarray, Tensor]</code> <p>Mean values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.</p> required <code>data_std</code> <code>Union[ndarray, Tensor]</code> <p>Standard deviation values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def set_data_stats(\n    self,\n    data_mean: Union[np.ndarray, torch.Tensor],\n    data_std: Union[np.ndarray, torch.Tensor],\n) -&gt; None:\n    \"\"\"Set the data mean and std for denormalization.\n    # TODO check this !!\n    Parameters\n    ----------\n    data_mean : Union[np.ndarray, torch.Tensor]\n        Mean values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.\n    data_std : Union[np.ndarray, torch.Tensor]\n        Standard deviation values for each channel. Will be reshaped to (1, C, 1, 1, 1) for broadcasting.\n    \"\"\"\n    # Convert to tensor if needed\n    self.data_mean = torch.as_tensor(data_mean, dtype=torch.float32)\n    self.data_std = torch.as_tensor(data_std, dtype=torch.float32)\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.likelihood_factory","title":"<code>likelihood_factory(config, noise_model=None)</code>","text":"<p>Factory function for creating likelihood modules.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[GaussianLikelihoodConfig, NMLikelihoodConfig]]</code> <p>The configuration object for the likelihood module.</p> required <code>noise_model</code> <code>Optional[NoiseModel]</code> <p>The noise model instance used to define the <code>NoiseModelLikelihood</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>The likelihood module.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def likelihood_factory(\n    config: Optional[Union[GaussianLikelihoodConfig, NMLikelihoodConfig]],\n    noise_model: Optional[NoiseModel] = None,\n):\n    \"\"\"\n    Factory function for creating likelihood modules.\n\n    Parameters\n    ----------\n    config: Union[GaussianLikelihoodConfig, NMLikelihoodConfig]\n        The configuration object for the likelihood module.\n    noise_model: Optional[NoiseModel]\n        The noise model instance used to define the `NoiseModelLikelihood`.\n\n    Returns\n    -------\n    nn.Module\n        The likelihood module.\n    \"\"\"\n    if config is None:\n        return None\n\n    if isinstance(config, GaussianLikelihoodConfig):\n        return GaussianLikelihood(\n            predict_logvar=config.predict_logvar,\n            logvar_lowerbound=config.logvar_lowerbound,\n        )\n    elif isinstance(config, NMLikelihoodConfig):\n        return NoiseModelLikelihood(\n            noise_model=noise_model,\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.log_normal","title":"<code>log_normal(x, mean, logvar)</code>","text":"<p>Compute the log-probability at <code>x</code> of a Gaussian distribution with parameters <code>(mean, exp(logvar))</code>.</p> <p>NOTE: In the case of LVAE, the log-likeihood formula becomes:     \\mathbb{E}{z_1\\sim{q\\phi}}[\\log{p_    heta(x|z_1)}]=-\frac{1}{2}(\\mathbb{E}{z_1\\sim{q\\phi}}[\\log{2\\pi\\sigma_{p,0}^2(z_1)}] +\\mathbb{E}{z_1\\sim{q\\phi}}[\frac{(x-\\mu_{p,0}(z_1))^2}{\\sigma_{p,0}^2(z_1)}])</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The ground-truth tensor. Shape is (batch, channels, dim1, dim2).</p> required <code>mean</code> <code>Tensor</code> <p>The inferred mean of distribution. Shape is (batch, channels, dim1, dim2).</p> required <code>logvar</code> <code>Tensor</code> <p>The inferred log-variance of distribution. Shape has to be either scalar or broadcastable.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_normal(\n    x: torch.Tensor, mean: torch.Tensor, logvar: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the log-probability at `x` of a Gaussian distribution\n    with parameters `(mean, exp(logvar))`.\n\n    NOTE: In the case of LVAE, the log-likeihood formula becomes:\n        \\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\\\log{p_\\theta(x|z_1)}]=-\\frac{1}{2}(\\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\\\log{2\\\\pi\\\\sigma_{p,0}^2(z_1)}] +\\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\frac{(x-\\\\mu_{p,0}(z_1))^2}{\\\\sigma_{p,0}^2(z_1)}])\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The ground-truth tensor. Shape is (batch, channels, dim1, dim2).\n    mean: torch.Tensor\n        The inferred mean of distribution. Shape is (batch, channels, dim1, dim2).\n    logvar: torch.Tensor\n        The inferred log-variance of distribution. Shape has to be either scalar or broadcastable.\n    \"\"\"\n    var = torch.exp(logvar)\n    log_prob = -0.5 * (\n        ((x - mean) ** 2) / var + logvar + torch.tensor(2 * math.pi).log()\n    )\n    return log_prob\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/","title":"lvae","text":"<p>Ladder VAE (LVAE) Model.</p> <p>The current implementation is based on \"Interpretable Unsupervised Diversity Denoising and Artefact Removal, Prakash et al.\"</p>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE","title":"<code>LadderVAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>int</code> <p>The size of the input image.</p> required <code>output_channels</code> <code>int</code> <p>The number of output channels.</p> required <code>multiscale_count</code> <code>int</code> <p>The number of scales for multiscale processing.</p> required <code>z_dims</code> <code>list[int]</code> <p>The dimensions of the latent space for each layer.</p> required <code>encoder_n_filters</code> <code>int</code> <p>The number of filters in the encoder.</p> required <code>decoder_n_filters</code> <code>int</code> <p>The number of filters in the decoder.</p> required <code>encoder_conv_strides</code> <code>list[int]</code> <p>The strides for the conv layers encoder.</p> required <code>decoder_conv_strides</code> <code>list[int]</code> <p>The strides for the conv layers decoder.</p> required <code>encoder_dropout</code> <code>float</code> <p>The dropout rate for the encoder.</p> required <code>decoder_dropout</code> <code>float</code> <p>The dropout rate for the decoder.</p> required <code>nonlinearity</code> <code>str</code> <p>The nonlinearity function to use.</p> required <code>predict_logvar</code> <code>bool</code> <p>Whether to predict the log variance.</p> required <code>analytical_kl</code> <code>bool</code> <p>Whether to use analytical KL divergence.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If only 2D convolutions are supported.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>class LadderVAE(nn.Module):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    input_shape : int\n        The size of the input image.\n    output_channels : int\n        The number of output channels.\n    multiscale_count : int\n        The number of scales for multiscale processing.\n    z_dims : list[int]\n        The dimensions of the latent space for each layer.\n    encoder_n_filters : int\n        The number of filters in the encoder.\n    decoder_n_filters : int\n        The number of filters in the decoder.\n    encoder_conv_strides : list[int]\n        The strides for the conv layers encoder.\n    decoder_conv_strides : list[int]\n        The strides for the conv layers decoder.\n    encoder_dropout : float\n        The dropout rate for the encoder.\n    decoder_dropout : float\n        The dropout rate for the decoder.\n    nonlinearity : str\n        The nonlinearity function to use.\n    predict_logvar : bool\n        Whether to predict the log variance.\n    analytical_kl : bool\n        Whether to use analytical KL divergence.\n\n    Raises\n    ------\n    NotImplementedError\n        If only 2D convolutions are supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_shape: int,\n        output_channels: int,\n        multiscale_count: int,\n        z_dims: list[int],\n        encoder_n_filters: int,\n        decoder_n_filters: int,\n        encoder_conv_strides: list[int],\n        decoder_conv_strides: list[int],\n        encoder_dropout: float,\n        decoder_dropout: float,\n        nonlinearity: str,\n        predict_logvar: bool,\n        analytical_kl: bool,\n    ):\n        super().__init__()\n\n        # -------------------------------------------------------\n        # Customizable attributes\n        self.image_size = input_shape\n        \"\"\"Input image size. (Z, Y, X) or (Y, X) if the data is 2D.\"\"\"\n        # TODO: we need to be careful with this since used to be an int.\n        # the tuple of shapes used to be `self.input_shape`.\n        self.target_ch = output_channels\n        self.encoder_conv_strides = encoder_conv_strides\n        self.decoder_conv_strides = decoder_conv_strides\n        self._multiscale_count = multiscale_count\n        self.z_dims = z_dims\n        self.encoder_n_filters = encoder_n_filters\n        self.decoder_n_filters = decoder_n_filters\n        self.encoder_dropout = encoder_dropout\n        self.decoder_dropout = decoder_dropout\n        self.nonlin = nonlinearity\n        self.predict_logvar = predict_logvar\n        self.analytical_kl = analytical_kl\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Model attributes -&gt; Hardcoded\n        self.model_type = ModelType.LadderVae  # TODO remove !\n        self.encoder_blocks_per_layer = 1\n        self.decoder_blocks_per_layer = 1\n        self.bottomup_batchnorm = True\n        self.topdown_batchnorm = True\n        self.topdown_conv2d_bias = True\n        self.gated = True\n        self.encoder_res_block_kernel = 3\n        self.decoder_res_block_kernel = 3\n        self.encoder_res_block_skip_padding = False\n        self.decoder_res_block_skip_padding = False\n        self.merge_type = \"residual\"\n        self.no_initial_downscaling = True\n        self.skip_bottomk_buvalues = 0\n        self.stochastic_skip = True\n        self.learn_top_prior = True\n        self.res_block_type = \"bacdbacd\"  # TODO remove !\n        self.mode_pred = False\n        self.logvar_lowerbound = -5\n        self._var_clip_max = 20\n        self._stochastic_use_naive_exponential = False\n        self._enable_topdown_normalize_factor = True\n\n        # Attributes that handle LC -&gt; Hardcoded\n        self.enable_multiscale = self._multiscale_count &gt; 1\n        self.multiscale_retain_spatial_dims = True\n        self.multiscale_lowres_separate_branch = False\n        self.multiscale_decoder_retain_spatial_dims = (\n            self.multiscale_retain_spatial_dims and self.enable_multiscale\n        )\n\n        # Derived attributes\n        self.n_layers = len(self.z_dims)\n\n        # Others...\n        self._tethered_to_input = False\n        self._tethered_ch1_scalar = self._tethered_ch2_scalar = None\n        if self._tethered_to_input:\n            target_ch = 1\n            requires_grad = False\n            self._tethered_ch1_scalar = nn.Parameter(\n                torch.ones(1) * 0.5, requires_grad=requires_grad\n            )\n            self._tethered_ch2_scalar = nn.Parameter(\n                torch.ones(1) * 2.0, requires_grad=requires_grad\n            )\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Data attributes\n        self.color_ch = 1  # TODO for now we only support 1 channel\n        self.normalized_input = True\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Loss attributes\n        # enabling reconstruction loss on mixed input\n        self.mixed_rec_w = 0\n        self.nbr_consistency_w = 0\n\n        # -------------------------------------------------------\n        # 3D related stuff\n        self._mode_3D = len(self.image_size) == 3  # TODO refac\n        self._model_3D_depth = self.image_size[0] if self._mode_3D else 1\n        self._decoder_mode_3D = len(self.decoder_conv_strides) == 3\n        if self._mode_3D and not self._decoder_mode_3D:\n            assert self._model_3D_depth % 2 == 1, \"3D model depth should be odd\"\n        assert (\n            self._mode_3D is True or self._decoder_mode_3D is False\n        ), \"Decoder cannot be 3D when encoder is 2D\"\n        self._squish3d = self._mode_3D and not self._decoder_mode_3D\n        self._3D_squisher = (\n            None\n            if not self._squish3d\n            else nn.ModuleList(\n                [\n                    GateLayer(\n                        channels=self.encoder_n_filters,\n                        conv_strides=self.encoder_conv_strides,\n                    )\n                    for k in range(len(self.z_dims))\n                ]\n            )\n        )\n        # TODO: this bit is in the Ashesh's confusing-hacky style... Can we do better?\n\n        # -------------------------------------------------------\n        # # Training attributes\n        # # can be used to tile the validation predictions\n        # self._val_idx_manager = val_idx_manager\n        # self._val_frame_creator = None\n        # # initialize the learning rate scheduler params.\n        # self.lr_scheduler_monitor = self.lr_scheduler_mode = None\n        # self._init_lr_scheduler_params(config)\n        # self._global_step = 0\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n\n        # Calculate the downsampling happening in the network\n        self.downsample = [1] * self.n_layers\n        self.overall_downscale_factor = np.power(2, sum(self.downsample))\n        if not self.no_initial_downscaling:  # by default do another downscaling\n            self.overall_downscale_factor *= 2\n\n        assert max(self.downsample) &lt;= self.encoder_blocks_per_layer\n        assert len(self.downsample) == self.n_layers\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        ### CREATE MODEL BLOCKS\n        # First bottom-up layer: change num channels + downsample by factor 2\n        # unless we want to prevent this\n        self.encoder_conv_op = getattr(nn, f\"Conv{len(self.encoder_conv_strides)}d\")\n        # TODO these should be defined for all layers here ?\n        self.decoder_conv_op = getattr(nn, f\"Conv{len(self.decoder_conv_strides)}d\")\n        # TODO: would be more readable to have a derived parameters to use like\n        # `conv_dims = len(self.encoder_conv_strides)` and then use `Conv{conv_dims}d`\n        stride = 1 if self.no_initial_downscaling else 2\n        self.first_bottom_up = self.create_first_bottom_up(stride)\n\n        # Input Branches for Lateral Contextualization\n        self.lowres_first_bottom_ups = None\n        self._init_multires()\n\n        # Other bottom-up layers\n        self.bottom_up_layers = self.create_bottom_up_layers(\n            self.multiscale_lowres_separate_branch\n        )\n\n        # Top-down layers\n        self.top_down_layers = self.create_top_down_layers()\n        self.final_top_down = self.create_final_topdown_layer(\n            not self.no_initial_downscaling\n        )\n\n        # Likelihood module\n        # self.likelihood = self.create_likelihood_module()\n\n        # Output layer --&gt; Project to target_ch many channels\n        logvar_ch_needed = self.predict_logvar is not None\n        self.output_layer = self.parameter_net = self.decoder_conv_op(\n            self.decoder_n_filters,\n            self.target_ch * (1 + logvar_ch_needed),\n            kernel_size=3,\n            padding=1,\n            bias=self.topdown_conv2d_bias,\n        )\n\n        # # gradient norms. updated while training. this is also logged.\n        # self.grad_norm_bottom_up = 0.0\n        # self.grad_norm_top_down = 0.0\n        # PSNR computation on validation.\n        # self.label1_psnr = RunningPSNR()\n        # self.label2_psnr = RunningPSNR()\n        # TODO: did you add this?\n\n        # msg =f'[{self.__class__.__name__}] Stoc:{not self.non_stochastic_version} RecMode:{self.reconstruction_mode} TethInput:{self._tethered_to_input}'\n        # msg += f' TargetCh: {self.target_ch}'\n        # print(msg)\n\n    ### SET OF METHODS TO CREATE MODEL BLOCKS\n    def create_first_bottom_up(\n        self,\n        init_stride: int,\n        num_res_blocks: int = 1,\n    ) -&gt; nn.Sequential:\n        \"\"\"\n        Method creates the first bottom-up block of the Encoder.\n\n        Its role is to perform a first image compression step.\n        It is composed by a sequence of nn.Conv2d + non-linearity +\n        BottomUpDeterministicResBlock (1 or more, default is 1).\n\n        Parameters\n        ----------\n        init_stride: int\n            The stride used by the intial Conv2d block.\n        num_res_blocks: int, optional\n            The number of BottomUpDeterministicResBlocks, default is 1.\n        \"\"\"\n        # From what I got from Ashesh, Z should not be touched in any case.\n        nonlin = get_activation(self.nonlin)\n        conv_block = self.encoder_conv_op(\n            in_channels=self.color_ch,\n            out_channels=self.encoder_n_filters,\n            kernel_size=self.encoder_res_block_kernel,\n            padding=(\n                0\n                if self.encoder_res_block_skip_padding\n                else self.encoder_res_block_kernel // 2\n            ),\n            stride=init_stride,\n        )\n\n        modules = [conv_block, nonlin]\n\n        for _ in range(num_res_blocks):\n            modules.append(\n                BottomUpDeterministicResBlock(\n                    conv_strides=self.encoder_conv_strides,\n                    c_in=self.encoder_n_filters,\n                    c_out=self.encoder_n_filters,\n                    nonlin=nonlin,\n                    downsample=False,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.encoder_res_block_kernel,\n                )\n            )\n\n        return nn.Sequential(*modules)\n\n    def create_bottom_up_layers(self, lowres_separate_branch: bool) -&gt; nn.ModuleList:\n        \"\"\"\n        Method creates the stack of bottom-up layers of the Encoder.\n\n        that are used to generate the so-called `bu_values`.\n\n        NOTE:\n            If `self._multiscale_count &lt; self.n_layers`, then LC is done only in the first\n            `self._multiscale_count` bottom-up layers (starting from the bottom).\n\n        Parameters\n        ----------\n        lowres_separate_branch: bool\n            Whether the residual block(s) used for encoding the low-res input are shared\n            (`False`) or not (`True`) with the \"same-size\" residual block(s) in the\n            `BottomUpLayer`'s primary flow.\n        \"\"\"\n        multiscale_lowres_size_factor = 1\n        nonlin = get_activation(self.nonlin)\n\n        bottom_up_layers = nn.ModuleList([])\n        for i in range(self.n_layers):\n            # Whether this is the top layer\n            is_top = i == self.n_layers - 1\n\n            # LC is applied only to the first (_multiscale_count - 1) bottom-up layers\n            layer_enable_multiscale = (\n                self.enable_multiscale and self._multiscale_count &gt; i + 1\n            )\n\n            # This factor determines the factor by which the low-resolution tensor is larger\n            # N.B. Only used if layer_enable_multiscale == True, so we updated it only in that case\n            multiscale_lowres_size_factor *= 1 + int(layer_enable_multiscale)\n\n            # TODO: check correctness of this\n            if self._multiscale_count &gt; 1:\n                output_expected_shape = (dim // 2 ** (i + 1) for dim in self.image_size)\n            else:\n                output_expected_shape = None\n\n            # Add bottom-up deterministic layer at level i.\n            # It's a sequence of residual blocks (BottomUpDeterministicResBlock), possibly with downsampling between them.\n            bottom_up_layers.append(\n                BottomUpLayer(\n                    n_res_blocks=self.encoder_blocks_per_layer,\n                    n_filters=self.encoder_n_filters,\n                    downsampling_steps=self.downsample[i],\n                    nonlin=nonlin,\n                    conv_strides=self.encoder_conv_strides,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.encoder_res_block_kernel,\n                    gated=self.gated,\n                    lowres_separate_branch=lowres_separate_branch,\n                    enable_multiscale=self.enable_multiscale,  # TODO: shouldn't the arg be `layer_enable_multiscale` here?\n                    multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n                    multiscale_lowres_size_factor=multiscale_lowres_size_factor,\n                    decoder_retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                    output_expected_shape=output_expected_shape,\n                )\n            )\n\n        return bottom_up_layers\n\n    def create_top_down_layers(self) -&gt; nn.ModuleList:\n        \"\"\"\n        Method creates the stack of top-down layers of the Decoder.\n\n        In these layer the `bu`_values` from the Encoder are merged with the `p_params` from the previous layer\n        of the Decoder to get `q_params`. Then, a stochastic layer generates a sample from the latent distribution\n        with parameters `q_params`. Finally, this sample is fed through a TopDownDeterministicResBlock to\n        compute the `p_params` for the layer below.\n\n        NOTE 1:\n            The algorithm for generative inference approximately works as follows:\n                - p_params = output of top-down layer above\n                - bu = inferred bottom-up value at this layer\n                - q_params = merge(bu, p_params)\n                - z = stochastic_layer(q_params)\n                - (optional) get and merge skip connection from prev top-down layer\n                - top-down deterministic ResNet\n\n        NOTE 2:\n            When doing unconditional generation, bu_value is not available. Hence the\n            merge layer is not used, and z is sampled directly from p_params.\n\n        \"\"\"\n        top_down_layers = nn.ModuleList([])\n        nonlin = get_activation(self.nonlin)\n        # NOTE: top-down layers are created starting from the bottom-most\n        for i in range(self.n_layers):\n            # Check if this is the top layer\n            is_top = i == self.n_layers - 1\n\n            if self._enable_topdown_normalize_factor:  # TODO: What is this?\n                normalize_latent_factor = (\n                    1 / np.sqrt(2 * (1 + i)) if len(self.z_dims) &gt; 4 else 1.0\n                )\n            else:\n                normalize_latent_factor = 1.0\n\n            top_down_layers.append(\n                TopDownLayer(\n                    z_dim=self.z_dims[i],\n                    n_res_blocks=self.decoder_blocks_per_layer,\n                    n_filters=self.decoder_n_filters,\n                    is_top_layer=is_top,\n                    conv_strides=self.decoder_conv_strides,\n                    upsampling_steps=self.downsample[i],\n                    nonlin=nonlin,\n                    merge_type=self.merge_type,\n                    batchnorm=self.topdown_batchnorm,\n                    dropout=self.decoder_dropout,\n                    stochastic_skip=self.stochastic_skip,\n                    learn_top_prior=self.learn_top_prior,\n                    top_prior_param_shape=self.get_top_prior_param_shape(),\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.decoder_res_block_kernel,\n                    gated=self.gated,\n                    analytical_kl=self.analytical_kl,\n                    vanilla_latent_hw=self.get_latent_spatial_size(i),\n                    retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                    input_image_shape=self.image_size,\n                    normalize_latent_factor=normalize_latent_factor,\n                    conv2d_bias=self.topdown_conv2d_bias,\n                    stochastic_use_naive_exponential=self._stochastic_use_naive_exponential,\n                )\n            )\n        return top_down_layers\n\n    def create_final_topdown_layer(self, upsample: bool) -&gt; nn.Sequential:\n        \"\"\"Create the final top-down layer of the Decoder.\n\n        NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation\n        instead of transposed convolution (like in other TD layers).\n\n        Parameters\n        ----------\n        upsample: bool\n            Whether to upsample the input of the final top-down layer\n            by bilinear interpolation with `scale_factor=2`.\n        \"\"\"\n        # Final top-down layer\n        modules = list()\n\n        if upsample:\n            modules.append(Interpolate(scale=2))\n\n        for i in range(self.decoder_blocks_per_layer):\n            modules.append(\n                TopDownDeterministicResBlock(\n                    c_in=self.decoder_n_filters,\n                    c_out=self.decoder_n_filters,\n                    nonlin=get_activation(self.nonlin),\n                    conv_strides=self.decoder_conv_strides,\n                    batchnorm=self.topdown_batchnorm,\n                    dropout=self.decoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.decoder_res_block_kernel,\n                    gated=self.gated,\n                    conv2d_bias=self.topdown_conv2d_bias,\n                )\n            )\n        return nn.Sequential(*modules)\n\n    def _init_multires(self, config=None) -&gt; nn.ModuleList:\n        \"\"\"\n        Method defines the input block/branch to encode/compress low-res lateral inputs.\n\n        at different hierarchical levels\n        in the multiresolution approach (LC). The role of the input branches is similar\n        to the one of the first bottom-up layer in the primary flow of the Encoder,\n        namely to compress the lateral input image to a degree that is compatible with\n        the one of the primary flow.\n\n        NOTE 1: Each input branch consists of a sequence of Conv2d + non-linearity\n        + BottomUpDeterministicResBlock. It is meaningful to observe that the\n        `BottomUpDeterministicResBlock` shares the same model attributes with the blocks\n        in the primary flow of the Encoder (e.g., c_in, c_out, dropout, etc. etc.).\n        Moreover, it does not perform downsampling.\n\n        NOTE 2: `_multiscale_count` attribute defines the total number of inputs to the\n        bottom-up pass. In other terms if we have the input patch and n_LC additional\n        lateral inputs, we will have a total of (n_LC + 1) inputs.\n        \"\"\"\n        stride = 1 if self.no_initial_downscaling else 2\n        nonlin = get_activation(self.nonlin)\n        if self._multiscale_count is None:\n            self._multiscale_count = 1\n\n        msg = (\n            f\"Multiscale count ({self._multiscale_count}) should not exceed the number\"\n            f\"of bottom up layers ({self.n_layers}) by more than 1.\\n\"\n        )\n        assert (\n            self._multiscale_count &lt;= 1 or self._multiscale_count &lt;= 1 + self.n_layers\n        ), msg  # TODO how ?\n\n        msg = (\n            \"Multiscale approach only supports monocrome images. \"\n            f\"Found instead color_ch={self.color_ch}.\"\n        )\n        # assert self._multiscale_count == 1 or self.color_ch == 1, msg\n\n        lowres_first_bottom_ups = []\n        for _ in range(1, self._multiscale_count):\n            first_bottom_up = nn.Sequential(\n                self.encoder_conv_op(\n                    in_channels=self.color_ch,\n                    out_channels=self.encoder_n_filters,\n                    kernel_size=5,\n                    padding=\"same\",\n                    stride=stride,\n                ),\n                nonlin,\n                BottomUpDeterministicResBlock(\n                    c_in=self.encoder_n_filters,\n                    c_out=self.encoder_n_filters,\n                    conv_strides=self.encoder_conv_strides,\n                    nonlin=nonlin,\n                    downsample=False,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                ),\n            )\n            lowres_first_bottom_ups.append(first_bottom_up)\n\n        self.lowres_first_bottom_ups = (\n            nn.ModuleList(lowres_first_bottom_ups)\n            if len(lowres_first_bottom_ups)\n            else None\n        )\n\n    ### SET OF FORWARD-LIKE METHODS\n    def bottomup_pass(self, inp: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"Wrapper of _bottomup_pass().\"\"\"\n        # TODO Remove wrapper\n        return self._bottomup_pass(\n            inp,\n            self.first_bottom_up,\n            self.lowres_first_bottom_ups,\n            self.bottom_up_layers,\n        )\n\n    def _bottomup_pass(\n        self,\n        inp: torch.Tensor,\n        first_bottom_up: nn.Sequential,\n        lowres_first_bottom_ups: nn.ModuleList,\n        bottom_up_layers: nn.ModuleList,\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Method defines the forward pass through the LVAE Encoder, the so-called.\n\n        Bottom-Up pass.\n\n        Parameters\n        ----------\n        inp: torch.Tensor\n            The input tensor to the bottom-up pass of shape (B, 1+n_LC, H, W), where n_LC\n            is the number of lateral low-res inputs used in the LC approach.\n            In particular, the first channel corresponds to the input patch, while the\n            remaining ones are associated to the lateral low-res inputs.\n        first_bottom_up: nn.Sequential\n            The module defining the first bottom-up layer of the Encoder.\n        lowres_first_bottom_ups: nn.ModuleList\n            The list of modules defining Lateral Contextualization.\n        bottom_up_layers: nn.ModuleList\n            The list of modules defining the stack of bottom-up layers of the Encoder.\n        \"\"\"\n        if self._multiscale_count &gt; 1:\n            x = first_bottom_up(inp[:, :1])\n        else:\n            x = first_bottom_up(inp)\n\n        # Loop from bottom to top layer, store all deterministic nodes we\n        # need for the top-down pass in bu_values list\n        bu_values = []\n        for i in range(self.n_layers):\n            lowres_x = None\n            if self._multiscale_count &gt; 1 and i + 1 &lt; inp.shape[1]:\n                lowres_x = lowres_first_bottom_ups[i](inp[:, i + 1 : i + 2])\n            x, bu_value = bottom_up_layers[i](x, lowres_x=lowres_x)\n            bu_values.append(bu_value)\n\n        return bu_values\n\n    def topdown_pass(\n        self,\n        bu_values: Union[torch.Tensor, None] = None,\n        n_img_prior: Union[torch.Tensor, None] = None,\n        constant_layers: Union[Iterable[int], None] = None,\n        forced_latent: Union[list[torch.Tensor], None] = None,\n        top_down_layers: Union[nn.ModuleList, None] = None,\n        final_top_down_layer: Union[nn.Sequential, None] = None,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Method defines the forward pass through the LVAE Decoder, the so-called.\n\n        Top-Down pass.\n\n        Parameters\n        ----------\n        bu_values: torch.Tensor, optional\n            Output of the bottom-up pass. It will have values from multiple layers of\n            the ladder.\n        n_img_prior: optional\n            When `bu_values` is `None`, `n_img_prior` indicates the number of images to\n            generate\n            from the prior (so bottom-up pass is not used at all here).\n        constant_layers: Iterable[int], optional\n            A sequence of indexes associated to the layers in which a single instance's\n            z is copied over the entire batch (bottom-up path is not used, so only prior\n            is used here). Set to `None` to avoid this behaviour.\n        forced_latent: list[torch.Tensor], optional\n            A list of tensors that are used as fixed latent variables (hence, sampling\n            doesn't take place in this case).\n        top_down_layers: nn.ModuleList, optional\n            A list of top-down layers to use in the top-down pass. If `None`, the method\n            uses the default layers defined in the constructor.\n        final_top_down_layer: nn.Sequential, optional\n            The last top-down layer of the top-down pass. If `None`, the method uses the\n            default layers defined in the constructor.\n        \"\"\"\n        if top_down_layers is None:\n            top_down_layers = self.top_down_layers\n        if final_top_down_layer is None:\n            final_top_down_layer = self.final_top_down\n\n        # Default: no layer is sampled from the distribution's mode\n        if constant_layers is None:\n            constant_layers = []\n        prior_experiment = len(constant_layers) &gt; 0\n\n        # If the bottom-up inference values are not given, don't do\n        # inference, sample from prior instead\n        inference_mode = bu_values is not None\n\n        # Check consistency of arguments\n        if inference_mode != (n_img_prior is None):\n            msg = (\n                \"Number of images for top-down generation has to be given \"\n                \"if and only if we're not doing inference\"\n            )\n            raise RuntimeError(msg)\n        if inference_mode and prior_experiment:\n            msg = (\n                \"Prior experiments (e.g. sampling from mode) are not\"\n                \" compatible with inference mode\"\n            )\n            raise RuntimeError(msg)\n\n        # Sampled latent variables at each layer\n        z = [None] * self.n_layers\n        # KL divergence of each layer\n        kl = [None] * self.n_layers\n        # Kl divergence restricted, only for the LC enabled setup denoiSplit.\n        kl_restricted = [None] * self.n_layers\n        # mean from which z is sampled.\n        q_mu = [None] * self.n_layers\n        # log(var) from which z is sampled.\n        q_lv = [None] * self.n_layers\n        # Spatial map of KL divergence for each layer\n        kl_spatial = [None] * self.n_layers\n        debug_qvar_max = [None] * self.n_layers\n        kl_channelwise = [None] * self.n_layers\n        if forced_latent is None:\n            forced_latent = [None] * self.n_layers\n\n        # Top-down inference/generation loop\n        out = None\n        for i in reversed(range(self.n_layers)):\n            # If available, get deterministic node from bottom-up inference\n            try:\n                bu_value = bu_values[i]\n            except TypeError:\n                bu_value = None\n\n            # Whether the current layer should be sampled from the mode\n            constant_out = i in constant_layers\n\n            # Input for skip connection\n            skip_input = out\n\n            # Full top-down layer, including sampling and deterministic part\n            out, aux = top_down_layers[i](\n                input_=out,\n                skip_connection_input=skip_input,\n                inference_mode=inference_mode,\n                bu_value=bu_value,\n                n_img_prior=n_img_prior,\n                force_constant_output=constant_out,\n                forced_latent=forced_latent[i],\n                mode_pred=self.mode_pred,\n                var_clip_max=self._var_clip_max,\n            )\n            # Save useful variables\n            z[i] = aux[\"z\"]  # sampled variable at this layer (batch, ch, h, w)\n            kl[i] = aux[\"kl_samplewise\"]  # (batch, )\n            kl_restricted[i] = aux[\"kl_samplewise_restricted\"]\n            kl_spatial[i] = aux[\"kl_spatial\"]  # (batch, h, w)\n            q_mu[i] = aux[\"q_mu\"]\n            q_lv[i] = aux[\"q_lv\"]\n\n            kl_channelwise[i] = aux[\"kl_channelwise\"]\n            debug_qvar_max[i] = aux[\"qvar_max\"]\n            # if self.mode_pred is False:\n            #     logprob_p += aux['logprob_p'].mean()  # mean over batch\n            # else:\n            #     logprob_p = None\n\n        # Final top-down layer\n        out = final_top_down_layer(out)\n\n        # Store useful variables in a dict to return them\n        data = {\n            \"z\": z,  # list of tensors with shape (batch, ch[i], h[i], w[i])\n            \"kl\": kl,  # list of tensors with shape (batch, )\n            \"kl_restricted\": kl_restricted,  # list of tensors with shape (batch, )\n            \"kl_spatial\": kl_spatial,  # list of tensors w shape (batch, h[i], w[i])\n            \"kl_channelwise\": kl_channelwise,  # list of tensors with shape (batch, ch[i])\n            # 'logprob_p': logprob_p,  # scalar, mean over batch\n            \"q_mu\": q_mu,\n            \"q_lv\": q_lv,\n            \"debug_qvar_max\": debug_qvar_max,\n        }\n        return out, data\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Forward pass through the LVAE model.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor of shape (B, C, H, W).\n        \"\"\"\n        img_size = x.size()[2:]\n\n        # Bottom-up inference: return list of length n_layers (bottom to top)\n        bu_values = self.bottomup_pass(x)\n        for i in range(0, self.skip_bottomk_buvalues):\n            bu_values[i] = None\n\n        if self._squish3d:\n            bu_values = [\n                torch.mean(self._3D_squisher[k](bu_value), dim=2)\n                for k, bu_value in enumerate(bu_values)\n            ]\n\n        # Top-down inference/generation\n        out, td_data = self.topdown_pass(bu_values)\n\n        if out.shape[-1] &gt; img_size[-1]:\n            # Restore original image size\n            out = crop_img_tensor(out, img_size)\n\n        out = self.output_layer(out)\n\n        return out, td_data\n\n    ### SET OF GETTERS\n    def get_padded_size(self, size):\n        \"\"\"\n        Returns the smallest size (H, W) of the image with actual size given\n        as input, such that H and W are powers of 2.\n        :param size: input size, tuple either (N, C, H, W) or (H, W)\n        :return: 2-tuple (H, W)\n        \"\"\"\n        # Make size argument into (heigth, width)\n        # assert len(size) in [2, 4, 5] # TODO commented out cuz it's weird\n        # We're only interested in the Y,X dimensions\n        size = size[-2:]\n\n        if self.multiscale_decoder_retain_spatial_dims is True:\n            # In this case, we can go much more deeper and so this is not required\n            # (in the way it is. ;). More work would be needed if this was to be correctly implemented )\n            return list(size)\n\n        # Overall downscale factor from input to top layer (power of 2)\n        dwnsc = self.overall_downscale_factor\n\n        # Output smallest powers of 2 that are larger than current sizes\n        padded_size = [((s - 1) // dwnsc + 1) * dwnsc for s in size]\n        # TODO Needed for pad/crop odd sizes. Move to dataset?\n        return padded_size\n\n    def get_latent_spatial_size(self, level_idx: int):\n        \"\"\"Level_idx: 0 is the bottommost layer, the highest resolution one.\"\"\"\n        actual_downsampling = level_idx + 1\n        dwnsc = 2**actual_downsampling\n        sz = self.get_padded_size(self.image_size)\n        h = sz[0] // dwnsc\n        w = sz[1] // dwnsc\n        assert h == w\n        return h\n\n    def get_top_prior_param_shape(self, n_imgs: int = 1):\n\n        # Compute the total downscaling performed in the Encoder\n        if self.multiscale_decoder_retain_spatial_dims is False:\n            dwnsc = self.overall_downscale_factor\n        else:\n            # LC allow the encoder latents to keep the same (H, W) size at different levels\n            actual_downsampling = self.n_layers + 1 - self._multiscale_count\n            dwnsc = 2**actual_downsampling\n\n        h = self.image_size[-2] // dwnsc\n        w = self.image_size[-1] // dwnsc\n        mu_logvar = self.z_dims[-1] * 2  # mu and logvar\n        top_layer_shape = (n_imgs, mu_logvar, h, w)\n        # TODO refactor!\n        if self._model_3D_depth &gt; 1 and self._decoder_mode_3D is True:\n            # TODO check if model_3D_depth is needed ?\n            top_layer_shape = (n_imgs, mu_logvar, self._model_3D_depth, h, w)\n        return top_layer_shape\n\n    def reset_for_inference(self, tile_size: tuple[int, int] | None = None):\n        \"\"\"Should be called if we want to predict for a different input/output size.\"\"\"\n        self.mode_pred = True\n        if tile_size is None:\n            tile_size = self.image_size\n        self.image_size = tile_size\n        for i in range(self.n_layers):\n            self.bottom_up_layers[i].output_expected_shape = (\n                ts // 2 ** (i + 1) for ts in tile_size\n            )\n            self.top_down_layers[i].latent_shape = tile_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.image_size","title":"<code>image_size = input_shape</code>  <code>instance-attribute</code>","text":"<p>Input image size. (Z, Y, X) or (Y, X) if the data is 2D.</p>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.bottomup_pass","title":"<code>bottomup_pass(inp)</code>","text":"<p>Wrapper of _bottomup_pass().</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def bottomup_pass(self, inp: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"Wrapper of _bottomup_pass().\"\"\"\n    # TODO Remove wrapper\n    return self._bottomup_pass(\n        inp,\n        self.first_bottom_up,\n        self.lowres_first_bottom_ups,\n        self.bottom_up_layers,\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_bottom_up_layers","title":"<code>create_bottom_up_layers(lowres_separate_branch)</code>","text":"<p>Method creates the stack of bottom-up layers of the Encoder.</p> <p>that are used to generate the so-called <code>bu_values</code>.</p> <p>NOTE:     If <code>self._multiscale_count &lt; self.n_layers</code>, then LC is done only in the first     <code>self._multiscale_count</code> bottom-up layers (starting from the bottom).</p> <p>Parameters:</p> Name Type Description Default <code>lowres_separate_branch</code> <code>bool</code> <p>Whether the residual block(s) used for encoding the low-res input are shared (<code>False</code>) or not (<code>True</code>) with the \"same-size\" residual block(s) in the <code>BottomUpLayer</code>'s primary flow.</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_bottom_up_layers(self, lowres_separate_branch: bool) -&gt; nn.ModuleList:\n    \"\"\"\n    Method creates the stack of bottom-up layers of the Encoder.\n\n    that are used to generate the so-called `bu_values`.\n\n    NOTE:\n        If `self._multiscale_count &lt; self.n_layers`, then LC is done only in the first\n        `self._multiscale_count` bottom-up layers (starting from the bottom).\n\n    Parameters\n    ----------\n    lowres_separate_branch: bool\n        Whether the residual block(s) used for encoding the low-res input are shared\n        (`False`) or not (`True`) with the \"same-size\" residual block(s) in the\n        `BottomUpLayer`'s primary flow.\n    \"\"\"\n    multiscale_lowres_size_factor = 1\n    nonlin = get_activation(self.nonlin)\n\n    bottom_up_layers = nn.ModuleList([])\n    for i in range(self.n_layers):\n        # Whether this is the top layer\n        is_top = i == self.n_layers - 1\n\n        # LC is applied only to the first (_multiscale_count - 1) bottom-up layers\n        layer_enable_multiscale = (\n            self.enable_multiscale and self._multiscale_count &gt; i + 1\n        )\n\n        # This factor determines the factor by which the low-resolution tensor is larger\n        # N.B. Only used if layer_enable_multiscale == True, so we updated it only in that case\n        multiscale_lowres_size_factor *= 1 + int(layer_enable_multiscale)\n\n        # TODO: check correctness of this\n        if self._multiscale_count &gt; 1:\n            output_expected_shape = (dim // 2 ** (i + 1) for dim in self.image_size)\n        else:\n            output_expected_shape = None\n\n        # Add bottom-up deterministic layer at level i.\n        # It's a sequence of residual blocks (BottomUpDeterministicResBlock), possibly with downsampling between them.\n        bottom_up_layers.append(\n            BottomUpLayer(\n                n_res_blocks=self.encoder_blocks_per_layer,\n                n_filters=self.encoder_n_filters,\n                downsampling_steps=self.downsample[i],\n                nonlin=nonlin,\n                conv_strides=self.encoder_conv_strides,\n                batchnorm=self.bottomup_batchnorm,\n                dropout=self.encoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.encoder_res_block_kernel,\n                gated=self.gated,\n                lowres_separate_branch=lowres_separate_branch,\n                enable_multiscale=self.enable_multiscale,  # TODO: shouldn't the arg be `layer_enable_multiscale` here?\n                multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n                multiscale_lowres_size_factor=multiscale_lowres_size_factor,\n                decoder_retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                output_expected_shape=output_expected_shape,\n            )\n        )\n\n    return bottom_up_layers\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_final_topdown_layer","title":"<code>create_final_topdown_layer(upsample)</code>","text":"<p>Create the final top-down layer of the Decoder.</p> <p>NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation instead of transposed convolution (like in other TD layers).</p> <p>Parameters:</p> Name Type Description Default <code>upsample</code> <code>bool</code> <p>Whether to upsample the input of the final top-down layer by bilinear interpolation with <code>scale_factor=2</code>.</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_final_topdown_layer(self, upsample: bool) -&gt; nn.Sequential:\n    \"\"\"Create the final top-down layer of the Decoder.\n\n    NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation\n    instead of transposed convolution (like in other TD layers).\n\n    Parameters\n    ----------\n    upsample: bool\n        Whether to upsample the input of the final top-down layer\n        by bilinear interpolation with `scale_factor=2`.\n    \"\"\"\n    # Final top-down layer\n    modules = list()\n\n    if upsample:\n        modules.append(Interpolate(scale=2))\n\n    for i in range(self.decoder_blocks_per_layer):\n        modules.append(\n            TopDownDeterministicResBlock(\n                c_in=self.decoder_n_filters,\n                c_out=self.decoder_n_filters,\n                nonlin=get_activation(self.nonlin),\n                conv_strides=self.decoder_conv_strides,\n                batchnorm=self.topdown_batchnorm,\n                dropout=self.decoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.decoder_res_block_kernel,\n                gated=self.gated,\n                conv2d_bias=self.topdown_conv2d_bias,\n            )\n        )\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_first_bottom_up","title":"<code>create_first_bottom_up(init_stride, num_res_blocks=1)</code>","text":"<p>Method creates the first bottom-up block of the Encoder.</p> <p>Its role is to perform a first image compression step. It is composed by a sequence of nn.Conv2d + non-linearity + BottomUpDeterministicResBlock (1 or more, default is 1).</p> <p>Parameters:</p> Name Type Description Default <code>init_stride</code> <code>int</code> <p>The stride used by the intial Conv2d block.</p> required <code>num_res_blocks</code> <code>int</code> <p>The number of BottomUpDeterministicResBlocks, default is 1.</p> <code>1</code> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_first_bottom_up(\n    self,\n    init_stride: int,\n    num_res_blocks: int = 1,\n) -&gt; nn.Sequential:\n    \"\"\"\n    Method creates the first bottom-up block of the Encoder.\n\n    Its role is to perform a first image compression step.\n    It is composed by a sequence of nn.Conv2d + non-linearity +\n    BottomUpDeterministicResBlock (1 or more, default is 1).\n\n    Parameters\n    ----------\n    init_stride: int\n        The stride used by the intial Conv2d block.\n    num_res_blocks: int, optional\n        The number of BottomUpDeterministicResBlocks, default is 1.\n    \"\"\"\n    # From what I got from Ashesh, Z should not be touched in any case.\n    nonlin = get_activation(self.nonlin)\n    conv_block = self.encoder_conv_op(\n        in_channels=self.color_ch,\n        out_channels=self.encoder_n_filters,\n        kernel_size=self.encoder_res_block_kernel,\n        padding=(\n            0\n            if self.encoder_res_block_skip_padding\n            else self.encoder_res_block_kernel // 2\n        ),\n        stride=init_stride,\n    )\n\n    modules = [conv_block, nonlin]\n\n    for _ in range(num_res_blocks):\n        modules.append(\n            BottomUpDeterministicResBlock(\n                conv_strides=self.encoder_conv_strides,\n                c_in=self.encoder_n_filters,\n                c_out=self.encoder_n_filters,\n                nonlin=nonlin,\n                downsample=False,\n                batchnorm=self.bottomup_batchnorm,\n                dropout=self.encoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.encoder_res_block_kernel,\n            )\n        )\n\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_top_down_layers","title":"<code>create_top_down_layers()</code>","text":"<p>Method creates the stack of top-down layers of the Decoder.</p> <p>In these layer the <code>bu</code>_values<code>from the Encoder are merged with the</code>p_params<code>from the previous layer of the Decoder to get</code>q_params<code>. Then, a stochastic layer generates a sample from the latent distribution with parameters</code>q_params<code>. Finally, this sample is fed through a TopDownDeterministicResBlock to compute the</code>p_params` for the layer below.</p> <p>NOTE 1:     The algorithm for generative inference approximately works as follows:         - p_params = output of top-down layer above         - bu = inferred bottom-up value at this layer         - q_params = merge(bu, p_params)         - z = stochastic_layer(q_params)         - (optional) get and merge skip connection from prev top-down layer         - top-down deterministic ResNet</p> <p>NOTE 2:     When doing unconditional generation, bu_value is not available. Hence the     merge layer is not used, and z is sampled directly from p_params.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_top_down_layers(self) -&gt; nn.ModuleList:\n    \"\"\"\n    Method creates the stack of top-down layers of the Decoder.\n\n    In these layer the `bu`_values` from the Encoder are merged with the `p_params` from the previous layer\n    of the Decoder to get `q_params`. Then, a stochastic layer generates a sample from the latent distribution\n    with parameters `q_params`. Finally, this sample is fed through a TopDownDeterministicResBlock to\n    compute the `p_params` for the layer below.\n\n    NOTE 1:\n        The algorithm for generative inference approximately works as follows:\n            - p_params = output of top-down layer above\n            - bu = inferred bottom-up value at this layer\n            - q_params = merge(bu, p_params)\n            - z = stochastic_layer(q_params)\n            - (optional) get and merge skip connection from prev top-down layer\n            - top-down deterministic ResNet\n\n    NOTE 2:\n        When doing unconditional generation, bu_value is not available. Hence the\n        merge layer is not used, and z is sampled directly from p_params.\n\n    \"\"\"\n    top_down_layers = nn.ModuleList([])\n    nonlin = get_activation(self.nonlin)\n    # NOTE: top-down layers are created starting from the bottom-most\n    for i in range(self.n_layers):\n        # Check if this is the top layer\n        is_top = i == self.n_layers - 1\n\n        if self._enable_topdown_normalize_factor:  # TODO: What is this?\n            normalize_latent_factor = (\n                1 / np.sqrt(2 * (1 + i)) if len(self.z_dims) &gt; 4 else 1.0\n            )\n        else:\n            normalize_latent_factor = 1.0\n\n        top_down_layers.append(\n            TopDownLayer(\n                z_dim=self.z_dims[i],\n                n_res_blocks=self.decoder_blocks_per_layer,\n                n_filters=self.decoder_n_filters,\n                is_top_layer=is_top,\n                conv_strides=self.decoder_conv_strides,\n                upsampling_steps=self.downsample[i],\n                nonlin=nonlin,\n                merge_type=self.merge_type,\n                batchnorm=self.topdown_batchnorm,\n                dropout=self.decoder_dropout,\n                stochastic_skip=self.stochastic_skip,\n                learn_top_prior=self.learn_top_prior,\n                top_prior_param_shape=self.get_top_prior_param_shape(),\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.decoder_res_block_kernel,\n                gated=self.gated,\n                analytical_kl=self.analytical_kl,\n                vanilla_latent_hw=self.get_latent_spatial_size(i),\n                retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                input_image_shape=self.image_size,\n                normalize_latent_factor=normalize_latent_factor,\n                conv2d_bias=self.topdown_conv2d_bias,\n                stochastic_use_naive_exponential=self._stochastic_use_naive_exponential,\n            )\n        )\n    return top_down_layers\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the LVAE model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (B, C, H, W).</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Forward pass through the LVAE model.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor of shape (B, C, H, W).\n    \"\"\"\n    img_size = x.size()[2:]\n\n    # Bottom-up inference: return list of length n_layers (bottom to top)\n    bu_values = self.bottomup_pass(x)\n    for i in range(0, self.skip_bottomk_buvalues):\n        bu_values[i] = None\n\n    if self._squish3d:\n        bu_values = [\n            torch.mean(self._3D_squisher[k](bu_value), dim=2)\n            for k, bu_value in enumerate(bu_values)\n        ]\n\n    # Top-down inference/generation\n    out, td_data = self.topdown_pass(bu_values)\n\n    if out.shape[-1] &gt; img_size[-1]:\n        # Restore original image size\n        out = crop_img_tensor(out, img_size)\n\n    out = self.output_layer(out)\n\n    return out, td_data\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.get_latent_spatial_size","title":"<code>get_latent_spatial_size(level_idx)</code>","text":"<p>Level_idx: 0 is the bottommost layer, the highest resolution one.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def get_latent_spatial_size(self, level_idx: int):\n    \"\"\"Level_idx: 0 is the bottommost layer, the highest resolution one.\"\"\"\n    actual_downsampling = level_idx + 1\n    dwnsc = 2**actual_downsampling\n    sz = self.get_padded_size(self.image_size)\n    h = sz[0] // dwnsc\n    w = sz[1] // dwnsc\n    assert h == w\n    return h\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.get_padded_size","title":"<code>get_padded_size(size)</code>","text":"<p>Returns the smallest size (H, W) of the image with actual size given as input, such that H and W are powers of 2. :param size: input size, tuple either (N, C, H, W) or (H, W) :return: 2-tuple (H, W)</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def get_padded_size(self, size):\n    \"\"\"\n    Returns the smallest size (H, W) of the image with actual size given\n    as input, such that H and W are powers of 2.\n    :param size: input size, tuple either (N, C, H, W) or (H, W)\n    :return: 2-tuple (H, W)\n    \"\"\"\n    # Make size argument into (heigth, width)\n    # assert len(size) in [2, 4, 5] # TODO commented out cuz it's weird\n    # We're only interested in the Y,X dimensions\n    size = size[-2:]\n\n    if self.multiscale_decoder_retain_spatial_dims is True:\n        # In this case, we can go much more deeper and so this is not required\n        # (in the way it is. ;). More work would be needed if this was to be correctly implemented )\n        return list(size)\n\n    # Overall downscale factor from input to top layer (power of 2)\n    dwnsc = self.overall_downscale_factor\n\n    # Output smallest powers of 2 that are larger than current sizes\n    padded_size = [((s - 1) // dwnsc + 1) * dwnsc for s in size]\n    # TODO Needed for pad/crop odd sizes. Move to dataset?\n    return padded_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.reset_for_inference","title":"<code>reset_for_inference(tile_size=None)</code>","text":"<p>Should be called if we want to predict for a different input/output size.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def reset_for_inference(self, tile_size: tuple[int, int] | None = None):\n    \"\"\"Should be called if we want to predict for a different input/output size.\"\"\"\n    self.mode_pred = True\n    if tile_size is None:\n        tile_size = self.image_size\n    self.image_size = tile_size\n    for i in range(self.n_layers):\n        self.bottom_up_layers[i].output_expected_shape = (\n            ts // 2 ** (i + 1) for ts in tile_size\n        )\n        self.top_down_layers[i].latent_shape = tile_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.topdown_pass","title":"<code>topdown_pass(bu_values=None, n_img_prior=None, constant_layers=None, forced_latent=None, top_down_layers=None, final_top_down_layer=None)</code>","text":"<p>Method defines the forward pass through the LVAE Decoder, the so-called.</p> <p>Top-Down pass.</p> <p>Parameters:</p> Name Type Description Default <code>bu_values</code> <code>Union[Tensor, None]</code> <p>Output of the bottom-up pass. It will have values from multiple layers of the ladder.</p> <code>None</code> <code>n_img_prior</code> <code>Union[Tensor, None]</code> <p>When <code>bu_values</code> is <code>None</code>, <code>n_img_prior</code> indicates the number of images to generate from the prior (so bottom-up pass is not used at all here).</p> <code>None</code> <code>constant_layers</code> <code>Union[Iterable[int], None]</code> <p>A sequence of indexes associated to the layers in which a single instance's z is copied over the entire batch (bottom-up path is not used, so only prior is used here). Set to <code>None</code> to avoid this behaviour.</p> <code>None</code> <code>forced_latent</code> <code>Union[list[Tensor], None]</code> <p>A list of tensors that are used as fixed latent variables (hence, sampling doesn't take place in this case).</p> <code>None</code> <code>top_down_layers</code> <code>Union[ModuleList, None]</code> <p>A list of top-down layers to use in the top-down pass. If <code>None</code>, the method uses the default layers defined in the constructor.</p> <code>None</code> <code>final_top_down_layer</code> <code>Union[Sequential, None]</code> <p>The last top-down layer of the top-down pass. If <code>None</code>, the method uses the default layers defined in the constructor.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def topdown_pass(\n    self,\n    bu_values: Union[torch.Tensor, None] = None,\n    n_img_prior: Union[torch.Tensor, None] = None,\n    constant_layers: Union[Iterable[int], None] = None,\n    forced_latent: Union[list[torch.Tensor], None] = None,\n    top_down_layers: Union[nn.ModuleList, None] = None,\n    final_top_down_layer: Union[nn.Sequential, None] = None,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Method defines the forward pass through the LVAE Decoder, the so-called.\n\n    Top-Down pass.\n\n    Parameters\n    ----------\n    bu_values: torch.Tensor, optional\n        Output of the bottom-up pass. It will have values from multiple layers of\n        the ladder.\n    n_img_prior: optional\n        When `bu_values` is `None`, `n_img_prior` indicates the number of images to\n        generate\n        from the prior (so bottom-up pass is not used at all here).\n    constant_layers: Iterable[int], optional\n        A sequence of indexes associated to the layers in which a single instance's\n        z is copied over the entire batch (bottom-up path is not used, so only prior\n        is used here). Set to `None` to avoid this behaviour.\n    forced_latent: list[torch.Tensor], optional\n        A list of tensors that are used as fixed latent variables (hence, sampling\n        doesn't take place in this case).\n    top_down_layers: nn.ModuleList, optional\n        A list of top-down layers to use in the top-down pass. If `None`, the method\n        uses the default layers defined in the constructor.\n    final_top_down_layer: nn.Sequential, optional\n        The last top-down layer of the top-down pass. If `None`, the method uses the\n        default layers defined in the constructor.\n    \"\"\"\n    if top_down_layers is None:\n        top_down_layers = self.top_down_layers\n    if final_top_down_layer is None:\n        final_top_down_layer = self.final_top_down\n\n    # Default: no layer is sampled from the distribution's mode\n    if constant_layers is None:\n        constant_layers = []\n    prior_experiment = len(constant_layers) &gt; 0\n\n    # If the bottom-up inference values are not given, don't do\n    # inference, sample from prior instead\n    inference_mode = bu_values is not None\n\n    # Check consistency of arguments\n    if inference_mode != (n_img_prior is None):\n        msg = (\n            \"Number of images for top-down generation has to be given \"\n            \"if and only if we're not doing inference\"\n        )\n        raise RuntimeError(msg)\n    if inference_mode and prior_experiment:\n        msg = (\n            \"Prior experiments (e.g. sampling from mode) are not\"\n            \" compatible with inference mode\"\n        )\n        raise RuntimeError(msg)\n\n    # Sampled latent variables at each layer\n    z = [None] * self.n_layers\n    # KL divergence of each layer\n    kl = [None] * self.n_layers\n    # Kl divergence restricted, only for the LC enabled setup denoiSplit.\n    kl_restricted = [None] * self.n_layers\n    # mean from which z is sampled.\n    q_mu = [None] * self.n_layers\n    # log(var) from which z is sampled.\n    q_lv = [None] * self.n_layers\n    # Spatial map of KL divergence for each layer\n    kl_spatial = [None] * self.n_layers\n    debug_qvar_max = [None] * self.n_layers\n    kl_channelwise = [None] * self.n_layers\n    if forced_latent is None:\n        forced_latent = [None] * self.n_layers\n\n    # Top-down inference/generation loop\n    out = None\n    for i in reversed(range(self.n_layers)):\n        # If available, get deterministic node from bottom-up inference\n        try:\n            bu_value = bu_values[i]\n        except TypeError:\n            bu_value = None\n\n        # Whether the current layer should be sampled from the mode\n        constant_out = i in constant_layers\n\n        # Input for skip connection\n        skip_input = out\n\n        # Full top-down layer, including sampling and deterministic part\n        out, aux = top_down_layers[i](\n            input_=out,\n            skip_connection_input=skip_input,\n            inference_mode=inference_mode,\n            bu_value=bu_value,\n            n_img_prior=n_img_prior,\n            force_constant_output=constant_out,\n            forced_latent=forced_latent[i],\n            mode_pred=self.mode_pred,\n            var_clip_max=self._var_clip_max,\n        )\n        # Save useful variables\n        z[i] = aux[\"z\"]  # sampled variable at this layer (batch, ch, h, w)\n        kl[i] = aux[\"kl_samplewise\"]  # (batch, )\n        kl_restricted[i] = aux[\"kl_samplewise_restricted\"]\n        kl_spatial[i] = aux[\"kl_spatial\"]  # (batch, h, w)\n        q_mu[i] = aux[\"q_mu\"]\n        q_lv[i] = aux[\"q_lv\"]\n\n        kl_channelwise[i] = aux[\"kl_channelwise\"]\n        debug_qvar_max[i] = aux[\"qvar_max\"]\n        # if self.mode_pred is False:\n        #     logprob_p += aux['logprob_p'].mean()  # mean over batch\n        # else:\n        #     logprob_p = None\n\n    # Final top-down layer\n    out = final_top_down_layer(out)\n\n    # Store useful variables in a dict to return them\n    data = {\n        \"z\": z,  # list of tensors with shape (batch, ch[i], h[i], w[i])\n        \"kl\": kl,  # list of tensors with shape (batch, )\n        \"kl_restricted\": kl_restricted,  # list of tensors with shape (batch, )\n        \"kl_spatial\": kl_spatial,  # list of tensors w shape (batch, h[i], w[i])\n        \"kl_channelwise\": kl_channelwise,  # list of tensors with shape (batch, ch[i])\n        # 'logprob_p': logprob_p,  # scalar, mean over batch\n        \"q_mu\": q_mu,\n        \"q_lv\": q_lv,\n        \"debug_qvar_max\": debug_qvar_max,\n    }\n    return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/","title":"noise_models","text":""},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel","title":"<code>GaussianMixtureNoiseModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Define a noise model parameterized as a mixture of gaussians.</p> <p>If <code>config.path</code> is not provided a new object is initialized from scratch. Otherwise, a model is loaded from <code>config.path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GaussianMixtureNMConfig</code> <p>A <code>pydantic</code> model that defines the configuration of the GMM noise model.</p> required <p>Attributes:</p> Name Type Description <code>min_signal</code> <code>float</code> <p>Minimum signal intensity expected in the image.</p> <code>max_signal</code> <code>float</code> <p>Maximum signal intensity expected in the image.</p> <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory where the trained noise model (*.npz) is saved in the <code>train</code> method.</p> <code>weight</code> <code>Parameter</code> <p>A [3*n_gaussian, n_coeff] sized array containing the values of the weights describing the GMM noise model, with each row corresponding to one parameter of each gaussian, namely [mean, standard deviation and weight]. Specifically, rows are organized as follows: - first n_gaussian rows correspond to the means - next n_gaussian rows correspond to the weights - last n_gaussian rows correspond to the standard deviations If <code>weight=None</code>, the weight array is initialized using the <code>min_signal</code> and <code>max_signal</code> parameters.</p> <code>n_gaussian</code> <code>int</code> <p>Number of gaussians in the mixture.</p> <code>n_coeff</code> <code>int</code> <p>Number of coefficients to describe the functional relationship between gaussian parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic relationship and so on.</p> <code>device</code> <code>device</code> <p>GPU device.</p> <code>min_sigma</code> <code>float</code> <p>All values of <code>standard deviation</code> below this are clamped to this value.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>class GaussianMixtureNoiseModel(nn.Module):\n    \"\"\"Define a noise model parameterized as a mixture of gaussians.\n\n    If `config.path` is not provided a new object is initialized from scratch.\n    Otherwise, a model is loaded from `config.path`.\n\n    Parameters\n    ----------\n    config : GaussianMixtureNMConfig\n        A `pydantic` model that defines the configuration of the GMM noise model.\n\n    Attributes\n    ----------\n    min_signal : float\n        Minimum signal intensity expected in the image.\n    max_signal : float\n        Maximum signal intensity expected in the image.\n    path: Union[str, Path]\n        Path to the directory where the trained noise model (*.npz) is saved in the `train` method.\n    weight : torch.nn.Parameter\n        A [3*n_gaussian, n_coeff] sized array containing the values of the weights\n        describing the GMM noise model, with each row corresponding to one\n        parameter of each gaussian, namely [mean, standard deviation and weight].\n        Specifically, rows are organized as follows:\n        - first n_gaussian rows correspond to the means\n        - next n_gaussian rows correspond to the weights\n        - last n_gaussian rows correspond to the standard deviations\n        If `weight=None`, the weight array is initialized using the `min_signal`\n        and `max_signal` parameters.\n    n_gaussian: int\n        Number of gaussians in the mixture.\n    n_coeff: int\n        Number of coefficients to describe the functional relationship between gaussian\n        parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic\n        relationship and so on.\n    device: device\n        GPU device.\n    min_sigma: float\n        All values of `standard deviation` below this are clamped to this value.\n    \"\"\"\n\n    # TODO training a NM relies on getting a clean data(N2V e.g,)\n    def __init__(self, config: GaussianMixtureNMConfig) -&gt; None:\n        super().__init__()\n        self.device = torch.device(\"cpu\")\n\n        if config.path is not None:\n            params = np.load(config.path)\n        else:\n            params = config.model_dump(exclude_none=True)\n\n        min_sigma = torch.tensor(params[\"min_sigma\"])\n        min_signal = torch.tensor(params[\"min_signal\"])\n        max_signal = torch.tensor(params[\"max_signal\"])\n        self.register_buffer(\"min_signal\", min_signal)\n        self.register_buffer(\"max_signal\", max_signal)\n        self.register_buffer(\"min_sigma\", min_sigma)\n        self.register_buffer(\"tolerance\", torch.tensor([1e-10]))\n\n        if \"trained_weight\" in params:\n            weight = torch.tensor(params[\"trained_weight\"])\n        elif \"weight\" in params and params[\"weight\"] is not None:\n            weight = torch.tensor(params[\"weight\"])\n        else:\n            weight = self._initialize_weights(\n                params[\"n_gaussian\"], params[\"n_coeff\"], max_signal, min_signal\n            )\n\n        self.n_gaussian = weight.shape[0] // 3\n        self.n_coeff = weight.shape[1]\n\n        self.register_parameter(\"weight\", nn.Parameter(weight))\n        self._set_model_mode(mode=\"prediction\")\n\n        print(f\"[{self.__class__.__name__}] min_sigma: {self.min_sigma}\")\n\n    def _initialize_weights(\n        self,\n        n_gaussian: int,\n        n_coeff: int,\n        max_signal: torch.Tensor,\n        min_signal: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create random weight initialization.\"\"\"\n        weight = torch.randn(n_gaussian * 3, n_coeff)\n        weight[n_gaussian : 2 * n_gaussian, 1] = torch.log(\n            max_signal - min_signal\n        ).float()\n        return weight\n\n    def to_device(self, device: torch.device):\n        self.device = device\n        self.to(device)\n\n    def _set_model_mode(self, mode: str) -&gt; None:\n        \"\"\"Move parameters to the device and set weights' requires_grad depending on the mode\"\"\"\n        if mode == \"train\":\n            self.weight.requires_grad = True\n        else:\n            self.weight.requires_grad = False\n\n    def polynomial_regressor(\n        self, weight_params: torch.Tensor, signals: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Combines `weight_params` and signal `signals` to regress for the gaussian parameter values.\n\n        Parameters\n        ----------\n        weight_params : Tensor\n            Corresponds to specific rows of the `self.weight`\n\n        signals : Tensor\n            Signals\n\n        Returns\n        -------\n        value : Tensor\n            Corresponds to either of mean, standard deviation or weight, evaluated at `signals`\n        \"\"\"\n        value = torch.zeros_like(signals)\n        for i in range(weight_params.shape[0]):\n            value += weight_params[i] * (\n                ((signals - self.min_signal) / (self.max_signal - self.min_signal)) ** i\n            )\n        return value\n\n    def normal_density(\n        self, x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the normal probability density at `x` given the mean `mean` and standard deviation `std`.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The ground-truth tensor. Shape is (batch, 1, dim1, dim2).\n        mean: torch.Tensor\n            The inferred mean of distribution. Shape is (batch, 1, dim1, dim2).\n        std: torch.Tensor\n            The inferred standard deviation of distribution. Shape is (batch, 1, dim1, dim2).\n\n        Returns\n        -------\n        tmp: torch.Tensor\n            Normal probability density of `x` given `mean` and `std`\n        \"\"\"\n        tmp = -((x - mean) ** 2)\n        tmp = tmp / (2.0 * std * std)\n        tmp = torch.exp(tmp)\n        tmp = tmp / torch.sqrt((2.0 * np.pi) * std * std)\n        return tmp\n\n    def likelihood(\n        self, observations: torch.Tensor, signals: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.\n\n        Parameters\n        ----------\n        observations : Tensor\n            Noisy observations. Shape is (batch, 1, dim1, dim2).\n        signals : Tensor\n            Underlying signals. Shape is (batch, 1, dim1, dim2).\n\n        Returns\n        -------\n        value: torch.Tensor:\n            Likelihood of observations given the signals and the GMM noise model\n        \"\"\"\n        gaussian_parameters: list[torch.Tensor] = self.get_gaussian_parameters(signals)\n        p = torch.zeros_like(observations)\n        for gaussian in range(self.n_gaussian):\n            # Ensure all tensors have compatible shapes\n            mean = gaussian_parameters[gaussian]\n            std = gaussian_parameters[self.n_gaussian + gaussian]\n            weight = gaussian_parameters[2 * self.n_gaussian + gaussian]\n\n            # Compute normal density\n            p += (\n                self.normal_density(\n                    observations,\n                    mean,\n                    std,\n                )\n                * weight\n            )\n        return p + self.tolerance\n\n    def get_gaussian_parameters(self, signals: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Returns the noise model for given signals\n\n        Parameters\n        ----------\n        signals : Tensor\n            Underlying signals\n\n        Returns\n        -------\n        noise_model: list of Tensor\n            Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n        \"\"\"\n        noise_model = []\n        mu = []\n        sigma = []\n        alpha = []\n        kernels = self.weight.shape[0] // 3\n        for num in range(kernels):\n            mu.append(self.polynomial_regressor(self.weight[num, :], signals))\n            expval = torch.exp(self.weight[kernels + num, :])\n            sigma_temp = self.polynomial_regressor(expval, signals)\n            sigma_temp = torch.clamp(sigma_temp, min=self.min_sigma)\n            sigma.append(torch.sqrt(sigma_temp))\n\n            expval = torch.exp(\n                self.polynomial_regressor(self.weight[2 * kernels + num, :], signals)\n                + self.tolerance\n            )\n            alpha.append(expval)\n\n        sum_alpha = 0\n        for al in range(kernels):\n            sum_alpha = alpha[al] + sum_alpha\n\n        # sum of alpha is forced to be 1.\n        for ker in range(kernels):\n            alpha[ker] = alpha[ker] / sum_alpha\n\n        sum_means = 0\n        # sum_means is the alpha weighted average of the means\n        for ker in range(kernels):\n            sum_means = alpha[ker] * mu[ker] + sum_means\n\n        # subtracting the alpha weighted average of the means from the means\n        # ensures that the GMM has the inclination to have the mean=signals.\n        # its like a residual conection. I don't understand why we need to learn the mean?\n        for ker in range(kernels):\n            mu[ker] = mu[ker] - sum_means + signals\n\n        for i in range(kernels):\n            noise_model.append(mu[i])\n        for j in range(kernels):\n            noise_model.append(sigma[j])\n        for k in range(kernels):\n            noise_model.append(alpha[k])\n\n        return noise_model\n\n    @staticmethod\n    def _fast_shuffle(series: torch.Tensor, num: int) -&gt; torch.Tensor:\n        \"\"\"Shuffle the inputs randomly num times\"\"\"\n        length = series.shape[0]\n        for _ in range(num):\n            idx = torch.randperm(length)\n            series = series[idx, :]\n        return series\n\n    def get_signal_observation_pairs(\n        self,\n        signal: NDArray,\n        observation: NDArray,\n        lower_clip: float,\n        upper_clip: float,\n    ) -&gt; torch.Tensor:\n        \"\"\"Returns the Signal-Observation pixel intensities as a two-column array\n\n        Parameters\n        ----------\n        signal : numpy array\n            Clean Signal Data\n        observation: numpy array\n            Noisy observation Data\n        lower_clip: float\n            Lower percentile bound for clipping.\n        upper_clip: float\n            Upper percentile bound for clipping.\n\n        Returns\n        -------\n        noise_model: list of torch floats\n            Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n        \"\"\"\n        lb = np.percentile(signal, lower_clip)\n        ub = np.percentile(signal, upper_clip)\n        stepsize = observation[0].size\n        n_observations = observation.shape[0]\n        n_signals = signal.shape[0]\n        sig_obs_pairs = np.zeros((n_observations * stepsize, 2))\n\n        for i in range(n_observations):\n            j = i // (n_observations // n_signals)\n            sig_obs_pairs[stepsize * i : stepsize * (i + 1), 0] = signal[j].ravel()\n            sig_obs_pairs[stepsize * i : stepsize * (i + 1), 1] = observation[i].ravel()\n        sig_obs_pairs = sig_obs_pairs[\n            (sig_obs_pairs[:, 0] &gt; lb) &amp; (sig_obs_pairs[:, 0] &lt; ub)\n        ]\n        sig_obs_pairs = sig_obs_pairs.astype(np.float32)\n        sig_obs_pairs = torch.from_numpy(sig_obs_pairs)\n        return self._fast_shuffle(sig_obs_pairs, 2)\n\n    def fit(\n        self,\n        signal: NDArray,\n        observation: NDArray,\n        learning_rate: float = 1e-1,\n        batch_size: int = 250000,\n        n_epochs: int = 2000,\n        lower_clip: float = 0.0,\n        upper_clip: float = 100.0,\n    ) -&gt; list[float]:\n        \"\"\"Training to learn the noise model from signal - observation pairs.\n\n        Parameters\n        ----------\n        signal: numpy array\n            Clean Signal Data\n        observation: numpy array\n            Noisy Observation Data\n        learning_rate: float\n            Learning rate. Default = 1e-1.\n        batch_size: int\n            Nini-batch size. Default = 250000.\n        n_epochs: int\n            Number of epochs. Default = 2000.\n        lower_clip : int\n            Lower percentile for clipping. Default is 0.\n        upper_clip : int\n            Upper percentile for clipping. Default is 100.\n        \"\"\"\n        self._set_model_mode(mode=\"train\")\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to_device(device)\n        optimizer = torch.optim.Adam([self.weight], lr=learning_rate)\n\n        sig_obs_pairs = self.get_signal_observation_pairs(\n            signal, observation, lower_clip, upper_clip\n        )\n\n        train_losses = []\n        counter = 0\n        for t in range(n_epochs):\n            if (counter + 1) * batch_size &gt;= sig_obs_pairs.shape[0]:\n                counter = 0\n                sig_obs_pairs = self._fast_shuffle(sig_obs_pairs, 1)\n\n            batch_vectors = sig_obs_pairs[\n                counter * batch_size : (counter + 1) * batch_size, :\n            ]\n            observations = batch_vectors[:, 1].to(self.device)\n            signals = batch_vectors[:, 0].to(self.device)\n\n            p = self.likelihood(observations, signals)\n\n            joint_loss = torch.mean(-torch.log(p))\n            train_losses.append(joint_loss.item())\n\n            if self.weight.isnan().any() or self.weight.isinf().any():\n                print(\n                    \"NaN or Inf detected in the weights. Aborting training at epoch: \",\n                    t,\n                )\n                break\n\n            if t % 100 == 0:\n                last_losses = train_losses[-100:]\n                print(t, np.mean(last_losses))\n\n            optimizer.zero_grad()\n            joint_loss.backward()\n            optimizer.step()\n            counter += 1\n\n        self._set_model_mode(mode=\"prediction\")\n        self.to_device(torch.device(\"cpu\"))\n        print(\"===================\\n\")\n        return train_losses\n\n    def sample_observation_from_signal(self, signal: NDArray) -&gt; NDArray:\n        \"\"\"\n        Sample an instance of observation based on an input signal using a\n        learned Gaussian Mixture Model. For each pixel in the input signal,\n        samples a corresponding noisy pixel.\n\n        Parameters\n        ----------\n        signal: numpy array\n            Clean 2D signal data.\n\n        Returns\n        -------\n        observation: numpy array\n            An instance of noisy observation data based on the input signal.\n        \"\"\"\n        assert len(signal.shape) == 2, \"Only 2D inputs are supported.\"\n\n        signal_tensor = torch.from_numpy(signal).to(torch.float32)\n        height, width = signal_tensor.shape\n\n        with torch.no_grad():\n            # Get gaussian parameters for each pixel\n            gaussian_params = self.get_gaussian_parameters(signal_tensor)\n            means = np.array(gaussian_params[: self.n_gaussian])\n            stds = np.array(gaussian_params[self.n_gaussian : self.n_gaussian * 2])\n            alphas = np.array(gaussian_params[self.n_gaussian * 2 :])\n\n            if self.n_gaussian == 1:\n                # Single gaussian case\n                observation = np.random.normal(\n                    loc=means[0], scale=stds[0], size=(height, width)\n                )\n            else:\n                # Multiple gaussians: sample component for each pixel\n                uniform = np.random.rand(1, height, width)\n                # Compute cumulative probabilities for component selection\n                cumulative_alphas = np.cumsum(\n                    alphas, axis=0\n                )  # Shape: (n_gaussian, height, width)\n                selected_component = np.argmax(\n                    uniform &lt; cumulative_alphas, axis=0, keepdims=True\n                )\n\n                # For every pixel, choose the corresponding gaussian\n                # and get the learned mu and sigma\n                selected_mus = np.take_along_axis(means, selected_component, axis=0)\n                selected_stds = np.take_along_axis(stds, selected_component, axis=0)\n                selected_mus = selected_mus.squeeze(0)\n                selected_stds = selected_stds.squeeze(0)\n\n                # Sample from the normal distribution with learned mu and sigma\n                observation = np.random.normal(\n                    selected_mus, selected_stds, size=(height, width)\n                )\n        return observation\n\n    def save(self, path: str, name: str) -&gt; None:\n        \"\"\"Save the trained parameters on the noise model.\n\n        Parameters\n        ----------\n        path : str\n            Path to save the trained parameters.\n        name : str\n            File name to save the trained parameters.\n        \"\"\"\n        os.makedirs(path, exist_ok=True)\n        np.savez(\n            os.path.join(path, name),\n            trained_weight=self.weight.numpy(),\n            min_signal=self.min_signal.numpy(),\n            max_signal=self.max_signal.numpy(),\n            min_sigma=self.min_sigma,\n        )\n        print(\"The trained parameters (\" + name + \") is saved at location: \" + path)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.fit","title":"<code>fit(signal, observation, learning_rate=0.1, batch_size=250000, n_epochs=2000, lower_clip=0.0, upper_clip=100.0)</code>","text":"<p>Training to learn the noise model from signal - observation pairs.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>NDArray</code> <p>Clean Signal Data</p> required <code>observation</code> <code>NDArray</code> <p>Noisy Observation Data</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate. Default = 1e-1.</p> <code>0.1</code> <code>batch_size</code> <code>int</code> <p>Nini-batch size. Default = 250000.</p> <code>250000</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs. Default = 2000.</p> <code>2000</code> <code>lower_clip</code> <code>int</code> <p>Lower percentile for clipping. Default is 0.</p> <code>0.0</code> <code>upper_clip</code> <code>int</code> <p>Upper percentile for clipping. Default is 100.</p> <code>100.0</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def fit(\n    self,\n    signal: NDArray,\n    observation: NDArray,\n    learning_rate: float = 1e-1,\n    batch_size: int = 250000,\n    n_epochs: int = 2000,\n    lower_clip: float = 0.0,\n    upper_clip: float = 100.0,\n) -&gt; list[float]:\n    \"\"\"Training to learn the noise model from signal - observation pairs.\n\n    Parameters\n    ----------\n    signal: numpy array\n        Clean Signal Data\n    observation: numpy array\n        Noisy Observation Data\n    learning_rate: float\n        Learning rate. Default = 1e-1.\n    batch_size: int\n        Nini-batch size. Default = 250000.\n    n_epochs: int\n        Number of epochs. Default = 2000.\n    lower_clip : int\n        Lower percentile for clipping. Default is 0.\n    upper_clip : int\n        Upper percentile for clipping. Default is 100.\n    \"\"\"\n    self._set_model_mode(mode=\"train\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.to_device(device)\n    optimizer = torch.optim.Adam([self.weight], lr=learning_rate)\n\n    sig_obs_pairs = self.get_signal_observation_pairs(\n        signal, observation, lower_clip, upper_clip\n    )\n\n    train_losses = []\n    counter = 0\n    for t in range(n_epochs):\n        if (counter + 1) * batch_size &gt;= sig_obs_pairs.shape[0]:\n            counter = 0\n            sig_obs_pairs = self._fast_shuffle(sig_obs_pairs, 1)\n\n        batch_vectors = sig_obs_pairs[\n            counter * batch_size : (counter + 1) * batch_size, :\n        ]\n        observations = batch_vectors[:, 1].to(self.device)\n        signals = batch_vectors[:, 0].to(self.device)\n\n        p = self.likelihood(observations, signals)\n\n        joint_loss = torch.mean(-torch.log(p))\n        train_losses.append(joint_loss.item())\n\n        if self.weight.isnan().any() or self.weight.isinf().any():\n            print(\n                \"NaN or Inf detected in the weights. Aborting training at epoch: \",\n                t,\n            )\n            break\n\n        if t % 100 == 0:\n            last_losses = train_losses[-100:]\n            print(t, np.mean(last_losses))\n\n        optimizer.zero_grad()\n        joint_loss.backward()\n        optimizer.step()\n        counter += 1\n\n    self._set_model_mode(mode=\"prediction\")\n    self.to_device(torch.device(\"cpu\"))\n    print(\"===================\\n\")\n    return train_losses\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.get_gaussian_parameters","title":"<code>get_gaussian_parameters(signals)</code>","text":"<p>Returns the noise model for given signals</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Tensor</code> <p>Underlying signals</p> required <p>Returns:</p> Name Type Description <code>noise_model</code> <code>list of Tensor</code> <p>Contains a list of <code>mu</code>, <code>sigma</code> and <code>alpha</code> for the <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def get_gaussian_parameters(self, signals: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Returns the noise model for given signals\n\n    Parameters\n    ----------\n    signals : Tensor\n        Underlying signals\n\n    Returns\n    -------\n    noise_model: list of Tensor\n        Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n    \"\"\"\n    noise_model = []\n    mu = []\n    sigma = []\n    alpha = []\n    kernels = self.weight.shape[0] // 3\n    for num in range(kernels):\n        mu.append(self.polynomial_regressor(self.weight[num, :], signals))\n        expval = torch.exp(self.weight[kernels + num, :])\n        sigma_temp = self.polynomial_regressor(expval, signals)\n        sigma_temp = torch.clamp(sigma_temp, min=self.min_sigma)\n        sigma.append(torch.sqrt(sigma_temp))\n\n        expval = torch.exp(\n            self.polynomial_regressor(self.weight[2 * kernels + num, :], signals)\n            + self.tolerance\n        )\n        alpha.append(expval)\n\n    sum_alpha = 0\n    for al in range(kernels):\n        sum_alpha = alpha[al] + sum_alpha\n\n    # sum of alpha is forced to be 1.\n    for ker in range(kernels):\n        alpha[ker] = alpha[ker] / sum_alpha\n\n    sum_means = 0\n    # sum_means is the alpha weighted average of the means\n    for ker in range(kernels):\n        sum_means = alpha[ker] * mu[ker] + sum_means\n\n    # subtracting the alpha weighted average of the means from the means\n    # ensures that the GMM has the inclination to have the mean=signals.\n    # its like a residual conection. I don't understand why we need to learn the mean?\n    for ker in range(kernels):\n        mu[ker] = mu[ker] - sum_means + signals\n\n    for i in range(kernels):\n        noise_model.append(mu[i])\n    for j in range(kernels):\n        noise_model.append(sigma[j])\n    for k in range(kernels):\n        noise_model.append(alpha[k])\n\n    return noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.get_signal_observation_pairs","title":"<code>get_signal_observation_pairs(signal, observation, lower_clip, upper_clip)</code>","text":"<p>Returns the Signal-Observation pixel intensities as a two-column array</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>numpy array</code> <p>Clean Signal Data</p> required <code>observation</code> <code>NDArray</code> <p>Noisy observation Data</p> required <code>lower_clip</code> <code>float</code> <p>Lower percentile bound for clipping.</p> required <code>upper_clip</code> <code>float</code> <p>Upper percentile bound for clipping.</p> required <p>Returns:</p> Name Type Description <code>noise_model</code> <code>list of torch floats</code> <p>Contains a list of <code>mu</code>, <code>sigma</code> and <code>alpha</code> for the <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def get_signal_observation_pairs(\n    self,\n    signal: NDArray,\n    observation: NDArray,\n    lower_clip: float,\n    upper_clip: float,\n) -&gt; torch.Tensor:\n    \"\"\"Returns the Signal-Observation pixel intensities as a two-column array\n\n    Parameters\n    ----------\n    signal : numpy array\n        Clean Signal Data\n    observation: numpy array\n        Noisy observation Data\n    lower_clip: float\n        Lower percentile bound for clipping.\n    upper_clip: float\n        Upper percentile bound for clipping.\n\n    Returns\n    -------\n    noise_model: list of torch floats\n        Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n    \"\"\"\n    lb = np.percentile(signal, lower_clip)\n    ub = np.percentile(signal, upper_clip)\n    stepsize = observation[0].size\n    n_observations = observation.shape[0]\n    n_signals = signal.shape[0]\n    sig_obs_pairs = np.zeros((n_observations * stepsize, 2))\n\n    for i in range(n_observations):\n        j = i // (n_observations // n_signals)\n        sig_obs_pairs[stepsize * i : stepsize * (i + 1), 0] = signal[j].ravel()\n        sig_obs_pairs[stepsize * i : stepsize * (i + 1), 1] = observation[i].ravel()\n    sig_obs_pairs = sig_obs_pairs[\n        (sig_obs_pairs[:, 0] &gt; lb) &amp; (sig_obs_pairs[:, 0] &lt; ub)\n    ]\n    sig_obs_pairs = sig_obs_pairs.astype(np.float32)\n    sig_obs_pairs = torch.from_numpy(sig_obs_pairs)\n    return self._fast_shuffle(sig_obs_pairs, 2)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.likelihood","title":"<code>likelihood(observations, signals)</code>","text":"<p>Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>Tensor</code> <p>Noisy observations. Shape is (batch, 1, dim1, dim2).</p> required <code>signals</code> <code>Tensor</code> <p>Underlying signals. Shape is (batch, 1, dim1, dim2).</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>torch.Tensor:</code> <p>Likelihood of observations given the signals and the GMM noise model</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def likelihood(\n    self, observations: torch.Tensor, signals: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.\n\n    Parameters\n    ----------\n    observations : Tensor\n        Noisy observations. Shape is (batch, 1, dim1, dim2).\n    signals : Tensor\n        Underlying signals. Shape is (batch, 1, dim1, dim2).\n\n    Returns\n    -------\n    value: torch.Tensor:\n        Likelihood of observations given the signals and the GMM noise model\n    \"\"\"\n    gaussian_parameters: list[torch.Tensor] = self.get_gaussian_parameters(signals)\n    p = torch.zeros_like(observations)\n    for gaussian in range(self.n_gaussian):\n        # Ensure all tensors have compatible shapes\n        mean = gaussian_parameters[gaussian]\n        std = gaussian_parameters[self.n_gaussian + gaussian]\n        weight = gaussian_parameters[2 * self.n_gaussian + gaussian]\n\n        # Compute normal density\n        p += (\n            self.normal_density(\n                observations,\n                mean,\n                std,\n            )\n            * weight\n        )\n    return p + self.tolerance\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.normal_density","title":"<code>normal_density(x, mean, std)</code>","text":"<p>Evaluates the normal probability density at <code>x</code> given the mean <code>mean</code> and standard deviation <code>std</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The ground-truth tensor. Shape is (batch, 1, dim1, dim2).</p> required <code>mean</code> <code>Tensor</code> <p>The inferred mean of distribution. Shape is (batch, 1, dim1, dim2).</p> required <code>std</code> <code>Tensor</code> <p>The inferred standard deviation of distribution. Shape is (batch, 1, dim1, dim2).</p> required <p>Returns:</p> Name Type Description <code>tmp</code> <code>Tensor</code> <p>Normal probability density of <code>x</code> given <code>mean</code> and <code>std</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def normal_density(\n    self, x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the normal probability density at `x` given the mean `mean` and standard deviation `std`.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The ground-truth tensor. Shape is (batch, 1, dim1, dim2).\n    mean: torch.Tensor\n        The inferred mean of distribution. Shape is (batch, 1, dim1, dim2).\n    std: torch.Tensor\n        The inferred standard deviation of distribution. Shape is (batch, 1, dim1, dim2).\n\n    Returns\n    -------\n    tmp: torch.Tensor\n        Normal probability density of `x` given `mean` and `std`\n    \"\"\"\n    tmp = -((x - mean) ** 2)\n    tmp = tmp / (2.0 * std * std)\n    tmp = torch.exp(tmp)\n    tmp = tmp / torch.sqrt((2.0 * np.pi) * std * std)\n    return tmp\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.polynomial_regressor","title":"<code>polynomial_regressor(weight_params, signals)</code>","text":"<p>Combines <code>weight_params</code> and signal <code>signals</code> to regress for the gaussian parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>weight_params</code> <code>Tensor</code> <p>Corresponds to specific rows of the <code>self.weight</code></p> required <code>signals</code> <code>Tensor</code> <p>Signals</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>Tensor</code> <p>Corresponds to either of mean, standard deviation or weight, evaluated at <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def polynomial_regressor(\n    self, weight_params: torch.Tensor, signals: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Combines `weight_params` and signal `signals` to regress for the gaussian parameter values.\n\n    Parameters\n    ----------\n    weight_params : Tensor\n        Corresponds to specific rows of the `self.weight`\n\n    signals : Tensor\n        Signals\n\n    Returns\n    -------\n    value : Tensor\n        Corresponds to either of mean, standard deviation or weight, evaluated at `signals`\n    \"\"\"\n    value = torch.zeros_like(signals)\n    for i in range(weight_params.shape[0]):\n        value += weight_params[i] * (\n            ((signals - self.min_signal) / (self.max_signal - self.min_signal)) ** i\n        )\n    return value\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.sample_observation_from_signal","title":"<code>sample_observation_from_signal(signal)</code>","text":"<p>Sample an instance of observation based on an input signal using a learned Gaussian Mixture Model. For each pixel in the input signal, samples a corresponding noisy pixel.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>NDArray</code> <p>Clean 2D signal data.</p> required <p>Returns:</p> Name Type Description <code>observation</code> <code>numpy array</code> <p>An instance of noisy observation data based on the input signal.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def sample_observation_from_signal(self, signal: NDArray) -&gt; NDArray:\n    \"\"\"\n    Sample an instance of observation based on an input signal using a\n    learned Gaussian Mixture Model. For each pixel in the input signal,\n    samples a corresponding noisy pixel.\n\n    Parameters\n    ----------\n    signal: numpy array\n        Clean 2D signal data.\n\n    Returns\n    -------\n    observation: numpy array\n        An instance of noisy observation data based on the input signal.\n    \"\"\"\n    assert len(signal.shape) == 2, \"Only 2D inputs are supported.\"\n\n    signal_tensor = torch.from_numpy(signal).to(torch.float32)\n    height, width = signal_tensor.shape\n\n    with torch.no_grad():\n        # Get gaussian parameters for each pixel\n        gaussian_params = self.get_gaussian_parameters(signal_tensor)\n        means = np.array(gaussian_params[: self.n_gaussian])\n        stds = np.array(gaussian_params[self.n_gaussian : self.n_gaussian * 2])\n        alphas = np.array(gaussian_params[self.n_gaussian * 2 :])\n\n        if self.n_gaussian == 1:\n            # Single gaussian case\n            observation = np.random.normal(\n                loc=means[0], scale=stds[0], size=(height, width)\n            )\n        else:\n            # Multiple gaussians: sample component for each pixel\n            uniform = np.random.rand(1, height, width)\n            # Compute cumulative probabilities for component selection\n            cumulative_alphas = np.cumsum(\n                alphas, axis=0\n            )  # Shape: (n_gaussian, height, width)\n            selected_component = np.argmax(\n                uniform &lt; cumulative_alphas, axis=0, keepdims=True\n            )\n\n            # For every pixel, choose the corresponding gaussian\n            # and get the learned mu and sigma\n            selected_mus = np.take_along_axis(means, selected_component, axis=0)\n            selected_stds = np.take_along_axis(stds, selected_component, axis=0)\n            selected_mus = selected_mus.squeeze(0)\n            selected_stds = selected_stds.squeeze(0)\n\n            # Sample from the normal distribution with learned mu and sigma\n            observation = np.random.normal(\n                selected_mus, selected_stds, size=(height, width)\n            )\n    return observation\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.save","title":"<code>save(path, name)</code>","text":"<p>Save the trained parameters on the noise model.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the trained parameters.</p> required <code>name</code> <code>str</code> <p>File name to save the trained parameters.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def save(self, path: str, name: str) -&gt; None:\n    \"\"\"Save the trained parameters on the noise model.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the trained parameters.\n    name : str\n        File name to save the trained parameters.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    np.savez(\n        os.path.join(path, name),\n        trained_weight=self.weight.numpy(),\n        min_signal=self.min_signal.numpy(),\n        max_signal=self.max_signal.numpy(),\n        min_sigma=self.min_sigma,\n    )\n    print(\"The trained parameters (\" + name + \") is saved at location: \" + path)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel","title":"<code>MultiChannelNoiseModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>class MultiChannelNoiseModel(nn.Module):\n    def __init__(self, nmodels: list[GaussianMixtureNoiseModel]):\n        \"\"\"Constructor.\n\n        To handle noise models and the relative likelihood computation for multiple\n        output channels (e.g., muSplit, denoiseSplit).\n\n        This class:\n        - receives as input a variable number of noise models, one for each channel.\n        - computes the likelihood of observations given signals for each channel.\n        - returns the concatenation of these likelihoods.\n\n        Parameters\n        ----------\n        nmodels : list[GaussianMixtureNoiseModel]\n            List of noise models, one for each output channel.\n        \"\"\"\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        for i, nmodel in enumerate(nmodels):  # TODO refactor this !!!\n            if nmodel is not None:\n                self.add_module(\n                    f\"nmodel_{i}\", nmodel\n                )  # TODO: wouldn't be easier to use a list?\n\n        self._nm_cnt = 0\n        for nmodel in nmodels:\n            if nmodel is not None:\n                self._nm_cnt += 1\n\n        print(f\"[{self.__class__.__name__}] Nmodels count:{self._nm_cnt}\")\n\n    def to_device(self, device: torch.device):\n        self.device = device\n        self.to(device)\n        for ch_idx in range(self._nm_cnt):\n            nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n            nmodel.to_device(device)\n\n    def likelihood(self, obs: torch.Tensor, signal: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the likelihood of observations given signals for each channel.\n\n        Parameters\n        ----------\n        obs : torch.Tensor\n            Noisy observations, i.e., the target(s). Specifically, the input noisy\n            image for HDN, or the noisy unmixed images used for supervision\n            for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of\n            unmixed channels.\n        signal : torch.Tensor\n            Underlying signals, i.e., the (clean) output of the model. Specifically, the\n            denoised image for HDN, or the unmixed images for denoiSplit.\n            Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.\n        \"\"\"\n        # Case 1: obs and signal have a single channel (e.g., denoising)\n        if obs.shape[1] == 1:\n            assert signal.shape[1] == 1\n            return self.nmodel_0.likelihood(obs, signal)\n\n        # Case 2: obs and signal have multiple channels (e.g., denoiSplit)\n        assert obs.shape[1] == self._nm_cnt, (\n            \"The number of channels in `obs` must match the number of noise models.\"\n            f\" Got instead: obs={obs.shape[1]},  nm={self._nm_cnt}\"\n        )\n        ll_list = []\n        for ch_idx in range(obs.shape[1]):\n            nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n            ll_list.append(\n                nmodel.likelihood(\n                    obs[:, ch_idx : ch_idx + 1], signal[:, ch_idx : ch_idx + 1]\n                )  # slicing to keep the channel dimension\n            )\n        return torch.cat(ll_list, dim=1)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel.__init__","title":"<code>__init__(nmodels)</code>","text":"<p>Constructor.</p> <p>To handle noise models and the relative likelihood computation for multiple output channels (e.g., muSplit, denoiseSplit).</p> <p>This class: - receives as input a variable number of noise models, one for each channel. - computes the likelihood of observations given signals for each channel. - returns the concatenation of these likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>nmodels</code> <code>list[GaussianMixtureNoiseModel]</code> <p>List of noise models, one for each output channel.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def __init__(self, nmodels: list[GaussianMixtureNoiseModel]):\n    \"\"\"Constructor.\n\n    To handle noise models and the relative likelihood computation for multiple\n    output channels (e.g., muSplit, denoiseSplit).\n\n    This class:\n    - receives as input a variable number of noise models, one for each channel.\n    - computes the likelihood of observations given signals for each channel.\n    - returns the concatenation of these likelihoods.\n\n    Parameters\n    ----------\n    nmodels : list[GaussianMixtureNoiseModel]\n        List of noise models, one for each output channel.\n    \"\"\"\n    super().__init__()\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for i, nmodel in enumerate(nmodels):  # TODO refactor this !!!\n        if nmodel is not None:\n            self.add_module(\n                f\"nmodel_{i}\", nmodel\n            )  # TODO: wouldn't be easier to use a list?\n\n    self._nm_cnt = 0\n    for nmodel in nmodels:\n        if nmodel is not None:\n            self._nm_cnt += 1\n\n    print(f\"[{self.__class__.__name__}] Nmodels count:{self._nm_cnt}\")\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel.likelihood","title":"<code>likelihood(obs, signal)</code>","text":"<p>Compute the likelihood of observations given signals for each channel.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Noisy observations, i.e., the target(s). Specifically, the input noisy image for HDN, or the noisy unmixed images used for supervision for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.</p> required <code>signal</code> <code>Tensor</code> <p>Underlying signals, i.e., the (clean) output of the model. Specifically, the denoised image for HDN, or the unmixed images for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def likelihood(self, obs: torch.Tensor, signal: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the likelihood of observations given signals for each channel.\n\n    Parameters\n    ----------\n    obs : torch.Tensor\n        Noisy observations, i.e., the target(s). Specifically, the input noisy\n        image for HDN, or the noisy unmixed images used for supervision\n        for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of\n        unmixed channels.\n    signal : torch.Tensor\n        Underlying signals, i.e., the (clean) output of the model. Specifically, the\n        denoised image for HDN, or the unmixed images for denoiSplit.\n        Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.\n    \"\"\"\n    # Case 1: obs and signal have a single channel (e.g., denoising)\n    if obs.shape[1] == 1:\n        assert signal.shape[1] == 1\n        return self.nmodel_0.likelihood(obs, signal)\n\n    # Case 2: obs and signal have multiple channels (e.g., denoiSplit)\n    assert obs.shape[1] == self._nm_cnt, (\n        \"The number of channels in `obs` must match the number of noise models.\"\n        f\" Got instead: obs={obs.shape[1]},  nm={self._nm_cnt}\"\n    )\n    ll_list = []\n    for ch_idx in range(obs.shape[1]):\n        nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n        ll_list.append(\n            nmodel.likelihood(\n                obs[:, ch_idx : ch_idx + 1], signal[:, ch_idx : ch_idx + 1]\n            )  # slicing to keep the channel dimension\n        )\n    return torch.cat(ll_list, dim=1)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.create_histogram","title":"<code>create_histogram(bins, min_val, max_val, observation, signal)</code>","text":"<p>Creates a 2D histogram from 'observation' and 'signal'.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>int</code> <p>Number of bins in x and y.</p> required <code>min_val</code> <code>float</code> <p>Lower bound of the lowest bin in x and y.</p> required <code>max_val</code> <code>float</code> <p>Upper bound of the highest bin in x and y.</p> required <code>observation</code> <code>ndarray</code> <p>3D numpy array (stack of 2D images). Observation.shape[0] must be divisible by signal.shape[0]. Assumes that n subsequent images in observation belong to one image in 'signal'.</p> required <code>signal</code> <code>ndarray</code> <p>3D numpy array (stack of 2D images).</p> required <p>Returns:</p> Name Type Description <code>histogram</code> <code>ndarray</code> <p>A 3D array: - histogram[0]: Normalized 2D counts. - histogram[1]: Lower boundaries of bins along y. - histogram[2]: Upper boundaries of bins along y.</p> <code>The values for x can be obtained by transposing 'histogram[1]' and 'histogram[2]'.</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def create_histogram(\n    bins: int, min_val: float, max_val: float, observation: NDArray, signal: NDArray\n) -&gt; NDArray:\n    \"\"\"\n    Creates a 2D histogram from 'observation' and 'signal'.\n\n    Parameters\n    ----------\n    bins : int\n        Number of bins in x and y.\n    min_val : float\n        Lower bound of the lowest bin in x and y.\n    max_val : float\n        Upper bound of the highest bin in x and y.\n    observation : np.ndarray\n        3D numpy array (stack of 2D images).\n        Observation.shape[0] must be divisible by signal.shape[0].\n        Assumes that n subsequent images in observation belong to one image in 'signal'.\n    signal : np.ndarray\n        3D numpy array (stack of 2D images).\n\n    Returns\n    -------\n    histogram : np.ndarray\n        A 3D array:\n        - histogram[0]: Normalized 2D counts.\n        - histogram[1]: Lower boundaries of bins along y.\n        - histogram[2]: Upper boundaries of bins along y.\n    The values for x can be obtained by transposing 'histogram[1]' and 'histogram[2]'.\n    \"\"\"\n    histogram = np.zeros((3, bins, bins))\n\n    value_range = [min_val, max_val]\n\n    # Compute mapping factor between observation and signal samples\n    obs_to_signal_shape_factor = int(observation.shape[0] / signal.shape[0])\n\n    # Flatten arrays and align signal values\n    signal_indices = np.arange(observation.shape[0]) // obs_to_signal_shape_factor\n    signal_values = signal[signal_indices].ravel()\n    observation_values = observation.ravel()\n\n    count_histogram, signal_edges, _ = np.histogram2d(\n        signal_values, observation_values, bins=bins, range=[value_range, value_range]\n    )\n\n    # Normalize rows to obtain probabilities\n    row_sums = count_histogram.sum(axis=1, keepdims=True)\n    count_histogram /= np.clip(row_sums, a_min=1e-20, a_max=None)\n\n    histogram[0] = count_histogram\n    histogram[1] = signal_edges[:-1][..., np.newaxis]\n    histogram[2] = signal_edges[1:][..., np.newaxis]\n\n    return histogram\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.noise_model_factory","title":"<code>noise_model_factory(model_config)</code>","text":"<p>Noise model factory.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>Optional[MultiChannelNMConfig]</code> <p>Noise model configuration, a <code>MultiChannelNMConfig</code> config that defines noise models for the different output channels.</p> required <p>Returns:</p> Type Description <code>Optional[MultiChannelNoiseModel]</code> <p>A noise model instance.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the chosen noise model <code>model_type</code> is not implemented. Currently only <code>GaussianMixtureNoiseModel</code> is implemented.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def noise_model_factory(\n    model_config: Optional[MultiChannelNMConfig],\n) -&gt; Optional[MultiChannelNoiseModel]:\n    \"\"\"Noise model factory.\n\n    Parameters\n    ----------\n    model_config : Optional[MultiChannelNMConfig]\n        Noise model configuration, a `MultiChannelNMConfig` config that defines\n        noise models for the different output channels.\n\n    Returns\n    -------\n    Optional[MultiChannelNoiseModel]\n        A noise model instance.\n\n    Raises\n    ------\n    NotImplementedError\n        If the chosen noise model `model_type` is not implemented.\n        Currently only `GaussianMixtureNoiseModel` is implemented.\n    \"\"\"\n    if model_config:\n        noise_models = []\n        for nm in model_config.noise_models:\n            if nm.path:\n                if nm.model_type == \"GaussianMixtureNoiseModel\":\n                    noise_models.append(GaussianMixtureNoiseModel(nm))\n                else:\n                    raise NotImplementedError(\n                        f\"Model {nm.model_type} is not implemented\"\n                    )\n\n            else:  # TODO this means signal/obs are provided. Controlled in pydantic model\n                # TODO train a new model. Config should always be provided?\n                if nm.model_type == \"GaussianMixtureNoiseModel\":\n                    # TODO one model for each channel all make this choise inside the model?\n                    # trained_nm = train_gm_noise_model(nm)\n                    # noise_models.append(trained_nm)\n                    raise NotImplementedError(\n                        \"GaussianMixtureNoiseModel model training is not implemented.\"\n                    )\n                else:\n                    raise NotImplementedError(\n                        f\"Model {nm.model_type} is not implemented\"\n                    )\n        return MultiChannelNoiseModel(noise_models)\n    return None\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.train_gm_noise_model","title":"<code>train_gm_noise_model(model_config, signal, observation)</code>","text":"<p>Train a Gaussian mixture noise model.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>GaussianMixtureNoiseModel</code> <p>description</p> required <p>Returns:</p> Type Description <code>_description_</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def train_gm_noise_model(\n    model_config: GaussianMixtureNMConfig,\n    signal: np.ndarray,\n    observation: np.ndarray,\n) -&gt; GaussianMixtureNoiseModel:\n    \"\"\"Train a Gaussian mixture noise model.\n\n    Parameters\n    ----------\n    model_config : GaussianMixtureNoiseModel\n        _description_\n\n    Returns\n    -------\n    _description_\n    \"\"\"\n    # TODO where to put train params?\n    # TODO any training params ? Different channels ?\n    noise_model = GaussianMixtureNoiseModel(model_config)\n    # TODO revisit config unpacking\n    noise_model.fit(signal, observation)\n    return noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/","title":"stochastic","text":"<p>Script containing the common basic blocks (nn.Module) reused by the LadderVAE architecture.</p>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock","title":"<code>NormalStochasticBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Stochastic block used in the Top-Down inference pass.</p> <p>Algorithm:     - map input parameters to q(z) and (optionally) p(z) via convolution     - sample a latent tensor z ~ q(z)     - feed z to convolution and return.</p> <p>NOTE 1:     If parameters for q are not given, sampling is done from p(z).</p> <p>NOTE 2:     The restricted KL divergence is obtained by first computing the element-wise KL divergence     (i.e., the KL computed for each element of the latent tensors). Then, the restricted version     is computed by summing over the channels and the spatial dimensions associated only to the     portion of the latent tensor that is used for prediction.</p> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>class NormalStochasticBlock(nn.Module):\n    \"\"\"\n    Stochastic block used in the Top-Down inference pass.\n\n    Algorithm:\n        - map input parameters to q(z) and (optionally) p(z) via convolution\n        - sample a latent tensor z ~ q(z)\n        - feed z to convolution and return.\n\n    NOTE 1:\n        If parameters for q are not given, sampling is done from p(z).\n\n    NOTE 2:\n        The restricted KL divergence is obtained by first computing the element-wise KL divergence\n        (i.e., the KL computed for each element of the latent tensors). Then, the restricted version\n        is computed by summing over the channels and the spatial dimensions associated only to the\n        portion of the latent tensor that is used for prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        c_in: int,\n        c_vars: int,\n        c_out: int,\n        conv_dims: int = 2,\n        kernel: int = 3,\n        transform_p_params: bool = True,\n        vanilla_latent_hw: int = None,\n        use_naive_exponential: bool = False,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        c_in: int\n            The number of channels of the input tensor.\n        c_vars: int\n            The number of channels of the latent space tensor.\n        c_out:  int\n            The output of the stochastic layer.\n            Note that this is different from the sampled latent z.\n        conv_dims: int, optional\n            The number of dimensions of the convolutional layers (2D or 3D).\n            Default is 2.\n        kernel: int, optional\n            The size of the kernel used in convolutional layers.\n            Default is 3.\n        transform_p_params: bool, optional\n            Whether a transformation should be applied to the `p_params` tensor.\n            The transformation consists in a 2D convolution ()`conv_in_p()`) that\n            maps the input to a larger number of channels.\n            Default is `True`.\n        vanilla_latent_hw: int, optional\n            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n            Default is `None`.\n        use_naive_exponential: bool, optional\n            If `False`, exponentials are computed according to the alternative definition\n            provided by `StableExponential` class. This should improve numerical stability\n            in the training process. Default is `False`.\n        \"\"\"\n        super().__init__()\n        assert kernel % 2 == 1\n        pad = kernel // 2\n        self.transform_p_params = transform_p_params\n        self.c_in = c_in\n        self.c_out = c_out\n        self.c_vars = c_vars\n        self.conv_dims = conv_dims\n        self._use_naive_exponential = use_naive_exponential\n        self._vanilla_latent_hw = vanilla_latent_hw\n\n        conv_layer: ConvType = getattr(nn, f\"Conv{conv_dims}d\")\n\n        if transform_p_params:\n            self.conv_in_p = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n        self.conv_in_q = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n        self.conv_out = conv_layer(c_vars, c_out, kernel, padding=pad)\n\n    def get_z(\n        self,\n        sampling_distrib: torch.distributions.normal.Normal,\n        forced_latent: Union[torch.Tensor, None],\n        mode_pred: bool,\n        use_uncond_mode: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample a latent tensor from the given latent distribution.\n\n        Latent tensor can be obtained is several ways:\n            - Sampled from the (Gaussian) latent distribution.\n            - Taken as a pre-defined forced latent.\n            - Taken as the mode (mean) of the latent distribution.\n            - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.\n\n        Parameters\n        ----------\n        sampling_distrib: torch.distributions.normal.Normal\n            The Gaussian distribution from which latent tensor is sampled.\n        forced_latent: torch.Tensor\n            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,\n            hence, sampling does not happen.\n        mode_pred: bool\n            Whether the model is prediction mode.\n        use_uncond_mode: bool\n            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n        \"\"\"\n        if forced_latent is None:\n            if mode_pred:\n                if use_uncond_mode:\n                    z = sampling_distrib.mean\n                else:\n                    z = sampling_distrib.rsample()\n            else:\n                z = sampling_distrib.rsample()\n        else:\n            z = forced_latent\n        return z\n\n    def sample_from_q(\n        self, q_params: torch.Tensor, var_clip_max: float\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Given an input parameter tensor defining q(z),\n        it processes it by calling `process_q_params()` method and\n        sample a latent tensor from the resulting distribution.\n\n        Parameters\n        ----------\n        q_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        _, _, q = self.process_q_params(q_params, var_clip_max)\n        return q.rsample()\n\n    def compute_kl_metrics(\n        self,\n        p: torch.distributions.normal.Normal,\n        p_params: torch.Tensor,\n        q: torch.distributions.normal.Normal,\n        q_params: torch.Tensor,\n        mode_pred: bool,\n        analytical_kl: bool,\n        z: torch.Tensor,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.\n        Specifically, the different versions of the KL loss terms are:\n            - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].\n            - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].\n            - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is\n            used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].\n            - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].\n            - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]\n\n        Parameters\n        ----------\n        p: torch.distributions.normal.Normal\n            The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).\n        p_params: torch.Tensor\n            The parameters of the prior generative distribution.\n        q: torch.distributions.normal.Normal\n            The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).\n        q_params: torch.Tensor\n            The parameters of the inference distribution.\n        mode_pred: bool\n            Whether the model is in prediction mode.\n        analytical_kl: bool\n            Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n        z: torch.Tensor\n            The sampled latent tensor.\n        \"\"\"\n        kl_samplewise_restricted = None\n        if mode_pred is False:  # if not predicting\n            if analytical_kl:\n                kl_elementwise = kl_divergence(q, p)\n            else:\n                kl_elementwise = kl_normal_mc(z, p_params, q_params)\n\n            all_dims = tuple(range(len(kl_elementwise.shape)))\n            kl_samplewise = kl_elementwise.sum(all_dims[1:])\n            kl_channelwise = kl_elementwise.sum(all_dims[2:])\n\n            # compute KL only on the portion of the latent space that is used for prediction.\n            pad = (kl_elementwise.shape[-1] - self._vanilla_latent_hw) // 2\n            if pad &gt; 0:\n                tmp = kl_elementwise[..., pad:-pad, pad:-pad]\n                kl_samplewise_restricted = tmp.sum(all_dims[1:])\n            else:\n                kl_samplewise_restricted = kl_samplewise\n\n            # Compute spatial KL analytically (but conditioned on samples from\n            # previous layers)\n            kl_spatial = kl_elementwise.sum(1)\n        else:  # if predicting, no need to compute KL\n            kl_elementwise = kl_samplewise = kl_spatial = kl_channelwise = None\n\n        kl_dict = {\n            \"kl_elementwise\": kl_elementwise,  # (batch, ch, h, w)\n            \"kl_samplewise\": kl_samplewise,  # (batch, )\n            \"kl_samplewise_restricted\": kl_samplewise_restricted,  # (batch, )\n            \"kl_spatial\": kl_spatial,  # (batch, h, w)\n            \"kl_channelwise\": kl_channelwise,  # (batch, ch)\n        }  # TODO revisit, check dims\n        return kl_dict\n\n    def process_p_params(\n        self, p_params: torch.Tensor, var_clip_max: float\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n        \"\"\"Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).\n\n        Processing consists in:\n            - (optionally) 2D convolution on the input tensor to increase number of channels.\n            - split the resulting tensor into two chunks, the mean and the log-variance.\n            - (optionally) clip the log-variance to an upper threshold.\n            - define the normal distribution p(z) given the parameter tensors above.\n\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        if self.transform_p_params:\n            p_params = self.conv_in_p(p_params)\n        else:\n            assert p_params.size(1) == 2 * self.c_vars\n\n        # Define p(z)\n        p_mu, p_lv = p_params.chunk(2, dim=1)\n        if var_clip_max is not None:\n            p_lv = torch.clip(p_lv, max=var_clip_max)\n\n        p_mu = StableMean(p_mu)\n        p_lv = StableLogVar(p_lv, enable_stable=not self._use_naive_exponential)\n        p = Normal(p_mu.get(), p_lv.get_std())\n        return p_mu, p_lv, p\n\n    def process_q_params(\n        self, q_params: torch.Tensor, var_clip_max: float, allow_oddsizes: bool = False\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n        \"\"\"\n        Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).\n\n        Processing consists in:\n            - convolution on the input tensor to double the number of channels.\n            - split the resulting tensor into 2 chunks, respectively mean and log-var.\n            - (optionally) clip the log-variance to an upper threshold.\n            - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.\n            - define the normal distribution q(z) given the parameter tensors above.\n\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        q_params = self.conv_in_q(q_params)\n\n        q_mu, q_lv = q_params.chunk(2, dim=1)\n        if var_clip_max is not None:\n            q_lv = torch.clip(q_lv, max=var_clip_max)\n\n        if q_mu.shape[-1] % 2 == 1 and allow_oddsizes is False:\n            q_mu = F.center_crop(q_mu, q_mu.shape[-1] - 1)\n            q_lv = F.center_crop(q_lv, q_lv.shape[-1] - 1)\n            # TODO revisit ?!\n        q_mu = StableMean(q_mu)\n        q_lv = StableLogVar(q_lv, enable_stable=not self._use_naive_exponential)\n        q = Normal(q_mu.get(), q_lv.get_std())\n        return q_mu, q_lv, q\n\n    def forward(\n        self,\n        p_params: torch.Tensor,\n        q_params: Union[torch.Tensor, None] = None,\n        forced_latent: Union[torch.Tensor, None] = None,\n        force_constant_output: bool = False,\n        analytical_kl: bool = False,\n        mode_pred: bool = False,\n        use_uncond_mode: bool = False,\n        var_clip_max: Union[float, None] = None,\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).\n        q_params: torch.Tensor, optional\n            The tensor resulting from merging the bu_value tensor at the same hierarchical level\n            from the bottom-up pass and the `p_params` tensor. Default is `None`.\n        forced_latent: torch.Tensor, optional\n            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent\n            tensor and, hence, sampling does not happen. Default is `None`.\n        force_constant_output: bool, optional\n            Whether to copy the first sample (and rel. distrib parameters) over the whole batch.\n            This is used when doing experiment from the prior - q is not used.\n            Default is `False`.\n        analytical_kl: bool, optional\n            Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n            Default is `False`.\n        mode_pred: bool, optional\n            Whether the model is in prediction mode. Default is `False`.\n        use_uncond_mode: bool, optional\n            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n            Default is `False`.\n        var_clip_max: float, optional\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped. Default is `None`.\n        \"\"\"\n        debug_qvar_max = 0\n\n        # Check sampling options consistency\n        assert forced_latent is None\n\n        # Get generative distribution p(z_i|z_{i+1})\n        p_mu, p_lv, p = self.process_p_params(p_params, var_clip_max)\n        p_params = (p_mu, p_lv)\n\n        if q_params is not None:\n            # Get inference distribution q(z_i|z_{i+1})\n            q_mu, q_lv, q = self.process_q_params(q_params, var_clip_max)\n            q_params = (q_mu, q_lv)\n            debug_qvar_max = torch.max(q_lv.get())\n            sampling_distrib = q\n            q_size = q_mu.get().shape[-1]\n            if p_mu.get().shape[-1] != q_size and mode_pred is False:\n                p_mu.centercrop_to_size(q_size)\n                p_lv.centercrop_to_size(q_size)\n        else:\n            sampling_distrib = p\n\n        # Sample latent variable\n        z = self.get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)\n\n        # TODO: not necessary, remove\n        # Copy one sample (and distrib parameters) over the whole batch.\n        # This is used when doing experiment from the prior - q is not used.\n        if force_constant_output:\n            z = z[0:1].expand_as(z).clone()\n            p_params = (\n                p_params[0][0:1].expand_as(p_params[0]).clone(),\n                p_params[1][0:1].expand_as(p_params[1]).clone(),\n            )\n\n        # Pass the sampled latent through the output convolution of stochastic block\n        out = self.conv_out(z)\n\n        if q_params is not None:\n            # Compute log q(z)\n            logprob_q = q.log_prob(z).sum(tuple(range(1, z.dim())))\n            # Compute KL divergence metrics\n            kl_dict = self.compute_kl_metrics(\n                p, p_params, q, q_params, mode_pred, analytical_kl, z\n            )\n        else:\n            kl_dict = {}\n            logprob_q = None\n\n        # Store meaningful quantities for later computation\n        data = kl_dict\n        data[\"z\"] = z  # sampled variable at this layer (B, C, [Z], Y, X)\n        data[\"p_params\"] = p_params  # (B, C, [Z], Y, X) where B is 1 or batch size\n        data[\"q_params\"] = q_params  # (B, C, [Z], Y, X)\n        data[\"logprob_q\"] = logprob_q  # (B, )\n        data[\"qvar_max\"] = debug_qvar_max\n        return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.__init__","title":"<code>__init__(c_in, c_vars, c_out, conv_dims=2, kernel=3, transform_p_params=True, vanilla_latent_hw=None, use_naive_exponential=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>c_in</code> <code>int</code> <p>The number of channels of the input tensor.</p> required <code>c_vars</code> <code>int</code> <p>The number of channels of the latent space tensor.</p> required <code>c_out</code> <code>int</code> <p>The output of the stochastic layer. Note that this is different from the sampled latent z.</p> required <code>conv_dims</code> <code>int</code> <p>The number of dimensions of the convolutional layers (2D or 3D). Default is 2.</p> <code>2</code> <code>kernel</code> <code>int</code> <p>The size of the kernel used in convolutional layers. Default is 3.</p> <code>3</code> <code>transform_p_params</code> <code>bool</code> <p>Whether a transformation should be applied to the <code>p_params</code> tensor. The transformation consists in a 2D convolution ()<code>conv_in_p()</code>) that maps the input to a larger number of channels. Default is <code>True</code>.</p> <code>True</code> <code>vanilla_latent_hw</code> <code>int</code> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> <code>None</code> <code>use_naive_exponential</code> <code>bool</code> <p>If <code>False</code>, exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> <code>False</code> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def __init__(\n    self,\n    c_in: int,\n    c_vars: int,\n    c_out: int,\n    conv_dims: int = 2,\n    kernel: int = 3,\n    transform_p_params: bool = True,\n    vanilla_latent_hw: int = None,\n    use_naive_exponential: bool = False,\n):\n    \"\"\"\n    Parameters\n    ----------\n    c_in: int\n        The number of channels of the input tensor.\n    c_vars: int\n        The number of channels of the latent space tensor.\n    c_out:  int\n        The output of the stochastic layer.\n        Note that this is different from the sampled latent z.\n    conv_dims: int, optional\n        The number of dimensions of the convolutional layers (2D or 3D).\n        Default is 2.\n    kernel: int, optional\n        The size of the kernel used in convolutional layers.\n        Default is 3.\n    transform_p_params: bool, optional\n        Whether a transformation should be applied to the `p_params` tensor.\n        The transformation consists in a 2D convolution ()`conv_in_p()`) that\n        maps the input to a larger number of channels.\n        Default is `True`.\n    vanilla_latent_hw: int, optional\n        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n        Default is `None`.\n    use_naive_exponential: bool, optional\n        If `False`, exponentials are computed according to the alternative definition\n        provided by `StableExponential` class. This should improve numerical stability\n        in the training process. Default is `False`.\n    \"\"\"\n    super().__init__()\n    assert kernel % 2 == 1\n    pad = kernel // 2\n    self.transform_p_params = transform_p_params\n    self.c_in = c_in\n    self.c_out = c_out\n    self.c_vars = c_vars\n    self.conv_dims = conv_dims\n    self._use_naive_exponential = use_naive_exponential\n    self._vanilla_latent_hw = vanilla_latent_hw\n\n    conv_layer: ConvType = getattr(nn, f\"Conv{conv_dims}d\")\n\n    if transform_p_params:\n        self.conv_in_p = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n    self.conv_in_q = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n    self.conv_out = conv_layer(c_vars, c_out, kernel, padding=pad)\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.compute_kl_metrics","title":"<code>compute_kl_metrics(p, p_params, q, q_params, mode_pred, analytical_kl, z)</code>","text":"<p>Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric. Specifically, the different versions of the KL loss terms are:     - <code>kl_elementwise</code>: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].     - <code>kl_samplewise</code>: KL term associated to each sample in the batch [Shape: (batch, )].     - <code>kl_samplewise_restricted</code>: KL term only associated to the portion of the latent tensor that is     used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].     - <code>kl_channelwise</code>: KL term associated to each sample and each channel [Shape: (batch, ch, )].     - <code>kl_spatial</code>: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Normal</code> <p>The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</p> required <code>p_params</code> <code>Tensor</code> <p>The parameters of the prior generative distribution.</p> required <code>q</code> <code>Normal</code> <p>The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</p> required <code>q_params</code> <code>Tensor</code> <p>The parameters of the inference distribution.</p> required <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode.</p> required <code>analytical_kl</code> <code>bool</code> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation.</p> required <code>z</code> <code>Tensor</code> <p>The sampled latent tensor.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def compute_kl_metrics(\n    self,\n    p: torch.distributions.normal.Normal,\n    p_params: torch.Tensor,\n    q: torch.distributions.normal.Normal,\n    q_params: torch.Tensor,\n    mode_pred: bool,\n    analytical_kl: bool,\n    z: torch.Tensor,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.\n    Specifically, the different versions of the KL loss terms are:\n        - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].\n        - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].\n        - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is\n        used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].\n        - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].\n        - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]\n\n    Parameters\n    ----------\n    p: torch.distributions.normal.Normal\n        The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).\n    p_params: torch.Tensor\n        The parameters of the prior generative distribution.\n    q: torch.distributions.normal.Normal\n        The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).\n    q_params: torch.Tensor\n        The parameters of the inference distribution.\n    mode_pred: bool\n        Whether the model is in prediction mode.\n    analytical_kl: bool\n        Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n    z: torch.Tensor\n        The sampled latent tensor.\n    \"\"\"\n    kl_samplewise_restricted = None\n    if mode_pred is False:  # if not predicting\n        if analytical_kl:\n            kl_elementwise = kl_divergence(q, p)\n        else:\n            kl_elementwise = kl_normal_mc(z, p_params, q_params)\n\n        all_dims = tuple(range(len(kl_elementwise.shape)))\n        kl_samplewise = kl_elementwise.sum(all_dims[1:])\n        kl_channelwise = kl_elementwise.sum(all_dims[2:])\n\n        # compute KL only on the portion of the latent space that is used for prediction.\n        pad = (kl_elementwise.shape[-1] - self._vanilla_latent_hw) // 2\n        if pad &gt; 0:\n            tmp = kl_elementwise[..., pad:-pad, pad:-pad]\n            kl_samplewise_restricted = tmp.sum(all_dims[1:])\n        else:\n            kl_samplewise_restricted = kl_samplewise\n\n        # Compute spatial KL analytically (but conditioned on samples from\n        # previous layers)\n        kl_spatial = kl_elementwise.sum(1)\n    else:  # if predicting, no need to compute KL\n        kl_elementwise = kl_samplewise = kl_spatial = kl_channelwise = None\n\n    kl_dict = {\n        \"kl_elementwise\": kl_elementwise,  # (batch, ch, h, w)\n        \"kl_samplewise\": kl_samplewise,  # (batch, )\n        \"kl_samplewise_restricted\": kl_samplewise_restricted,  # (batch, )\n        \"kl_spatial\": kl_spatial,  # (batch, h, w)\n        \"kl_channelwise\": kl_channelwise,  # (batch, ch)\n    }  # TODO revisit, check dims\n    return kl_dict\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.forward","title":"<code>forward(p_params, q_params=None, forced_latent=None, force_constant_output=False, analytical_kl=False, mode_pred=False, use_uncond_mode=False, var_clip_max=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>p_params</code> <code>Tensor</code> <p>The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</p> required <code>q_params</code> <code>Union[Tensor, None]</code> <p>The tensor resulting from merging the bu_value tensor at the same hierarchical level from the bottom-up pass and the <code>p_params</code> tensor. Default is <code>None</code>.</p> <code>None</code> <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> <code>None</code> <code>force_constant_output</code> <code>bool</code> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> <code>False</code> <code>analytical_kl</code> <code>bool</code> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation. Default is <code>False</code>.</p> <code>False</code> <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>var_clip_max</code> <code>Union[float, None]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def forward(\n    self,\n    p_params: torch.Tensor,\n    q_params: Union[torch.Tensor, None] = None,\n    forced_latent: Union[torch.Tensor, None] = None,\n    force_constant_output: bool = False,\n    analytical_kl: bool = False,\n    mode_pred: bool = False,\n    use_uncond_mode: bool = False,\n    var_clip_max: Union[float, None] = None,\n) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).\n    q_params: torch.Tensor, optional\n        The tensor resulting from merging the bu_value tensor at the same hierarchical level\n        from the bottom-up pass and the `p_params` tensor. Default is `None`.\n    forced_latent: torch.Tensor, optional\n        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent\n        tensor and, hence, sampling does not happen. Default is `None`.\n    force_constant_output: bool, optional\n        Whether to copy the first sample (and rel. distrib parameters) over the whole batch.\n        This is used when doing experiment from the prior - q is not used.\n        Default is `False`.\n    analytical_kl: bool, optional\n        Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n        Default is `False`.\n    mode_pred: bool, optional\n        Whether the model is in prediction mode. Default is `False`.\n    use_uncond_mode: bool, optional\n        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n        Default is `False`.\n    var_clip_max: float, optional\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped. Default is `None`.\n    \"\"\"\n    debug_qvar_max = 0\n\n    # Check sampling options consistency\n    assert forced_latent is None\n\n    # Get generative distribution p(z_i|z_{i+1})\n    p_mu, p_lv, p = self.process_p_params(p_params, var_clip_max)\n    p_params = (p_mu, p_lv)\n\n    if q_params is not None:\n        # Get inference distribution q(z_i|z_{i+1})\n        q_mu, q_lv, q = self.process_q_params(q_params, var_clip_max)\n        q_params = (q_mu, q_lv)\n        debug_qvar_max = torch.max(q_lv.get())\n        sampling_distrib = q\n        q_size = q_mu.get().shape[-1]\n        if p_mu.get().shape[-1] != q_size and mode_pred is False:\n            p_mu.centercrop_to_size(q_size)\n            p_lv.centercrop_to_size(q_size)\n    else:\n        sampling_distrib = p\n\n    # Sample latent variable\n    z = self.get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)\n\n    # TODO: not necessary, remove\n    # Copy one sample (and distrib parameters) over the whole batch.\n    # This is used when doing experiment from the prior - q is not used.\n    if force_constant_output:\n        z = z[0:1].expand_as(z).clone()\n        p_params = (\n            p_params[0][0:1].expand_as(p_params[0]).clone(),\n            p_params[1][0:1].expand_as(p_params[1]).clone(),\n        )\n\n    # Pass the sampled latent through the output convolution of stochastic block\n    out = self.conv_out(z)\n\n    if q_params is not None:\n        # Compute log q(z)\n        logprob_q = q.log_prob(z).sum(tuple(range(1, z.dim())))\n        # Compute KL divergence metrics\n        kl_dict = self.compute_kl_metrics(\n            p, p_params, q, q_params, mode_pred, analytical_kl, z\n        )\n    else:\n        kl_dict = {}\n        logprob_q = None\n\n    # Store meaningful quantities for later computation\n    data = kl_dict\n    data[\"z\"] = z  # sampled variable at this layer (B, C, [Z], Y, X)\n    data[\"p_params\"] = p_params  # (B, C, [Z], Y, X) where B is 1 or batch size\n    data[\"q_params\"] = q_params  # (B, C, [Z], Y, X)\n    data[\"logprob_q\"] = logprob_q  # (B, )\n    data[\"qvar_max\"] = debug_qvar_max\n    return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.get_z","title":"<code>get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)</code>","text":"<p>Sample a latent tensor from the given latent distribution.</p> <p>Latent tensor can be obtained is several ways:     - Sampled from the (Gaussian) latent distribution.     - Taken as a pre-defined forced latent.     - Taken as the mode (mean) of the latent distribution.     - In prediction mode (<code>mode_pred==True</code>), can be either sample or taken as the distribution mode.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_distrib</code> <code>Normal</code> <p>The Gaussian distribution from which latent tensor is sampled.</p> required <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen.</p> required <code>mode_pred</code> <code>bool</code> <p>Whether the model is prediction mode.</p> required <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def get_z(\n    self,\n    sampling_distrib: torch.distributions.normal.Normal,\n    forced_latent: Union[torch.Tensor, None],\n    mode_pred: bool,\n    use_uncond_mode: bool,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a latent tensor from the given latent distribution.\n\n    Latent tensor can be obtained is several ways:\n        - Sampled from the (Gaussian) latent distribution.\n        - Taken as a pre-defined forced latent.\n        - Taken as the mode (mean) of the latent distribution.\n        - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.\n\n    Parameters\n    ----------\n    sampling_distrib: torch.distributions.normal.Normal\n        The Gaussian distribution from which latent tensor is sampled.\n    forced_latent: torch.Tensor\n        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,\n        hence, sampling does not happen.\n    mode_pred: bool\n        Whether the model is prediction mode.\n    use_uncond_mode: bool\n        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n    \"\"\"\n    if forced_latent is None:\n        if mode_pred:\n            if use_uncond_mode:\n                z = sampling_distrib.mean\n            else:\n                z = sampling_distrib.rsample()\n        else:\n            z = sampling_distrib.rsample()\n    else:\n        z = forced_latent\n    return z\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.process_p_params","title":"<code>process_p_params(p_params, var_clip_max)</code>","text":"<p>Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).</p> <p>Processing consists in:     - (optionally) 2D convolution on the input tensor to increase number of channels.     - split the resulting tensor into two chunks, the mean and the log-variance.     - (optionally) clip the log-variance to an upper threshold.     - define the normal distribution p(z) given the parameter tensors above.</p> <p>Parameters:</p> Name Type Description Default <code>p_params</code> <code>Tensor</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def process_p_params(\n    self, p_params: torch.Tensor, var_clip_max: float\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n    \"\"\"Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).\n\n    Processing consists in:\n        - (optionally) 2D convolution on the input tensor to increase number of channels.\n        - split the resulting tensor into two chunks, the mean and the log-variance.\n        - (optionally) clip the log-variance to an upper threshold.\n        - define the normal distribution p(z) given the parameter tensors above.\n\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    if self.transform_p_params:\n        p_params = self.conv_in_p(p_params)\n    else:\n        assert p_params.size(1) == 2 * self.c_vars\n\n    # Define p(z)\n    p_mu, p_lv = p_params.chunk(2, dim=1)\n    if var_clip_max is not None:\n        p_lv = torch.clip(p_lv, max=var_clip_max)\n\n    p_mu = StableMean(p_mu)\n    p_lv = StableLogVar(p_lv, enable_stable=not self._use_naive_exponential)\n    p = Normal(p_mu.get(), p_lv.get_std())\n    return p_mu, p_lv, p\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.process_q_params","title":"<code>process_q_params(q_params, var_clip_max, allow_oddsizes=False)</code>","text":"<p>Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).</p> <p>Processing consists in:     - convolution on the input tensor to double the number of channels.     - split the resulting tensor into 2 chunks, respectively mean and log-var.     - (optionally) clip the log-variance to an upper threshold.     - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.     - define the normal distribution q(z) given the parameter tensors above.</p> <p>Parameters:</p> Name Type Description Default <code>p_params</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def process_q_params(\n    self, q_params: torch.Tensor, var_clip_max: float, allow_oddsizes: bool = False\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n    \"\"\"\n    Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).\n\n    Processing consists in:\n        - convolution on the input tensor to double the number of channels.\n        - split the resulting tensor into 2 chunks, respectively mean and log-var.\n        - (optionally) clip the log-variance to an upper threshold.\n        - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.\n        - define the normal distribution q(z) given the parameter tensors above.\n\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    q_params = self.conv_in_q(q_params)\n\n    q_mu, q_lv = q_params.chunk(2, dim=1)\n    if var_clip_max is not None:\n        q_lv = torch.clip(q_lv, max=var_clip_max)\n\n    if q_mu.shape[-1] % 2 == 1 and allow_oddsizes is False:\n        q_mu = F.center_crop(q_mu, q_mu.shape[-1] - 1)\n        q_lv = F.center_crop(q_lv, q_lv.shape[-1] - 1)\n        # TODO revisit ?!\n    q_mu = StableMean(q_mu)\n    q_lv = StableLogVar(q_lv, enable_stable=not self._use_naive_exponential)\n    q = Normal(q_mu.get(), q_lv.get_std())\n    return q_mu, q_lv, q\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.sample_from_q","title":"<code>sample_from_q(q_params, var_clip_max)</code>","text":"<p>Given an input parameter tensor defining q(z), it processes it by calling <code>process_q_params()</code> method and sample a latent tensor from the resulting distribution.</p> <p>Parameters:</p> Name Type Description Default <code>q_params</code> <code>Tensor</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def sample_from_q(\n    self, q_params: torch.Tensor, var_clip_max: float\n) -&gt; torch.Tensor:\n    \"\"\"\n    Given an input parameter tensor defining q(z),\n    it processes it by calling `process_q_params()` method and\n    sample a latent tensor from the resulting distribution.\n\n    Parameters\n    ----------\n    q_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    _, _, q = self.process_q_params(q_params, var_clip_max)\n    return q.rsample()\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/","title":"utils","text":"<p>Script for utility functions needed by the LVAE model.</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.Interpolate","title":"<code>Interpolate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper for torch.nn.functional.interpolate.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class Interpolate(nn.Module):\n    \"\"\"Wrapper for torch.nn.functional.interpolate.\"\"\"\n\n    def __init__(self, size=None, scale=None, mode=\"bilinear\", align_corners=False):\n        super().__init__()\n        assert (size is None) == (scale is not None)\n        self.size = size\n        self.scale = scale\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        out = F.interpolate(\n            x,\n            size=self.size,\n            scale_factor=self.scale,\n            mode=self.mode,\n            align_corners=self.align_corners,\n        )\n        return out\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableExponential","title":"<code>StableExponential</code>","text":"<p>Class that redefines the definition of exp() to increase numerical stability. Naturally, also the definition of log() must change accordingly. However, it is worth noting that the two operations remain one the inverse of the other, meaning that x = log(exp(x)) and x = exp(log(x)) are always true.</p> <p>Definition:     exp(x) = {         exp(x) if x&lt;=0         x+1    if x&gt;0     }</p> <pre><code>log(x) = {\n    x        if x&lt;=0\n    log(1+x) if x&gt;0\n}\n</code></pre> <p>NOTE 1:     Within the class everything is done on the tensor given as input to the constructor.     Therefore, when exp() is called, self._tensor.exp() is computed.     When log() is called, torch.log(self._tensor.exp()) is computed instead.</p> <p>NOTE 2:     Given the output from exp(), torch.log() or the log() method of the class give identical results.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableExponential:\n    \"\"\"\n    Class that redefines the definition of exp() to increase numerical stability.\n    Naturally, also the definition of log() must change accordingly.\n    However, it is worth noting that the two operations remain one the inverse of the other,\n    meaning that x = log(exp(x)) and x = exp(log(x)) are always true.\n\n    Definition:\n        exp(x) = {\n            exp(x) if x&lt;=0\n            x+1    if x&gt;0\n        }\n\n        log(x) = {\n            x        if x&lt;=0\n            log(1+x) if x&gt;0\n        }\n\n    NOTE 1:\n        Within the class everything is done on the tensor given as input to the constructor.\n        Therefore, when exp() is called, self._tensor.exp() is computed.\n        When log() is called, torch.log(self._tensor.exp()) is computed instead.\n\n    NOTE 2:\n        Given the output from exp(), torch.log() or the log() method of the class give identical results.\n    \"\"\"\n\n    def __init__(self, tensor):\n        self._raw_tensor = tensor\n        posneg_dic = self.posneg_separation(self._raw_tensor)\n        self.pos_f, self.neg_f = posneg_dic[\"filter\"]\n        self.pos_data, self.neg_data = posneg_dic[\"value\"]\n\n    def posneg_separation(self, tensor):\n        pos = tensor &gt; 0\n        pos_tensor = torch.clip(tensor, min=0)\n\n        neg = tensor &lt;= 0\n        neg_tensor = torch.clip(tensor, max=0)\n\n        return {\"filter\": [pos, neg], \"value\": [pos_tensor, neg_tensor]}\n\n    def exp(self):\n        return torch.exp(self.neg_data) * self.neg_f + (1 + self.pos_data) * self.pos_f\n\n    def log(self):\n        return self.neg_data * self.neg_f + torch.log(1 + self.pos_data) * self.pos_f\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar","title":"<code>StableLogVar</code>","text":"<p>Class that provides a numerically stable implementation of Log-Variance. Specifically, it uses the exp() and log() formulas defined in <code>StableExponential</code> class.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableLogVar:\n    \"\"\"\n    Class that provides a numerically stable implementation of Log-Variance.\n    Specifically, it uses the exp() and log() formulas defined in `StableExponential` class.\n    \"\"\"\n\n    def __init__(\n        self, logvar: torch.Tensor, enable_stable: bool = True, var_eps: float = 1e-6\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        logvar: torch.Tensor\n            The input (true) logvar vector, to be converted in the Stable version.\n        enable_stable: bool, optional\n            Whether to compute the stable version of log-variance. Default is `True`.\n        var_eps: float, optional\n            The minimum value attainable by the variance. Default is `1e-6`.\n        \"\"\"\n        self._lv = logvar\n        self._enable_stable = enable_stable\n        self._eps = var_eps\n\n    def get(self) -&gt; torch.Tensor:\n        if self._enable_stable is False:\n            return self._lv\n\n        return torch.log(self.get_var())\n\n    def get_var(self) -&gt; torch.Tensor:\n        \"\"\"\n        Get Variance from Log-Variance.\n        \"\"\"\n        if self._enable_stable is False:\n            return torch.exp(self._lv)\n        return StableExponential(self._lv).exp() + self._eps\n\n    def get_std(self) -&gt; torch.Tensor:\n        return torch.sqrt(self.get_var())\n\n    @property\n    def is_3D(self) -&gt; bool:\n        \"\"\"Check if the _lv tensor is 3D.\n\n        Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).\n        \"\"\"\n        return self._lv.dim() == 5\n\n    def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n        \"\"\"\n        Centercrop the log-variance tensor to the desired size.\n\n        Parameters\n        ----------\n        size: torch.Tensor\n            The desired size of the log-variance tensor.\n        \"\"\"\n        assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n        if self._lv.shape[-1] == size:\n            return\n\n        diff = self._lv.shape[-1] - size\n        assert diff &gt; 0 and diff % 2 == 0\n        self._lv = F.center_crop(self._lv, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.is_3D","title":"<code>is_3D</code>  <code>property</code>","text":"<p>Check if the _lv tensor is 3D.</p> <p>Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.__init__","title":"<code>__init__(logvar, enable_stable=True, var_eps=1e-06)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>logvar</code> <code>Tensor</code> <p>The input (true) logvar vector, to be converted in the Stable version.</p> required <code>enable_stable</code> <code>bool</code> <p>Whether to compute the stable version of log-variance. Default is <code>True</code>.</p> <code>True</code> <code>var_eps</code> <code>float</code> <p>The minimum value attainable by the variance. Default is <code>1e-6</code>.</p> <code>1e-06</code> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def __init__(\n    self, logvar: torch.Tensor, enable_stable: bool = True, var_eps: float = 1e-6\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    logvar: torch.Tensor\n        The input (true) logvar vector, to be converted in the Stable version.\n    enable_stable: bool, optional\n        Whether to compute the stable version of log-variance. Default is `True`.\n    var_eps: float, optional\n        The minimum value attainable by the variance. Default is `1e-6`.\n    \"\"\"\n    self._lv = logvar\n    self._enable_stable = enable_stable\n    self._eps = var_eps\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.centercrop_to_size","title":"<code>centercrop_to_size(size)</code>","text":"<p>Centercrop the log-variance tensor to the desired size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Sequence[int]</code> <p>The desired size of the log-variance tensor.</p> required Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n    \"\"\"\n    Centercrop the log-variance tensor to the desired size.\n\n    Parameters\n    ----------\n    size: torch.Tensor\n        The desired size of the log-variance tensor.\n    \"\"\"\n    assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n    if self._lv.shape[-1] == size:\n        return\n\n    diff = self._lv.shape[-1] - size\n    assert diff &gt; 0 and diff % 2 == 0\n    self._lv = F.center_crop(self._lv, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.get_var","title":"<code>get_var()</code>","text":"<p>Get Variance from Log-Variance.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def get_var(self) -&gt; torch.Tensor:\n    \"\"\"\n    Get Variance from Log-Variance.\n    \"\"\"\n    if self._enable_stable is False:\n        return torch.exp(self._lv)\n    return StableExponential(self._lv).exp() + self._eps\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean","title":"<code>StableMean</code>","text":"Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableMean:\n\n    def __init__(self, mean):\n        self._mean = mean\n\n    def get(self) -&gt; torch.Tensor:\n        return self._mean\n\n    @property\n    def is_3D(self) -&gt; bool:\n        \"\"\"Check if the _mean tensor is 3D.\n\n        Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).\n        \"\"\"\n        return self._mean.dim() == 5\n\n    def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n        \"\"\"Centercrop the mean tensor to the desired size.\n\n        Implemented only in the case of 2D tensors.\n\n        Parameters\n        ----------\n        size: torch.Tensor\n            The desired size of the log-variance tensor.\n        \"\"\"\n        assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n        if self._mean.shape[-1] == size:\n            return\n\n        diff = self._mean.shape[-1] - size\n        assert diff &gt; 0 and diff % 2 == 0\n        self._mean = F.center_crop(self._mean, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean.is_3D","title":"<code>is_3D</code>  <code>property</code>","text":"<p>Check if the _mean tensor is 3D.</p> <p>Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean.centercrop_to_size","title":"<code>centercrop_to_size(size)</code>","text":"<p>Centercrop the mean tensor to the desired size.</p> <p>Implemented only in the case of 2D tensors.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Sequence[int]</code> <p>The desired size of the log-variance tensor.</p> required Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n    \"\"\"Centercrop the mean tensor to the desired size.\n\n    Implemented only in the case of 2D tensors.\n\n    Parameters\n    ----------\n    size: torch.Tensor\n        The desired size of the log-variance tensor.\n    \"\"\"\n    assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n    if self._mean.shape[-1] == size:\n        return\n\n    diff = self._mean.shape[-1] - size\n    assert diff &gt; 0 and diff % 2 == 0\n    self._mean = F.center_crop(self._mean, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.allow_numpy","title":"<code>allow_numpy(func)</code>","text":"<p>All optional arguments are passed as is. positional arguments are checked. if they are numpy array, they are converted to torch Tensor.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def allow_numpy(func):\n    \"\"\"\n    All optional arguments are passed as is. positional arguments are checked. if they are numpy array,\n    they are converted to torch Tensor.\n    \"\"\"\n\n    def numpy_wrapper(*args, **kwargs):\n        new_args = []\n        for arg in args:\n            if isinstance(arg, np.ndarray):\n                arg = torch.Tensor(arg)\n            new_args.append(arg)\n        new_args = tuple(new_args)\n\n        output = func(*new_args, **kwargs)\n        return output\n\n    return numpy_wrapper\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.crop_img_tensor","title":"<code>crop_img_tensor(x, size)</code>","text":"<p>Crops a tensor. Crops a tensor of shape (batch, channels, h, w) to a desired height and width given by a tuple. Args:     x (torch.Tensor): Input image     size (list or tuple): Desired size (height, width)</p> <p>Returns:</p> Type Description <code>    The cropped tensor</code> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def crop_img_tensor(x, size) -&gt; torch.Tensor:\n    \"\"\"Crops a tensor.\n    Crops a tensor of shape (batch, channels, h, w) to a desired height and width\n    given by a tuple.\n    Args:\n        x (torch.Tensor): Input image\n        size (list or tuple): Desired size (height, width)\n\n    Returns\n    -------\n        The cropped tensor\n    \"\"\"\n    return _pad_crop_img(x, size, \"crop\")\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.kl_normal_mc","title":"<code>kl_normal_mc(z, p_mulv, q_mulv)</code>","text":"<p>One-sample estimation of element-wise KL between two diagonal multivariate normal distributions. Any number of dimensions, broadcasting supported (be careful). :param z: :param p_mulv: :param q_mulv: :return:</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def kl_normal_mc(z, p_mulv, q_mulv):\n    \"\"\"\n    One-sample estimation of element-wise KL between two diagonal\n    multivariate normal distributions. Any number of dimensions,\n    broadcasting supported (be careful).\n    :param z:\n    :param p_mulv:\n    :param q_mulv:\n    :return:\n    \"\"\"\n    assert isinstance(p_mulv, tuple)\n    assert isinstance(q_mulv, tuple)\n    p_mu, p_lv = p_mulv\n    q_mu, q_lv = q_mulv\n\n    p_std = p_lv.get_std()\n    q_std = q_lv.get_std()\n\n    p_distrib = Normal(p_mu.get(), p_std)\n    q_distrib = Normal(q_mu.get(), q_std)\n    return q_distrib.log_prob(z) - p_distrib.log_prob(z)\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.pad_img_tensor","title":"<code>pad_img_tensor(x, size)</code>","text":"<p>Pads a tensor</p> <p>Pads a tensor of shape (B, C, [Z], Y, X) to desired spatial dimensions.</p> Parameters: <pre><code>x (torch.Tensor): Input image of shape (B, C, [Z], Y, X)\nsize (list or tuple): Desired size  ([Z*], Y*, X*)\n</code></pre> Returns: <pre><code>The padded tensor\n</code></pre> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def pad_img_tensor(x: torch.Tensor, size: Sequence[int]) -&gt; torch.Tensor:\n    \"\"\"Pads a tensor\n\n    Pads a tensor of shape (B, C, [Z], Y, X) to desired spatial dimensions.\n\n    Parameters:\n    -----------\n        x (torch.Tensor): Input image of shape (B, C, [Z], Y, X)\n        size (list or tuple): Desired size  ([Z*], Y*, X*)\n\n    Returns:\n    --------\n        The padded tensor\n    \"\"\"\n    return _pad_crop_img(x, size, \"pad\")\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/","title":"lvae_prediction","text":"<p>Module containing pytorch implementations for obtaining predictions from an LVAE.</p>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_mmse_tiled_batch","title":"<code>lvae_predict_mmse_tiled_batch(model, likelihood_obj, input, mmse_count)</code>","text":"<p>Generate the MMSE (minimum mean squared error) prediction, for a given input.</p> <p>This is calculated from the mean of multiple single sample predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>torch.tensor | tuple of (torch.tensor, Any, ...)</code> <p>Input to generate prediction for. This can include auxilary inputs such as <code>TileInformation</code>, but the model input is always the first item of the tuple. Expected shape of the model input is (S, C, Y, X).</p> required <code>mmse_count</code> <code>int</code> <p>Number of samples to generate to calculate MMSE (minimum mean squared error).</p> required <p>Returns:</p> Type Description <code>tuple of (tuple of (torch.Tensor[Any], Any, ...))</code> <p>A tuple of 3 elements. The first element contains the MMSE prediction, the second contains the standard deviation of the samples used to create the MMSE prediction. Finally the last element contains the log-variance of the likelihood, this will be <code>None</code> if <code>likelihood.predict_logvar</code> is <code>None</code>. Any auxillary data included in the input will also be include with all of the MMSE prediction, the standard deviation, and the log-variance.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_mmse_tiled_batch(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: tuple[Any],\n    mmse_count: int,\n) -&gt; tuple[tuple[Any], tuple[Any], tuple[Any] | None]:\n    # TODO: fix docstring return types, ... hard to make readable\n    \"\"\"\n    Generate the MMSE (minimum mean squared error) prediction, for a given input.\n\n    This is calculated from the mean of multiple single sample predictions.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor | tuple of (torch.tensor, Any, ...)\n        Input to generate prediction for. This can include auxilary inputs such as\n        `TileInformation`, but the model input is always the first item of the tuple.\n        Expected shape of the model input is (S, C, Y, X).\n    mmse_count : int\n        Number of samples to generate to calculate MMSE (minimum mean squared error).\n\n    Returns\n    -------\n    tuple of (tuple of (torch.Tensor[Any], Any, ...))\n        A tuple of 3 elements. The first element contains the MMSE prediction, the\n        second contains the standard deviation of the samples used to create the MMSE\n        prediction. Finally the last element contains the log-variance of the\n        likelihood, this will be `None` if `likelihood.predict_logvar` is `None`.\n        Any auxillary data included in the input will also be include with all of the\n        MMSE prediction, the standard deviation, and the log-variance.\n    \"\"\"\n    if mmse_count &lt;= 0:\n        raise ValueError(\"MMSE count must be greater than zero.\")\n\n    x: torch.Tensor\n    aux: list[Any]\n    x, *aux = input\n\n    input_shape = x.shape\n    output_shape = (input_shape[0], model.target_ch, *input_shape[2:])\n    log_var: torch.Tensor | None = None\n    # pre-declare empty array to fill with individual sample predictions\n    sample_predictions = torch.zeros(size=(mmse_count, *output_shape))\n    for mmse_idx in range(mmse_count):\n        sample_prediction, lv = lvae_predict_single_sample(\n            model=model, likelihood_obj=likelihood_obj, input=x\n        )\n        # only keep the log variance of the first sample prediction\n        if mmse_idx == 0:\n            log_var = lv\n\n        # store sample predictions\n        sample_predictions[mmse_idx, ...] = sample_prediction\n\n    mmse_prediction = torch.mean(sample_predictions, dim=0)\n    mmse_prediction_std = torch.std(sample_predictions, dim=0)\n\n    log_var_output = (log_var, *aux) if log_var is not None else None\n    return (mmse_prediction, *aux), (mmse_prediction_std, *aux), log_var_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_single_sample","title":"<code>lvae_predict_single_sample(model, likelihood_obj, input)</code>","text":"<p>Generate a single sample prediction from an LVAE model, for a given input.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>tensor</code> <p>Input to generate prediction for. Expected shape is (S, C, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of (torch.tensor, optional torch.tensor)</code> <p>The first element is the sample prediction, and the second element is the log-variance. The log-variance will be None if <code>model.predict_logvar is None</code>.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_single_sample(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"\n    Generate a single sample prediction from an LVAE model, for a given input.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor\n        Input to generate prediction for. Expected shape is (S, C, Y, X).\n\n    Returns\n    -------\n    tuple of (torch.tensor, optional torch.tensor)\n        The first element is the sample prediction, and the second element is the\n        log-variance. The log-variance will be None if `model.predict_logvar is None`.\n    \"\"\"\n    model.eval()  # Not in original predict code: effects batch_norm and dropout layers\n    with torch.no_grad():\n        output: torch.Tensor\n        output, _ = model(input)  # 2nd item is top-down data dict\n\n    # presently, get_mean_lv just splits the output in 2 if predict_logvar=True,\n    #   optionally clips the logvavr if logvar_lowerbound is not None\n    # TODO: consider refactoring to remove use of the likelihood object\n    sample_prediction, log_var = likelihood_obj.get_mean_lv(output)\n\n    # TODO: output denormalization using target stats that will be saved in data config\n    # -&gt; Don't think we need this, saw it in a random bit of code somewhere.\n\n    return sample_prediction, log_var\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_tiled_batch","title":"<code>lvae_predict_tiled_batch(model, likelihood_obj, input)</code>","text":"<p>Generate a single sample prediction from an LVAE model, for a given input.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>torch.tensor | tuple of (torch.tensor, Any, ...)</code> <p>Input to generate prediction for. This can include auxilary inputs such as <code>TileInformation</code>, but the model input is always the first item of the tuple. Expected shape of the model input is (S, C, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of ((torch.tensor, Any, ...), optional tuple of (torch.tensor, Any, ...))</code> <p>The first element is the sample prediction, and the second element is the log-variance. The log-variance will be None if <code>model.predict_logvar is None</code>. Any auxillary data included in the input will also be include with both the sample prediction and the log-variance.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_tiled_batch(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: tuple[Any],\n) -&gt; tuple[tuple[Any], tuple[Any] | None]:\n    # TODO: fix docstring return types, ... too many output options\n    \"\"\"\n    Generate a single sample prediction from an LVAE model, for a given input.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor | tuple of (torch.tensor, Any, ...)\n        Input to generate prediction for. This can include auxilary inputs such as\n        `TileInformation`, but the model input is always the first item of the tuple.\n        Expected shape of the model input is (S, C, Y, X).\n\n    Returns\n    -------\n    tuple of ((torch.tensor, Any, ...), optional tuple of (torch.tensor, Any, ...))\n        The first element is the sample prediction, and the second element is the\n        log-variance. The log-variance will be None if `model.predict_logvar is None`.\n        Any auxillary data included in the input will also be include with both the\n        sample prediction and the log-variance.\n    \"\"\"\n    x: torch.Tensor\n    aux: list[Any]\n    x, *aux = input\n\n    sample_prediction, log_var = lvae_predict_single_sample(\n        model=model, likelihood_obj=likelihood_obj, input=x\n    )\n\n    log_var_output = (log_var, *aux) if log_var is not None else None\n    return (sample_prediction, *aux), log_var_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_tiling_manager/","title":"lvae_tiling_manager","text":"<p>Module contiaing tiling manager class.</p>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/","title":"prediction_outputs","text":"<p>Module containing functions to convert prediction outputs to desired form.</p>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.combine_batches","title":"<code>combine_batches(predictions, tiled)</code>","text":"<pre><code>combine_batches(predictions: list[Any], tiled: Literal[True]) -&gt; tuple[list[NDArray], list[TileInformation]]\n</code></pre><pre><code>combine_batches(predictions: list[Any], tiled: Literal[False]) -&gt; list[NDArray]\n</code></pre><pre><code>combine_batches(predictions: list[Any], tiled: Union[bool, Literal[True], Literal[False]]) -&gt; Union[list[NDArray], tuple[list[NDArray], list[TileInformation]]]\n</code></pre> <p>If predictions are in batches, they will be combined.</p>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.combine_batches--todo-improve-description","title":"TODO improve description!","text":"<p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>Predictions that are output from <code>Trainer.predict</code>.</p> required <code>tiled</code> <code>bool</code> <p>Whether the predictions are tiled.</p> required <p>Returns:</p> Type Description <code>(list of numpy.ndarray) or tuple of (list of numpy.ndarray, list of TileInformation)</code> <p>Combined batches.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def combine_batches(\n    predictions: list[Any], tiled: bool\n) -&gt; Union[list[NDArray], tuple[list[NDArray], list[TileInformation]]]:\n    \"\"\"\n    If predictions are in batches, they will be combined.\n\n    # TODO improve description!\n\n    Parameters\n    ----------\n    predictions : list\n        Predictions that are output from `Trainer.predict`.\n    tiled : bool\n        Whether the predictions are tiled.\n\n    Returns\n    -------\n    (list of numpy.ndarray) or tuple of (list of numpy.ndarray, list of TileInformation)\n        Combined batches.\n    \"\"\"\n    if tiled:\n        return _combine_tiled_batches(predictions)\n    else:\n        return _combine_array_batches(predictions)\n</code></pre>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.convert_outputs","title":"<code>convert_outputs(predictions, tiled)</code>","text":"<p>Convert the Lightning trainer outputs to the desired form.</p> <p>This method allows stitching back together tiled predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>Predictions that are output from <code>Trainer.predict</code>.</p> required <code>tiled</code> <code>bool</code> <p>Whether the predictions are tiled.</p> required <p>Returns:</p> Type Description <code>list of numpy.ndarray or numpy.ndarray</code> <p>list of arrays with the axes SC(Z)YX. If there is only 1 output it will not be in a list.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def convert_outputs(predictions: list[Any], tiled: bool) -&gt; list[NDArray]:\n    \"\"\"\n    Convert the Lightning trainer outputs to the desired form.\n\n    This method allows stitching back together tiled predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Predictions that are output from `Trainer.predict`.\n    tiled : bool\n        Whether the predictions are tiled.\n\n    Returns\n    -------\n    list of numpy.ndarray or numpy.ndarray\n        list of arrays with the axes SC(Z)YX. If there is only 1 output it will not\n        be in a list.\n    \"\"\"\n    if len(predictions) == 0:\n        return predictions\n\n    # this layout is to stop mypy complaining\n    if tiled:\n        predictions_comb = combine_batches(predictions, tiled)\n        predictions_output = stitch_prediction(*predictions_comb)\n    else:\n        predictions_output = combine_batches(predictions, tiled)\n\n    return predictions_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.convert_outputs_microsplit","title":"<code>convert_outputs_microsplit(predictions, dataset)</code>","text":"<p>Convert microsplit Lightning trainer outputs using eval_utils stitching functions.</p> <p>This function processes microsplit predictions that return (tile_prediction, tile_std) tuples and stitches them back together using the same logic as get_single_file_mmse.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list of tuple[NDArray, NDArray]</code> <p>Predictions from Lightning trainer for microsplit. Each element is a tuple of (tile_prediction, tile_std) where both are numpy arrays from predict_step.</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset object used for prediction, needed for stitching function selection and stitching process.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>A tuple of (stitched_predictions, stitched_stds) representing the full stitched predictions and standard deviations.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def convert_outputs_microsplit(\n    predictions: list[tuple[NDArray, NDArray]], dataset\n) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Convert microsplit Lightning trainer outputs using eval_utils stitching functions.\n\n    This function processes microsplit predictions that return (tile_prediction,\n    tile_std) tuples and stitches them back together using the same logic as\n    get_single_file_mmse.\n\n    Parameters\n    ----------\n    predictions : list of tuple[NDArray, NDArray]\n        Predictions from Lightning trainer for microsplit. Each element is a tuple of\n        (tile_prediction, tile_std) where both are numpy arrays from predict_step.\n    dataset : Dataset\n        The dataset object used for prediction, needed for stitching function selection\n        and stitching process.\n\n    Returns\n    -------\n    tuple[NDArray, NDArray]\n        A tuple of (stitched_predictions, stitched_stds) representing the full\n        stitched predictions and standard deviations.\n    \"\"\"\n    if len(predictions) == 0:\n        raise ValueError(\"No predictions provided\")\n\n    # Separate predictions and stds from the list of tuples\n    tile_predictions = [pred for pred, _ in predictions]\n    tile_stds = [std for _, std in predictions]\n\n    # Concatenate all tiles exactly like get_single_file_mmse\n    tiles_arr = np.concatenate(tile_predictions, axis=0)\n    tile_stds_arr = np.concatenate(tile_stds, axis=0)\n\n    # Apply stitching using stitch_predictions_new\n    stitched_predictions = stitch_prediction_vae(tiles_arr, dataset)\n    stitched_stds = stitch_prediction_vae(tile_stds_arr, dataset)\n\n    return stitched_predictions, stitched_stds\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/","title":"stitch_prediction","text":"<p>Prediction utility functions.</p>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.TilingMode","title":"<code>TilingMode</code>","text":"<p>Enum for the tiling mode.</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>class TilingMode:\n    \"\"\"Enum for the tiling mode.\"\"\"\n\n    TrimBoundary = 0\n    PadBoundary = 1\n    ShiftBoundary = 2\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.stitch_prediction","title":"<code>stitch_prediction(tiles, tile_infos)</code>","text":"<p>Stitch tiles back together to form a full image(s).</p> <p>Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a singleton dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list of numpy.ndarray</code> <p>Cropped tiles and their respective stitching coordinates. Can contain tiles from multiple images.</p> required <code>tile_infos</code> <code>list of TileInformation</code> <p>List of information and coordinates obtained from <code>dataset.tiled_patching.extract_tiles</code>.</p> required <p>Returns:</p> Type Description <code>list of numpy.ndarray</code> <p>Full image(s).</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>def stitch_prediction(\n    tiles: list[np.ndarray],\n    tile_infos: list[TileInformation],\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Stitch tiles back together to form a full image(s).\n\n    Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a\n    singleton dimension.\n\n    Parameters\n    ----------\n    tiles : list of numpy.ndarray\n        Cropped tiles and their respective stitching coordinates. Can contain tiles\n        from multiple images.\n    tile_infos : list of TileInformation\n        List of information and coordinates obtained from\n        `dataset.tiled_patching.extract_tiles`.\n\n    Returns\n    -------\n    list of numpy.ndarray\n        Full image(s).\n    \"\"\"\n    # Find where to split the lists so that only info from one image is contained.\n    # Do this by locating the last tiles of each image.\n    last_tiles = [tile_info.last_tile for tile_info in tile_infos]\n    last_tile_position = np.where(last_tiles)[0]\n    image_slices = [\n        slice(\n            None if i == 0 else last_tile_position[i - 1] + 1, last_tile_position[i] + 1\n        )\n        for i in range(len(last_tile_position))\n    ]\n    image_predictions = []\n    # slice the lists and apply stitch_prediction_single to each in turn.\n    for image_slice in image_slices:\n        image_predictions.append(\n            stitch_prediction_single(tiles[image_slice], tile_infos[image_slice])\n        )\n    return image_predictions\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.stitch_prediction_single","title":"<code>stitch_prediction_single(tiles, tile_infos)</code>","text":"<p>Stitch tiles back together to form a full image.</p> <p>Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a singleton dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list of numpy.ndarray</code> <p>Cropped tiles and their respective stitching coordinates.</p> required <code>tile_infos</code> <code>list of TileInformation</code> <p>List of information and coordinates obtained from <code>dataset.tiled_patching.extract_tiles</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Full image, with dimensions SC(Z)YX.</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>def stitch_prediction_single(\n    tiles: list[NDArray],\n    tile_infos: list[TileInformation],\n) -&gt; NDArray:\n    \"\"\"\n    Stitch tiles back together to form a full image.\n\n    Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a\n    singleton dimension.\n\n    Parameters\n    ----------\n    tiles : list of numpy.ndarray\n        Cropped tiles and their respective stitching coordinates.\n    tile_infos : list of TileInformation\n        List of information and coordinates obtained from\n        `dataset.tiled_patching.extract_tiles`.\n\n    Returns\n    -------\n    numpy.ndarray\n        Full image, with dimensions SC(Z)YX.\n    \"\"\"\n    # TODO: this is hacky... need a better way to deal with when input channels and\n    #   target channels do not match\n    if len(tile_infos[0].array_shape) == 4:\n        # 4 dimensions =&gt; 3 spatial dimensions so -4 is channel dimension\n        tile_channels = tiles[0].shape[-4]\n    elif len(tile_infos[0].array_shape) == 3:\n        # 3 dimensions =&gt; 2 spatial dimensions so -3 is channel dimension\n        tile_channels = tiles[0].shape[-3]\n    else:\n        # Note pretty sure this is unreachable because array shape is already\n        #   validated by TileInformation\n        raise ValueError(\n            f\"Unsupported number of output dimension {len(tile_infos[0].array_shape)}\"\n        )\n    # retrieve whole array size, add S dim and use number of channels in tile\n    input_shape = (1, tile_channels, *tile_infos[0].array_shape[1:])\n    predicted_image = np.zeros(input_shape, dtype=np.float32)\n\n    for tile, tile_info in zip(tiles, tile_infos, strict=False):\n\n        # Compute coordinates for cropping predicted tile\n        crop_slices: tuple[Union[builtins.ellipsis, slice], ...] = (\n            ...,\n            *[slice(c[0], c[1]) for c in tile_info.overlap_crop_coords],\n        )\n\n        # Crop predited tile according to overlap coordinates\n        cropped_tile = tile[crop_slices]\n\n        # Insert cropped tile into predicted image using stitch coordinates\n        image_slices = (..., *[slice(c[0], c[1]) for c in tile_info.stitch_coords])\n\n        # TODO fix mypy error here, potentially due to numpy 2\n        predicted_image[image_slices] = cropped_tile.astype(np.float32)  # type: ignore\n\n    return predicted_image\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.stitch_prediction_vae","title":"<code>stitch_prediction_vae(predictions, dset)</code>","text":"<p>Stitch predictions back together using dataset's index manager.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Array of predictions with shape (n_tiles, channels, height, width).</p> required <code>dset</code> <code>Dataset</code> <p>Dataset object with idx_manager containing tiling information.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Stitched array with shape matching the original data shape.</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>def stitch_prediction_vae(predictions, dset) -&gt; NDArray:\n    \"\"\"\n    Stitch predictions back together using dataset's index manager.\n\n    Parameters\n    ----------\n    predictions : np.ndarray\n        Array of predictions with shape (n_tiles, channels, height, width).\n    dset : Dataset\n        Dataset object with idx_manager containing tiling information.\n\n    Returns\n    -------\n    np.ndarray\n        Stitched array with shape matching the original data shape.\n    \"\"\"\n    mng = dset.idx_manager\n\n    # if there are more channels, use all of them.\n    shape = list(dset.get_data_shape())\n    shape[-1] = max(shape[-1], predictions.shape[1])\n\n    output = np.zeros(shape, dtype=predictions.dtype)\n    # frame_shape = dset.get_data_shape()[:-1]\n    for dset_idx in range(predictions.shape[0]):\n        # loc = get_location_from_idx(dset, dset_idx, predictions.shape[-2],\n        # predictions.shape[-1])\n        # grid start, grid end\n        gs = np.array(mng.get_location_from_dataset_idx(dset_idx), dtype=int)\n        ge = gs + mng.grid_shape\n\n        # patch start, patch end\n        ps = gs - mng.patch_offset()\n        pe = ps + mng.patch_shape\n\n        # valid grid start, valid grid end\n        vgs = np.array([max(0, x) for x in gs], dtype=int)\n        vge = np.array(\n            [min(x, y) for x, y in zip(ge, mng.data_shape, strict=False)], dtype=int\n        )\n\n        if mng.tiling_mode == TilingMode.ShiftBoundary:\n            for dim in range(len(vgs)):\n                if ps[dim] == 0:\n                    vgs[dim] = 0\n                if pe[dim] == mng.data_shape[dim]:\n                    vge[dim] = mng.data_shape[dim]\n\n        # relative start, relative end. This will be used on pred_tiled\n        rs = vgs - ps\n        re = rs + (vge - vgs)\n\n        for ch_idx in range(predictions.shape[1]):\n            if len(output.shape) == 4:\n                # channel dimension is the last one.\n                output[vgs[0] : vge[0], vgs[1] : vge[1], vgs[2] : vge[2], ch_idx] = (\n                    predictions[dset_idx][ch_idx, rs[1] : re[1], rs[2] : re[2]]\n                )\n            elif len(output.shape) == 5:\n                # channel dimension is the last one.\n                assert vge[0] - vgs[0] == 1, \"Only one frame is supported\"\n                output[\n                    vgs[0], vgs[1] : vge[1], vgs[2] : vge[2], vgs[3] : vge[3], ch_idx\n                ] = predictions[dset_idx][\n                    ch_idx, rs[1] : re[1], rs[2] : re[2], rs[3] : re[3]\n                ]\n            else:\n                raise ValueError(f\"Unsupported shape {output.shape}\")\n\n    return output\n</code></pre>"},{"location":"reference/careamics/transforms/compose/","title":"compose","text":"<p>A class chaining transforms together.</p>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose","title":"<code>Compose</code>","text":"<p>A class chaining transforms together.</p> <p>Parameters:</p> Name Type Description Default <code>transform_list</code> <code>list[TransformModel]</code> <p>A list of dictionaries where each dictionary contains the name of a transform and its parameters.</p> required <p>Attributes:</p> Name Type Description <code>_callable_transforms</code> <code>Callable</code> <p>A callable that applies the transforms to the input data.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>class Compose:\n    \"\"\"A class chaining transforms together.\n\n    Parameters\n    ----------\n    transform_list : list[TransformModel]\n        A list of dictionaries where each dictionary contains the name of a\n        transform and its parameters.\n\n    Attributes\n    ----------\n    _callable_transforms : Callable\n        A callable that applies the transforms to the input data.\n    \"\"\"\n\n    def __init__(self, transform_list: list[NORM_AND_SPATIAL_UNION]) -&gt; None:\n        \"\"\"Instantiate a Compose object.\n\n        Parameters\n        ----------\n        transform_list : list[NORM_AND_SPATIAL_UNION]\n            A list of dictionaries where each dictionary contains the name of a\n            transform and its parameters.\n        \"\"\"\n        # retrieve all available transforms\n        # TODO: correctly type hint get_all_transforms function output\n        all_transforms: dict[str, type[Transform]] = get_all_transforms()\n\n        # instantiate all transforms\n        self.transforms: list[Transform] = [\n            all_transforms[t.name](**t.model_dump()) for t in transform_list\n        ]\n\n    def _chain_transforms(\n        self, patch: NDArray, target: NDArray | None\n    ) -&gt; tuple[NDArray | None, ...]:\n        \"\"\"Chain transforms on the input data.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Input data.\n        target : Optional[np.ndarray]\n            Target data, by default None.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            The output of the transformations.\n        \"\"\"\n        params: Union[tuple[NDArray, NDArray | None],] = (patch, target)\n\n        for t in self.transforms:\n            *params, _ = t(*params)  # ignore additional_arrays dict\n\n        # avoid None values that create problems for collating\n        # TODO: removing None should be handled in dataset, not here\n        return tuple(p for p in params if p is not None)\n\n    def _chain_transforms_additional_arrays(\n        self,\n        patch: NDArray,\n        target: NDArray | None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n        \"\"\"Chain transforms on the input data, with additional arrays.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Input data.\n        target : Optional[np.ndarray]\n            Target data, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            The output of the transformations.\n        \"\"\"\n        params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n        for t in self.transforms:\n            patch, target, additional_arrays = t(**params)\n            params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n        return patch, target, additional_arrays\n\n    def __call__(\n        self, patch: NDArray, target: NDArray | None = None\n    ) -&gt; tuple[NDArray, ...]:\n        \"\"\"Apply the transforms to the input data.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            The input data.\n        target : Optional[np.ndarray], optional\n            Target data, by default None.\n\n        Returns\n        -------\n        tuple[np.ndarray, ...]\n            The output of the transformations.\n        \"\"\"\n        # TODO: solve casting Compose.__call__ ouput\n        return cast(tuple[NDArray, ...], self._chain_transforms(patch, target))\n\n    def transform_with_additional_arrays(\n        self,\n        patch: NDArray,\n        target: NDArray | None = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n        \"\"\"Apply the transforms to the input data, including additional arrays.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            The input data.\n        target : Optional[np.ndarray], optional\n            Target data, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        NDArray\n            The transformed patch.\n        NDArray | None\n            The transformed target.\n        dict of {str, NDArray}\n            Transformed additional arrays. Keys correspond to the keyword argument\n            names.\n        \"\"\"\n        return self._chain_transforms_additional_arrays(\n            patch, target, **additional_arrays\n        )\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.__call__","title":"<code>__call__(patch, target=None)</code>","text":"<p>Apply the transforms to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>The input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ...]</code> <p>The output of the transformations.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def __call__(\n    self, patch: NDArray, target: NDArray | None = None\n) -&gt; tuple[NDArray, ...]:\n    \"\"\"Apply the transforms to the input data.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        The input data.\n    target : Optional[np.ndarray], optional\n        Target data, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray, ...]\n        The output of the transformations.\n    \"\"\"\n    # TODO: solve casting Compose.__call__ ouput\n    return cast(tuple[NDArray, ...], self._chain_transforms(patch, target))\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.__init__","title":"<code>__init__(transform_list)</code>","text":"<p>Instantiate a Compose object.</p> <p>Parameters:</p> Name Type Description Default <code>transform_list</code> <code>list[NORM_AND_SPATIAL_UNION]</code> <p>A list of dictionaries where each dictionary contains the name of a transform and its parameters.</p> required Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def __init__(self, transform_list: list[NORM_AND_SPATIAL_UNION]) -&gt; None:\n    \"\"\"Instantiate a Compose object.\n\n    Parameters\n    ----------\n    transform_list : list[NORM_AND_SPATIAL_UNION]\n        A list of dictionaries where each dictionary contains the name of a\n        transform and its parameters.\n    \"\"\"\n    # retrieve all available transforms\n    # TODO: correctly type hint get_all_transforms function output\n    all_transforms: dict[str, type[Transform]] = get_all_transforms()\n\n    # instantiate all transforms\n    self.transforms: list[Transform] = [\n        all_transforms[t.name](**t.model_dump()) for t in transform_list\n    ]\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.transform_with_additional_arrays","title":"<code>transform_with_additional_arrays(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transforms to the input data, including additional arrays.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>The input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>The transformed patch.</p> <code>NDArray | None</code> <p>The transformed target.</p> <code>dict of {str, NDArray}</code> <p>Transformed additional arrays. Keys correspond to the keyword argument names.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def transform_with_additional_arrays(\n    self,\n    patch: NDArray,\n    target: NDArray | None = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n    \"\"\"Apply the transforms to the input data, including additional arrays.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        The input data.\n    target : Optional[np.ndarray], optional\n        Target data, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    NDArray\n        The transformed patch.\n    NDArray | None\n        The transformed target.\n    dict of {str, NDArray}\n        Transformed additional arrays. Keys correspond to the keyword argument\n        names.\n    \"\"\"\n    return self._chain_transforms_additional_arrays(\n        patch, target, **additional_arrays\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.get_all_transforms","title":"<code>get_all_transforms()</code>","text":"<p>Return all the transforms accepted by CAREamics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with all the transforms accepted by CAREamics, where the keys are the transform names and the values are the transform classes.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def get_all_transforms() -&gt; dict[str, type]:\n    \"\"\"Return all the transforms accepted by CAREamics.\n\n    Returns\n    -------\n    dict\n        A dictionary with all the transforms accepted by CAREamics, where the keys are\n        the transform names and the values are the transform classes.\n    \"\"\"\n    return ALL_TRANSFORMS\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/","title":"n2v_manipulate","text":"<p>N2V manipulation transform.</p>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate","title":"<code>N2VManipulate</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Default augmentation for the N2V model.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <code>int</code> <p>Size of the replacement area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask, by default 0.2.</p> <code>0.2</code> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median, by default uniform.</p> <code>value</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch, by default True.</p> <code>True</code> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>StructN2V mask axis, by default \"none\".</p> <code>'none'</code> <code>struct_mask_span</code> <code>int</code> <p>StructN2V mask span, by default 5.</p> <code>5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask.</p> <code>roi_size</code> <code>int</code> <p>Size of the replacement area.</p> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median.</p> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch.</p> <code>struct_mask</code> <code>Optional[StructMaskParameters]</code> <p>StructN2V mask parameters.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>class N2VManipulate(Transform):\n    \"\"\"\n    Default augmentation for the N2V model.\n\n    This transform expects C(Z)YX dimensions.\n\n    Parameters\n    ----------\n    roi_size : int, optional\n        Size of the replacement area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels to mask, by default 0.2.\n    strategy : Literal[ \"uniform\", \"median\" ], optional\n        Replaccement strategy, uniform or median, by default uniform.\n    remove_center : bool, optional\n        Whether to remove central pixel from patch, by default True.\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        StructN2V mask axis, by default \"none\".\n    struct_mask_span : int, optional\n        StructN2V mask span, by default 5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n\n    Attributes\n    ----------\n    masked_pixel_percentage : float\n        Percentage of pixels to mask.\n    roi_size : int\n        Size of the replacement area.\n    strategy : Literal[ \"uniform\", \"median\" ]\n        Replaccement strategy, uniform or median.\n    remove_center : bool\n        Whether to remove central pixel from patch.\n    struct_mask : Optional[StructMaskParameters]\n        StructN2V mask parameters.\n    rng : Generator\n        Random number generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        roi_size: int = 11,\n        masked_pixel_percentage: float = 0.2,\n        strategy: Literal[\n            \"uniform\", \"median\"\n        ] = SupportedPixelManipulation.UNIFORM.value,\n        remove_center: bool = True,\n        struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n        struct_mask_span: int = 5,\n        seed: int | None = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        roi_size : int, optional\n            Size of the replacement area, by default 11.\n        masked_pixel_percentage : float, optional\n            Percentage of pixels to mask, by default 0.2.\n        strategy : Literal[ \"uniform\", \"median\" ], optional\n            Replaccement strategy, uniform or median, by default uniform.\n        remove_center : bool, optional\n            Whether to remove central pixel from patch, by default True.\n        struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n            StructN2V mask axis, by default \"none\".\n        struct_mask_span : int, optional\n            StructN2V mask span, by default 5.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        \"\"\"\n        self.masked_pixel_percentage = masked_pixel_percentage\n        self.roi_size = roi_size\n        self.strategy = strategy\n        self.remove_center = remove_center  # TODO is this ever used?\n\n        if struct_mask_axis == SupportedStructAxis.NONE:\n            self.struct_mask: StructMaskParameters | None = None\n        else:\n            self.struct_mask = StructMaskParameters(\n                axis=0 if struct_mask_axis == SupportedStructAxis.HORIZONTAL else 1,\n                span=struct_mask_span,\n            )\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self, patch: NDArray, *args: Any, **kwargs: Any\n    ) -&gt; tuple[NDArray, NDArray, NDArray]:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        *args : Any\n            Additional arguments, unused.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray, np.ndarray]\n            Masked patch, original patch, and mask.\n        \"\"\"\n        masked = np.zeros_like(patch)\n        mask = np.zeros_like(patch)\n        if self.strategy == SupportedPixelManipulation.UNIFORM:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(patch.shape[0]):\n                masked[c, ...], mask[c, ...] = uniform_manipulate(\n                    patch=patch[c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    remove_center=self.remove_center,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        elif self.strategy == SupportedPixelManipulation.MEDIAN:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(patch.shape[0]):\n                masked[c, ...], mask[c, ...] = median_manipulate(\n                    patch=patch[c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        else:\n            raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n        # TODO: Output does not match other transforms, how to resolve?\n        #     - Don't include in Compose and apply after if algorithm is N2V?\n        #     - or just don't return patch? but then mask is in the target position\n        # TODO why return patch?\n        return masked, patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate.__call__","title":"<code>__call__(patch, *args, **kwargs)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape C(Z)YX.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments, unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray]</code> <p>Masked patch, original patch, and mask.</p> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>def __call__(\n    self, patch: NDArray, *args: Any, **kwargs: Any\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape C(Z)YX.\n    *args : Any\n        Additional arguments, unused.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray, np.ndarray]\n        Masked patch, original patch, and mask.\n    \"\"\"\n    masked = np.zeros_like(patch)\n    mask = np.zeros_like(patch)\n    if self.strategy == SupportedPixelManipulation.UNIFORM:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(patch.shape[0]):\n            masked[c, ...], mask[c, ...] = uniform_manipulate(\n                patch=patch[c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                remove_center=self.remove_center,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    elif self.strategy == SupportedPixelManipulation.MEDIAN:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(patch.shape[0]):\n            masked[c, ...], mask[c, ...] = median_manipulate(\n                patch=patch[c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    else:\n        raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n    # TODO: Output does not match other transforms, how to resolve?\n    #     - Don't include in Compose and apply after if algorithm is N2V?\n    #     - or just don't return patch? but then mask is in the target position\n    # TODO why return patch?\n    return masked, patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate.__init__","title":"<code>__init__(roi_size=11, masked_pixel_percentage=0.2, strategy=SupportedPixelManipulation.UNIFORM.value, remove_center=True, struct_mask_axis='none', struct_mask_span=5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <code>int</code> <p>Size of the replacement area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask, by default 0.2.</p> <code>0.2</code> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median, by default uniform.</p> <code>value</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch, by default True.</p> <code>True</code> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>StructN2V mask axis, by default \"none\".</p> <code>'none'</code> <code>struct_mask_span</code> <code>int</code> <p>StructN2V mask span, by default 5.</p> <code>5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>def __init__(\n    self,\n    roi_size: int = 11,\n    masked_pixel_percentage: float = 0.2,\n    strategy: Literal[\n        \"uniform\", \"median\"\n    ] = SupportedPixelManipulation.UNIFORM.value,\n    remove_center: bool = True,\n    struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_mask_span: int = 5,\n    seed: int | None = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    roi_size : int, optional\n        Size of the replacement area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels to mask, by default 0.2.\n    strategy : Literal[ \"uniform\", \"median\" ], optional\n        Replaccement strategy, uniform or median, by default uniform.\n    remove_center : bool, optional\n        Whether to remove central pixel from patch, by default True.\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        StructN2V mask axis, by default \"none\".\n    struct_mask_span : int, optional\n        StructN2V mask span, by default 5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n    self.masked_pixel_percentage = masked_pixel_percentage\n    self.roi_size = roi_size\n    self.strategy = strategy\n    self.remove_center = remove_center  # TODO is this ever used?\n\n    if struct_mask_axis == SupportedStructAxis.NONE:\n        self.struct_mask: StructMaskParameters | None = None\n    else:\n        self.struct_mask = StructMaskParameters(\n            axis=0 if struct_mask_axis == SupportedStructAxis.HORIZONTAL else 1,\n            span=struct_mask_span,\n        )\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/","title":"n2v_manipulate_torch","text":"<p>N2V manipulation transform for PyTorch.</p>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch","title":"<code>N2VManipulateTorch</code>","text":"<p>Default augmentation for the N2V model.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>n2v_manipulate_config</code> <code>N2VManipulateConfig</code> <p>N2V manipulation configuration.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask.</p> <code>roi_size</code> <code>int</code> <p>Size of the replacement area.</p> <code>strategy</code> <code>Literal[uniform, median]</code> <p>Replacement strategy, uniform or median.</p> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch.</p> <code>struct_mask</code> <code>Optional[StructMaskParameters]</code> <p>StructN2V mask parameters.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>class N2VManipulateTorch:\n    \"\"\"\n    Default augmentation for the N2V model.\n\n    This transform expects C(Z)YX dimensions.\n\n    Parameters\n    ----------\n    n2v_manipulate_config : N2VManipulateConfig\n        N2V manipulation configuration.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    device : str\n        The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".\n\n    Attributes\n    ----------\n    masked_pixel_percentage : float\n        Percentage of pixels to mask.\n    roi_size : int\n        Size of the replacement area.\n    strategy : Literal[ \"uniform\", \"median\" ]\n        Replacement strategy, uniform or median.\n    remove_center : bool\n        Whether to remove central pixel from patch.\n    struct_mask : Optional[StructMaskParameters]\n        StructN2V mask parameters.\n    rng : Generator\n        Random number generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        n2v_manipulate_config: N2VManipulateModel,\n        seed: int | None = None,\n        device: str | None = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        n2v_manipulate_config : N2VManipulateModel\n            N2V manipulation configuration.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        device : str\n            The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".\n        \"\"\"\n        self.masked_pixel_percentage = n2v_manipulate_config.masked_pixel_percentage\n        self.roi_size = n2v_manipulate_config.roi_size\n        self.strategy = n2v_manipulate_config.strategy\n        self.remove_center = n2v_manipulate_config.remove_center\n\n        if n2v_manipulate_config.struct_mask_axis == SupportedStructAxis.NONE:\n            self.struct_mask: StructMaskParameters | None = None\n        else:\n            self.struct_mask = StructMaskParameters(\n                axis=(\n                    0\n                    if n2v_manipulate_config.struct_mask_axis\n                    == SupportedStructAxis.HORIZONTAL\n                    else 1\n                ),\n                span=n2v_manipulate_config.struct_mask_span,\n            )\n\n        # PyTorch random generator\n        # TODO refactor into careamics.utils.torch_utils.get_device\n        if device is None:\n            if torch.cuda.is_available():\n                device = \"cuda\"\n            elif torch.backends.mps.is_available() and platform.processor() in (\n                \"arm\",\n                \"arm64\",\n            ):\n                device = \"mps\"\n            else:\n                device = \"cpu\"\n\n        self.rng = (\n            torch.Generator(device=device).manual_seed(seed)\n            if seed is not None\n            else torch.Generator(device=device)\n        )\n\n    def __call__(\n        self, batch: torch.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Batch if image patches, 2D or 3D, shape BC(Z)YX.\n        *args : Any\n            Additional arguments, unused.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Returns\n        -------\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n            Masked patch, original patch, and mask.\n        \"\"\"\n        masked = torch.zeros_like(batch)\n        mask = torch.zeros_like(batch, dtype=torch.uint8)\n\n        if self.strategy == SupportedPixelManipulation.UNIFORM:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(batch.shape[1]):\n                masked[:, c, ...], mask[:, c, ...] = uniform_manipulate_torch(\n                    patch=batch[:, c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    remove_center=self.remove_center,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        elif self.strategy == SupportedPixelManipulation.MEDIAN:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(batch.shape[1]):\n                masked[:, c, ...], mask[:, c, ...] = median_manipulate_torch(\n                    batch=batch[:, c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        else:\n            raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n        return masked, batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch.__call__","title":"<code>__call__(batch, *args, **kwargs)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch if image patches, 2D or 3D, shape BC(Z)YX.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments, unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>Masked patch, original patch, and mask.</p> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>def __call__(\n    self, batch: torch.Tensor, *args: Any, **kwargs: Any\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Batch if image patches, 2D or 3D, shape BC(Z)YX.\n    *args : Any\n        Additional arguments, unused.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n        Masked patch, original patch, and mask.\n    \"\"\"\n    masked = torch.zeros_like(batch)\n    mask = torch.zeros_like(batch, dtype=torch.uint8)\n\n    if self.strategy == SupportedPixelManipulation.UNIFORM:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(batch.shape[1]):\n            masked[:, c, ...], mask[:, c, ...] = uniform_manipulate_torch(\n                patch=batch[:, c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                remove_center=self.remove_center,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    elif self.strategy == SupportedPixelManipulation.MEDIAN:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(batch.shape[1]):\n            masked[:, c, ...], mask[:, c, ...] = median_manipulate_torch(\n                batch=batch[:, c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    else:\n        raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n    return masked, batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch.__init__","title":"<code>__init__(n2v_manipulate_config, seed=None, device=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n2v_manipulate_config</code> <code>N2VManipulateModel</code> <p>N2V manipulation configuration.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <code>device</code> <code>str</code> <p>The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".</p> <code>None</code> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>def __init__(\n    self,\n    n2v_manipulate_config: N2VManipulateModel,\n    seed: int | None = None,\n    device: str | None = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    n2v_manipulate_config : N2VManipulateModel\n        N2V manipulation configuration.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    device : str\n        The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".\n    \"\"\"\n    self.masked_pixel_percentage = n2v_manipulate_config.masked_pixel_percentage\n    self.roi_size = n2v_manipulate_config.roi_size\n    self.strategy = n2v_manipulate_config.strategy\n    self.remove_center = n2v_manipulate_config.remove_center\n\n    if n2v_manipulate_config.struct_mask_axis == SupportedStructAxis.NONE:\n        self.struct_mask: StructMaskParameters | None = None\n    else:\n        self.struct_mask = StructMaskParameters(\n            axis=(\n                0\n                if n2v_manipulate_config.struct_mask_axis\n                == SupportedStructAxis.HORIZONTAL\n                else 1\n            ),\n            span=n2v_manipulate_config.struct_mask_span,\n        )\n\n    # PyTorch random generator\n    # TODO refactor into careamics.utils.torch_utils.get_device\n    if device is None:\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        elif torch.backends.mps.is_available() and platform.processor() in (\n            \"arm\",\n            \"arm64\",\n        ):\n            device = \"mps\"\n        else:\n            device = \"cpu\"\n\n    self.rng = (\n        torch.Generator(device=device).manual_seed(seed)\n        if seed is not None\n        else torch.Generator(device=device)\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/","title":"normalize","text":"<p>Normalization and denormalization transforms for image patches.</p>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize","title":"<code>Denormalize</code>","text":"<p>Denormalize an image.</p> <p>Denormalization is performed expecting a zero mean and unit variance input. This transform expects C(Z)YX dimensions.</p> <p>Note that an epsilon value of 1e-6 is added to the standard deviation to avoid division by zero during the normalization step, which is taken into account during denormalization.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list or tuple of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list or tuple of float</code> <p>Standard deviation value per channel.</p> required Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>class Denormalize:\n    \"\"\"\n    Denormalize an image.\n\n    Denormalization is performed expecting a zero mean and unit variance input. This\n    transform expects C(Z)YX dimensions.\n\n    Note that an epsilon value of 1e-6 is added to the standard deviation to avoid\n    division by zero during the normalization step, which is taken into account during\n    denormalization.\n\n    Parameters\n    ----------\n    image_means : list or tuple of float\n        Mean value per channel.\n    image_stds : list or tuple of float\n        Standard deviation value per channel.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        image_means: list[float],\n        image_stds: list[float],\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        image_means : list of float\n            Mean value per channel.\n        image_stds : list of float\n            Standard deviation value per channel.\n        \"\"\"\n        self.image_means = image_means\n        self.image_stds = image_stds\n\n        self.eps = 1e-6\n\n    def __call__(self, patch: NDArray) -&gt; NDArray:\n        \"\"\"Reverse the normalization operation for a batch of patches.\n\n        Parameters\n        ----------\n        patch : NDArray\n            Patch, 2D or 3D, shape BC(Z)YX.\n\n        Returns\n        -------\n        NDArray\n            Transformed array.\n        \"\"\"\n        if len(self.image_means) != patch.shape[1]:\n            raise ValueError(\n                f\"Number of means (got a list of size {len(self.image_means)}) and \"\n                f\"number of channels (got shape {patch.shape} for BC(Z)YX) do not \"\n                f\"match.\"\n            )\n\n        means = _reshape_stats(self.image_means, patch.ndim)\n        stds = _reshape_stats(self.image_stds, patch.ndim)\n\n        denorm_array = self._apply(\n            patch,\n            np.swapaxes(means, 0, 1),  # swap axes as C channel is axis 1\n            np.swapaxes(stds, 0, 1),\n        )\n\n        return denorm_array.astype(np.float32)\n\n    def _apply(self, array: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n        \"\"\"\n        Apply the transform to the image.\n\n        Parameters\n        ----------\n        array : NDArray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        mean : NDArray\n            Mean values.\n        std : NDArray\n            Standard deviations.\n\n        Returns\n        -------\n        NDArray\n            Denormalized image array.\n        \"\"\"\n        return array * (std + self.eps) + mean\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize.__call__","title":"<code>__call__(patch)</code>","text":"<p>Reverse the normalization operation for a batch of patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>NDArray</code> <p>Patch, 2D or 3D, shape BC(Z)YX.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Transformed array.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __call__(self, patch: NDArray) -&gt; NDArray:\n    \"\"\"Reverse the normalization operation for a batch of patches.\n\n    Parameters\n    ----------\n    patch : NDArray\n        Patch, 2D or 3D, shape BC(Z)YX.\n\n    Returns\n    -------\n    NDArray\n        Transformed array.\n    \"\"\"\n    if len(self.image_means) != patch.shape[1]:\n        raise ValueError(\n            f\"Number of means (got a list of size {len(self.image_means)}) and \"\n            f\"number of channels (got shape {patch.shape} for BC(Z)YX) do not \"\n            f\"match.\"\n        )\n\n    means = _reshape_stats(self.image_means, patch.ndim)\n    stds = _reshape_stats(self.image_stds, patch.ndim)\n\n    denorm_array = self._apply(\n        patch,\n        np.swapaxes(means, 0, 1),  # swap axes as C channel is axis 1\n        np.swapaxes(stds, 0, 1),\n    )\n\n    return denorm_array.astype(np.float32)\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize.__init__","title":"<code>__init__(image_means, image_stds)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __init__(\n    self,\n    image_means: list[float],\n    image_stds: list[float],\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    \"\"\"\n    self.image_means = image_means\n    self.image_stds = image_stds\n\n    self.eps = 1e-6\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize","title":"<code>Normalize</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Normalize an image or image patch.</p> <p>Normalization is a zero mean and unit variance. This transform expects C(Z)YX dimensions.</p> <p>Not that an epsilon value of 1e-6 is added to the standard deviation to avoid division by zero and that it returns a float32 image.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required <code>target_means</code> <code>list of float</code> <p>Target mean value per channel, by default None.</p> <code>None</code> <code>target_stds</code> <code>list of float</code> <p>Target standard deviation value per channel, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> <code>target_means</code> <code>list of float, optional</code> <p>Target mean value per channel, by default None.</p> <code>target_stds</code> <code>list of float, optional</code> <p>Target standard deviation value per channel, by default None.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>class Normalize(Transform):\n    \"\"\"\n    Normalize an image or image patch.\n\n    Normalization is a zero mean and unit variance. This transform expects C(Z)YX\n    dimensions.\n\n    Not that an epsilon value of 1e-6 is added to the standard deviation to avoid\n    division by zero and that it returns a float32 image.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means : list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n\n    Attributes\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means :list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_means: list[float],\n        image_stds: list[float],\n        target_means: list[float] | None = None,\n        target_stds: list[float] | None = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        image_means : list of float\n            Mean value per channel.\n        image_stds : list of float\n            Standard deviation value per channel.\n        target_means : list of float, optional\n            Target mean value per channel, by default None.\n        target_stds : list of float, optional\n            Target standard deviation value per channel, by default None.\n        \"\"\"\n        self.image_means = image_means\n        self.image_stds = image_stds\n        self.target_means = target_means\n        self.target_stds = target_stds\n\n        self.eps = 1e-6\n\n    def __call__(\n        self,\n        patch: np.ndarray,\n        target: NDArray | None = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : NDArray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : NDArray, optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple of NDArray\n            Transformed patch and target, the target can be returned as `None`.\n        \"\"\"\n        if len(self.image_means) != patch.shape[0]:\n            raise ValueError(\n                f\"Number of means (got a list of size {len(self.image_means)}) and \"\n                f\"number of channels (got shape {patch.shape} for C(Z)YX) do not match.\"\n            )\n        if len(additional_arrays) != 0:\n            raise NotImplementedError(\n                \"Transforming additional arrays is currently not supported for \"\n                \"`Normalize`.\"\n            )\n\n        # reshape mean and std and apply the normalization to the patch\n        means = _reshape_stats(self.image_means, patch.ndim)\n        stds = _reshape_stats(self.image_stds, patch.ndim)\n        norm_patch = self._apply(patch, means, stds)\n\n        # same for the target patch\n        if target is None:\n            norm_target = None\n        else:\n            if not self.target_means or not self.target_stds:\n                raise ValueError(\n                    \"Target means and standard deviations must be provided \"\n                    \"if target is not None.\"\n                )\n            if len(self.target_means) == 0 and len(self.target_stds) == 0:\n                raise ValueError(\n                    \"Target means and standard deviations must be provided \"\n                    \"if target is not None.\"\n                )\n            if len(self.target_means) != target.shape[0]:\n                raise ValueError(\n                    \"Target means and standard deviations must have the same length \"\n                    \"as the target.\"\n                )\n            target_means = _reshape_stats(self.target_means, target.ndim)\n            target_stds = _reshape_stats(self.target_stds, target.ndim)\n            norm_target = self._apply(target, target_means, target_stds)\n\n        return norm_patch, norm_target, additional_arrays\n\n    def _apply(self, patch: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n        \"\"\"\n        Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : NDArray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        mean : NDArray\n            Mean values.\n        std : NDArray\n            Standard deviations.\n\n        Returns\n        -------\n        NDArray\n            Normalized image patch.\n        \"\"\"\n        return ((patch - mean) / (std + self.eps)).astype(np.float32)\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>NDArray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>NDArray</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple of NDArray</code> <p>Transformed patch and target, the target can be returned as <code>None</code>.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __call__(\n    self,\n    patch: np.ndarray,\n    target: NDArray | None = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : NDArray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : NDArray, optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    tuple of NDArray\n        Transformed patch and target, the target can be returned as `None`.\n    \"\"\"\n    if len(self.image_means) != patch.shape[0]:\n        raise ValueError(\n            f\"Number of means (got a list of size {len(self.image_means)}) and \"\n            f\"number of channels (got shape {patch.shape} for C(Z)YX) do not match.\"\n        )\n    if len(additional_arrays) != 0:\n        raise NotImplementedError(\n            \"Transforming additional arrays is currently not supported for \"\n            \"`Normalize`.\"\n        )\n\n    # reshape mean and std and apply the normalization to the patch\n    means = _reshape_stats(self.image_means, patch.ndim)\n    stds = _reshape_stats(self.image_stds, patch.ndim)\n    norm_patch = self._apply(patch, means, stds)\n\n    # same for the target patch\n    if target is None:\n        norm_target = None\n    else:\n        if not self.target_means or not self.target_stds:\n            raise ValueError(\n                \"Target means and standard deviations must be provided \"\n                \"if target is not None.\"\n            )\n        if len(self.target_means) == 0 and len(self.target_stds) == 0:\n            raise ValueError(\n                \"Target means and standard deviations must be provided \"\n                \"if target is not None.\"\n            )\n        if len(self.target_means) != target.shape[0]:\n            raise ValueError(\n                \"Target means and standard deviations must have the same length \"\n                \"as the target.\"\n            )\n        target_means = _reshape_stats(self.target_means, target.ndim)\n        target_stds = _reshape_stats(self.target_stds, target.ndim)\n        norm_target = self._apply(target, target_means, target_stds)\n\n    return norm_patch, norm_target, additional_arrays\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize.__init__","title":"<code>__init__(image_means, image_stds, target_means=None, target_stds=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required <code>target_means</code> <code>list of float</code> <p>Target mean value per channel, by default None.</p> <code>None</code> <code>target_stds</code> <code>list of float</code> <p>Target standard deviation value per channel, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __init__(\n    self,\n    image_means: list[float],\n    image_stds: list[float],\n    target_means: list[float] | None = None,\n    target_stds: list[float] | None = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means : list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n    \"\"\"\n    self.image_means = image_means\n    self.image_stds = image_stds\n    self.target_means = target_means\n    self.target_stds = target_stds\n\n    self.eps = 1e-6\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/","title":"pixel_manipulation","text":"<p>Pixel manipulation methods.</p> <p>Pixel manipulation is used in N2V and similar algorithm to replace the value of masked pixels.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation.median_manipulate","title":"<code>median_manipulate(patch, mask_pixel_percentage, subpatch_size=11, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with the median of their surrounding subpatch.</p> <p>N2V2 version, manipulated pixels are selected randomly away from a grid with an approximate uniform probability to be selected across the whole patch.</p> <p>If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>floar</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>Generator or None</code> <p>Random number generato, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>tuple containing the manipulated patch, the original patch and the mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def median_manipulate(\n    patch: np.ndarray,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    struct_params: StructMaskParameters | None = None,\n    rng: np.random.Generator | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Manipulate pixels by replacing them with the median of their surrounding subpatch.\n\n    N2V2 version, manipulated pixels are selected randomly away from a grid with an\n    approximate uniform probability to be selected across the whole patch.\n\n    If `struct_params` is not None, an additional structN2V mask is applied to the data,\n    replacing the pixels in the mask with random values (excluding the pixel already\n    manipulated).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : floar\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    struct_params : StructMaskParameters or None, optional\n        Parameters for the structN2V mask (axis and span).\n    rng : np.random.Generator or None, optional\n        Random number generato, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n           tuple containing the manipulated patch, the original patch and the mask.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    transformed_patch = patch.copy()\n\n    # Get the coordinates of the pixels to be replaced\n    subpatch_centers = _get_stratified_coords(mask_pixel_percentage, patch.shape, rng)\n\n    # Generate coordinate grid for subpatch\n    roi_span = np.array(\n        [-np.floor(subpatch_size / 2), np.ceil(subpatch_size / 2)]\n    ).astype(np.int32)\n\n    subpatch_crops_span_full = subpatch_centers[np.newaxis, ...].T + roi_span\n\n    # Dimensions n dims, n centers, (min, max)\n    subpatch_crops_span_clipped = np.clip(\n        subpatch_crops_span_full,\n        a_min=np.zeros_like(patch.shape)[:, np.newaxis, np.newaxis],\n        a_max=np.array(patch.shape)[:, np.newaxis, np.newaxis],\n    )\n\n    for idx in range(subpatch_crops_span_clipped.shape[1]):\n        subpatch_coords = subpatch_crops_span_clipped[:, idx, ...]\n        idxs = [\n            slice(x[0], x[1]) if x[1] - x[0] &gt; 0 else slice(0, 1)\n            for x in subpatch_coords\n        ]\n        subpatch = patch[tuple(idxs)]\n        subpatch_center_adjusted = subpatch_centers[idx] - subpatch_coords[:, 0]\n\n        if struct_params is None:\n            subpatch_mask = _create_subpatch_center_mask(\n                subpatch, subpatch_center_adjusted\n            )\n        else:\n            subpatch_mask = _create_subpatch_struct_mask(\n                subpatch, subpatch_center_adjusted, struct_params\n            )\n        transformed_patch[tuple(subpatch_centers[idx])] = np.median(\n            subpatch[subpatch_mask]\n        )\n\n    mask = np.where(transformed_patch != patch, 1, 0).astype(np.uint8)\n\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask(\n            transformed_patch, subpatch_centers, struct_params\n        )\n\n    return (\n        transformed_patch,\n        mask,\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation.uniform_manipulate","title":"<code>uniform_manipulate(patch, mask_pixel_percentage, subpatch_size=11, remove_center=True, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with a neighbor values.</p> <p>Manipulated pixels are selected unformly selected in a subpatch, away from a grid with an approximate uniform probability to be selected across the whole patch. If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove the center pixel from the subpatch, by default False.</p> <code>True</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>Generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>tuple containing the manipulated patch and the corresponding mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def uniform_manipulate(\n    patch: np.ndarray,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    remove_center: bool = True,\n    struct_params: StructMaskParameters | None = None,\n    rng: np.random.Generator | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Manipulate pixels by replacing them with a neighbor values.\n\n    Manipulated pixels are selected unformly selected in a subpatch, away from a grid\n    with an approximate uniform probability to be selected across the whole patch.\n    If `struct_params` is not None, an additional structN2V mask is applied to the\n    data, replacing the pixels in the mask with random values (excluding the pixel\n    already manipulated).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    remove_center : bool\n        Whether to remove the center pixel from the subpatch, by default False.\n    struct_params : StructMaskParameters or None\n        Parameters for the structN2V mask (axis and span).\n    rng : np.random.Generator or None\n        Random number generator.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        tuple containing the manipulated patch and the corresponding mask.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    # Get the coordinates of the pixels to be replaced\n    transformed_patch = patch.copy()\n\n    subpatch_centers = _get_stratified_coords(mask_pixel_percentage, patch.shape, rng)\n\n    # Generate coordinate grid for subpatch\n    roi_span_full = np.arange(\n        -np.floor(subpatch_size / 2), np.ceil(subpatch_size / 2)\n    ).astype(np.int32)\n\n    # Remove the center pixel from the grid if needed\n    roi_span = roi_span_full[roi_span_full != 0] if remove_center else roi_span_full\n\n    # Randomly select coordinates from the grid\n    random_increment = rng.choice(roi_span, size=subpatch_centers.shape)\n\n    # Clip the coordinates to the patch size\n    replacement_coords = np.clip(\n        subpatch_centers + random_increment,\n        0,\n        [patch.shape[i] - 1 for i in range(len(patch.shape))],\n    )\n\n    # Get the replacement pixels from all subpatchs\n    replacement_pixels = patch[tuple(replacement_coords.T.tolist())]\n\n    # Replace the original pixels with the replacement pixels\n    transformed_patch[tuple(subpatch_centers.T.tolist())] = replacement_pixels\n    mask = np.where(transformed_patch != patch, 1, 0).astype(np.uint8)\n\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask(\n            transformed_patch, subpatch_centers, struct_params\n        )\n\n    return (\n        transformed_patch,\n        mask,\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/","title":"pixel_manipulation_torch","text":"<p>N2V manipulation functions for PyTorch.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.median_manipulate_torch","title":"<code>median_manipulate_torch(batch, mask_pixel_percentage, subpatch_size=11, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with the median of their surrounding subpatch.</p> <p>N2V2 version, manipulated pixels are selected randomly away from a grid with an approximate uniform probability to be selected across the whole patch.</p> <p>If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>default_generator or None</code> <p>Random number generator, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple containing the manipulated patch, the original patch and the mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def median_manipulate_torch(\n    batch: torch.Tensor,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    struct_params: StructMaskParameters | None = None,\n    rng: torch.Generator | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Manipulate pixels by replacing them with the median of their surrounding subpatch.\n\n    N2V2 version, manipulated pixels are selected randomly away from a grid with an\n    approximate uniform probability to be selected across the whole patch.\n\n    If `struct_params` is not None, an additional structN2V mask is applied to the data,\n    replacing the pixels in the mask with random values (excluding the pixel already\n    manipulated).\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    struct_params : StructMaskParameters or None, optional\n        Parameters for the structN2V mask (axis and span).\n    rng : torch.default_generator or None, optional\n        Random number generator, by default None.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n           tuple containing the manipulated patch, the original patch and the mask.\n    \"\"\"\n    # get the coordinates of the future ROI centers\n    subpatch_center_coordinates = _get_stratified_coords_torch(\n        mask_pixel_percentage, batch.shape, rng\n    ).to(\n        device=batch.device\n    )  # (num_coordinates, batch + num_spatial_dims)\n\n    # Calculate the padding value for the input tensor\n    pad_value = subpatch_size // 2\n\n    # Generate all offsets for the ROIs. Iteration starting from 1 to skip the batch\n    offsets = torch.meshgrid(\n        [\n            torch.arange(-pad_value, pad_value + 1, device=batch.device)\n            for _ in range(1, subpatch_center_coordinates.shape[1])\n        ],\n        indexing=\"ij\",\n    )\n    offsets = torch.stack(\n        [axis_offset.flatten() for axis_offset in offsets], dim=1\n    )  # (subpatch_size**2, num_spatial_dims)\n\n    # Create the list to assemble coordinates of the ROIs centers for each axis\n    coords_axes = []\n    # Create the list to assemble the span of coordinates defining the ROIs for each\n    # axis\n    coords_expands = []\n    for d in range(subpatch_center_coordinates.shape[1]):\n        coords_axes.append(subpatch_center_coordinates[:, d])\n        if d == 0:\n            # For batch dimension coordinates are not expanded (no offsets)\n            coords_expands.append(\n                subpatch_center_coordinates[:, d]\n                .unsqueeze(1)\n                .expand(-1, subpatch_size ** offsets.shape[1])\n            )  # (num_coordinates, subpatch_size**num_spacial_dims)\n        else:\n            # For spatial dimensions, coordinates are expanded with offsets, creating\n            # spans\n            coords_expands.append(\n                (\n                    subpatch_center_coordinates[:, d].unsqueeze(1) + offsets[:, d - 1]\n                ).clamp(0, batch.shape[d] - 1)\n            )  # (num_coordinates, subpatch_size**num_spacial_dims)\n\n    # create array of rois by indexing the batch with gathered coordinates\n    rois = batch[\n        tuple(coords_expands)\n    ]  # (num_coordinates, subpatch_size**num_spacial_dims)\n\n    if struct_params is not None:\n        # Create the structN2V mask\n        h, w = torch.meshgrid(\n            torch.arange(subpatch_size), torch.arange(subpatch_size), indexing=\"ij\"\n        )\n        center_idx = subpatch_size // 2\n        halfspan = (struct_params.span - 1) // 2\n\n        # Determine the axis along which to apply the mask\n        if struct_params.axis == 0:\n            center_axis = h\n            span_axis = w\n        else:\n            center_axis = w\n            span_axis = h\n\n        # Create the mask\n        struct_mask = (\n            ~(\n                (center_axis == center_idx)\n                &amp; (span_axis &gt;= center_idx - halfspan)\n                &amp; (span_axis &lt;= center_idx + halfspan)\n            )\n        ).flatten()\n        rois_filtered = rois[:, struct_mask]\n    else:\n        # Remove the center pixel value from the rois\n        center_idx = (subpatch_size ** offsets.shape[1]) // 2\n        rois_filtered = torch.cat(\n            [rois[:, :center_idx], rois[:, center_idx + 1 :]], dim=1\n        )\n\n    # compute the medians.\n    medians = rois_filtered.median(dim=1).values  # (num_coordinates,)\n\n    # Update the output tensor with medians\n    output_batch = batch.clone()\n    output_batch[tuple(coords_axes)] = medians\n    mask = torch.where(output_batch != batch, 1, 0).to(torch.uint8)\n\n    if struct_params is not None:\n        output_batch = _apply_struct_mask_torch(\n            output_batch, subpatch_center_coordinates, struct_params\n        )\n\n    return output_batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.uniform_manipulate_torch","title":"<code>uniform_manipulate_torch(patch, mask_pixel_percentage, subpatch_size=11, remove_center=True, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with a neighbor values.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.uniform_manipulate_torch--todo-add-more-details-especially-about-batch","title":"TODO add more details, especially about batch","text":"<p>Manipulated pixels are selected uniformly selected in a subpatch, away from a grid with an approximate uniform probability to be selected across the whole patch. If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>Tensor</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x). # TODO batch and channel.</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove the center pixel from the subpatch, by default False.</p> <code>True</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>default_generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple containing the manipulated patch and the corresponding mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def uniform_manipulate_torch(\n    patch: torch.Tensor,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    remove_center: bool = True,\n    struct_params: StructMaskParameters | None = None,\n    rng: torch.Generator | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Manipulate pixels by replacing them with a neighbor values.\n\n    # TODO add more details, especially about batch\n\n    Manipulated pixels are selected uniformly selected in a subpatch, away from a grid\n    with an approximate uniform probability to be selected across the whole patch.\n    If `struct_params` is not None, an additional structN2V mask is applied to the\n    data, replacing the pixels in the mask with random values (excluding the pixel\n    already manipulated).\n\n    Parameters\n    ----------\n    patch : torch.Tensor\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x). # TODO batch and channel.\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    remove_center : bool\n        Whether to remove the center pixel from the subpatch, by default False.\n    struct_params : StructMaskParameters or None\n        Parameters for the structN2V mask (axis and span).\n    rng : torch.default_generator or None\n        Random number generator.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        tuple containing the manipulated patch and the corresponding mask.\n    \"\"\"\n    if rng is None:\n        rng = torch.Generator(device=patch.device)\n        # TODO do we need seed ?\n\n    # create a copy of the patch\n    transformed_patch = patch.clone()\n\n    # get the coordinates of the pixels to be masked\n    subpatch_centers = _get_stratified_coords_torch(\n        mask_pixel_percentage, patch.shape, rng\n    )\n    subpatch_centers = subpatch_centers.to(device=patch.device)\n\n    # TODO refactor with non negative indices?\n    # arrange the list of indices to represent the ROI around the pixel to be masked\n    roi_span_full = torch.arange(\n        -(subpatch_size // 2),\n        subpatch_size // 2 + 1,\n        dtype=torch.int32,\n        device=patch.device,\n    )\n\n    # remove the center pixel from the ROI\n    roi_span = roi_span_full[roi_span_full != 0] if remove_center else roi_span_full\n\n    # create a random increment to select the replacement value\n    # this increment is added to the center coordinates\n    random_increment = roi_span[\n        torch.randint(\n            low=min(roi_span),\n            high=max(roi_span) + 1,\n            # one less coord dim: we shouldn't add a random increment to the batch coord\n            size=(subpatch_centers.shape[0], subpatch_centers.shape[1] - 1),\n            generator=rng,\n            device=patch.device,\n        )\n    ]\n\n    # compute the replacement pixel coordinates\n    replacement_coords = subpatch_centers.clone()\n    # only add random increment to the spatial dimensions, not the batch dimension\n    replacement_coords[:, 1:] = torch.clamp(\n        replacement_coords[:, 1:] + random_increment,\n        torch.zeros_like(torch.tensor(patch.shape[1:])).to(device=patch.device),\n        torch.tensor([v - 1 for v in patch.shape[1:]]).to(device=patch.device),\n    )\n\n    # replace the pixels in the patch\n    # tuples and transpose are needed for proper indexing\n    replacement_pixels = patch[tuple(replacement_coords.T)]\n    transformed_patch[tuple(subpatch_centers.T)] = replacement_pixels\n\n    # create a mask representing the masked pixels\n    mask = (transformed_patch != patch).to(dtype=torch.uint8)\n\n    # apply structN2V mask if needed\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask_torch(\n            transformed_patch, subpatch_centers, struct_params, rng\n        )\n\n    return transformed_patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/struct_mask_parameters/","title":"struct_mask_parameters","text":"<p>Class representing the parameters of structN2V masks.</p>"},{"location":"reference/careamics/transforms/struct_mask_parameters/#careamics.transforms.struct_mask_parameters.StructMaskParameters","title":"<code>StructMaskParameters</code>  <code>dataclass</code>","text":"<p>Parameters of structN2V masks.</p> <p>Attributes:</p> Name Type Description <code>axis</code> <code>Literal[0, 1]</code> <p>Axis along which to apply the mask, horizontal (0) or vertical (1).</p> <code>span</code> <code>int</code> <p>Span of the mask.</p> Source code in <code>src/careamics/transforms/struct_mask_parameters.py</code> <pre><code>@dataclass\nclass StructMaskParameters:\n    \"\"\"Parameters of structN2V masks.\n\n    Attributes\n    ----------\n    axis : Literal[0, 1]\n        Axis along which to apply the mask, horizontal (0) or vertical (1).\n    span : int\n        Span of the mask.\n    \"\"\"\n\n    axis: Literal[0, 1]\n    span: int\n</code></pre>"},{"location":"reference/careamics/transforms/transform/","title":"transform","text":"<p>A general parent class for transforms.</p>"},{"location":"reference/careamics/transforms/transform/#careamics.transforms.transform.Transform","title":"<code>Transform</code>","text":"<p>A general parent class for transforms.</p> Source code in <code>src/careamics/transforms/transform.py</code> <pre><code>class Transform:\n    \"\"\"A general parent class for transforms.\"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Apply the transform.\n\n        Parameters\n        ----------\n        *args : Any\n            Arguments.\n        **kwargs : Any\n            Keyword arguments.\n\n        Returns\n        -------\n        Any\n            Transformed data.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/careamics/transforms/transform/#careamics.transforms.transform.Transform.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Apply the transform.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Transformed data.</p> Source code in <code>src/careamics/transforms/transform.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Apply the transform.\n\n    Parameters\n    ----------\n    *args : Any\n        Arguments.\n    **kwargs : Any\n        Keyword arguments.\n\n    Returns\n    -------\n    Any\n        Transformed data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics/transforms/tta/","title":"tta","text":"<p>Test-time augmentations.</p>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA","title":"<code>ImageRestorationTTA</code>","text":"<p>Test-time augmentation for image restoration tasks.</p> <p>The augmentation is performed using all 90 deg rotations and their flipped version, as well as the original image flipped.</p> <p>Tensors should be of shape SC(Z)YX.</p> <p>This transformation is used in the LightningModule in order to perform test-time augmentation.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>class ImageRestorationTTA:\n    \"\"\"\n    Test-time augmentation for image restoration tasks.\n\n    The augmentation is performed using all 90 deg rotations and their flipped version,\n    as well as the original image flipped.\n\n    Tensors should be of shape SC(Z)YX.\n\n    This transformation is used in the LightningModule in order to perform test-time\n    augmentation.\n    \"\"\"\n\n    def forward(self, input_tensor: Tensor) -&gt; list[Tensor]:\n        \"\"\"\n        Apply test-time augmentation to the input tensor.\n\n        Parameters\n        ----------\n        input_tensor : Tensor\n            Input tensor, shape SC(Z)YX.\n\n        Returns\n        -------\n        list of torch.Tensor\n            List of augmented tensors.\n        \"\"\"\n        # axes: only applies to YX axes\n        axes = (-2, -1)\n\n        augmented = [\n            # original\n            input_tensor,\n            # rotations\n            rot90(input_tensor, 1, dims=axes),\n            rot90(input_tensor, 2, dims=axes),\n            rot90(input_tensor, 3, dims=axes),\n            # original flipped\n            flip(input_tensor, dims=(axes[0],)),\n            flip(input_tensor, dims=(axes[1],)),\n        ]\n\n        # rotated once, flipped\n        augmented.extend(\n            [\n                flip(augmented[1], dims=(axes[0],)),\n                flip(augmented[1], dims=(axes[1],)),\n            ]\n        )\n\n        return augmented\n\n    def backward(self, x: list[Tensor]) -&gt; Tensor:\n        \"\"\"Undo the test-time augmentation.\n\n        Parameters\n        ----------\n        x : Any\n            List of augmented tensors of shape SC(Z)YX.\n\n        Returns\n        -------\n        Any\n            Original tensor.\n        \"\"\"\n        axes = (-2, -1)\n\n        reverse = [\n            # original\n            x[0],\n            # rotated\n            rot90(x[1], -1, dims=axes),\n            rot90(x[2], -2, dims=axes),\n            rot90(x[3], -3, dims=axes),\n            # original flipped\n            flip(x[4], dims=(axes[0],)),\n            flip(x[5], dims=(axes[1],)),\n            # rotated once, flipped\n            rot90(flip(x[6], dims=(axes[0],)), -1, dims=axes),\n            rot90(flip(x[7], dims=(axes[1],)), -1, dims=axes),\n        ]\n\n        return mean(stack(reverse), dim=0)\n</code></pre>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA.backward","title":"<code>backward(x)</code>","text":"<p>Undo the test-time augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>List of augmented tensors of shape SC(Z)YX.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Original tensor.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>def backward(self, x: list[Tensor]) -&gt; Tensor:\n    \"\"\"Undo the test-time augmentation.\n\n    Parameters\n    ----------\n    x : Any\n        List of augmented tensors of shape SC(Z)YX.\n\n    Returns\n    -------\n    Any\n        Original tensor.\n    \"\"\"\n    axes = (-2, -1)\n\n    reverse = [\n        # original\n        x[0],\n        # rotated\n        rot90(x[1], -1, dims=axes),\n        rot90(x[2], -2, dims=axes),\n        rot90(x[3], -3, dims=axes),\n        # original flipped\n        flip(x[4], dims=(axes[0],)),\n        flip(x[5], dims=(axes[1],)),\n        # rotated once, flipped\n        rot90(flip(x[6], dims=(axes[0],)), -1, dims=axes),\n        rot90(flip(x[7], dims=(axes[1],)), -1, dims=axes),\n    ]\n\n    return mean(stack(reverse), dim=0)\n</code></pre>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Apply test-time augmentation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor, shape SC(Z)YX.</p> required <p>Returns:</p> Type Description <code>list of torch.Tensor</code> <p>List of augmented tensors.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>def forward(self, input_tensor: Tensor) -&gt; list[Tensor]:\n    \"\"\"\n    Apply test-time augmentation to the input tensor.\n\n    Parameters\n    ----------\n    input_tensor : Tensor\n        Input tensor, shape SC(Z)YX.\n\n    Returns\n    -------\n    list of torch.Tensor\n        List of augmented tensors.\n    \"\"\"\n    # axes: only applies to YX axes\n    axes = (-2, -1)\n\n    augmented = [\n        # original\n        input_tensor,\n        # rotations\n        rot90(input_tensor, 1, dims=axes),\n        rot90(input_tensor, 2, dims=axes),\n        rot90(input_tensor, 3, dims=axes),\n        # original flipped\n        flip(input_tensor, dims=(axes[0],)),\n        flip(input_tensor, dims=(axes[1],)),\n    ]\n\n    # rotated once, flipped\n    augmented.extend(\n        [\n            flip(augmented[1], dims=(axes[0],)),\n            flip(augmented[1], dims=(axes[1],)),\n        ]\n    )\n\n    return augmented\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/","title":"xy_flip","text":"<p>XY flip transform.</p>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip","title":"<code>XYFlip</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Flip image along X and Y axis, one at a time.</p> <p>This transform randomly flips one of the last two axes.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Attributes:</p> Name Type Description <code>axis_indices</code> <code>List[int]</code> <p>Indices of the axes that can be flipped.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <p>Parameters:</p> Name Type Description Default <code>flip_x</code> <code>bool</code> <p>Whether to flip along the X axis, by default True.</p> <code>True</code> <code>flip_y</code> <code>bool</code> <p>Whether to flip along the Y axis, by default True.</p> <code>True</code> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>class XYFlip(Transform):\n    \"\"\"Flip image along X and Y axis, one at a time.\n\n    This transform randomly flips one of the last two axes.\n\n    This transform expects C(Z)YX dimensions.\n\n    Attributes\n    ----------\n    axis_indices : List[int]\n        Indices of the axes that can be flipped.\n    rng : np.random.Generator\n        Random number generator.\n    p : float\n        Probability of applying the transform.\n    seed : Optional[int]\n        Random seed.\n\n    Parameters\n    ----------\n    flip_x : bool, optional\n        Whether to flip along the X axis, by default True.\n    flip_y : bool, optional\n        Whether to flip along the Y axis, by default True.\n    p : float, optional\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        flip_x: bool = True,\n        flip_y: bool = True,\n        p: float = 0.5,\n        seed: int | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        flip_x : bool, optional\n            Whether to flip along the X axis, by default True.\n        flip_y : bool, optional\n            Whether to flip along the Y axis, by default True.\n        p : float\n            Probability of applying the transform, by default 0.5.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        \"\"\"\n        if p &lt; 0 or p &gt; 1:\n            raise ValueError(\"Probability must be in [0, 1].\")\n\n        if not flip_x and not flip_y:\n            raise ValueError(\"At least one axis must be flippable.\")\n\n        # probability to apply the transform\n        self.p = p\n\n        # \"flippable\" axes\n        self.axis_indices = []\n\n        if flip_y:\n            self.axis_indices.append(-2)\n        if flip_x:\n            self.axis_indices.append(-1)\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self,\n        patch: NDArray,\n        target: NDArray | None = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : Optional[np.ndarray], optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        Tuple[np.ndarray, Optional[np.ndarray]]\n            Transformed patch and target.\n        \"\"\"\n        if self.rng.random() &gt; self.p:\n            return patch, target, additional_arrays\n\n        # choose an axis to flip\n        axis = self.rng.choice(self.axis_indices)\n\n        patch_transformed = self._apply(patch, axis)\n        target_transformed = self._apply(target, axis) if target is not None else None\n        additional_transformed = {\n            key: self._apply(array, axis) for key, array in additional_arrays.items()\n        }\n\n        return patch_transformed, target_transformed, additional_transformed\n\n    def _apply(self, patch: NDArray, axis: int) -&gt; NDArray:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        axis : int\n            Axis to flip.\n\n        Returns\n        -------\n        np.ndarray\n            Flipped image patch.\n        \"\"\"\n        return np.ascontiguousarray(np.flip(patch, axis=axis))\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Optional[ndarray]]</code> <p>Transformed patch and target.</p> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>def __call__(\n    self,\n    patch: NDArray,\n    target: NDArray | None = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : Optional[np.ndarray], optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    Tuple[np.ndarray, Optional[np.ndarray]]\n        Transformed patch and target.\n    \"\"\"\n    if self.rng.random() &gt; self.p:\n        return patch, target, additional_arrays\n\n    # choose an axis to flip\n    axis = self.rng.choice(self.axis_indices)\n\n    patch_transformed = self._apply(patch, axis)\n    target_transformed = self._apply(target, axis) if target is not None else None\n    additional_transformed = {\n        key: self._apply(array, axis) for key, array in additional_arrays.items()\n    }\n\n    return patch_transformed, target_transformed, additional_transformed\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip.__init__","title":"<code>__init__(flip_x=True, flip_y=True, p=0.5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>flip_x</code> <code>bool</code> <p>Whether to flip along the X axis, by default True.</p> <code>True</code> <code>flip_y</code> <code>bool</code> <p>Whether to flip along the Y axis, by default True.</p> <code>True</code> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>def __init__(\n    self,\n    flip_x: bool = True,\n    flip_y: bool = True,\n    p: float = 0.5,\n    seed: int | None = None,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    flip_x : bool, optional\n        Whether to flip along the X axis, by default True.\n    flip_y : bool, optional\n        Whether to flip along the Y axis, by default True.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n    if p &lt; 0 or p &gt; 1:\n        raise ValueError(\"Probability must be in [0, 1].\")\n\n    if not flip_x and not flip_y:\n        raise ValueError(\"At least one axis must be flippable.\")\n\n    # probability to apply the transform\n    self.p = p\n\n    # \"flippable\" axes\n    self.axis_indices = []\n\n    if flip_y:\n        self.axis_indices.append(-2)\n    if flip_x:\n        self.axis_indices.append(-1)\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/","title":"xy_random_rotate90","text":"<p>Patch transform applying XY random 90 degrees rotations.</p>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90","title":"<code>XYRandomRotate90</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Applies random 90 degree rotations to the YX axis.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Attributes:</p> Name Type Description <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>class XYRandomRotate90(Transform):\n    \"\"\"Applies random 90 degree rotations to the YX axis.\n\n    This transform expects C(Z)YX dimensions.\n\n    Attributes\n    ----------\n    rng : np.random.Generator\n        Random number generator.\n    p : float\n        Probability of applying the transform.\n    seed : Optional[int]\n        Random seed.\n\n    Parameters\n    ----------\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Random seed, by default None.\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, seed: int | None = None):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        p : float\n            Probability of applying the transform, by default 0.5.\n        seed : Optional[int]\n            Random seed, by default None.\n        \"\"\"\n        if p &lt; 0 or p &gt; 1:\n            raise ValueError(\"Probability must be in [0, 1].\")\n\n        # probability to apply the transform\n        self.p = p\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self,\n        patch: NDArray,\n        target: NDArray | None = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : Optional[np.ndarray], optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            Transformed patch and target.\n        \"\"\"\n        if self.rng.random() &gt; self.p:\n            return patch, target, additional_arrays\n\n        # number of rotations\n        n_rot = int(self.rng.integers(1, 4))\n\n        axes = (-2, -1)\n        patch_transformed = self._apply(patch, n_rot, axes)\n        target_transformed = (\n            self._apply(target, n_rot, axes) if target is not None else None\n        )\n        additional_transformed = {\n            key: self._apply(array, n_rot, axes)\n            for key, array in additional_arrays.items()\n        }\n\n        return patch_transformed, target_transformed, additional_transformed\n\n    def _apply(self, patch: NDArray, n_rot: int, axes: tuple[int, int]) -&gt; NDArray:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image or image patch, 2D or 3D, shape C(Z)YX.\n        n_rot : int\n            Number of 90 degree rotations.\n        axes : tuple[int, int]\n            Axes along which to rotate the patch.\n\n        Returns\n        -------\n        np.ndarray\n            Transformed patch.\n        \"\"\"\n        return np.ascontiguousarray(np.rot90(patch, k=n_rot, axes=axes))\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>Transformed patch and target.</p> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>def __call__(\n    self,\n    patch: NDArray,\n    target: NDArray | None = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, NDArray | None, dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : Optional[np.ndarray], optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        Transformed patch and target.\n    \"\"\"\n    if self.rng.random() &gt; self.p:\n        return patch, target, additional_arrays\n\n    # number of rotations\n    n_rot = int(self.rng.integers(1, 4))\n\n    axes = (-2, -1)\n    patch_transformed = self._apply(patch, n_rot, axes)\n    target_transformed = (\n        self._apply(target, n_rot, axes) if target is not None else None\n    )\n    additional_transformed = {\n        key: self._apply(array, n_rot, axes)\n        for key, array in additional_arrays.items()\n    }\n\n    return patch_transformed, target_transformed, additional_transformed\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90.__init__","title":"<code>__init__(p=0.5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>def __init__(self, p: float = 0.5, seed: int | None = None):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Random seed, by default None.\n    \"\"\"\n    if p &lt; 0 or p &gt; 1:\n        raise ValueError(\"Probability must be in [0, 1].\")\n\n    # probability to apply the transform\n    self.p = p\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/utils/autocorrelation/","title":"autocorrelation","text":"<p>Autocorrelation function.</p>"},{"location":"reference/careamics/utils/autocorrelation/#careamics.utils.autocorrelation.autocorrelation","title":"<code>autocorrelation(image)</code>","text":"<p>Compute the autocorrelation of an image.</p> <p>This method is used to explore spatial correlations in images, in particular in the noise.</p> <p>The autocorrelation is normalized to the zero-shift value, which is centered in the resulting images.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>Input image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Autocorrelation of the input image.</p> Source code in <code>src/careamics/utils/autocorrelation.py</code> <pre><code>def autocorrelation(image: NDArray) -&gt; NDArray:\n    \"\"\"Compute the autocorrelation of an image.\n\n    This method is used to explore spatial correlations in images,\n    in particular in the noise.\n\n    The autocorrelation is normalized to the zero-shift value, which is centered in\n    the resulting images.\n\n    Parameters\n    ----------\n    image : NDArray\n        Input image.\n\n    Returns\n    -------\n    numpy.ndarray\n        Autocorrelation of the input image.\n    \"\"\"\n    # normalize image\n    image = (image - np.mean(image)) / np.std(image)\n\n    # compute autocorrelation in fourier space\n    image = np.fft.fftn(image)\n    image = np.abs(image) ** 2\n    image = np.fft.ifftn(image).real\n\n    # normalize to zero shift value\n    image = image / image.flat[0]\n\n    # shift zero frequency to center\n    image = np.fft.fftshift(image)\n\n    return image\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/","title":"base_enum","text":"<p>A base class for Enum that allows checking if a value is in the Enum.</p>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum","title":"<code>BaseEnum</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Base Enum class, allowing checking if a value is in the enum.</p> Example <p>from careamics.utils.base_enum import BaseEnum</p> Source code in <code>src/careamics/utils/base_enum.py</code> <pre><code>class BaseEnum(Enum, metaclass=_ContainerEnum):\n    \"\"\"Base Enum class, allowing checking if a value is in the enum.\n\n    Example\n    -------\n    &gt;&gt;&gt; from careamics.utils.base_enum import BaseEnum\n    &gt;&gt;&gt; # Define a new enum\n    &gt;&gt;&gt; class BaseEnumExtension(BaseEnum):\n    ...     VALUE = \"value\"\n    &gt;&gt;&gt; # Check if value is in the enum\n    &gt;&gt;&gt; \"value\" in BaseEnumExtension\n    True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum--define-a-new-enum","title":"Define a new enum","text":"<p>class BaseEnumExtension(BaseEnum): ...     VALUE = \"value\"</p>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum--check-if-value-is-in-the-enum","title":"Check if value is in the enum","text":"<p>\"value\" in BaseEnumExtension True</p>"},{"location":"reference/careamics/utils/context/","title":"context","text":"<p>Context submodule.</p> <p>A convenience function to change the working directory in order to save data.</p>"},{"location":"reference/careamics/utils/context/#careamics.utils.context.cwd","title":"<code>cwd(path)</code>","text":"<p>Change the current working directory to the given path.</p> <p>This method can be used to generate files in a specific directory, once out of the context, the working directory is set back to the original one.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>New working directory path.</p> required <p>Returns:</p> Type Description <code>Iterator[None]</code> <p>None values.</p> <p>Examples:</p> <p>The context is whcnaged within the block and then restored to the original one.</p> <pre><code>&gt;&gt;&gt; with cwd(my_path):\n...     pass # do something\n</code></pre> Source code in <code>src/careamics/utils/context.py</code> <pre><code>@contextmanager\ndef cwd(path: Union[str, Path]) -&gt; Iterator[None]:\n    \"\"\"\n    Change the current working directory to the given path.\n\n    This method can be used to generate files in a specific directory, once out of the\n    context, the working directory is set back to the original one.\n\n    Parameters\n    ----------\n    path : Union[str,Path]\n        New working directory path.\n\n    Returns\n    -------\n    Iterator[None]\n        None values.\n\n    Examples\n    --------\n    The context is whcnaged within the block and then restored to the original one.\n\n    &gt;&gt;&gt; with cwd(my_path):\n    ...     pass # do something\n    \"\"\"\n    path = Path(path)\n\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n\n    old_pwd = Path(\".\").absolute()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(old_pwd)\n</code></pre>"},{"location":"reference/careamics/utils/context/#careamics.utils.context.get_careamics_home","title":"<code>get_careamics_home()</code>","text":"<p>Return the CAREamics home directory.</p> <p>CAREamics home directory is a hidden folder in home.</p> <p>Returns:</p> Type Description <code>Path</code> <p>CAREamics home directory path.</p> Source code in <code>src/careamics/utils/context.py</code> <pre><code>def get_careamics_home() -&gt; Path:\n    \"\"\"Return the CAREamics home directory.\n\n    CAREamics home directory is a hidden folder in home.\n\n    Returns\n    -------\n    Path\n        CAREamics home directory path.\n    \"\"\"\n    home = Path.home() / \".careamics\"\n\n    if not home.exists():\n        home.mkdir(parents=True, exist_ok=True)\n\n    return home\n</code></pre>"},{"location":"reference/careamics/utils/lightning_utils/","title":"lightning_utils","text":"<p>PyTorch lightning utilities.</p>"},{"location":"reference/careamics/utils/lightning_utils/#careamics.utils.lightning_utils.read_csv_logger","title":"<code>read_csv_logger(experiment_name, log_folder)</code>","text":"<p>Return the loss curves from the csv logs.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>log_folder</code> <code>Path or str</code> <p>Path to the folder containing the csv logs.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the loss curves, with keys \"train_epoch\", \"val_epoch\", \"train_loss\" and \"val_loss\".</p> Source code in <code>src/careamics/utils/lightning_utils.py</code> <pre><code>def read_csv_logger(experiment_name: str, log_folder: Union[str, Path]) -&gt; dict:\n    \"\"\"Return the loss curves from the csv logs.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    log_folder : Path or str\n        Path to the folder containing the csv logs.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the loss curves, with keys \"train_epoch\", \"val_epoch\",\n        \"train_loss\" and \"val_loss\".\n    \"\"\"\n    path = Path(log_folder) / experiment_name\n\n    # find the most recent of version_* folders\n    versions = [int(v.name.split(\"_\")[-1]) for v in path.iterdir() if v.is_dir()]\n    version = max(versions)\n\n    path_log = path / f\"version_{version}\" / \"metrics.csv\"\n\n    with open(path_log) as f:\n        lines = f.readlines()\n\n        header = lines[0].strip().split(\",\")\n        metrics: dict[str, list] = {value: [] for value in header}\n        print(metrics)\n\n        for single_line in lines[1:]:\n            values = single_line.strip().split(\",\")\n\n            for k, v in zip(header, values, strict=False):\n                metrics[k].append(v)\n\n    # train and val are not logged on the same row and can have different lengths\n    train_epoch = [\n        int(metrics[\"epoch\"][i])\n        for i in range(len(metrics[\"epoch\"]))\n        if metrics[\"train_loss_epoch\"][i] != \"\"\n    ]\n    val_epoch = [\n        int(metrics[\"epoch\"][i])\n        for i in range(len(metrics[\"epoch\"]))\n        if metrics[\"val_loss\"][i] != \"\"\n    ]\n    train_losses = [\n        float(metrics[\"train_loss_epoch\"][i])\n        for i in range(len(metrics[\"train_loss_epoch\"]))\n        if metrics[\"train_loss_epoch\"][i] != \"\"\n    ]\n    val_losses = [\n        float(metrics[\"val_loss\"][i])\n        for i in range(len(metrics[\"val_loss\"]))\n        if metrics[\"val_loss\"][i] != \"\"\n    ]\n\n    return {\n        \"train_epoch\": train_epoch,\n        \"val_epoch\": val_epoch,\n        \"train_loss\": train_losses,\n        \"val_loss\": val_losses,\n    }\n</code></pre>"},{"location":"reference/careamics/utils/logging/","title":"logging","text":"<p>Logging submodule.</p> <p>The methods are responsible for the in-console logger.</p>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar","title":"<code>ProgressBar</code>","text":"<p>Keras style progress bar.</p> <p>Adapted from https://github.com/yueyericardo/pkbar.</p> <p>Parameters:</p> Name Type Description Default <code>max_value</code> <code>Optional[int]</code> <p>Maximum progress bar value, by default None.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>Zero-indexed current epoch, by default None.</p> <code>None</code> <code>num_epochs</code> <code>Optional[int]</code> <p>Total number of epochs, by default None.</p> <code>None</code> <code>stateful_metrics</code> <code>Optional[list]</code> <p>Iterable of string names of metrics that should not be averaged over time. Metrics in this list will be displayed as-is. All others will be averaged by the progress bar before display, by default None.</p> <code>None</code> <code>always_stateful</code> <code>bool</code> <pre><code>Whether to set all metrics to be stateful, by default False.\n</code></pre> <code>False</code> <code>mode</code> <code>str</code> <p>Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".</p> <code>'train'</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>class ProgressBar:\n    \"\"\"\n    Keras style progress bar.\n\n    Adapted from https://github.com/yueyericardo/pkbar.\n\n    Parameters\n    ----------\n    max_value : Optional[int], optional\n        Maximum progress bar value, by default None.\n    epoch : Optional[int], optional\n        Zero-indexed current epoch, by default None.\n    num_epochs : Optional[int], optional\n        Total number of epochs, by default None.\n    stateful_metrics : Optional[list], optional\n        Iterable of string names of metrics that should *not* be averaged over time.\n        Metrics in this list will be displayed as-is. All others will be averaged by\n        the progress bar before display, by default None.\n    always_stateful : bool, optional\n            Whether to set all metrics to be stateful, by default False.\n    mode : str, optional\n        Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n    \"\"\"\n\n    def __init__(\n        self,\n        max_value: int | None = None,\n        epoch: int | None = None,\n        num_epochs: int | None = None,\n        stateful_metrics: list | None = None,\n        always_stateful: bool = False,\n        mode: str = \"train\",\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        max_value : Optional[int], optional\n            Maximum progress bar value, by default None.\n        epoch : Optional[int], optional\n            Zero-indexed current epoch, by default None.\n        num_epochs : Optional[int], optional\n            Total number of epochs, by default None.\n        stateful_metrics : Optional[list], optional\n            Iterable of string names of metrics that should *not* be averaged over time.\n            Metrics in this list will be displayed as-is. All others will be averaged by\n            the progress bar before display, by default None.\n        always_stateful : bool, optional\n             Whether to set all metrics to be stateful, by default False.\n        mode : str, optional\n            Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n        \"\"\"\n        self.max_value = max_value\n        # Width of the progress bar\n        self.width = 30\n        self.always_stateful = always_stateful\n\n        if (epoch is not None) and (num_epochs is not None):\n            print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()\n\n        self._dynamic_display = (\n            (hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty())\n            or \"ipykernel\" in sys.modules\n            or \"posix\" in sys.modules\n        )\n        self._total_width = 0\n        self._seen_so_far = 0\n        # We use a dict + list to avoid garbage collection\n        # issues found in OrderedDict\n        self._values: dict[Any, Any] = {}\n        self._values_order: list[Any] = []\n        self._start = time.time()\n        self._last_update = 0.0\n        self.spin = self.spinning_cursor() if self.max_value is None else None\n        if mode == \"train\" and self.max_value is None:\n            self.message = \"Estimating dataset size\"\n        elif mode == \"val\":\n            self.message = \"Validating\"\n        elif mode == \"predict\":\n            self.message = \"Denoising\"\n\n    def update(\n        self, current_step: int, batch_size: int = 1, values: list | None = None\n    ) -&gt; None:\n        \"\"\"\n        Update the progress bar.\n\n        Parameters\n        ----------\n        current_step : int\n            Index of the current step.\n        batch_size : int, optional\n            Batch size, by default 1.\n        values : Optional[list], optional\n            Updated metrics values, by default None.\n        \"\"\"\n        values = values or []\n        for k, v in values:\n            # if torch tensor, convert it to numpy\n            if str(type(v)) == \"&lt;class 'torch.Tensor'&gt;\":\n                v = v.detach().cpu().numpy()\n\n            if k not in self._values_order:\n                self._values_order.append(k)\n            if k not in self.stateful_metrics and not self.always_stateful:\n                if k not in self._values:\n                    self._values[k] = [\n                        v * (current_step - self._seen_so_far),\n                        current_step - self._seen_so_far,\n                    ]\n                else:\n                    self._values[k][0] += v * (current_step - self._seen_so_far)\n                    self._values[k][1] += current_step - self._seen_so_far\n            else:\n                # Stateful metrics output a numeric value. This representation\n                # means \"take an average from a single value\" but keeps the\n                # numeric formatting.\n                self._values[k] = [v, 1]\n\n        self._seen_so_far = current_step\n\n        now = time.time()\n        info = f\" - {(now - self._start):.0f}s\"\n\n        prev_total_width = self._total_width\n        if self._dynamic_display:\n            sys.stdout.write(\"\\b\" * prev_total_width)\n            sys.stdout.write(\"\\r\")\n        else:\n            sys.stdout.write(\"\\n\")\n\n        if self.max_value is not None:\n            bar = f\"{current_step}/{self.max_value} [\"\n            progress = float(current_step) / self.max_value\n            progress_width = int(self.width * progress)\n            if progress_width &gt; 0:\n                bar += \"=\" * (progress_width - 1)\n                if current_step &lt; self.max_value:\n                    bar += \"&gt;\"\n                else:\n                    bar += \"=\"\n            bar += \".\" * (self.width - progress_width)\n            bar += \"]\"\n        else:\n            bar = (\n                f\"{self.message} {next(self.spin)}, tile \"  # type: ignore\n                f\"No. {current_step * batch_size}\"\n            )\n\n        self._total_width = len(bar)\n        sys.stdout.write(bar)\n\n        if current_step &gt; 0:\n            time_per_unit = (now - self._start) / current_step\n        else:\n            time_per_unit = 0\n\n        if time_per_unit &gt;= 1 or time_per_unit == 0:\n            info += f\" {time_per_unit:.0f}s/step\"\n        elif time_per_unit &gt;= 1e-3:\n            info += f\" {time_per_unit * 1e3:.0f}ms/step\"\n        else:\n            info += f\" {time_per_unit * 1e6:.0f}us/step\"\n\n        for k in self._values_order:\n            info += f\" - {k}:\"\n            if isinstance(self._values[k], list):\n                avg = self._values[k][0] / max(1, self._values[k][1])\n                if abs(avg) &gt; 1e-3:\n                    info += f\" {avg:.4f}\"\n                else:\n                    info += f\" {avg:.4e}\"\n            else:\n                info += f\" {self._values[k]}s\"\n\n        self._total_width += len(info)\n        if prev_total_width &gt; self._total_width:\n            info += \" \" * (prev_total_width - self._total_width)\n\n        if self.max_value is not None and current_step &gt;= self.max_value:\n            info += \"\\n\"\n\n        sys.stdout.write(info)\n        sys.stdout.flush()\n\n        self._last_update = now\n\n    def add(self, n: int, values: list | None = None) -&gt; None:\n        \"\"\"\n        Update the progress bar by n steps.\n\n        Parameters\n        ----------\n        n : int\n            Number of steps to increase the progress bar with.\n        values : Optional[list], optional\n            Updated metrics values, by default None.\n        \"\"\"\n        self.update(self._seen_so_far + n, 1, values=values)\n\n    def spinning_cursor(self) -&gt; Generator:\n        \"\"\"\n        Generate a spinning cursor animation.\n\n        Taken from https://github.com/manrajgrover/py-spinners/tree/master.\n\n        Returns\n        -------\n        Generator\n            Generator of animation frames.\n        \"\"\"\n        while True:\n            yield from [\n                \"\u2593 ----- \u2592\",\n                \"\u2593 ----- \u2592\",\n                \"\u2593 ----- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n            ]\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.__init__","title":"<code>__init__(max_value=None, epoch=None, num_epochs=None, stateful_metrics=None, always_stateful=False, mode='train')</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>max_value</code> <code>Optional[int]</code> <p>Maximum progress bar value, by default None.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>Zero-indexed current epoch, by default None.</p> <code>None</code> <code>num_epochs</code> <code>Optional[int]</code> <p>Total number of epochs, by default None.</p> <code>None</code> <code>stateful_metrics</code> <code>Optional[list]</code> <p>Iterable of string names of metrics that should not be averaged over time. Metrics in this list will be displayed as-is. All others will be averaged by the progress bar before display, by default None.</p> <code>None</code> <code>always_stateful</code> <code>bool</code> <p>Whether to set all metrics to be stateful, by default False.</p> <code>False</code> <code>mode</code> <code>str</code> <p>Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".</p> <code>'train'</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def __init__(\n    self,\n    max_value: int | None = None,\n    epoch: int | None = None,\n    num_epochs: int | None = None,\n    stateful_metrics: list | None = None,\n    always_stateful: bool = False,\n    mode: str = \"train\",\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    max_value : Optional[int], optional\n        Maximum progress bar value, by default None.\n    epoch : Optional[int], optional\n        Zero-indexed current epoch, by default None.\n    num_epochs : Optional[int], optional\n        Total number of epochs, by default None.\n    stateful_metrics : Optional[list], optional\n        Iterable of string names of metrics that should *not* be averaged over time.\n        Metrics in this list will be displayed as-is. All others will be averaged by\n        the progress bar before display, by default None.\n    always_stateful : bool, optional\n         Whether to set all metrics to be stateful, by default False.\n    mode : str, optional\n        Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n    \"\"\"\n    self.max_value = max_value\n    # Width of the progress bar\n    self.width = 30\n    self.always_stateful = always_stateful\n\n    if (epoch is not None) and (num_epochs is not None):\n        print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n\n    if stateful_metrics:\n        self.stateful_metrics = set(stateful_metrics)\n    else:\n        self.stateful_metrics = set()\n\n    self._dynamic_display = (\n        (hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty())\n        or \"ipykernel\" in sys.modules\n        or \"posix\" in sys.modules\n    )\n    self._total_width = 0\n    self._seen_so_far = 0\n    # We use a dict + list to avoid garbage collection\n    # issues found in OrderedDict\n    self._values: dict[Any, Any] = {}\n    self._values_order: list[Any] = []\n    self._start = time.time()\n    self._last_update = 0.0\n    self.spin = self.spinning_cursor() if self.max_value is None else None\n    if mode == \"train\" and self.max_value is None:\n        self.message = \"Estimating dataset size\"\n    elif mode == \"val\":\n        self.message = \"Validating\"\n    elif mode == \"predict\":\n        self.message = \"Denoising\"\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.add","title":"<code>add(n, values=None)</code>","text":"<p>Update the progress bar by n steps.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of steps to increase the progress bar with.</p> required <code>values</code> <code>Optional[list]</code> <p>Updated metrics values, by default None.</p> <code>None</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def add(self, n: int, values: list | None = None) -&gt; None:\n    \"\"\"\n    Update the progress bar by n steps.\n\n    Parameters\n    ----------\n    n : int\n        Number of steps to increase the progress bar with.\n    values : Optional[list], optional\n        Updated metrics values, by default None.\n    \"\"\"\n    self.update(self._seen_so_far + n, 1, values=values)\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.spinning_cursor","title":"<code>spinning_cursor()</code>","text":"<p>Generate a spinning cursor animation.</p> <p>Taken from https://github.com/manrajgrover/py-spinners/tree/master.</p> <p>Returns:</p> Type Description <code>Generator</code> <p>Generator of animation frames.</p> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def spinning_cursor(self) -&gt; Generator:\n    \"\"\"\n    Generate a spinning cursor animation.\n\n    Taken from https://github.com/manrajgrover/py-spinners/tree/master.\n\n    Returns\n    -------\n    Generator\n        Generator of animation frames.\n    \"\"\"\n    while True:\n        yield from [\n            \"\u2593 ----- \u2592\",\n            \"\u2593 ----- \u2592\",\n            \"\u2593 ----- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n        ]\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.update","title":"<code>update(current_step, batch_size=1, values=None)</code>","text":"<p>Update the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>current_step</code> <code>int</code> <p>Index of the current step.</p> required <code>batch_size</code> <code>int</code> <p>Batch size, by default 1.</p> <code>1</code> <code>values</code> <code>Optional[list]</code> <p>Updated metrics values, by default None.</p> <code>None</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def update(\n    self, current_step: int, batch_size: int = 1, values: list | None = None\n) -&gt; None:\n    \"\"\"\n    Update the progress bar.\n\n    Parameters\n    ----------\n    current_step : int\n        Index of the current step.\n    batch_size : int, optional\n        Batch size, by default 1.\n    values : Optional[list], optional\n        Updated metrics values, by default None.\n    \"\"\"\n    values = values or []\n    for k, v in values:\n        # if torch tensor, convert it to numpy\n        if str(type(v)) == \"&lt;class 'torch.Tensor'&gt;\":\n            v = v.detach().cpu().numpy()\n\n        if k not in self._values_order:\n            self._values_order.append(k)\n        if k not in self.stateful_metrics and not self.always_stateful:\n            if k not in self._values:\n                self._values[k] = [\n                    v * (current_step - self._seen_so_far),\n                    current_step - self._seen_so_far,\n                ]\n            else:\n                self._values[k][0] += v * (current_step - self._seen_so_far)\n                self._values[k][1] += current_step - self._seen_so_far\n        else:\n            # Stateful metrics output a numeric value. This representation\n            # means \"take an average from a single value\" but keeps the\n            # numeric formatting.\n            self._values[k] = [v, 1]\n\n    self._seen_so_far = current_step\n\n    now = time.time()\n    info = f\" - {(now - self._start):.0f}s\"\n\n    prev_total_width = self._total_width\n    if self._dynamic_display:\n        sys.stdout.write(\"\\b\" * prev_total_width)\n        sys.stdout.write(\"\\r\")\n    else:\n        sys.stdout.write(\"\\n\")\n\n    if self.max_value is not None:\n        bar = f\"{current_step}/{self.max_value} [\"\n        progress = float(current_step) / self.max_value\n        progress_width = int(self.width * progress)\n        if progress_width &gt; 0:\n            bar += \"=\" * (progress_width - 1)\n            if current_step &lt; self.max_value:\n                bar += \"&gt;\"\n            else:\n                bar += \"=\"\n        bar += \".\" * (self.width - progress_width)\n        bar += \"]\"\n    else:\n        bar = (\n            f\"{self.message} {next(self.spin)}, tile \"  # type: ignore\n            f\"No. {current_step * batch_size}\"\n        )\n\n    self._total_width = len(bar)\n    sys.stdout.write(bar)\n\n    if current_step &gt; 0:\n        time_per_unit = (now - self._start) / current_step\n    else:\n        time_per_unit = 0\n\n    if time_per_unit &gt;= 1 or time_per_unit == 0:\n        info += f\" {time_per_unit:.0f}s/step\"\n    elif time_per_unit &gt;= 1e-3:\n        info += f\" {time_per_unit * 1e3:.0f}ms/step\"\n    else:\n        info += f\" {time_per_unit * 1e6:.0f}us/step\"\n\n    for k in self._values_order:\n        info += f\" - {k}:\"\n        if isinstance(self._values[k], list):\n            avg = self._values[k][0] / max(1, self._values[k][1])\n            if abs(avg) &gt; 1e-3:\n                info += f\" {avg:.4f}\"\n            else:\n                info += f\" {avg:.4e}\"\n        else:\n            info += f\" {self._values[k]}s\"\n\n    self._total_width += len(info)\n    if prev_total_width &gt; self._total_width:\n        info += \" \" * (prev_total_width - self._total_width)\n\n    if self.max_value is not None and current_step &gt;= self.max_value:\n        info += \"\\n\"\n\n    sys.stdout.write(info)\n    sys.stdout.flush()\n\n    self._last_update = now\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.get_logger","title":"<code>get_logger(name, log_level=logging.INFO, log_path=None)</code>","text":"<p>Create a python logger instance with configured handlers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger.</p> required <code>log_level</code> <code>int</code> <p>Log level (info, error etc.), by default logging.INFO.</p> <code>INFO</code> <code>log_path</code> <code>Optional[Union[str, Path]]</code> <p>Path in which to save the log, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Logger.</p> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def get_logger(\n    name: str,\n    log_level: int = logging.INFO,\n    log_path: Union[str, Path] | None = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Create a python logger instance with configured handlers.\n\n    Parameters\n    ----------\n    name : str\n        Name of the logger.\n    log_level : int, optional\n        Log level (info, error etc.), by default logging.INFO.\n    log_path : Optional[Union[str, Path]], optional\n        Path in which to save the log, by default None.\n\n    Returns\n    -------\n    logging.Logger\n        Logger.\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.propagate = False\n\n    if name in LOGGERS:\n        return logger\n\n    for logger_name in LOGGERS:\n        if name.startswith(logger_name):\n            return logger\n\n    logger.propagate = False\n\n    if log_path:\n        handlers = [\n            logging.StreamHandler(),\n            logging.FileHandler(log_path),\n        ]\n    else:\n        handlers = [logging.StreamHandler()]\n\n    formatter = logging.Formatter(\"%(message)s\")\n\n    for handler in handlers:\n        handler.setFormatter(formatter)  # type: ignore\n        handler.setLevel(log_level)  # type: ignore\n        logger.addHandler(handler)  # type: ignore\n\n    logger.setLevel(log_level)\n    LOGGERS[name] = True\n\n    logger.propagate = False\n\n    return logger\n</code></pre>"},{"location":"reference/careamics/utils/metrics/","title":"metrics","text":"<p>Metrics submodule.</p> <p>This module contains various metrics and a metrics tracking class.</p>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR","title":"<code>RunningPSNR</code>","text":"<p>Compute the running PSNR during validation step in training.</p> <p>This class allows to compute the PSNR on the entire validation set one batch at the time.</p> <p>Attributes:</p> Name Type Description <code>N</code> <code>int</code> <p>Number of elements seen so far during the epoch.</p> <code>mse_sum</code> <code>float</code> <p>Running sum of the MSE over the N elements seen so far.</p> <code>max</code> <code>float</code> <p>Running max value of the N target images seen so far.</p> <code>min</code> <code>float</code> <p>Running min value of the N target images seen so far.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>class RunningPSNR:\n    \"\"\"Compute the running PSNR during validation step in training.\n\n    This class allows to compute the PSNR on the entire validation set\n    one batch at the time.\n\n    Attributes\n    ----------\n    N : int\n        Number of elements seen so far during the epoch.\n    mse_sum : float\n        Running sum of the MSE over the N elements seen so far.\n    max : float\n        Running max value of the N target images seen so far.\n    min : float\n        Running min value of the N target images seen so far.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Constructor.\"\"\"\n        self.N = None\n        self.mse_sum = None\n        self.max = self.min = None\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the running PSNR computation.\n\n        Usually called at the end of each epoch.\n        \"\"\"\n        self.mse_sum = 0\n        self.N = 0\n        self.max = self.min = None\n\n    def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n        \"\"\"Update the running PSNR statistics given a new batch.\n\n        Parameters\n        ----------\n        rec : torch.Tensor\n            Reconstructed batch.\n        tar : torch.Tensor\n            Target batch.\n        \"\"\"\n        ins_max = torch.max(tar).item()\n        ins_min = torch.min(tar).item()\n        if self.max is None:\n            assert self.min is None\n            self.max = ins_max\n            self.min = ins_min\n        else:\n            self.max = max(self.max, ins_max)\n            self.min = min(self.min, ins_min)\n\n        mse = (rec - tar) ** 2\n        elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n        self.mse_sum += torch.nansum(elementwise_mse)\n        self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n\n    def get(self) -&gt; torch.Tensor | None:\n        \"\"\"Get the actual PSNR value given the running statistics.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            PSNR value.\n        \"\"\"\n        if self.N == 0 or self.N is None:\n            return None\n        rmse = torch.sqrt(self.mse_sum / self.N)\n        return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.__init__","title":"<code>__init__()</code>","text":"<p>Constructor.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    self.N = None\n    self.mse_sum = None\n    self.max = self.min = None\n    self.reset()\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.get","title":"<code>get()</code>","text":"<p>Get the actual PSNR value given the running statistics.</p> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def get(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the actual PSNR value given the running statistics.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        PSNR value.\n    \"\"\"\n    if self.N == 0 or self.N is None:\n        return None\n    rmse = torch.sqrt(self.mse_sum / self.N)\n    return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.reset","title":"<code>reset()</code>","text":"<p>Reset the running PSNR computation.</p> <p>Usually called at the end of each epoch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the running PSNR computation.\n\n    Usually called at the end of each epoch.\n    \"\"\"\n    self.mse_sum = 0\n    self.N = 0\n    self.max = self.min = None\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.update","title":"<code>update(rec, tar)</code>","text":"<p>Update the running PSNR statistics given a new batch.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>Tensor</code> <p>Reconstructed batch.</p> required <code>tar</code> <code>Tensor</code> <p>Target batch.</p> required Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n    \"\"\"Update the running PSNR statistics given a new batch.\n\n    Parameters\n    ----------\n    rec : torch.Tensor\n        Reconstructed batch.\n    tar : torch.Tensor\n        Target batch.\n    \"\"\"\n    ins_max = torch.max(tar).item()\n    ins_min = torch.min(tar).item()\n    if self.max is None:\n        assert self.min is None\n        self.max = ins_max\n        self.min = ins_min\n    else:\n        self.max = max(self.max, ins_max)\n        self.min = min(self.min, ins_min)\n\n    mse = (rec - tar) ** 2\n    elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n    self.mse_sum += torch.nansum(elementwise_mse)\n    self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_psnr","title":"<code>avg_psnr(target, prediction)</code>","text":"<p>Compute the average PSNR over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average PSNR value over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_psnr(target: np.ndarray, prediction: np.ndarray) -&gt; float:\n    \"\"\"Compute the average PSNR over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    float\n        Average PSNR value over the batch.\n    \"\"\"\n    return _avg_psnr(target, prediction, psnr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_range_inv_psnr","title":"<code>avg_range_inv_psnr(target, prediction)</code>","text":"<p>Compute the average range-invariant PSNR over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average range-invariant PSNR value over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_range_inv_psnr(target: np.ndarray, prediction: np.ndarray) -&gt; float:\n    \"\"\"Compute the average range-invariant PSNR over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    float\n        Average range-invariant PSNR value over the batch.\n    \"\"\"\n    return _avg_psnr(target, prediction, scale_invariant_psnr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_range_invariant_psnr","title":"<code>avg_range_invariant_psnr(pred, target)</code>","text":"<p>Compute the average range-invariant PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>ndarray</code> <p>Predicted images.</p> required <code>target</code> <code>ndarray</code> <p>Target images.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average range-invariant PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_range_invariant_psnr(\n    pred: np.ndarray,\n    target: np.ndarray,\n) -&gt; float:\n    \"\"\"Compute the average range-invariant PSNR.\n\n    Parameters\n    ----------\n    pred : np.ndarray\n        Predicted images.\n    target : np.ndarray\n        Target images.\n\n    Returns\n    -------\n    float\n        Average range-invariant PSNR value.\n    \"\"\"\n    psnr_arr = []\n    for i in range(pred.shape[0]):\n        psnr_arr.append(scale_invariant_psnr(pred[i], target[i]))\n    return np.mean(psnr_arr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_ssim","title":"<code>avg_ssim(target, prediction)</code>","text":"<p>Compute the average Structural Similarity (SSIM) over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>Mean and standard deviation of SSIM values over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_ssim(\n    target: Union[np.ndarray, torch.Tensor], prediction: Union[np.ndarray, torch.Tensor]\n) -&gt; tuple[float, float]:\n    \"\"\"Compute the average Structural Similarity (SSIM) over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    tuple[float, float]\n        Mean and standard deviation of SSIM values over the batch.\n    \"\"\"\n    ssim = [\n        structural_similarity(\n            target[i], prediction[i], data_range=(target[i].max() - target[i].min())\n        )\n        for i in range(len(target))\n    ]\n    return np.mean(ssim), np.std(ssim)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.multiscale_ssim","title":"<code>multiscale_ssim(gt_, pred_, range_invariant=True)</code>","text":"<p>Compute channel-wise multiscale SSIM for each channel.</p> <p>It allows to use either standard multiscale SSIM or its range-invariant version.</p> <p>NOTE: images fed to this function should have channels dimension as the last one.</p>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.multiscale_ssim--todo-do-we-want-to-allow-this-behavior-or-we-want-the-usual-n-c-h-w","title":"TODO: do we want to allow this behavior? or we want the usual (N, C, H, W)?","text":"<p>Parameters:</p> Name Type Description Default <code>gt_</code> <code>Union[ndarray, Tensor]</code> <p>Ground truth image with shape (N, H, W, C).</p> required <code>pred_</code> <code>Union[ndarray, Tensor]</code> <p>Predicted image with shape (N, H, W, C).</p> required <code>range_invariant</code> <code>bool</code> <p>Whether to use standard or range invariant multiscale SSIM.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of SSIM values for each channel.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def multiscale_ssim(\n    gt_: Union[np.ndarray, torch.Tensor],\n    pred_: Union[np.ndarray, torch.Tensor],\n    range_invariant: bool = True,\n) -&gt; list[Union[float, None]]:\n    \"\"\"Compute channel-wise multiscale SSIM for each channel.\n\n    It allows to use either standard multiscale SSIM or its range-invariant version.\n\n    NOTE: images fed to this function should have channels dimension as the last one.\n    # TODO: do we want to allow this behavior? or we want the usual (N, C, H, W)?\n\n    Parameters\n    ----------\n    gt_ : Union[np.ndarray, torch.Tensor]\n        Ground truth image with shape (N, H, W, C).\n    pred_ : Union[np.ndarray, torch.Tensor]\n        Predicted image with shape (N, H, W, C).\n    range_invariant : bool\n        Whether to use standard or range invariant multiscale SSIM.\n\n    Returns\n    -------\n    list[float]\n        List of SSIM values for each channel.\n    \"\"\"\n    ms_ssim_values = {}\n    for ch_idx in range(gt_.shape[-1]):\n        tar_tmp = gt_[..., ch_idx]\n        pred_tmp = pred_[..., ch_idx]\n        if range_invariant:\n            ms_ssim_values[ch_idx] = _range_invariant_multiscale_ssim(\n                gt_=tar_tmp, pred_=pred_tmp\n            )\n        else:\n            ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n                data_range=tar_tmp.max() - tar_tmp.min()\n            )\n            ms_ssim_values[ch_idx] = ms_ssim(\n                torch.Tensor(pred_tmp[:, None]), torch.Tensor(tar_tmp[:, None])\n            ).item()\n\n    return [ms_ssim_values[i] for i in range(gt_.shape[-1])]  # type: ignore\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.psnr","title":"<code>psnr(gt, pred, data_range)</code>","text":"<p>Peak Signal to Noise Ratio.</p> <p>This method calls skimage.metrics.peak_signal_noise_ratio. See: https://scikit-image.org/docs/dev/api/skimage.metrics.html.</p> <p>NOTE: to avoid unwanted behaviors (e.g., data_range inferred from array dtype), the data_range parameter is mandatory.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>ndarray</code> <p>Ground truth array.</p> required <code>pred</code> <code>ndarray</code> <p>Predicted array.</p> required <code>data_range</code> <code>float</code> <p>The images pixel range.</p> required <p>Returns:</p> Type Description <code>float</code> <p>PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def psnr(gt: np.ndarray, pred: np.ndarray, data_range: float) -&gt; float:\n    \"\"\"\n    Peak Signal to Noise Ratio.\n\n    This method calls skimage.metrics.peak_signal_noise_ratio. See:\n    https://scikit-image.org/docs/dev/api/skimage.metrics.html.\n\n    NOTE: to avoid unwanted behaviors (e.g., data_range inferred from array dtype),\n    the data_range parameter is mandatory.\n\n    Parameters\n    ----------\n    gt : np.ndarray\n        Ground truth array.\n    pred : np.ndarray\n        Predicted array.\n    data_range : float\n        The images pixel range.\n\n    Returns\n    -------\n    float\n        PSNR value.\n    \"\"\"\n    return peak_signal_noise_ratio(gt, pred, data_range=data_range)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.scale_invariant_psnr","title":"<code>scale_invariant_psnr(gt, pred)</code>","text":"<p>Scale invariant PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>ndarray</code> <p>Ground truth image.</p> required <code>pred</code> <code>ndarray</code> <p>Predicted image.</p> required <p>Returns:</p> Type Description <code>Union[float, tensor]</code> <p>Scale invariant PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def scale_invariant_psnr(\n    gt: np.ndarray, pred: np.ndarray\n) -&gt; Union[float, torch.tensor]:\n    \"\"\"\n    Scale invariant PSNR.\n\n    Parameters\n    ----------\n    gt : np.ndarray\n        Ground truth image.\n    pred : np.ndarray\n        Predicted image.\n\n    Returns\n    -------\n    Union[float, torch.tensor]\n        Scale invariant PSNR value.\n    \"\"\"\n    range_parameter = (np.max(gt) - np.min(gt)) / np.std(gt)\n    gt_ = _zero_mean(gt) / np.std(gt)\n    return psnr(_zero_mean(gt_), _fix(gt_, pred), range_parameter)\n</code></pre>"},{"location":"reference/careamics/utils/path_utils/","title":"path_utils","text":"<p>Utility functions for paths.</p>"},{"location":"reference/careamics/utils/path_utils/#careamics.utils.path_utils.check_path_exists","title":"<code>check_path_exists(path)</code>","text":"<p>Check if a path exists. If not, raise an error.</p> <p>Note that it returns <code>path</code> as a Path object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path as a Path object.</p> Source code in <code>src/careamics/utils/path_utils.py</code> <pre><code>def check_path_exists(path: Union[str, Path]) -&gt; Path:\n    \"\"\"Check if a path exists. If not, raise an error.\n\n    Note that it returns `path` as a Path object.\n\n    Parameters\n    ----------\n    path : Union[str, Path]\n        Path to check.\n\n    Returns\n    -------\n    Path\n        Path as a Path object.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Data path {path} is incorrect or does not exist.\")\n\n    return path\n</code></pre>"},{"location":"reference/careamics/utils/plotting/","title":"plotting","text":"<p>Plotting utilities.</p>"},{"location":"reference/careamics/utils/plotting/#careamics.utils.plotting.plot_noise_model_probability_distribution","title":"<code>plot_noise_model_probability_distribution(noise_model, signalBinIndex, histogram, channel=None, number_of_bins=100)</code>","text":"<p>Plot probability distribution P(x|s) for a certain ground truth signal.</p> <p>Predictions from both Histogram and GMM-based Noise models are displayed for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>noise_model</code> <code>GaussianMixtureNoiseModel</code> <p>Trained GaussianMixtureNoiseModel.</p> required <code>signalBinIndex</code> <code>int</code> <p>Index of signal bin. Values go from 0 to number of bins (<code>n_bin</code>).</p> required <code>histogram</code> <code>NDArray</code> <p>Histogram based noise model.</p> required <code>channel</code> <code>Optional[str]</code> <p>Channel name used for plotting. Default is None.</p> <code>None</code> <code>number_of_bins</code> <code>int</code> <p>Number of bins in the resulting histogram. Default is 100.</p> <code>100</code> Source code in <code>src/careamics/utils/plotting.py</code> <pre><code>def plot_noise_model_probability_distribution(\n    noise_model: GaussianMixtureNoiseModel,\n    signalBinIndex: int,\n    histogram: NDArray,\n    channel: str | None = None,\n    number_of_bins: int = 100,\n) -&gt; None:\n    \"\"\"Plot probability distribution P(x|s) for a certain ground truth signal.\n\n    Predictions from both Histogram and GMM-based\n    Noise models are displayed for comparison.\n\n    Parameters\n    ----------\n    noise_model : GaussianMixtureNoiseModel\n        Trained GaussianMixtureNoiseModel.\n    signalBinIndex : int\n        Index of signal bin. Values go from 0 to number of bins (`n_bin`).\n    histogram : NDArray\n        Histogram based noise model.\n    channel : Optional[str], optional\n        Channel name used for plotting. Default is None.\n    number_of_bins : int, optional\n        Number of bins in the resulting histogram. Default is 100.\n    \"\"\"\n    min_signal = noise_model.min_signal.item()\n    max_signal = noise_model.max_signal.item()\n    bin_size = (max_signal - min_signal) / number_of_bins\n\n    query_signal_normalized = signalBinIndex / number_of_bins\n    query_signal = query_signal_normalized * (max_signal - min_signal) + min_signal\n    query_signal += bin_size / 2\n    query_signal = torch.tensor(query_signal)\n\n    query_observations = torch.arange(min_signal, max_signal, bin_size)\n    query_observations += bin_size / 2\n\n    likelihoods = noise_model.likelihood(\n        observations=query_observations, signals=query_signal\n    ).numpy()\n\n    plt.figure(figsize=(12, 5))\n    if channel:\n        plt.suptitle(f\"Noise model for channel {channel}\")\n    else:\n        plt.suptitle(\"Noise model\")\n\n    plt.subplot(1, 2, 1)\n    plt.xlabel(\"Observation Bin\")\n    plt.ylabel(\"Signal Bin\")\n    plt.imshow(histogram**0.25, cmap=\"gray\")\n    plt.axhline(y=signalBinIndex + 0.5, linewidth=5, color=\"blue\", alpha=0.5)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        query_observations,\n        likelihoods,\n        label=\"GMM : \" + \" signal = \" + str(np.round(query_signal, 2)),\n        marker=\".\",\n        color=\"red\",\n        linewidth=2,\n    )\n    plt.xlabel(\"Observations (x) for signal s = \" + str(query_signal))\n    plt.ylabel(\"Probability Density\")\n    plt.title(\"Probability Distribution P(x|s) at signal =\" + str(query_signal))\n    plt.legend()\n</code></pre>"},{"location":"reference/careamics/utils/ram/","title":"ram","text":"<p>Utility function to get RAM size.</p>"},{"location":"reference/careamics/utils/ram/#careamics.utils.ram.get_ram_size","title":"<code>get_ram_size()</code>","text":"<p>Get RAM size in mbytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>RAM size in mbytes.</p> Source code in <code>src/careamics/utils/ram.py</code> <pre><code>def get_ram_size() -&gt; int:\n    \"\"\"\n    Get RAM size in mbytes.\n\n    Returns\n    -------\n    int\n        RAM size in mbytes.\n    \"\"\"\n    return psutil.virtual_memory().available / 1024**2\n</code></pre>"},{"location":"reference/careamics/utils/receptive_field/","title":"receptive_field","text":"<p>Receptive field calculation for computing the tile overlap.</p>"},{"location":"reference/careamics/utils/serializers/","title":"serializers","text":"<p>A script for serializers in the careamics package.</p>"},{"location":"reference/careamics/utils/torch_utils/","title":"torch_utils","text":"<p>Convenience functions using torch.</p> <p>These functions are used to control certain aspects and behaviours of PyTorch.</p>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.filter_parameters","title":"<code>filter_parameters(func, user_params)</code>","text":"<p>Filter parameters according to the function signature.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>type</code> <p>Class object.</p> required <code>user_params</code> <code>dict</code> <p>User provided parameters.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Parameters matching <code>func</code>'s signature.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def filter_parameters(\n    func: type,\n    user_params: dict,\n) -&gt; dict:\n    \"\"\"\n    Filter parameters according to the function signature.\n\n    Parameters\n    ----------\n    func : type\n        Class object.\n    user_params : dict\n        User provided parameters.\n\n    Returns\n    -------\n    dict\n        Parameters matching `func`'s signature.\n    \"\"\"\n    # Get the list of all default parameters\n    default_params = list(inspect.signature(func).parameters.keys())\n\n    # Filter matching parameters\n    params_to_be_used = set(user_params.keys()) &amp; set(default_params)\n\n    return {key: user_params[key] for key in params_to_be_used}\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_device","title":"<code>get_device()</code>","text":"<p>Get the device on which operations take place.</p> <p>Returns:</p> Type Description <code>str</code> <p>The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_device() -&gt; str:\n    \"\"\"\n    Get the device on which operations take place.\n\n    Returns\n    -------\n    str\n        The device on which operations take place, e.g. \"cuda\", \"cpu\" or \"mps\".\n    \"\"\"\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available() and platform.processor() in (\n        \"arm\",\n        \"arm64\",\n    ):\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    return device\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_optimizer","title":"<code>get_optimizer(name)</code>","text":"<p>Return the optimizer class given its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optimizer name.</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>Optimizer class.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_optimizer(name: str) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Return the optimizer class given its name.\n\n    Parameters\n    ----------\n    name : str\n        Optimizer name.\n\n    Returns\n    -------\n    torch.nn.Optimizer\n        Optimizer class.\n    \"\"\"\n    if name not in SupportedOptimizer:\n        raise NotImplementedError(f\"Optimizer {name} is not yet supported.\")\n\n    return getattr(torch.optim, name)\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_optimizers","title":"<code>get_optimizers()</code>","text":"<p>Return the list of all optimizers available in torch.optim.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Optimizers available in torch.optim.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_optimizers() -&gt; dict[str, str]:\n    \"\"\"\n    Return the list of all optimizers available in torch.optim.\n\n    Returns\n    -------\n    dict\n        Optimizers available in torch.optim.\n    \"\"\"\n    optims = {}\n    for name, obj in inspect.getmembers(torch.optim):\n        if inspect.isclass(obj) and issubclass(obj, torch.optim.Optimizer):\n            if name != \"Optimizer\":\n                optims[name] = name\n    return optims\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_scheduler","title":"<code>get_scheduler(name)</code>","text":"<p>Return the scheduler class given its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scheduler name.</p> required <p>Returns:</p> Type Description <code>Union</code> <p>Scheduler class.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_scheduler(\n    name: str,\n) -&gt; Union[\n    # torch.optim.lr_scheduler.LRScheduler,\n    torch.optim.lr_scheduler.ReduceLROnPlateau,\n]:\n    \"\"\"\n    Return the scheduler class given its name.\n\n    Parameters\n    ----------\n    name : str\n        Scheduler name.\n\n    Returns\n    -------\n    Union\n        Scheduler class.\n    \"\"\"\n    if name not in SupportedScheduler:\n        raise NotImplementedError(f\"Scheduler {name} is not yet supported.\")\n\n    return getattr(torch.optim.lr_scheduler, name)\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_schedulers","title":"<code>get_schedulers()</code>","text":"<p>Return the list of all schedulers available in torch.optim.lr_scheduler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Schedulers available in torch.optim.lr_scheduler.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_schedulers() -&gt; dict[str, str]:\n    \"\"\"\n    Return the list of all schedulers available in torch.optim.lr_scheduler.\n\n    Returns\n    -------\n    dict\n        Schedulers available in torch.optim.lr_scheduler.\n    \"\"\"\n    schedulers = {}\n    for name, obj in inspect.getmembers(torch.optim.lr_scheduler):\n        if inspect.isclass(obj) and issubclass(\n            obj, torch.optim.lr_scheduler.LRScheduler\n        ):\n            if \"LRScheduler\" not in name:\n                schedulers[name] = name\n        elif name == \"ReduceLROnPlateau\":  # somewhat not a subclass of LRScheduler\n            schedulers[name] = name\n    return schedulers\n</code></pre>"},{"location":"reference/careamics/utils/version/","title":"version","text":"<p>Version utility.</p>"},{"location":"reference/careamics/utils/version/#careamics.utils.version.get_careamics_version","title":"<code>get_careamics_version()</code>","text":"<p>Get clean CAREamics version.</p> <p>This method returns the latest <code>Major.Minor.Patch</code> version of CAREamics, removing any local version identifier.</p> <p>Returns:</p> Type Description <code>str</code> <p>Clean CAREamics version.</p> Source code in <code>src/careamics/utils/version.py</code> <pre><code>def get_careamics_version() -&gt; str:\n    \"\"\"Get clean CAREamics version.\n\n    This method returns the latest `Major.Minor.Patch` version of CAREamics, removing\n    any local version identifier.\n\n    Returns\n    -------\n    str\n        Clean CAREamics version.\n    \"\"\"\n    parts = __version__.split(\".\")\n\n    # for local installs that do not detect the latest versions via tags\n    # (typically our CI will install `0.1.devX&lt;hash&gt;` versions)\n    if \"dev\" in parts[-1]:\n        parts[-1] = \"*\"\n        clean_version = \".\".join(parts[:3])\n\n        logger.warning(\n            f\"Your CAREamics version seems to be a locally modified version \"\n            f\"({__version__}). The recorded version for loading models will be \"\n            f\"{clean_version}, which may not exist. If you want to ensure \"\n            f\"exporting the model with an existing version, please install the \"\n            f\"closest CAREamics version from PyPI or conda-forge.\"\n        )\n\n    # Remove any local version identifier)\n    return \".\".join(parts[:3])\n</code></pre>"},{"location":"reference/careamics_napari/","title":"CAREamics napari","text":"<p>Use the navigation index on the left to explore the documentation.</p>"},{"location":"reference/careamics_napari/_version/","title":"_version","text":""},{"location":"reference/careamics_napari/base_plugin/","title":"base_plugin","text":""},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin","title":"<code>BasePlugin</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>CAREamics Base plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>class BasePlugin(QWidget):\n    \"\"\"CAREamics Base plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(\n        self,\n        napari_viewer: napari.Viewer | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__()\n        self.viewer = napari_viewer\n        self.careamist: CAREamist | None = None  # to hold trained careamist\n        self.careamist_loaded: CAREamist | None = None  # to hold loaded careamist\n\n        # create statuses, used to keep track of the threads statuses\n        self.train_status = TrainingStatus()  # type: ignore\n        self.pred_status = PredictionStatus()  # type: ignore\n\n        # create a careamics config (n2v by default)\n        self.careamics_config = get_default_n2v_config()\n\n        # create queues, used to communicate between the threads and the UI\n        self._training_queue: Queue = Queue(10)\n        self._prediction_queue: Queue = Queue(10)\n\n        # changes from the training and prediction\n        self.train_status.events.state.connect(self._training_state_changed)\n        self.pred_status.events.state.connect(self._prediction_state_changed)\n\n        # main layout\n        self.base_layout = QVBoxLayout()\n        # scrolling content\n        scroll_content = QWidget()\n        scroll_content.setLayout(self.base_layout)\n        scroll = ScrollWidgetWrapper(scroll_content)\n        vbox = QVBoxLayout()\n        vbox.addWidget(scroll)\n        self.setLayout(vbox)\n        self.setMinimumWidth(200)\n\n        # calling add_*_ui methods will be happened in sub-classes\n        # to allow more flexibility while saving some code duplication.\n\n    def add_careamics_banner(self, desc: str = \"\") -&gt; None:\n        \"\"\"Add the CAREamics banner and GPU label to the plugin.\"\"\"\n        if len(desc) == 0:\n            desc = \"CAREamics UI for training denoising models.\"\n        self.base_layout.addWidget(\n            CAREamicsBanner(\n                title=\"CAREamics\",\n                short_desc=(desc),\n            )\n        )\n        # GPU label\n        gpu_button = create_gpu_label()\n        gpu_button.setAlignment(Qt.AlignmentFlag.AlignRight)\n        gpu_button.setContentsMargins(0, 5, 0, 0)  # top margin\n        self.base_layout.addWidget(gpu_button)\n\n    def add_train_input_ui(self, use_target: bool = False) -&gt; None:\n        \"\"\"Add the train input data selection UI to the plugin.\"\"\"\n        self.input_data_widget = TrainDataWidget(\n            careamics_config=self.careamics_config, use_target=use_target\n        )\n        self.base_layout.addWidget(self.input_data_widget)\n\n    def add_config_ui(self) -&gt; None:\n        \"\"\"Add the training configuration UI to the plugin.\"\"\"\n        self.config_widget = ConfigurationWidget(self.careamics_config)\n        self.config_widget.enable_3d_chkbox.clicked.connect(self._set_pred_3d)\n        self.config_widget.show_advanced_config.connect(self.show_advanced_config)\n        self.base_layout.addWidget(self.config_widget)\n\n    def add_train_button_ui(self) -&gt; None:\n        \"\"\"Add the training button UI to the plugin.\"\"\"\n        self.train_widget = TrainingWidget(self.train_status)\n        self.progress_widget = TrainProgressWidget(\n            self.careamics_config, self.train_status\n        )\n        self.base_layout.addWidget(self.train_widget)\n        self.base_layout.addWidget(self.progress_widget)\n\n    def add_prediction_ui(self) -&gt; None:\n        \"\"\"Add the prediction UI to the plugin.\"\"\"\n        self.prediction_widget = PredictionWidget(\n            self.careamics_config,\n            self.train_status,\n            self.pred_status,\n            self._prediction_queue,\n        )\n        self.base_layout.addWidget(self.prediction_widget)\n        # to get loaded careamist\n        self.prediction_widget.careamist_loaded.connect(self._on_careamist_loaded)\n        self.prediction_widget.model_from_disk.connect(self._model_selection_changed)\n\n    def add_model_export_ui(self) -&gt; None:\n        \"\"\"Add the model saving UI to the plugin.\"\"\"\n        self.saving_widget = SavingWidget(\n            self.careamics_config,\n            self.train_status,\n        )\n        self.saving_widget.export_model.connect(self.export_model)\n        self.base_layout.addWidget(self.saving_widget)\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration from the UI.\"\"\"\n        if self.config_widget is not None:\n            self.config_widget.update_config()\n\n        if self.prediction_widget is not None:\n            self.prediction_widget.update_config()\n\n        print(f\"update_config:\\n{self.careamics_config}\")\n\n    def export_model(self, destination: Path, export_type: str) -&gt; None:\n        \"\"\"Export the trained model.\"\"\"\n        if self.careamist is None:\n            if _has_napari:\n                ntf.show_info(\"No trained model is available for exporting.\")\n            return\n\n        dims = \"3D\" if self.careamics_config.is_3D else \"2D\"\n        algo_name = self.careamics_config.algorithm_config.get_algorithm_friendly_name()\n        name = f\"{algo_name}_{dims}_{self.careamics_config.experiment_name}\"\n\n        try:\n            if export_type == ExportType.BMZ.value:\n                self._prepare_export_to_bmz(destination, name)\n            else:\n                name = name + \".ckpt\"\n                self.careamist.trainer.save_checkpoint(\n                    destination.joinpath(name),\n                )\n                print(f\"Model exported at {destination}\")\n                if _has_napari:\n                    ntf.show_info(f\"Model exported at {destination}\")\n\n        except Exception as e:\n            traceback.print_exc()\n            if _has_napari:\n                ntf.show_error(str(e))\n\n    def show_advanced_config(self):\n        \"\"\"Show advanced configuration options.\"\"\"\n        raise NotImplementedError(\"Advanced configuration options are not implemented.\")\n\n    def _set_pred_3d(self, state: bool) -&gt; None:\n        \"\"\"Set the 3D mode flag in the prediction widget.\n\n        Parameters\n        ----------\n        state : bool\n            3D mode.\n        \"\"\"\n        if self.prediction_widget is not None:\n            self.prediction_widget.set_3d(state)\n\n    def _training_state_changed(self, state: TrainingState) -&gt; None:\n        \"\"\"Handle training state changes.\n\n        This includes starting and stopping training.\n\n        Parameters\n        ----------\n        state : TrainingState\n            New state.\n        \"\"\"\n        if state == TrainingState.TRAINING:\n            # get data sources\n            data_sources = self.input_data_widget.get_data_sources()\n            if data_sources is None:\n                ntf.show_info(\"Please set the training data first.\")\n                self.train_status.state = TrainingState.IDLE\n                self.train_widget.train_button.setText(\"Train\")\n                return\n\n            # update configuration from ui\n            self.update_config()\n            print(self.careamics_config)\n\n            # start the training thread\n            self.train_worker = train_worker(\n                self.careamics_config,\n                data_sources,\n                self._training_queue,\n                self._prediction_queue,\n                self.careamist,\n                self.pred_status,\n            )\n            self.train_worker.yielded.connect(self._update_from_training)\n            self.train_worker.start()\n\n        elif state == TrainingState.STOPPED:\n            if self.careamist is not None:\n                self.careamist.stop_training()\n\n        elif state == TrainingState.CRASHED or state == TrainingState.IDLE:\n            del self.careamist\n            self.careamist = None\n\n        # update prediction widget\n        if self.prediction_widget is not None:\n            self.prediction_widget.update_button_from_train(state)\n\n    def _prediction_state_changed(self, state: PredictionState) -&gt; None:\n        \"\"\"Handle prediction state changes.\n\n        Parameters\n        ----------\n        state : PredictionState\n            New state.\n        \"\"\"\n        # if self.careamist is None and self.careamist_loaded is None:\n        #     ntf.show_info(\"No trained or loaded model is available for prediction.\")\n        #     self.pred_status.state = PredictionState.STOPPED\n        #     return\n        careamist = self._which_careamist()\n        if careamist is None:\n            self.pred_status.state = PredictionState.STOPPED\n            return\n\n        if state == PredictionState.PREDICTING:\n            # get the prediction data\n            data_source = self.prediction_widget.get_data_source()\n            if data_source is None:\n                ntf.show_info(\"Please set the prediction data first.\")\n                self.pred_status.state = PredictionState.IDLE\n                self.prediction_widget.predict_button.setText(\"Predict\")\n                return\n\n            # update configuration from ui\n            self.update_config()\n\n            # start the prediction thread\n            self.pred_worker = predict_worker(\n                careamist,\n                data_source,\n                self.careamics_config,\n                self._prediction_queue,\n            )\n            self.pred_worker.yielded.connect(self._update_from_prediction)\n            self.pred_worker.start()\n\n        elif state == PredictionState.STOPPED:\n            # prediction stopped: reset the progress bar\n            self._prediction_queue.put(\n                PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, -1)\n            )\n\n    def _on_careamist_loaded(self, careamist: CAREamist) -&gt; None:\n        \"\"\"Event handler called when a CAREamics instance has been loaded.\"\"\"\n        self.careamist_loaded = careamist\n        print(\n            f\"CAREamics instance loaded: \"\n            f\"{self.careamist_loaded.cfg.get_algorithm_friendly_name()}\"\n        )\n        if _has_napari:\n            ntf.show_info(\"CAREamics model loaded successfully!\")\n\n    def _model_selection_changed(self, from_disk: bool) -&gt; None:\n        \"\"\"Event handler called when user changed the model selection.\"\"\"\n        # update the prediction and stop buttons\n        if not from_disk:\n            self.prediction_widget.update_button_from_train(self.train_status.state)\n        elif self.careamist_loaded is not None:\n            self.prediction_widget.predict_button.setEnabled(True)\n            self.prediction_widget.stop_button.setEnabled(False)\n\n    def _which_careamist(self) -&gt; CAREamist | None:\n        \"\"\"Which careamist to use? Trained one or the loaded one.\"\"\"\n        # if load from disk option is selected\n        if self.prediction_widget.load_from_disk:\n            careamist = self.careamist_loaded\n            if careamist is None:\n                ntf.show_warning(\"No model was loaded from disk!\")\n        else:\n            careamist = self.careamist\n            if careamist is None:\n                ntf.show_warning(\"No trained model is available.\")\n\n        return careamist\n\n    def _update_from_training(self, update: TrainUpdate) -&gt; None:\n        \"\"\"Update the training status from the training worker.\n\n        This method receives the updates from the training worker.\n\n        Parameters\n        ----------\n        update : TrainUpdate\n            Update.\n        \"\"\"\n        if update.type == TrainUpdateType.CAREAMIST:\n            if isinstance(update.value, CAREamist):\n                self.careamist = update.value\n        elif update.type == TrainUpdateType.DEBUG:\n            print(update.value)\n        elif update.type == TrainUpdateType.EXCEPTION:\n            self.train_status.state = TrainingState.CRASHED\n\n            if isinstance(update.value, Exception):\n                raise update.value\n        else:\n            self.train_status.update(update)\n\n    def _update_from_prediction(self, update: PredictionUpdate) -&gt; None:\n        \"\"\"Update the signal from the prediction worker.\n\n        This method receives the updates from the prediction worker.\n\n        Parameters\n        ----------\n        update : PredictionUpdate\n            Update.\n        \"\"\"\n        if update.type == PredictionUpdateType.DEBUG:\n            print(update.value)\n        elif update.type == PredictionUpdateType.EXCEPTION:\n            self.pred_status.state = PredictionState.CRASHED\n            # print exception without raising it\n            print(f\"Error: {update.value}\")\n            if _has_napari:\n                ntf.show_error(\n                    f\"An error occurred during prediction: \\n {update.value} \\n\"\n                    f\"Note: if you get an error due to the sizes of \"\n                    f\"Tensors, try using tiling.\"\n                )\n        else:\n            if update.type == PredictionUpdateType.SAMPLE:\n                # add image to napari\n                # TODO keep scaling?\n                if self.viewer is not None:\n                    # value is either a numpy array or\n                    # a list of numpy arrays with each sample/time-point as an element\n                    if isinstance(update.value, list):\n                        # combine all samples\n                        samples = np.concatenate(update.value, axis=0)\n                    else:\n                        samples = update.value\n\n                    # reshape the prediction to match the input axes\n                    samples = reshape_prediction(\n                        samples,  # type: ignore\n                        self.careamics_config.data_config.axes,  # type: ignore\n                        self.careamics_config.is_3D,\n                    )\n                    self.viewer.add_image(samples, name=\"Prediction\")\n            else:\n                self.pred_status.update(update)\n\n    def _show_bmz_dialog(\n        self, bmz_path: Path, cover: Path, sample_input: np.ndarray\n    ) -&gt; None:\n        \"\"\"Show the BMZ export dialog window.\"\"\"\n        # ask user for bmz model specs\n        bmz_window = BMZExportWidget(self, cover_image_path=cover)\n        bmz_window.accepted.connect(\n            lambda: self._export_to_bmz(bmz_window, bmz_path, sample_input)\n        )\n        bmz_window.show()\n\n    def _prepare_export_to_bmz(self, destination: Path, name: str) -&gt; None:\n        \"\"\"Export the trained model to BMZ format.\"\"\"\n        if self.careamist is None:\n            if _has_napari:\n                ntf.show_info(\"No trained model is available for exporting.\")\n            return\n\n        bmz_path = destination.joinpath(name + \".zip\")\n\n        data_sources = self.input_data_widget.get_data_sources()\n        if data_sources is not None:\n            train_data = data_sources[\"train\"][0]\n            if not isinstance(train_data, np.ndarray):\n                raise NotImplementedError(\n                    \"BMZ export from tiff data source is not implemented yet.\"\n                )\n        if train_data.ndim == 2:\n            sample_input = train_data[:256, :256]\n        else:\n            sample_input = train_data[0, :256, :256]\n\n        # make a default cover image\n        output_patches = self.careamist.predict(\n            sample_input,\n            data_type=\"array\",\n            tta_transforms=False,\n        )\n        sample_output = np.concatenate(output_patches, axis=0)\n        cover_path = create_cover(\n            directory=self.careamics_config.work_dir,\n            array_in=sample_input[np.newaxis, np.newaxis, ...],\n            array_out=sample_output,\n        )\n\n        # show the bmz export dialog\n        self._show_bmz_dialog(bmz_path, cover_path, sample_input)\n\n    def _export_to_bmz(\n        self, bmz_window: BMZExportWidget, bmz_path: Path, sample_input: np.ndarray\n    ) -&gt; None:\n        bmz_data = {\n            \"model_name\": bmz_window.model_name,\n            \"description\": bmz_window.general_description,\n            \"data_description\": bmz_window.data_description,\n            \"authors\": bmz_window.authors,\n            \"cover\": bmz_window.cover_image,\n        }\n\n        try:\n            self.careamist.export_to_bmz(  # type: ignore\n                path_to_archive=bmz_path,\n                input_array=sample_input,\n                friendly_model_name=bmz_data[\"model_name\"],\n                general_description=bmz_data[\"description\"],\n                data_description=bmz_data[\"data_description\"],\n                authors=bmz_data[\"authors\"],\n                covers=[bmz_data[\"cover\"]],\n            )\n            print(f\"Model exported at {bmz_path}\")\n            if _has_napari:\n                ntf.show_info(f\"Model exported at {bmz_path}\")\n\n        except Exception as e:\n            traceback.print_exc()\n            if _has_napari:\n                ntf.show_error(str(e))\n\n    def closeEvent(self, event) -&gt; None:\n        \"\"\"Close the plugin.\n\n        Parameters\n        ----------\n        event : QCloseEvent\n            Close event.\n        \"\"\"\n        super().closeEvent(event)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def __init__(\n    self,\n    napari_viewer: napari.Viewer | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__()\n    self.viewer = napari_viewer\n    self.careamist: CAREamist | None = None  # to hold trained careamist\n    self.careamist_loaded: CAREamist | None = None  # to hold loaded careamist\n\n    # create statuses, used to keep track of the threads statuses\n    self.train_status = TrainingStatus()  # type: ignore\n    self.pred_status = PredictionStatus()  # type: ignore\n\n    # create a careamics config (n2v by default)\n    self.careamics_config = get_default_n2v_config()\n\n    # create queues, used to communicate between the threads and the UI\n    self._training_queue: Queue = Queue(10)\n    self._prediction_queue: Queue = Queue(10)\n\n    # changes from the training and prediction\n    self.train_status.events.state.connect(self._training_state_changed)\n    self.pred_status.events.state.connect(self._prediction_state_changed)\n\n    # main layout\n    self.base_layout = QVBoxLayout()\n    # scrolling content\n    scroll_content = QWidget()\n    scroll_content.setLayout(self.base_layout)\n    scroll = ScrollWidgetWrapper(scroll_content)\n    vbox = QVBoxLayout()\n    vbox.addWidget(scroll)\n    self.setLayout(vbox)\n    self.setMinimumWidth(200)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_careamics_banner","title":"<code>add_careamics_banner(desc='')</code>","text":"<p>Add the CAREamics banner and GPU label to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_careamics_banner(self, desc: str = \"\") -&gt; None:\n    \"\"\"Add the CAREamics banner and GPU label to the plugin.\"\"\"\n    if len(desc) == 0:\n        desc = \"CAREamics UI for training denoising models.\"\n    self.base_layout.addWidget(\n        CAREamicsBanner(\n            title=\"CAREamics\",\n            short_desc=(desc),\n        )\n    )\n    # GPU label\n    gpu_button = create_gpu_label()\n    gpu_button.setAlignment(Qt.AlignmentFlag.AlignRight)\n    gpu_button.setContentsMargins(0, 5, 0, 0)  # top margin\n    self.base_layout.addWidget(gpu_button)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_config_ui","title":"<code>add_config_ui()</code>","text":"<p>Add the training configuration UI to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_config_ui(self) -&gt; None:\n    \"\"\"Add the training configuration UI to the plugin.\"\"\"\n    self.config_widget = ConfigurationWidget(self.careamics_config)\n    self.config_widget.enable_3d_chkbox.clicked.connect(self._set_pred_3d)\n    self.config_widget.show_advanced_config.connect(self.show_advanced_config)\n    self.base_layout.addWidget(self.config_widget)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_model_export_ui","title":"<code>add_model_export_ui()</code>","text":"<p>Add the model saving UI to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_model_export_ui(self) -&gt; None:\n    \"\"\"Add the model saving UI to the plugin.\"\"\"\n    self.saving_widget = SavingWidget(\n        self.careamics_config,\n        self.train_status,\n    )\n    self.saving_widget.export_model.connect(self.export_model)\n    self.base_layout.addWidget(self.saving_widget)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_prediction_ui","title":"<code>add_prediction_ui()</code>","text":"<p>Add the prediction UI to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_prediction_ui(self) -&gt; None:\n    \"\"\"Add the prediction UI to the plugin.\"\"\"\n    self.prediction_widget = PredictionWidget(\n        self.careamics_config,\n        self.train_status,\n        self.pred_status,\n        self._prediction_queue,\n    )\n    self.base_layout.addWidget(self.prediction_widget)\n    # to get loaded careamist\n    self.prediction_widget.careamist_loaded.connect(self._on_careamist_loaded)\n    self.prediction_widget.model_from_disk.connect(self._model_selection_changed)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_train_button_ui","title":"<code>add_train_button_ui()</code>","text":"<p>Add the training button UI to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_train_button_ui(self) -&gt; None:\n    \"\"\"Add the training button UI to the plugin.\"\"\"\n    self.train_widget = TrainingWidget(self.train_status)\n    self.progress_widget = TrainProgressWidget(\n        self.careamics_config, self.train_status\n    )\n    self.base_layout.addWidget(self.train_widget)\n    self.base_layout.addWidget(self.progress_widget)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.add_train_input_ui","title":"<code>add_train_input_ui(use_target=False)</code>","text":"<p>Add the train input data selection UI to the plugin.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def add_train_input_ui(self, use_target: bool = False) -&gt; None:\n    \"\"\"Add the train input data selection UI to the plugin.\"\"\"\n    self.input_data_widget = TrainDataWidget(\n        careamics_config=self.careamics_config, use_target=use_target\n    )\n    self.base_layout.addWidget(self.input_data_widget)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.closeEvent","title":"<code>closeEvent(event)</code>","text":"<p>Close the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QCloseEvent</code> <p>Close event.</p> required Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def closeEvent(self, event) -&gt; None:\n    \"\"\"Close the plugin.\n\n    Parameters\n    ----------\n    event : QCloseEvent\n        Close event.\n    \"\"\"\n    super().closeEvent(event)\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.export_model","title":"<code>export_model(destination, export_type)</code>","text":"<p>Export the trained model.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def export_model(self, destination: Path, export_type: str) -&gt; None:\n    \"\"\"Export the trained model.\"\"\"\n    if self.careamist is None:\n        if _has_napari:\n            ntf.show_info(\"No trained model is available for exporting.\")\n        return\n\n    dims = \"3D\" if self.careamics_config.is_3D else \"2D\"\n    algo_name = self.careamics_config.algorithm_config.get_algorithm_friendly_name()\n    name = f\"{algo_name}_{dims}_{self.careamics_config.experiment_name}\"\n\n    try:\n        if export_type == ExportType.BMZ.value:\n            self._prepare_export_to_bmz(destination, name)\n        else:\n            name = name + \".ckpt\"\n            self.careamist.trainer.save_checkpoint(\n                destination.joinpath(name),\n            )\n            print(f\"Model exported at {destination}\")\n            if _has_napari:\n                ntf.show_info(f\"Model exported at {destination}\")\n\n    except Exception as e:\n        traceback.print_exc()\n        if _has_napari:\n            ntf.show_error(str(e))\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.show_advanced_config","title":"<code>show_advanced_config()</code>","text":"<p>Show advanced configuration options.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def show_advanced_config(self):\n    \"\"\"Show advanced configuration options.\"\"\"\n    raise NotImplementedError(\"Advanced configuration options are not implemented.\")\n</code></pre>"},{"location":"reference/careamics_napari/base_plugin/#careamics_napari.base_plugin.BasePlugin.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration from the UI.</p> Source code in <code>src/careamics_napari/base_plugin.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration from the UI.\"\"\"\n    if self.config_widget is not None:\n        self.config_widget.update_config()\n\n    if self.prediction_widget is not None:\n        self.prediction_widget.update_config()\n\n    print(f\"update_config:\\n{self.careamics_config}\")\n</code></pre>"},{"location":"reference/careamics_napari/care_plugin/","title":"care_plugin","text":"<p>N2V plugin.</p>"},{"location":"reference/careamics_napari/care_plugin/#careamics_napari.care_plugin.CAREPlugin","title":"<code>CAREPlugin</code>","text":"<p>               Bases: <code>BasePlugin</code></p> <p>CAREamics CARE plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/care_plugin.py</code> <pre><code>class CAREPlugin(BasePlugin):\n    \"\"\"CAREamics CARE plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(\n        self,\n        napari_viewer: napari.Viewer | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__(napari_viewer)\n        self.viewer = napari_viewer\n\n        # create a care config\n        self.careamics_config = get_default_care_config()\n        # advanced care config\n        self.advanced_config = CAREAdvancedConfig()\n\n        # assemble plugin ui\n        self.add_careamics_banner(\"CAREamics UI for training CARE denoising models.\")\n        self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n        self.add_config_ui()\n        self.add_train_button_ui()\n        self.add_prediction_ui()\n        self.add_model_export_ui()\n\n    def show_advanced_config(self) -&gt; None:\n        \"\"\"Show advanced configuration.\"\"\"\n        # update axes in configuration\n        self.config_widget.axes_widget.update_config()\n\n        # show window with advanced options\n        win = CAREConfigurationWindow(self, self.careamics_config, self.advanced_config)\n        win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n        win.show()\n</code></pre>"},{"location":"reference/careamics_napari/care_plugin/#careamics_napari.care_plugin.CAREPlugin.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/care_plugin.py</code> <pre><code>def __init__(\n    self,\n    napari_viewer: napari.Viewer | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__(napari_viewer)\n    self.viewer = napari_viewer\n\n    # create a care config\n    self.careamics_config = get_default_care_config()\n    # advanced care config\n    self.advanced_config = CAREAdvancedConfig()\n\n    # assemble plugin ui\n    self.add_careamics_banner(\"CAREamics UI for training CARE denoising models.\")\n    self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n    self.add_config_ui()\n    self.add_train_button_ui()\n    self.add_prediction_ui()\n    self.add_model_export_ui()\n</code></pre>"},{"location":"reference/careamics_napari/care_plugin/#careamics_napari.care_plugin.CAREPlugin.show_advanced_config","title":"<code>show_advanced_config()</code>","text":"<p>Show advanced configuration.</p> Source code in <code>src/careamics_napari/care_plugin.py</code> <pre><code>def show_advanced_config(self) -&gt; None:\n    \"\"\"Show advanced configuration.\"\"\"\n    # update axes in configuration\n    self.config_widget.axes_widget.update_config()\n\n    # show window with advanced options\n    win = CAREConfigurationWindow(self, self.careamics_config, self.advanced_config)\n    win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n    win.show()\n</code></pre>"},{"location":"reference/careamics_napari/n2n_plugin/","title":"n2n_plugin","text":"<p>N2V plugin.</p>"},{"location":"reference/careamics_napari/n2n_plugin/#careamics_napari.n2n_plugin.N2NPlugin","title":"<code>N2NPlugin</code>","text":"<p>               Bases: <code>BasePlugin</code></p> <p>CAREamics N2N plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/n2n_plugin.py</code> <pre><code>class N2NPlugin(BasePlugin):\n    \"\"\"CAREamics N2N plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(\n        self,\n        napari_viewer: napari.Viewer | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__(napari_viewer)\n        self.viewer = napari_viewer\n\n        # create a n2n config\n        self.careamics_config = get_default_n2n_config()\n        # advanced n2n config\n        self.advanced_config = N2NAdvancedConfig()\n\n        # assemble plugin ui\n        self.add_careamics_banner(\n            \"CAREamics UI for training Noise2Noise (N2N) denoising models.\"\n        )\n        self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n        self.add_config_ui()\n        self.add_train_button_ui()\n        self.add_prediction_ui()\n        self.add_model_export_ui()\n\n    def show_advanced_config(self) -&gt; None:\n        \"\"\"Show advanced configuration.\"\"\"\n        # update axes in configuration\n        self.config_widget.axes_widget.update_config()\n\n        # show window with advanced options\n        win = N2NConfigurationWindow(self, self.careamics_config, self.advanced_config)\n        win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n        win.show()\n</code></pre>"},{"location":"reference/careamics_napari/n2n_plugin/#careamics_napari.n2n_plugin.N2NPlugin.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/n2n_plugin.py</code> <pre><code>def __init__(\n    self,\n    napari_viewer: napari.Viewer | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__(napari_viewer)\n    self.viewer = napari_viewer\n\n    # create a n2n config\n    self.careamics_config = get_default_n2n_config()\n    # advanced n2n config\n    self.advanced_config = N2NAdvancedConfig()\n\n    # assemble plugin ui\n    self.add_careamics_banner(\n        \"CAREamics UI for training Noise2Noise (N2N) denoising models.\"\n    )\n    self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n    self.add_config_ui()\n    self.add_train_button_ui()\n    self.add_prediction_ui()\n    self.add_model_export_ui()\n</code></pre>"},{"location":"reference/careamics_napari/n2n_plugin/#careamics_napari.n2n_plugin.N2NPlugin.show_advanced_config","title":"<code>show_advanced_config()</code>","text":"<p>Show advanced configuration.</p> Source code in <code>src/careamics_napari/n2n_plugin.py</code> <pre><code>def show_advanced_config(self) -&gt; None:\n    \"\"\"Show advanced configuration.\"\"\"\n    # update axes in configuration\n    self.config_widget.axes_widget.update_config()\n\n    # show window with advanced options\n    win = N2NConfigurationWindow(self, self.careamics_config, self.advanced_config)\n    win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n    win.show()\n</code></pre>"},{"location":"reference/careamics_napari/n2v_plugin/","title":"n2v_plugin","text":"<p>N2V plugin.</p>"},{"location":"reference/careamics_napari/n2v_plugin/#careamics_napari.n2v_plugin.N2VPlugin","title":"<code>N2VPlugin</code>","text":"<p>               Bases: <code>BasePlugin</code></p> <p>CAREamics N2V plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/n2v_plugin.py</code> <pre><code>class N2VPlugin(BasePlugin):\n    \"\"\"CAREamics N2V plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(\n        self,\n        napari_viewer: napari.Viewer | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__(napari_viewer)\n        self.viewer = napari_viewer\n\n        # create a n2v config\n        self.careamics_config = get_default_n2v_config()\n        # advanced n2v config\n        self.advanced_config = N2VAdvancedConfig()\n\n        # assemble plugin ui\n        self.add_careamics_banner(\n            \"CAREamics UI for training Noise2Void (N2V) denoising models.\"\n        )\n        self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n        self.add_config_ui()\n        self.add_train_button_ui()\n        self.add_prediction_ui()\n        self.add_model_export_ui()\n\n    def show_advanced_config(self) -&gt; None:\n        \"\"\"Show advanced configuration.\"\"\"\n        # update axes in configuration\n        self.config_widget.axes_widget.update_config()\n\n        # show window with advanced options\n        win = N2VConfigurationWindow(self, self.careamics_config, self.advanced_config)\n        win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n        win.show()\n</code></pre>"},{"location":"reference/careamics_napari/n2v_plugin/#careamics_napari.n2v_plugin.N2VPlugin.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/n2v_plugin.py</code> <pre><code>def __init__(\n    self,\n    napari_viewer: napari.Viewer | None = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__(napari_viewer)\n    self.viewer = napari_viewer\n\n    # create a n2v config\n    self.careamics_config = get_default_n2v_config()\n    # advanced n2v config\n    self.advanced_config = N2VAdvancedConfig()\n\n    # assemble plugin ui\n    self.add_careamics_banner(\n        \"CAREamics UI for training Noise2Void (N2V) denoising models.\"\n    )\n    self.add_train_input_ui(use_target=self.careamics_config.needs_gt)\n    self.add_config_ui()\n    self.add_train_button_ui()\n    self.add_prediction_ui()\n    self.add_model_export_ui()\n</code></pre>"},{"location":"reference/careamics_napari/n2v_plugin/#careamics_napari.n2v_plugin.N2VPlugin.show_advanced_config","title":"<code>show_advanced_config()</code>","text":"<p>Show advanced configuration.</p> Source code in <code>src/careamics_napari/n2v_plugin.py</code> <pre><code>def show_advanced_config(self) -&gt; None:\n    \"\"\"Show advanced configuration.\"\"\"\n    # update axes in configuration\n    self.config_widget.axes_widget.update_config()\n\n    # show window with advanced options\n    win = N2VConfigurationWindow(self, self.careamics_config, self.advanced_config)\n    win.finished.connect(lambda: print(self.advanced_config, self.careamics_config))\n    win.show()\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/","title":"sample_data","text":"<p>Sample data for the careamics napari plugin.</p>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.care_u2os_data","title":"<code>care_u2os_data()</code>","text":"<p>Load CARE U2OS data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def care_u2os_data() -&gt; LayerDataTuple:\n    \"\"\"Load CARE U2OS data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_u2os_care()\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.n2n_sem_data","title":"<code>n2n_sem_data()</code>","text":"<p>Load N2N SEM data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def n2n_sem_data() -&gt; LayerDataTuple:\n    \"\"\"Load N2N SEM data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_sem_n2n()\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.n2v_sem_data","title":"<code>n2v_sem_data()</code>","text":"<p>Load N2V SEM data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def n2v_sem_data() -&gt; LayerDataTuple:\n    \"\"\"Load N2V SEM data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_sem_n2v()\n</code></pre>"},{"location":"reference/careamics_napari/bmz/author_widget/","title":"author_widget","text":""},{"location":"reference/careamics_napari/bmz/author_widget/#careamics_napari.bmz.author_widget.AuthorWidget","title":"<code>AuthorWidget</code>","text":"<p>               Bases: <code>QDialog</code></p> <p>Author form widget.</p> Source code in <code>src/careamics_napari/bmz/author_widget.py</code> <pre><code>class AuthorWidget(QDialog):\n    \"\"\"Author form widget.\"\"\"\n\n    submit = Signal(AuthorModel, name=\"submit\")\n\n    def __init__(\n        self, parent: QWidget | None = None, author: AuthorModel | None = None\n    ) -&gt; None:\n        super().__init__(parent)\n\n        # author data (for editing mode)\n        self.author = author\n\n        # ui\n        self.name_txtbox = QLineEdit()\n        self.name_txtbox.setToolTip(\"Name of the author (Cannot have / or \\\\).\")\n        self.email_txtbox = QLineEdit()\n        self.affiliation_txtbox = QLineEdit()\n        self.git_user_txtbox = QLineEdit()\n        self.git_user_txtbox.setToolTip(\"Github username\")\n        self.orcid_txtbox = QLineEdit()\n        self.orcid_txtbox.setToolTip(\n            markdown(AuthorModel.model_fields[\"orcid\"].description)  # type: ignore\n        )\n        # buttons\n        self.submit_button = QPushButton(\"&amp;Submit\")\n        self.submit_button.setMaximumWidth(120)\n        self.submit_button.clicked.connect(self.submit_author)\n        self.cancel_button = QPushButton(\"&amp;Cancel\")\n        self.cancel_button.setMaximumWidth(120)\n        self.cancel_button.clicked.connect(lambda: self.close())  # type: ignore\n\n        # set values for editing mode\n        if self.author is not None:\n            self.name_txtbox.setText(self.author.name)\n            self.email_txtbox.setText(self.author.email)\n            self.affiliation_txtbox.setText(self.author.affiliation)\n            self.git_user_txtbox.setText(self.author.github_user)\n            self.orcid_txtbox.setText(self.author.orcid)\n\n        # layout\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        form.addRow(\"Name (required):\", self.name_txtbox)\n        form.addRow(\"Email:\", self.email_txtbox)\n        form.addRow(\"Affiliation:\", self.affiliation_txtbox)\n        form.addRow(\"Github User:\", self.git_user_txtbox)\n        form.addRow(\"ORCID:\", self.orcid_txtbox)\n\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.submit_button)\n        hbox.addWidget(self.cancel_button)\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(form)\n        vbox.addSpacing(10)\n        vbox.addLayout(hbox)\n        self.setLayout(vbox)\n\n    def submit_author(self) -&gt; None:\n        \"\"\"Validate and submit the entered author's profile.\"\"\"\n        author_data = {\n            \"name\": self._get_value(self.name_txtbox.text()),\n            \"email\": self._get_value(self.email_txtbox.text()),\n            \"affiliation\": self._get_value(self.affiliation_txtbox.text()),\n            \"github_user\": self._get_value(self.git_user_txtbox.text()),\n            \"orcid\": self._get_value(self.orcid_txtbox.text()),\n        }\n        # validation\n        try:\n            author = AuthorModel.model_validate(author_data)\n        except Exception as err:\n            print(err)\n            if _has_napari:\n                ntf.show_error(\"Not a valid author!\")\n            return\n\n        # submit\n        self.submit.emit(author)\n        self.close()\n\n    def _get_value(self, str) -&gt; str | None:\n        if not str.strip():\n            return None\n        else:\n            return str.strip()\n</code></pre>"},{"location":"reference/careamics_napari/bmz/author_widget/#careamics_napari.bmz.author_widget.AuthorWidget.submit_author","title":"<code>submit_author()</code>","text":"<p>Validate and submit the entered author's profile.</p> Source code in <code>src/careamics_napari/bmz/author_widget.py</code> <pre><code>def submit_author(self) -&gt; None:\n    \"\"\"Validate and submit the entered author's profile.\"\"\"\n    author_data = {\n        \"name\": self._get_value(self.name_txtbox.text()),\n        \"email\": self._get_value(self.email_txtbox.text()),\n        \"affiliation\": self._get_value(self.affiliation_txtbox.text()),\n        \"github_user\": self._get_value(self.git_user_txtbox.text()),\n        \"orcid\": self._get_value(self.orcid_txtbox.text()),\n    }\n    # validation\n    try:\n        author = AuthorModel.model_validate(author_data)\n    except Exception as err:\n        print(err)\n        if _has_napari:\n            ntf.show_error(\"Not a valid author!\")\n        return\n\n    # submit\n    self.submit.emit(author)\n    self.close()\n</code></pre>"},{"location":"reference/careamics_napari/bmz/bmz_export_widget/","title":"bmz_export_widget","text":""},{"location":"reference/careamics_napari/bmz/bmz_export_widget/#careamics_napari.bmz.bmz_export_widget.BMZExportWidget","title":"<code>BMZExportWidget</code>","text":"<p>               Bases: <code>QDialog</code></p> <p>A dialog to get information about the model to export.</p> Source code in <code>src/careamics_napari/bmz/bmz_export_widget.py</code> <pre><code>class BMZExportWidget(QDialog):\n    \"\"\"A dialog to get information about the model to export.\"\"\"\n\n    def __init__(\n        self,\n        parent: QWidget | None = None,\n        cover_image_path: str | Path | None = None,\n    ):\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        parent : QWidget\n            Parent widget. Default is None.\n        careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n        \"\"\"\n        super().__init__(parent)\n        self.setWindowModality(Qt.ApplicationModal)  # type: ignore\n\n        self.default_cover = str(cover_image_path)\n        self.uploaded_cover = None\n        self.authors: list[AuthorModel] = []\n\n        # ui\n        # model name\n        self.name_textbox = QLineEdit()\n        self.name_textbox.setPlaceholderText(\"At least 5 characters\")\n\n        # general description\n        self.description_textbox = QPlainTextEdit()\n        self.description_textbox.setFixedHeight(90)\n        self.description_textbox.setPlaceholderText(\n            \"Enter a short description of your model.\"\n        )\n\n        # data description\n        self.data_description_textbox = QPlainTextEdit()\n        self.data_description_textbox.setFixedHeight(120)\n        self.data_description_textbox.setPlaceholderText(\n            \"Describe the data that you used for training the model.\\n\"\n            \"Use Markdown formatting and must include a 'Validation' header \"\n            \"(like ### Validation) describing validation metrics &amp; methods.\"\n        )\n\n        # authors\n        self.authors_listview = QListWidget()\n        self.authors_listview.setFixedHeight(90)\n        self.add_author_button = QPushButton(\"Add\")\n        self.add_author_button.clicked.connect(self._open_new_author)\n        self.edit_author_button = QPushButton(\"Edit\")\n        self.edit_author_button.clicked.connect(self._open_edit_author)\n        self.del_author_button = QPushButton(\"Remove\")\n        self.del_author_button.clicked.connect(self._del_author)\n\n        # cover image\n        self.image_label = QLabel()\n        self.image_label.setBackgroundRole(QPalette.Dark)\n        self.image_label.setSizePolicy(QSizePolicy.Ignored, QSizePolicy.Ignored)\n        self.image_label.setScaledContents(True)\n\n        self.image_scroll = QScrollArea()\n        self.image_scroll.setBackgroundRole(QPalette.Dark)\n        self.image_scroll.setWidget(self.image_label)\n        self.image_scroll.setFixedHeight(300)\n        self.image_scroll.setStyleSheet(\"border: 1px solid grey;\")\n\n        self.default_cover_button = QPushButton(\"Default Cover Image\")\n        self.default_cover_button.setMaximumWidth(150)\n        self.default_cover_button.clicked.connect(self._set_default_cover_image)\n        self.upload_image_button = QPushButton(\"Upload Cover Image\")\n        self.upload_image_button.setMaximumWidth(150)\n        self.upload_image_button.clicked.connect(self._upload_cover_image)\n\n        # submit &amp; cancel buttons\n        self.submit_button = QPushButton(\"&amp;Export\")\n        self.submit_button.setMaximumWidth(120)\n        self.submit_button.clicked.connect(self._submit)\n        self.cancel_button = QPushButton(\"&amp;Cancel\")\n        self.cancel_button.setMaximumWidth(120)\n        self.cancel_button.clicked.connect(lambda: self.reject())  # type: ignore\n\n        # layout\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        form.addRow(\"Model Name: \", self.name_textbox)\n        form.addRow(\"General Description:\", self.description_textbox)\n        form.addRow(\"Data Description:\", self.data_description_textbox)\n        vbox = QVBoxLayout()\n        vbox.addWidget(self.add_author_button)\n        vbox.addWidget(self.edit_author_button)\n        vbox.addWidget(self.del_author_button)\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.authors_listview)\n        hbox.addLayout(vbox)\n        form.addRow(\"Authors:\", hbox)\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.default_cover_button)  # type: ignore\n        hbox.addWidget(self.upload_image_button)  # type: ignore\n        vbox = QVBoxLayout()\n        vbox.addWidget(self.image_scroll)\n        vbox.addLayout(hbox)\n        vbox.addSpacing(15)\n        form.addRow(\"Cover Image:\", vbox)\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.submit_button)\n        hbox.addWidget(self.cancel_button)\n        form.addRow(hbox)\n\n        self.setLayout(form)\n        self.setWindowTitle(\"BMZ Export\")\n\n        self._bind_properties()\n        self._set_default_cover_image()\n\n    def _bind_properties(self) -&gt; None:\n        \"\"\"Bind the properties to the UI elements.\"\"\"\n        type(self).model_name = bind(self.name_textbox, \"text\")\n        type(self).general_description = bind(self.description_textbox, \"plainText\")\n        type(self).data_description = bind(self.data_description_textbox, \"plainText\")\n        type(self).cover_image = property(\n            fget=lambda self: self.uploaded_cover\n            if self.uploaded_cover is not None\n            else self.default_cover,\n        )\n\n    def _populate_authors_list(self) -&gt; None:\n        \"\"\"Populates the authors' listview widget with the list of authors.\"\"\"\n        self.authors_listview.clear()\n        self.authors_listview.addItems(author.name for author in self.authors)\n\n    def _open_new_author(self) -&gt; None:\n        \"\"\"Show author's form to add a new author.\"\"\"\n        author_win = AuthorWidget(self)\n        author_win.setWindowModality(Qt.ApplicationModal)  # type: ignore\n        author_win.submit.connect(self._add_author)\n        author_win.show()\n\n    def _open_edit_author(self) -&gt; None:\n        \"\"\"Show author's form to modify an existing author.\"\"\"\n        selected_index = self.authors_listview.currentRow()\n        if selected_index &gt; -1:\n            author = self.authors[selected_index]\n            author_win = AuthorWidget(self, author)\n            author_win.setWindowModality(Qt.ApplicationModal)  # type: ignore\n            author_win.submit.connect(\n                lambda author: self._update_author(selected_index, author)\n            )\n            author_win.show()\n\n    def _del_author(self) -&gt; None:\n        \"\"\"Remove the selected author.\"\"\"\n        selected_index = self.authors_listview.currentRow()\n        if selected_index &gt; -1:\n            reply, del_row = self._remove_from_listview(\n                self.authors_listview,\n                \"Are you sure you want to remove the selected author?\",\n            )\n            if reply:\n                del self.authors[del_row]\n                self._populate_authors_list()\n\n    def _add_author(self, author: AuthorModel) -&gt; None:\n        \"\"\"Add a new author to the list.\"\"\"\n        self.authors.append(author)\n        self._populate_authors_list()\n\n    def _update_author(self, index: int, author: AuthorModel) -&gt; None:\n        \"\"\"Update the author at the given index with the given data.\"\"\"\n        self.authors[index] = author\n        self._populate_authors_list()\n\n    def _remove_from_listview(\n        self, list_widget: QListWidget, msg: str | None = None\n    ) -&gt; tuple[QMessageBox.StandardButton, int]:\n        \"\"\"Removes the selected item from the given listview widget.\"\"\"\n        curr_row = list_widget.currentRow()\n        if curr_row &gt; -1:\n            reply = QMessageBox.warning(\n                self,\n                \"CAREamics\",\n                msg or \"Are you sure you want to remove the selected item from the list?\",\n                QMessageBox.Yes | QMessageBox.No,\n                QMessageBox.No,\n            )\n            if reply == QMessageBox.Yes:\n                list_widget.takeItem(curr_row)\n\n        return reply == QMessageBox.Yes, curr_row\n\n    def _set_default_cover_image(self) -&gt; None:\n        \"\"\"Set the default cover image.\"\"\"\n        self.image_label.setPixmap(QPixmap(self.default_cover))\n        self.image_label.adjustSize()\n\n    def _upload_cover_image(self) -&gt; None:\n        \"\"\"Upload a cover image.\"\"\"\n        file_path, _ = QFileDialog.getOpenFileName(\n            self,\n            \"Select Cover Image\",\n            \"\",\n            \"Image Files (*.png *.jpg *.bmp);;All Files (*)\",\n        )\n        if file_path:\n            self.uploaded_cover = file_path\n            self.image_label.setPixmap(QPixmap(file_path))\n            self.image_label.adjustSize()\n\n    def _submit(self) -&gt; None:\n        \"\"\"Submit the entered information.\"\"\"\n        if len(self.model_name.strip()) &lt; 5:\n            QMessageBox.critical(\n                self, \"CAREamics\", \"Model name must be at least 5 characters long!\"\n            )\n            return\n        if not self.general_description.strip():\n            QMessageBox.critical(self, \"CAREamics\", \"General description is required!\")\n            return\n        if not self.data_description.strip():\n            QMessageBox.critical(self, \"CAREamics\", \"Data description is required!\")\n            return\n        if not self.authors:\n            QMessageBox.critical(self, \"CAREamics\", \"At least one author is required!\")\n            return\n        if self.cover_image is None:\n            QMessageBox.critical(self, \"CAREamics\", \"A cover image is required!\")\n            return\n\n        self.accept()\n</code></pre>"},{"location":"reference/careamics_napari/bmz/bmz_export_widget/#careamics_napari.bmz.bmz_export_widget.BMZExportWidget.__init__","title":"<code>__init__(parent=None, cover_image_path=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>Parent widget. Default is None.</p> <code>None</code> <code>careamics_config</code> <code>BaseConfig</code> <p>The configuration for the CAREamics algorithm.</p> required Source code in <code>src/careamics_napari/bmz/bmz_export_widget.py</code> <pre><code>def __init__(\n    self,\n    parent: QWidget | None = None,\n    cover_image_path: str | Path | None = None,\n):\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    parent : QWidget\n        Parent widget. Default is None.\n    careamics_config : BaseConfig\n        The configuration for the CAREamics algorithm.\n    \"\"\"\n    super().__init__(parent)\n    self.setWindowModality(Qt.ApplicationModal)  # type: ignore\n\n    self.default_cover = str(cover_image_path)\n    self.uploaded_cover = None\n    self.authors: list[AuthorModel] = []\n\n    # ui\n    # model name\n    self.name_textbox = QLineEdit()\n    self.name_textbox.setPlaceholderText(\"At least 5 characters\")\n\n    # general description\n    self.description_textbox = QPlainTextEdit()\n    self.description_textbox.setFixedHeight(90)\n    self.description_textbox.setPlaceholderText(\n        \"Enter a short description of your model.\"\n    )\n\n    # data description\n    self.data_description_textbox = QPlainTextEdit()\n    self.data_description_textbox.setFixedHeight(120)\n    self.data_description_textbox.setPlaceholderText(\n        \"Describe the data that you used for training the model.\\n\"\n        \"Use Markdown formatting and must include a 'Validation' header \"\n        \"(like ### Validation) describing validation metrics &amp; methods.\"\n    )\n\n    # authors\n    self.authors_listview = QListWidget()\n    self.authors_listview.setFixedHeight(90)\n    self.add_author_button = QPushButton(\"Add\")\n    self.add_author_button.clicked.connect(self._open_new_author)\n    self.edit_author_button = QPushButton(\"Edit\")\n    self.edit_author_button.clicked.connect(self._open_edit_author)\n    self.del_author_button = QPushButton(\"Remove\")\n    self.del_author_button.clicked.connect(self._del_author)\n\n    # cover image\n    self.image_label = QLabel()\n    self.image_label.setBackgroundRole(QPalette.Dark)\n    self.image_label.setSizePolicy(QSizePolicy.Ignored, QSizePolicy.Ignored)\n    self.image_label.setScaledContents(True)\n\n    self.image_scroll = QScrollArea()\n    self.image_scroll.setBackgroundRole(QPalette.Dark)\n    self.image_scroll.setWidget(self.image_label)\n    self.image_scroll.setFixedHeight(300)\n    self.image_scroll.setStyleSheet(\"border: 1px solid grey;\")\n\n    self.default_cover_button = QPushButton(\"Default Cover Image\")\n    self.default_cover_button.setMaximumWidth(150)\n    self.default_cover_button.clicked.connect(self._set_default_cover_image)\n    self.upload_image_button = QPushButton(\"Upload Cover Image\")\n    self.upload_image_button.setMaximumWidth(150)\n    self.upload_image_button.clicked.connect(self._upload_cover_image)\n\n    # submit &amp; cancel buttons\n    self.submit_button = QPushButton(\"&amp;Export\")\n    self.submit_button.setMaximumWidth(120)\n    self.submit_button.clicked.connect(self._submit)\n    self.cancel_button = QPushButton(\"&amp;Cancel\")\n    self.cancel_button.setMaximumWidth(120)\n    self.cancel_button.clicked.connect(lambda: self.reject())  # type: ignore\n\n    # layout\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    form.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    form.addRow(\"Model Name: \", self.name_textbox)\n    form.addRow(\"General Description:\", self.description_textbox)\n    form.addRow(\"Data Description:\", self.data_description_textbox)\n    vbox = QVBoxLayout()\n    vbox.addWidget(self.add_author_button)\n    vbox.addWidget(self.edit_author_button)\n    vbox.addWidget(self.del_author_button)\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.authors_listview)\n    hbox.addLayout(vbox)\n    form.addRow(\"Authors:\", hbox)\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.default_cover_button)  # type: ignore\n    hbox.addWidget(self.upload_image_button)  # type: ignore\n    vbox = QVBoxLayout()\n    vbox.addWidget(self.image_scroll)\n    vbox.addLayout(hbox)\n    vbox.addSpacing(15)\n    form.addRow(\"Cover Image:\", vbox)\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.submit_button)\n    hbox.addWidget(self.cancel_button)\n    form.addRow(hbox)\n\n    self.setLayout(form)\n    self.setWindowTitle(\"BMZ Export\")\n\n    self._bind_properties()\n    self._set_default_cover_image()\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/","title":"algorithms","text":"<p>Utilities for handling algorithms shorthand and friendly names.</p>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.UNSUPPORTED","title":"<code>UNSUPPORTED = 'I would prefer not to.'</code>  <code>module-attribute</code>","text":"<p>Label for algorithm not currently supported in the napari plugin.</p>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_algorithm","title":"<code>get_algorithm(friendly_name)</code>","text":"<p>Return the algorithm corresponding to the friendly name.</p> <p>The string returned by this method can directly be used with CAREamics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>friendly_name</code> <code>str</code> <p>Friendly name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Algorithm.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_algorithm(friendly_name: str) -&gt; str:\n    \"\"\"Return the algorithm corresponding to the friendly name.\n\n    The string returned by this method can directly be used with CAREamics\n    configuration.\n\n    Parameters\n    ----------\n    friendly_name : str\n        Friendly name.\n\n    Returns\n    -------\n    str\n        Algorithm.\n    \"\"\"\n    if friendly_name == \"Noise2Void\":\n        return SupportedAlgorithm.N2V.value\n    elif friendly_name == \"CARE\":\n        return SupportedAlgorithm.CARE.value\n    elif friendly_name == \"Noise2Noise\":\n        return SupportedAlgorithm.N2N.value\n    else:\n        raise ValueError(f\"Unsupported algorithm: {friendly_name}\")\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_available_algorithms","title":"<code>get_available_algorithms()</code>","text":"<p>Return the available algorithms friendly names.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of available algorithms.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_available_algorithms() -&gt; list[str]:\n    \"\"\"Return the available algorithms friendly names.\n\n    Returns\n    -------\n    list of str\n        A list of available algorithms.\n    \"\"\"\n    return [\n        get_friendly_name(algorithm)\n        for algorithm in SupportedAlgorithm\n        if get_friendly_name(algorithm) != UNSUPPORTED\n    ]\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_friendly_name","title":"<code>get_friendly_name(algorithm)</code>","text":"<p>Return the friendly name of an algorithm.</p> <p>Friendly names are spelling out the algorithm names in a human-readable way.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>SupportedAlgorithm</code> <p>Algorithm.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Friendly name.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_friendly_name(algorithm: SupportedAlgorithm) -&gt; str:\n    \"\"\"Return the friendly name of an algorithm.\n\n    Friendly names are spelling out the algorithm names in a human-readable way.\n\n    Parameters\n    ----------\n    algorithm : SupportedAlgorithm\n        Algorithm.\n\n    Returns\n    -------\n    str\n        Friendly name.\n    \"\"\"\n    if algorithm == SupportedAlgorithm.N2V:\n        return \"Noise2Void\"\n    elif algorithm == SupportedAlgorithm.CARE:\n        return \"CARE\"\n    elif algorithm == SupportedAlgorithm.N2N:\n        return \"Noise2Noise\"\n    else:\n        return UNSUPPORTED\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/","title":"callbacks","text":"<p>PyTorch Lightning callback used to update GUI with progress.</p>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.PredictionStoppedException","title":"<code>PredictionStoppedException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when prediction is stopped by user.</p> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>class PredictionStoppedException(Exception):\n    \"\"\"Exception raised when prediction is stopped by user.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.StopPredictionCallback","title":"<code>StopPredictionCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback to stop prediction when signaled.</p> <p>This callback monitors a PredictionStatus object and stops the trainer when the state is set to STOPPED, allowing for graceful interruption of prediction processes.</p> <p>Parameters:</p> Name Type Description Default <code>pred_status</code> <code>PredictionStatus</code> <p>Prediction status object that when set to STOPPED, signals the prediction to stop.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>class StopPredictionCallback(Callback):\n    \"\"\"PyTorch Lightning callback to stop prediction when signaled.\n\n    This callback monitors a PredictionStatus object and stops the trainer\n    when the state is set to STOPPED, allowing for graceful interruption of\n    prediction processes.\n\n    Parameters\n    ----------\n    pred_status : PredictionStatus\n        Prediction status object that when set to STOPPED, signals the prediction to stop.\n    \"\"\"\n\n    def __init__(self, pred_status: PredictionStatus) -&gt; None:\n        \"\"\"Initialize the callback.\n\n        Parameters\n        ----------\n        pred_status : PredictionStatus\n            Prediction status object that when set to STOPPED,\n            signals the prediction to stop.\n        \"\"\"\n        super().__init__()\n        self.pred_status = pred_status\n\n    def on_predict_batch_start(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Check for stop signal at the start of each prediction batch.\n\n        Parameters\n        ----------\n        trainer : pl.Trainer\n            The PyTorch Lightning trainer.\n        pl_module : pl.LightningModule\n            The Lightning module being used.\n        batch : Any\n            The current batch of data.\n        batch_idx : int\n            Index of the current batch.\n        dataloader_idx : int, optional\n            Index of the current dataloader, by default 0.\n        \"\"\"\n        if self.pred_status.state == PredictionState.STOPPED:\n            print(\"Stop signal received, stopping prediction...\")\n            trainer.should_stop = True\n            # For prediction, we need to raise an exception to actually stop\n            raise PredictionStoppedException(\"Prediction stopped by user\")\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.StopPredictionCallback.__init__","title":"<code>__init__(pred_status)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>pred_status</code> <code>PredictionStatus</code> <p>Prediction status object that when set to STOPPED, signals the prediction to stop.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def __init__(self, pred_status: PredictionStatus) -&gt; None:\n    \"\"\"Initialize the callback.\n\n    Parameters\n    ----------\n    pred_status : PredictionStatus\n        Prediction status object that when set to STOPPED,\n        signals the prediction to stop.\n    \"\"\"\n    super().__init__()\n    self.pred_status = pred_status\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.StopPredictionCallback.on_predict_batch_start","title":"<code>on_predict_batch_start(trainer, pl_module, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Check for stop signal at the start of each prediction batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The Lightning module being used.</p> required <code>batch</code> <code>Any</code> <p>The current batch of data.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the current batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader, by default 0.</p> <code>0</code> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_predict_batch_start(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n    \"\"\"Check for stop signal at the start of each prediction batch.\n\n    Parameters\n    ----------\n    trainer : pl.Trainer\n        The PyTorch Lightning trainer.\n    pl_module : pl.LightningModule\n        The Lightning module being used.\n    batch : Any\n        The current batch of data.\n    batch_idx : int\n        Index of the current batch.\n    dataloader_idx : int, optional\n        Index of the current dataloader, by default 0.\n    \"\"\"\n    if self.pred_status.state == PredictionState.STOPPED:\n        print(\"Stop signal received, stopping prediction...\")\n        trainer.should_stop = True\n        # For prediction, we need to raise an exception to actually stop\n        raise PredictionStoppedException(\"Prediction stopped by user\")\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack","title":"<code>UpdaterCallBack</code>","text":"<p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback for updating training and prediction UI states.</p> <p>Parameters:</p> Name Type Description Default <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> required <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> required <p>Attributes:</p> Name Type Description <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>class UpdaterCallBack(Callback):\n    \"\"\"PyTorch Lightning callback for updating training and prediction UI states.\n\n    Parameters\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n\n    Attributes\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n    \"\"\"\n\n    def __init__(self, training_queue: Queue, prediction_queue: Queue) -&gt; None:\n        \"\"\"Initialize the callback.\n\n        Parameters\n        ----------\n        training_queue : Queue\n            Training queue used to pass updates between threads.\n        prediction_queue : Queue\n            Prediction queue used to pass updates between threads.\n        \"\"\"\n        self.training_queue = training_queue\n        self.prediction_queue = prediction_queue\n\n    def get_train_queue(self) -&gt; Queue:\n        \"\"\"Return the training queue.\n\n        Returns\n        -------\n        Queue\n            Training queue.\n        \"\"\"\n        return self.training_queue\n\n    def get_predict_queue(self) -&gt; Queue:\n        \"\"\"Return the prediction queue.\n\n        Returns\n        -------\n        Queue\n            Prediction queue.\n        \"\"\"\n        return self.prediction_queue\n\n    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the beginning of the training.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        # compute the number of batches\n        len_dataloader = len(trainer.train_dataloader)  # type: ignore\n\n        self.training_queue.put(\n            TrainUpdate(\n                TrainUpdateType.MAX_BATCH,\n                int(len_dataloader / trainer.accumulate_grad_batches),\n            )\n        )\n\n        # register number of epochs\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.MAX_EPOCH, trainer.max_epochs)\n        )\n\n    def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the beginning of each epoch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        self.training_queue.put(TrainUpdate(TrainUpdateType.EPOCH, trainer.current_epoch))\n\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the end of each epoch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        metrics = trainer.progress_bar_metrics\n\n        if \"train_loss_epoch\" in metrics:\n            self.training_queue.put(\n                TrainUpdate(TrainUpdateType.LOSS, metrics[\"train_loss_epoch\"])\n            )\n\n        if \"val_loss\" in metrics:\n            self.training_queue.put(\n                TrainUpdate(TrainUpdateType.VAL_LOSS, metrics[\"val_loss\"])\n            )\n\n    def on_train_batch_start(\n        self, trainer: Trainer, pl_module: LightningModule, batch: Any, batch_idx: int\n    ) -&gt; None:\n        \"\"\"Method called at the beginning of each batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        batch : Any\n            Batch.\n        batch_idx : int\n            Index of the batch.\n        \"\"\"\n        self.training_queue.put(TrainUpdate(TrainUpdateType.BATCH, batch_idx))\n\n    def on_predict_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the beginning of the prediction.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        # lightning returns a number of batches per dataloader\n        # if data is loading from disk, the IterableDataset length is not defined.\n        n_batches = trainer.num_predict_batches[0]\n        if n_batches == np.inf:\n            n_batches = \"?\"\n        else:\n            n_batches = int(n_batches)\n\n        self.prediction_queue.put(\n            PredictionUpdate(\n                PredictionUpdateType.MAX_SAMPLES,\n                n_batches,\n            )\n        )\n\n    def on_predict_batch_start(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Method called at the beginning of each prediction batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        batch : Any\n            Batch.\n        batch_idx : int\n            Index of the batch.\n        dataloader_idx : int, default=0\n            Index of the dataloader.\n        \"\"\"\n        self.prediction_queue.put(\n            PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, batch_idx)\n        )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.__init__","title":"<code>__init__(training_queue, prediction_queue)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> required <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def __init__(self, training_queue: Queue, prediction_queue: Queue) -&gt; None:\n    \"\"\"Initialize the callback.\n\n    Parameters\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n    \"\"\"\n    self.training_queue = training_queue\n    self.prediction_queue = prediction_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.get_predict_queue","title":"<code>get_predict_queue()</code>","text":"<p>Return the prediction queue.</p> <p>Returns:</p> Type Description <code>Queue</code> <p>Prediction queue.</p> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def get_predict_queue(self) -&gt; Queue:\n    \"\"\"Return the prediction queue.\n\n    Returns\n    -------\n    Queue\n        Prediction queue.\n    \"\"\"\n    return self.prediction_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.get_train_queue","title":"<code>get_train_queue()</code>","text":"<p>Return the training queue.</p> <p>Returns:</p> Type Description <code>Queue</code> <p>Training queue.</p> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def get_train_queue(self) -&gt; Queue:\n    \"\"\"Return the training queue.\n\n    Returns\n    -------\n    Queue\n        Training queue.\n    \"\"\"\n    return self.training_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_predict_batch_start","title":"<code>on_predict_batch_start(trainer, pl_module, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Method called at the beginning of each prediction batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>batch</code> <code>Any</code> <p>Batch.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the dataloader.</p> <code>0</code> Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_predict_batch_start(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n    \"\"\"Method called at the beginning of each prediction batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    batch : Any\n        Batch.\n    batch_idx : int\n        Index of the batch.\n    dataloader_idx : int, default=0\n        Index of the dataloader.\n    \"\"\"\n    self.prediction_queue.put(\n        PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, batch_idx)\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_predict_start","title":"<code>on_predict_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of the prediction.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_predict_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the beginning of the prediction.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    # lightning returns a number of batches per dataloader\n    # if data is loading from disk, the IterableDataset length is not defined.\n    n_batches = trainer.num_predict_batches[0]\n    if n_batches == np.inf:\n        n_batches = \"?\"\n    else:\n        n_batches = int(n_batches)\n\n    self.prediction_queue.put(\n        PredictionUpdate(\n            PredictionUpdateType.MAX_SAMPLES,\n            n_batches,\n        )\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Method called at the beginning of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>batch</code> <code>Any</code> <p>Batch.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_train_batch_start(\n    self, trainer: Trainer, pl_module: LightningModule, batch: Any, batch_idx: int\n) -&gt; None:\n    \"\"\"Method called at the beginning of each batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    batch : Any\n        Batch.\n    batch_idx : int\n        Index of the batch.\n    \"\"\"\n    self.training_queue.put(TrainUpdate(TrainUpdateType.BATCH, batch_idx))\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Method called at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the end of each epoch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    metrics = trainer.progress_bar_metrics\n\n    if \"train_loss_epoch\" in metrics:\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.LOSS, metrics[\"train_loss_epoch\"])\n        )\n\n    if \"val_loss\" in metrics:\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.VAL_LOSS, metrics[\"val_loss\"])\n        )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_train_epoch_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the beginning of each epoch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    self.training_queue.put(TrainUpdate(TrainUpdateType.EPOCH, trainer.current_epoch))\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callbacks/#careamics_napari.careamics_utils.callbacks.UpdaterCallBack.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of the training.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callbacks.py</code> <pre><code>def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the beginning of the training.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    # compute the number of batches\n    len_dataloader = len(trainer.train_dataloader)  # type: ignore\n\n    self.training_queue.put(\n        TrainUpdate(\n            TrainUpdateType.MAX_BATCH,\n            int(len_dataloader / trainer.accumulate_grad_batches),\n        )\n    )\n\n    # register number of epochs\n    self.training_queue.put(\n        TrainUpdate(TrainUpdateType.MAX_EPOCH, trainer.max_epochs)\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/care_configs/","title":"care_configs","text":""},{"location":"reference/careamics_napari/careamics_utils/care_configs/#careamics_napari.careamics_utils.care_configs.CAREAdvancedConfig","title":"<code>CAREAdvancedConfig</code>","text":"<p>               Bases: <code>AdvancedConfig</code></p> <p>CARE advanced configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/care_configs.py</code> <pre><code>class CAREAdvancedConfig(AdvancedConfig):\n    \"\"\"CARE advanced configuration.\"\"\"\n\n    n_channels_in: int = 1\n    \"\"\"Number of input channels.\"\"\"\n\n    n_channels_out: int = 1\n    \"\"\"Number of output channels.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/care_configs/#careamics_napari.careamics_utils.care_configs.CAREAdvancedConfig.n_channels_in","title":"<code>n_channels_in = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of input channels.</p>"},{"location":"reference/careamics_napari/careamics_utils/care_configs/#careamics_napari.careamics_utils.care_configs.CAREAdvancedConfig.n_channels_out","title":"<code>n_channels_out = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of output channels.</p>"},{"location":"reference/careamics_napari/careamics_utils/care_configs/#careamics_napari.careamics_utils.care_configs.get_default_care_config","title":"<code>get_default_care_config()</code>","text":"<p>Return a default CARE configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/care_configs.py</code> <pre><code>def get_default_care_config() -&gt; BaseConfig:\n    \"\"\"Return a default CARE configuration.\"\"\"\n    num_workers = get_num_workers()\n\n    config = create_care_configuration(\n        experiment_name=\"careamics_care\",\n        data_type=\"array\",\n        axes=\"YX\",\n        patch_size=[64, 64],\n        batch_size=16,\n        num_epochs=30,\n        independent_channels=True,\n        train_dataloader_params={\"num_workers\": num_workers},\n        val_dataloader_params={\"num_workers\": num_workers},\n    )\n    config = BaseConfig(**config.model_dump(), needs_gt=True)\n\n    return config\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/configs/","title":"configs","text":""},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.AdvancedConfig","title":"<code>AdvancedConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Advanced configuration class.</p> Source code in <code>src/careamics_napari/careamics_utils/configs.py</code> <pre><code>class AdvancedConfig(BaseModel):\n    \"\"\"Advanced configuration class.\"\"\"\n\n    x_flip: bool = True\n    \"\"\"Whether to apply flipping along the X dimension during augmentation.\"\"\"\n\n    y_flip: bool = True\n    \"\"\"Whether to apply flipping along the Y dimension during augmentation.\"\"\"\n\n    rotations: bool = True\n    \"\"\"Whether to apply rotations during augmentation.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.AdvancedConfig.rotations","title":"<code>rotations = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply rotations during augmentation.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.AdvancedConfig.x_flip","title":"<code>x_flip = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply flipping along the X dimension during augmentation.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.AdvancedConfig.y_flip","title":"<code>y_flip = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply flipping along the Y dimension during augmentation.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>               Bases: <code>Configuration</code></p> <p>Base configuration class.</p> Source code in <code>src/careamics_napari/careamics_utils/configs.py</code> <pre><code>class BaseConfig(Configuration):\n    \"\"\"Base configuration class.\"\"\"\n\n    needs_gt: Annotated[bool, Field(exclude=True)] = False\n    \"\"\"Whether the algorithm requires ground truth (for training).\"\"\"\n\n    use_channels: Annotated[bool, Field(exclude=True)] = False\n    \"\"\"Whether the data has channels.\"\"\"\n\n    is_3D: Annotated[bool, Field(exclude=True)] = False\n    \"\"\"Whether the data is 3D.\"\"\"\n\n    work_dir: Annotated[Path, Field(exclude=True)] = HOME\n    \"\"\"Directory where the checkpoints and logs are saved.\"\"\"\n\n    # training parameters\n    val_percentage: Annotated[float, Field(exclude=True)] = 0.1\n    \"\"\"Percentage of the training data used for validation.\"\"\"\n\n    val_minimum_split: Annotated[int, Field(exclude=True)] = 1\n    \"\"\"Minimum number of patches or images in the validation set.\"\"\"\n\n    # prediction parameters\n    tile_size: Annotated[\n        tuple[int, int] | tuple[int, int, int] | None, Field(exclude=True)\n    ] = None\n    \"\"\"Size of the tiles to predict on.\"\"\"\n\n    tile_overlap_xy: Annotated[int, Field(exclude=True)] = 48\n    \"\"\"Overlap between the tiles along the X and Y dimensions.\"\"\"\n\n    tile_overlap_z: Annotated[int, Field(exclude=True)] = 4\n    \"\"\"Overlap between the tiles along the Z dimension.\"\"\"\n\n    pred_batch_size: Annotated[int, Field(exclude=True)] = 1\n    \"\"\"Batch size for prediction.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.is_3D","title":"<code>is_3D = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the data is 3D.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.needs_gt","title":"<code>needs_gt = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the algorithm requires ground truth (for training).</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.pred_batch_size","title":"<code>pred_batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for prediction.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.tile_overlap_xy","title":"<code>tile_overlap_xy = 48</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between the tiles along the X and Y dimensions.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.tile_overlap_z","title":"<code>tile_overlap_z = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between the tiles along the Z dimension.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.tile_size","title":"<code>tile_size = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the tiles to predict on.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.use_channels","title":"<code>use_channels = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the data has channels.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.val_minimum_split","title":"<code>val_minimum_split = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum number of patches or images in the validation set.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.val_percentage","title":"<code>val_percentage = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Percentage of the training data used for validation.</p>"},{"location":"reference/careamics_napari/careamics_utils/configs/#careamics_napari.careamics_utils.configs.BaseConfig.work_dir","title":"<code>work_dir = HOME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Directory where the checkpoints and logs are saved.</p>"},{"location":"reference/careamics_napari/careamics_utils/free_memory/","title":"free_memory","text":"<p>Utility to free GPU memory.</p>"},{"location":"reference/careamics_napari/careamics_utils/free_memory/#careamics_napari.careamics_utils.free_memory.free_memory","title":"<code>free_memory(careamist)</code>","text":"<p>Free memory from CAREamics instance.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamics instance.</p> required Source code in <code>src/careamics_napari/careamics_utils/free_memory.py</code> <pre><code>def free_memory(careamist: CAREamist) -&gt; None:\n    \"\"\"Free memory from CAREamics instance.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamics instance.\n    \"\"\"\n    if (\n        careamist is not None\n        and careamist.trainer is not None\n        and careamist.trainer.model is not None\n    ):\n        careamist.trainer.model.cpu()\n        del careamist.trainer.model\n        del careamist.trainer\n        del careamist\n\n        gc.collect()\n        empty_cache()\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/n2n_configs/","title":"n2n_configs","text":""},{"location":"reference/careamics_napari/careamics_utils/n2n_configs/#careamics_napari.careamics_utils.n2n_configs.N2NAdvancedConfig","title":"<code>N2NAdvancedConfig</code>","text":"<p>               Bases: <code>AdvancedConfig</code></p> <p>N2N advanced configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/n2n_configs.py</code> <pre><code>class N2NAdvancedConfig(AdvancedConfig):\n    \"\"\"N2N advanced configuration.\"\"\"\n\n    n_channels_in: int = 1\n    \"\"\"Number of input channels.\"\"\"\n\n    n_channels_out: int = 1\n    \"\"\"Number of output channels.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/n2n_configs/#careamics_napari.careamics_utils.n2n_configs.N2NAdvancedConfig.n_channels_in","title":"<code>n_channels_in = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of input channels.</p>"},{"location":"reference/careamics_napari/careamics_utils/n2n_configs/#careamics_napari.careamics_utils.n2n_configs.N2NAdvancedConfig.n_channels_out","title":"<code>n_channels_out = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of output channels.</p>"},{"location":"reference/careamics_napari/careamics_utils/n2n_configs/#careamics_napari.careamics_utils.n2n_configs.get_default_n2n_config","title":"<code>get_default_n2n_config()</code>","text":"<p>Return a default N2N configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/n2n_configs.py</code> <pre><code>def get_default_n2n_config() -&gt; BaseConfig:\n    \"\"\"Return a default N2N configuration.\"\"\"\n    num_workers = get_num_workers()\n\n    config = create_n2n_configuration(\n        experiment_name=\"careamics_care\",\n        data_type=\"array\",\n        axes=\"YX\",\n        patch_size=[64, 64],\n        batch_size=16,\n        num_epochs=30,\n        independent_channels=True,\n        train_dataloader_params={\"num_workers\": num_workers},\n        val_dataloader_params={\"num_workers\": num_workers},\n    )\n    config = BaseConfig(**config.model_dump(), needs_gt=True)\n\n    return config\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/","title":"n2v_configs","text":""},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.N2VAdvancedConfig","title":"<code>N2VAdvancedConfig</code>","text":"<p>               Bases: <code>AdvancedConfig</code></p> <p>N2V advanced configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/n2v_configs.py</code> <pre><code>class N2VAdvancedConfig(AdvancedConfig):\n    \"\"\"N2V advanced configuration.\"\"\"\n\n    use_n2v2: bool = False\n    \"\"\"To use N2V2\"\"\"\n\n    roi_size: int = 11\n    \"\"\"The size of the area around each pixel that will be manipulated by N2V.\"\"\"\n\n    masked_pixel_percentage: float = 0.2\n    \"\"\"How many pixels per patch will be manipulated.\"\"\"\n\n    n_channels: int | None = None\n    \"\"\"Number of channels in the input image (C must be in axes).\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.N2VAdvancedConfig.masked_pixel_percentage","title":"<code>masked_pixel_percentage = 0.2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many pixels per patch will be manipulated.</p>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.N2VAdvancedConfig.n_channels","title":"<code>n_channels = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels in the input image (C must be in axes).</p>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.N2VAdvancedConfig.roi_size","title":"<code>roi_size = 11</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The size of the area around each pixel that will be manipulated by N2V.</p>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.N2VAdvancedConfig.use_n2v2","title":"<code>use_n2v2 = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>To use N2V2</p>"},{"location":"reference/careamics_napari/careamics_utils/n2v_configs/#careamics_napari.careamics_utils.n2v_configs.get_default_n2v_config","title":"<code>get_default_n2v_config()</code>","text":"<p>Return a default N2V configuration.</p> Source code in <code>src/careamics_napari/careamics_utils/n2v_configs.py</code> <pre><code>def get_default_n2v_config() -&gt; BaseConfig:\n    \"\"\"Return a default N2V configuration.\"\"\"\n    num_workers = get_num_workers()\n\n    config = create_n2v_configuration(\n        experiment_name=\"careamics_n2v\",\n        data_type=\"array\",\n        axes=\"YX\",\n        patch_size=[64, 64],\n        batch_size=16,\n        num_epochs=30,\n        independent_channels=True,\n        train_dataloader_params={\"num_workers\": num_workers},\n        val_dataloader_params={\"num_workers\": num_workers},\n    )\n    config = BaseConfig(**config.model_dump(), needs_gt=False)\n\n    return config\n</code></pre>"},{"location":"reference/careamics_napari/resources/resources/","title":"resources","text":"<p>Logo and icons.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_CAREAMICS","title":"<code>ICON_CAREAMICS = str(Path(Path(__file__).parent, 'logo_careamics_v2_128.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the CAREamics logo.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_GEAR","title":"<code>ICON_GEAR = str(Path(Path(__file__).parent, 'gear_16.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the gear icon.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_GITHUB","title":"<code>ICON_GITHUB = str(Path(Path(__file__).parent, 'GitHub-Mark-Light-32px.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the GitHub icon.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_TF","title":"<code>ICON_TF = str(Path(Path(__file__).parent, 'TF_White_Icon.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the TensorFlow icon.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/","title":"prediction_status","text":"<p>Status and updates generated by the prediction worker.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState","title":"<code>PredictionState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Prediction state.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionState(IntEnum):\n    \"\"\"Prediction state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Prediction is idle.\"\"\"\n\n    PREDICTING = 1\n    \"\"\"Prediction is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Prediction is done.\"\"\"\n\n    STOPPED = 3\n    \"\"\"Prediction was stopped.\"\"\"\n\n    CRASHED = 4\n    \"\"\"Prediction crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.CRASHED","title":"<code>CRASHED = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction crashed.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is done.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is idle.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.PREDICTING","title":"<code>PREDICTING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is ongoing.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.STOPPED","title":"<code>STOPPED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction was stopped.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus","title":"<code>PredictionStatus</code>  <code>dataclass</code>","text":"<p>Status of the prediction thread.</p> <p>This dataclass is used to update the prediction UI with the current status and progress of the prediction. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>@evented\n@dataclass\nclass PredictionStatus:\n    \"\"\"Status of the prediction thread.\n\n    This dataclass is used to update the prediction UI with the current status and\n    progress of the prediction. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: PredictionStatusSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    max_samples: int = -1\n    \"\"\"Number of samples.\"\"\"\n\n    sample_idx: int = -1\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    state: PredictionState = PredictionState.IDLE\n    \"\"\"Current state of the prediction process.\"\"\"\n\n    def update(self, new_update: PredictionUpdate) -&gt; None:\n        \"\"\"Update the status with the new values.\n\n        Exceptions, debugging messages and samples are ignored.\n\n        Parameters\n        ----------\n        new_update : PredictionUpdate\n            New update to apply.\n        \"\"\"\n        if (\n            new_update.type != PredictionUpdateType.EXCEPTION\n            and new_update.type != PredictionUpdateType.DEBUG\n            and new_update.type != PredictionUpdateType.SAMPLE\n        ):\n            setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.max_samples","title":"<code>max_samples = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.sample_idx","title":"<code>sample_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.state","title":"<code>state = PredictionState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new values.</p> <p>Exceptions, debugging messages and samples are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>PredictionUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>def update(self, new_update: PredictionUpdate) -&gt; None:\n    \"\"\"Update the status with the new values.\n\n    Exceptions, debugging messages and samples are ignored.\n\n    Parameters\n    ----------\n    new_update : PredictionUpdate\n        New update to apply.\n    \"\"\"\n    if (\n        new_update.type != PredictionUpdateType.EXCEPTION\n        and new_update.type != PredictionUpdateType.DEBUG\n        and new_update.type != PredictionUpdateType.SAMPLE\n    ):\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup","title":"<code>PredictionStatusSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the prediction status dataclass.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionStatusSignalGroup(SignalGroup):\n    \"\"\"Signal group for the prediction status dataclass.\"\"\"\n\n    max_samples: SignalInstance\n    \"\"\"Number of samples.\"\"\"\n\n    sample_idx: SignalInstance\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the prediction process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.max_samples","title":"<code>max_samples</code>  <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.sample_idx","title":"<code>sample_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate","title":"<code>PredictionUpdate</code>  <code>dataclass</code>","text":"<p>Update from the prediction worker.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>@dataclass\nclass PredictionUpdate:\n    \"\"\"Update from the prediction worker.\"\"\"\n\n    type: PredictionUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    value: Optional[Union[int, float, str, NDArray, PredictionState, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType","title":"<code>PredictionUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of prediction update.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionUpdateType(str, Enum):\n    \"\"\"Type of prediction update.\"\"\"\n\n    MAX_SAMPLES = \"max_samples\"\n    \"\"\"Number of samples.\"\"\"\n\n    SAMPLE_IDX = \"sample_idx\"\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    SAMPLE = \"sample\"\n    \"\"\"Prediction result.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the prediction process.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the prediction process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.MAX_SAMPLES","title":"<code>MAX_SAMPLES = 'max_samples'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.SAMPLE","title":"<code>SAMPLE = 'sample'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction result.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.SAMPLE_IDX","title":"<code>SAMPLE_IDX = 'sample_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/","title":"saving_signal","text":"<p>Saving parameters set by the user.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType","title":"<code>ExportType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Type of model export.</p> Source code in <code>src/careamics_napari/signals/saving_signal.py</code> <pre><code>class ExportType(Enum):\n    \"\"\"Type of model export.\"\"\"\n\n    CKPT = \"Checkpoint\"\n    \"\"\"PyTorch Lightning checkpoint.\"\"\"\n\n    BMZ = \"Bioimage.io\"\n    \"\"\"Bioimage.io model format.\"\"\"\n\n    @classmethod\n    def list(cls) -&gt; list[str]:\n        \"\"\"List of all available export types.\n\n        Returns\n        -------\n        list of str\n            List of all available export types.\n        \"\"\"\n        return [c.value for c in cls]\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.BMZ","title":"<code>BMZ = 'Bioimage.io'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bioimage.io model format.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.CKPT","title":"<code>CKPT = 'Checkpoint'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PyTorch Lightning checkpoint.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.list","title":"<code>list()</code>  <code>classmethod</code>","text":"<p>List of all available export types.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of all available export types.</p> Source code in <code>src/careamics_napari/signals/saving_signal.py</code> <pre><code>@classmethod\ndef list(cls) -&gt; list[str]:\n    \"\"\"List of all available export types.\n\n    Returns\n    -------\n    list of str\n        List of all available export types.\n    \"\"\"\n    return [c.value for c in cls]\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/","title":"saving_status","text":"<p>Status and updates generated by the saving worker.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingSignalGroup","title":"<code>SavingSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the saving status dataclass.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingSignalGroup(SignalGroup):\n    \"\"\"Signal group for the saving status dataclass.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the saving process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState","title":"<code>SavingState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Saving state.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingState(IntEnum):\n    \"\"\"Saving state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Saving is idle.\"\"\"\n\n    SAVING = 1\n    \"\"\"Saving is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Saving is done.\"\"\"\n\n    CRASHED = 3\n    \"\"\"Saving has crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.CRASHED","title":"<code>CRASHED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving has crashed.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is done.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is idle.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.SAVING","title":"<code>SAVING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is ongoing.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus","title":"<code>SavingStatus</code>  <code>dataclass</code>","text":"<p>Status of the saving thread.</p> <p>This dataclass is used to update the saving UI with the current status and progress of the saving. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>@evented\n@dataclass\nclass SavingStatus:\n    \"\"\"Status of the saving thread.\n\n    This dataclass is used to update the saving UI with the current status and\n    progress of the saving. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: SavingSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    state: SavingState = SavingState.IDLE\n    \"\"\"Current state of the saving process.\"\"\"\n\n    def update(self, new_update: SavingUpdate) -&gt; None:\n        \"\"\"Update the status with the new update.\n\n        Parameters\n        ----------\n        new_update : SavingUpdate\n            New update to apply.\n        \"\"\"\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.state","title":"<code>state = SavingState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new update.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>SavingUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>def update(self, new_update: SavingUpdate) -&gt; None:\n    \"\"\"Update the status with the new update.\n\n    Parameters\n    ----------\n    new_update : SavingUpdate\n        New update to apply.\n    \"\"\"\n    setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate","title":"<code>SavingUpdate</code>  <code>dataclass</code>","text":"<p>Update from the saving worker.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>@dataclass\nclass SavingUpdate:\n    \"\"\"Update from the saving worker.\"\"\"\n\n    type: SavingUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    value: Optional[Union[str, SavingState, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType","title":"<code>SavingUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of saving update.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingUpdateType(str, Enum):\n    \"\"\"Type of saving update.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the saving process.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the saving process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/training_status/","title":"training_status","text":"<p>Status and updates generated by the training worker.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate","title":"<code>TrainUpdate</code>  <code>dataclass</code>","text":"<p>Update from the training worker.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>@dataclass\nclass TrainUpdate:\n    \"\"\"Update from the training worker.\"\"\"\n\n    type: TrainUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    # TODO should we split into subclasses to make the value type more specific?\n    value: Optional[Union[int, float, str, TrainingState, CAREamist, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType","title":"<code>TrainUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of training update.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainUpdateType(str, Enum):\n    \"\"\"Type of training update.\"\"\"\n\n    MAX_EPOCH = \"max_epochs\"\n    \"\"\"Number of epochs.\"\"\"\n\n    EPOCH = \"epoch_idx\"\n    \"\"\"Index of the current epoch.\"\"\"\n\n    MAX_BATCH = \"max_batches\"\n    \"\"\"Number of batches.\"\"\"\n\n    BATCH = \"batch_idx\"\n    \"\"\"Index of the current batch.\"\"\"\n\n    LOSS = \"loss\"\n    \"\"\"Current loss value.\"\"\"\n\n    VAL_LOSS = \"val_loss\"\n    \"\"\"Current validation loss value.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the training process.\"\"\"\n\n    CAREAMIST = \"careamist\"\n    \"\"\"CAREamist instance.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the training process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.BATCH","title":"<code>BATCH = 'batch_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.CAREAMIST","title":"<code>CAREAMIST = 'careamist'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CAREamist instance.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.EPOCH","title":"<code>EPOCH = 'epoch_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.LOSS","title":"<code>LOSS = 'loss'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.MAX_BATCH","title":"<code>MAX_BATCH = 'max_batches'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.MAX_EPOCH","title":"<code>MAX_EPOCH = 'max_epochs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.VAL_LOSS","title":"<code>VAL_LOSS = 'val_loss'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState","title":"<code>TrainingState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Training state.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainingState(IntEnum):\n    \"\"\"Training state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Training is idle.\"\"\"\n\n    TRAINING = 1\n    \"\"\"Training is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Training is done.\"\"\"\n\n    STOPPED = 3\n    \"\"\"Training was stopped.\"\"\"\n\n    CRASHED = 4\n    \"\"\"Training crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.CRASHED","title":"<code>CRASHED = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training crashed.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is done.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is idle.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.STOPPED","title":"<code>STOPPED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training was stopped.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.TRAINING","title":"<code>TRAINING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is ongoing.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus","title":"<code>TrainingStatus</code>  <code>dataclass</code>","text":"<p>Status of the training thread.</p> <p>This dataclass is used to update the training UI with the current status and progress of the training. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>@evented\n@dataclass\nclass TrainingStatus:\n    \"\"\"Status of the training thread.\n\n    This dataclass is used to update the training UI with the current status and\n    progress of the training. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: TrainingStatusSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    max_epochs: int = -1\n    \"\"\"Number of epochs.\"\"\"\n\n    max_batches: int = -1\n    \"\"\"Number of batches.\"\"\"\n\n    epoch_idx: int = -1\n    \"\"\"Index of the current epoch.\"\"\"\n\n    batch_idx: int = -1\n    \"\"\"Index of the current batch.\"\"\"\n\n    loss: float = -1\n    \"\"\"Current loss value.\"\"\"\n\n    val_loss: float = -1\n    \"\"\"Current validation loss value.\"\"\"\n\n    state: TrainingState = TrainingState.IDLE\n    \"\"\"Current state of the training process.\"\"\"\n\n    def update(self, new_update: TrainUpdate) -&gt; None:\n        \"\"\"Update the status with the new values.\n\n        Exceptions, debugging messages and CAREamist instances are ignored.\n\n        Parameters\n        ----------\n        new_update : PredictionUpdate\n            New update to apply.\n        \"\"\"\n        if (\n            new_update.type != TrainUpdateType.CAREAMIST\n            and new_update.type != TrainUpdateType.EXCEPTION\n            and new_update.type != TrainUpdateType.DEBUG\n        ):\n            setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.batch_idx","title":"<code>batch_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.epoch_idx","title":"<code>epoch_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.loss","title":"<code>loss = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.max_batches","title":"<code>max_batches = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.max_epochs","title":"<code>max_epochs = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.state","title":"<code>state = TrainingState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.val_loss","title":"<code>val_loss = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new values.</p> <p>Exceptions, debugging messages and CAREamist instances are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>PredictionUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>def update(self, new_update: TrainUpdate) -&gt; None:\n    \"\"\"Update the status with the new values.\n\n    Exceptions, debugging messages and CAREamist instances are ignored.\n\n    Parameters\n    ----------\n    new_update : PredictionUpdate\n        New update to apply.\n    \"\"\"\n    if (\n        new_update.type != TrainUpdateType.CAREAMIST\n        and new_update.type != TrainUpdateType.EXCEPTION\n        and new_update.type != TrainUpdateType.DEBUG\n    ):\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup","title":"<code>TrainingStatusSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the training status dataclass.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainingStatusSignalGroup(SignalGroup):\n    \"\"\"Signal group for the training status dataclass.\"\"\"\n\n    max_epochs: SignalInstance\n    \"\"\"Number of epochs.\"\"\"\n\n    max_batches: SignalInstance\n    \"\"\"Number of batches.\"\"\"\n\n    epoch_idx: SignalInstance\n    \"\"\"Index of the current epoch.\"\"\"\n\n    batch_idx: SignalInstance\n    \"\"\"Index of the current batch.\"\"\"\n\n    loss: SignalInstance\n    \"\"\"Current loss value.\"\"\"\n\n    val_loss: SignalInstance\n    \"\"\"Current validation loss value.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the training process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.batch_idx","title":"<code>batch_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.epoch_idx","title":"<code>epoch_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.loss","title":"<code>loss</code>  <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.max_batches","title":"<code>max_batches</code>  <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.max_epochs","title":"<code>max_epochs</code>  <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.val_loss","title":"<code>val_loss</code>  <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/","title":"axes_utils","text":"<p>Utilities to check axes validity.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.NAPARI_AXES","title":"<code>NAPARI_AXES = 'TSZYXC'</code>  <code>module-attribute</code>","text":"<p>Axes used in Napari.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.REF_AXES","title":"<code>REF_AXES = 'STCZYX'</code>  <code>module-attribute</code>","text":"<p>References axes in CAREamics.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.are_axes_valid","title":"<code>are_axes_valid(axes)</code>","text":"<p>Check if axes are valid.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the axes are valid.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def are_axes_valid(axes: str) -&gt; bool:\n    \"\"\"Check if axes are valid.\n\n    Parameters\n    ----------\n    axes : str\n        Axes to check.\n\n    Returns\n    -------\n    bool\n        Whether the axes are valid.\n    \"\"\"\n    _axes = axes.upper()\n\n    # length 0 and &gt; 6\n    if 0 &gt; len(_axes) &gt; 6:\n        return False\n\n    # all characters must be in REF_AXES = 'STZYXC'\n    if not all(s in REF_AXES for s in _axes):\n        return False\n\n    # check for repeating characters\n    for i, s in enumerate(_axes):\n        if i != _axes.rfind(s):\n            return False\n\n    # prior: X and Y contiguous\n    return (\"XY\" in _axes) or (\"YX\" in _axes)\n</code></pre>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.filter_dimensions","title":"<code>filter_dimensions(shape_length, is_3D)</code>","text":"<p>Filter axes based on shape and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>shape_length</code> <code>int</code> <p>Number of dimensions.</p> required <code>is_3D</code> <code>bool</code> <p>Whether the dimensions include Z.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of valid axes.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def filter_dimensions(shape_length: int, is_3D: bool) -&gt; list[str]:\n    \"\"\"Filter axes based on shape and dimensions.\n\n    Parameters\n    ----------\n    shape_length : int\n        Number of dimensions.\n    is_3D : bool\n        Whether the dimensions include Z.\n\n    Returns\n    -------\n    list of str\n        List of valid axes.\n    \"\"\"\n    axes = list(REF_AXES)\n    n = shape_length\n\n    if not is_3D:  # if not 3D, remove it from the\n        axes.remove(\"Z\")\n\n    if n &gt; len(axes):\n        warnings.warn(\"Data shape length is too large.\", stacklevel=3)\n        return []\n    else:\n        all_permutations = [\"\".join(p) for p in permutations(axes, n)]\n\n        # X and Y must be in each permutation and contiguous (#FancyComments)\n        all_permutations = [p for p in all_permutations if (\"XY\" in p) or (\"YX\" in p)]\n\n        if is_3D:\n            all_permutations = [p for p in all_permutations if \"Z\" in p]\n\n        if len(all_permutations) == 0 and not is_3D:\n            all_permutations = [\"YX\"]\n\n        return all_permutations\n</code></pre>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.reshape_prediction","title":"<code>reshape_prediction(prediction, axes, is_3d)</code>","text":"<p>Reshape the prediction to match the input axes.</p> <p>The default axes of the model prediction is SC(Z)YX.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>NDArray</code> <p>Prediction.</p> required <code>axes</code> <code>str</code> <p>Axes of the input data.</p> required <code>is_3d</code> <code>bool</code> <p>Whether the data is 3D.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Reshaped prediction.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def reshape_prediction(prediction: NDArray, axes: str, is_3d: bool) -&gt; NDArray:\n    \"\"\"Reshape the prediction to match the input axes.\n\n    The default axes of the model prediction is SC(Z)YX.\n\n    Parameters\n    ----------\n    prediction : NDArray\n        Prediction.\n    axes : str\n        Axes of the input data.\n    is_3d : bool\n        Whether the data is 3D.\n\n    Returns\n    -------\n    NDArray\n        Reshaped prediction.\n    \"\"\"\n    # model outputs SC(Z)YX\n    pred_axes = \"SCZYX\" if is_3d else \"SCYX\"\n\n    # transpose the axes\n    # TODO: during prediction T and S are merged. Check how to handle this\n    input_axes = axes.replace(\"T\", \"S\")\n    remove_c, remove_s = False, False\n\n    if \"C\" not in input_axes:\n        # add C if missing\n        input_axes = \"C\" + input_axes\n        remove_c = True\n\n    if \"S\" not in input_axes:\n        # add S if missing\n        input_axes = \"S\" + input_axes\n        remove_s = True\n\n    # TODO: check if all axes are present\n    assert all(ax in input_axes for ax in pred_axes)\n\n    indices = [pred_axes.index(ax) for ax in input_axes]\n    prediction = np.transpose(prediction, indices)\n\n    # remove S if not present in the input axes\n    if remove_c:\n        prediction = prediction[0]\n\n    # remove C if not present in the input axes\n    if remove_s:\n        prediction = prediction[0]\n\n    return prediction\n</code></pre>"},{"location":"reference/careamics_napari/utils/gpu_utils/","title":"gpu_utils","text":"<p>Utilities to test GPU availability with torch.</p>"},{"location":"reference/careamics_napari/utils/gpu_utils/#careamics_napari.utils.gpu_utils.is_gpu_available","title":"<code>is_gpu_available()</code>","text":"<p>Check if GPU is available with torch.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if GPU is available, False otherwise.</p> Source code in <code>src/careamics_napari/utils/gpu_utils.py</code> <pre><code>def is_gpu_available() -&gt; bool:\n    \"\"\"Check if GPU is available with torch.\n\n    Returns\n    -------\n    bool\n        True if GPU is available, False otherwise.\n    \"\"\"\n    if platform.system() == \"Darwin\":\n        # adapted from Lightning\n        # pytorch-lightning/src/lightning/fabric/accelerators/mps.py\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\n        return (\n            not mps_disabled\n            and backends.mps.is_available()\n            and platform.processor() in (\"arm\", \"arm64\")\n        )\n    else:\n        return cuda.is_available()\n</code></pre>"},{"location":"reference/careamics_napari/utils/workers/","title":"workers","text":""},{"location":"reference/careamics_napari/utils/workers/#careamics_napari.utils.workers.get_num_workers","title":"<code>get_num_workers()</code>","text":"<p>Utility function to set dataloader's num_workers based on OS.</p> Source code in <code>src/careamics_napari/utils/workers.py</code> <pre><code>def get_num_workers():\n    \"\"\"Utility function to set dataloader's num_workers based on OS.\"\"\"\n    if platform.system() == \"Windows\" or platform.system() == \"Darwin\":\n        return 0\n    else:\n        return 3  # or any other number suitable for your system\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/","title":"advanced_config","text":"<p>A dialog widget allowing modifying advanced settings.</p>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow","title":"<code>AdvancedConfigurationWindow</code>","text":"<p>               Bases: <code>QDialog</code></p> <p>A dialog widget allowing modifying advanced settings.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>Parent widget.</p> required <code>careamics_config</code> <code>BaseConfig</code> <pre><code>The configuration for the CAREamics algorithm.\n</code></pre> required Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>class AdvancedConfigurationWindow(QDialog):\n    \"\"\"A dialog widget allowing modifying advanced settings.\n\n    Parameters\n    ----------\n    parent : QWidget\n        Parent widget.\n    careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n    \"\"\"\n\n    def __init__(\n        self,\n        parent: QWidget | None,\n        careamics_config: BaseConfig,\n        algorithm_config: AdvancedConfig | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        parent : QWidget\n            Parent widget.\n        careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n        \"\"\"\n        super().__init__(parent)\n        self.setWindowModality(Qt.ApplicationModal)  # type: ignore\n\n        self.configuration = careamics_config\n        self.advanced_configuration = algorithm_config\n\n        self.tabs = QTabWidget()\n        self.add_common_tab()\n\n        self.save_button = QPushButton(\"Save\")\n        self.save_button.setMaximumWidth(120)\n        self.save_button.clicked.connect(self.save)\n\n        vbox = QVBoxLayout()\n        vbox.addWidget(self.tabs)\n        vbox.addWidget(self.save_button)\n        self.setLayout(vbox)\n\n        # self.bind_properties()\n\n    def add_common_tab(self) -&gt; None:\n        \"\"\"Add common advanced setting tab.\"\"\"\n        # tab widget\n        tab_widget = QWidget()\n\n        # experiment name text box\n        label = QLabel(\"Experiment Name:\")\n        self.experiment_txtbox = QLineEdit()\n        self.experiment_txtbox.setText(self.configuration.experiment_name)\n        self.experiment_txtbox.setToolTip(\n            \"Name of the experiment. It will be used to save the model\\n\"\n            \"and the training history.\"\n        )\n\n        # validation\n        val_grpbox = QGroupBox(\"Validation\")\n        self.val_perc_spin = create_double_spinbox(\n            0.01, 1, self.configuration.val_percentage, 0.01, n_decimal=2\n        )\n        self.val_perc_spin.setToolTip(\n            \"Percentage of the training data used for validation.\"\n        )\n        self.val_split_spin = create_int_spinbox(\n            1, 100, self.configuration.val_minimum_split, 1\n        )\n        self.val_split_spin.setToolTip(\n            \"Minimum number of patches or images in the validation set.\"\n        )\n\n        # augmentations group box, with x_flip, y_flip and rotations\n        _x_flip = True\n        _y_flip = True\n        _rotations = True\n        if self.advanced_configuration is not None:\n            _x_flip = self.advanced_configuration.x_flip\n            _y_flip = self.advanced_configuration.y_flip\n            _rotations = self.advanced_configuration.rotations\n\n        augment_grpbox = QGroupBox(\"Augmentations\")\n        self.x_flip_chkbox = QCheckBox(\"X Flip\")\n        self.x_flip_chkbox.setChecked(_x_flip)\n        self.x_flip_chkbox.setToolTip(\n            \"Check to add augmentation that flips the image\\nalong the x-axis\"\n        )\n        self.y_flip_chkbox = QCheckBox(\"Y Flip\")\n        self.y_flip_chkbox.setChecked(_y_flip)\n        self.y_flip_chkbox.setToolTip(\n            \"Check to add augmentation that flips the image\\nalong the y-axis\"\n        )\n        self.rotations_chkbox = QCheckBox(\"90 Rotations\")\n        self.rotations_chkbox.setChecked(_rotations)\n        self.rotations_chkbox.setToolTip(\n            \"Check to add augmentation that rotates the image\\n\"\n            \"in 90 degree increments in XY\"\n        )\n\n        # model params\n        _depth = 2\n        _num_filters = 32\n        _indi_channels = True\n        if isinstance(self.configuration.algorithm_config.model, UNetModel):\n            _depth = self.configuration.algorithm_config.model.depth\n            _num_filters = self.configuration.algorithm_config.model.num_channels_init\n            _indi_channels = (\n                self.configuration.algorithm_config.model.independent_channels\n            )\n\n        model_grpbox = QGroupBox(\"UNet parameters\")\n        self.model_depth_spin = create_int_spinbox(2, 5, _depth, 1)\n        self.model_depth_spin.setToolTip(\"Depth of the U-Net model.\")\n        self.num_conv_filters_spin = create_int_spinbox(8, 1024, _num_filters, 8)\n        self.num_conv_filters_spin.setToolTip(\n            \"Number of convolutional filters in the first layer.\"\n        )\n\n        # independent channels checkbox\n        self.indi_channels_chkbox = QCheckBox(\"Independent Channels\")\n        self.indi_channels_chkbox.setChecked(_indi_channels)\n        self.indi_channels_chkbox.setToolTip(\n            \"Check to treat the channels independently during\\ntraining.\"\n        )\n        self.indi_channels_chkbox.setEnabled(\n            \"C\" in self.configuration.data_config.axes  # type: ignore\n        )\n\n        # layout\n        layout = QVBoxLayout()\n        hbox = QHBoxLayout()\n        hbox.addWidget(label)\n        hbox.addWidget(self.experiment_txtbox)\n        layout.addLayout(hbox)\n        layout.addSpacing(10)\n\n        validation_layout = QFormLayout()\n        validation_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        validation_layout.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        validation_layout.addRow(\"Percentage\", self.val_perc_spin)\n        validation_layout.addRow(\"Minimum split\", self.val_split_spin)\n        val_grpbox.setLayout(validation_layout)\n        layout.addWidget(val_grpbox)\n        layout.addSpacing(10)\n\n        augmentations_layout = QFormLayout()\n        augmentations_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        augmentations_layout.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        augmentations_layout.addRow(self.x_flip_chkbox)\n        augmentations_layout.addRow(self.y_flip_chkbox)\n        augmentations_layout.addRow(self.rotations_chkbox)\n        augment_grpbox.setLayout(augmentations_layout)\n        layout.addWidget(augment_grpbox)\n        layout.addSpacing(10)\n\n        model_params_layout = QFormLayout()\n        model_params_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        model_params_layout.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        model_params_layout.addRow(\"Depth\", self.model_depth_spin)\n        model_params_layout.addRow(\"# Filters\", self.num_conv_filters_spin)\n        model_grpbox.setLayout(model_params_layout)\n        layout.addWidget(model_grpbox)\n        layout.addSpacing(10)\n\n        layout.addWidget(self.indi_channels_chkbox)\n\n        tab_widget.setLayout(layout)\n        self.tabs.addTab(tab_widget, \"Common\")\n\n    def add_algorithm_specific_tab(self) -&gt; None:\n        \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n        raise NotImplementedError(\"Should be implemented by subclasses.\")\n\n    def save(self) -&gt; None:\n        \"\"\"Save and close the window.\"\"\"\n        self.update_config()\n        # self.close()  # should be closed in a subclass\n\n    def bind_properties(self) -&gt; None:\n        \"\"\"Create and bind class properties to the UI elements.\"\"\"\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        type(self).experiment_name = bind(self.experiment_txtbox, \"text\")\n        type(self).val_percentage = bind(self.val_perc_spin, \"value\")\n        type(self).val_split = bind(self.val_split_spin, \"value\")\n        type(self).x_flip = bind(self.x_flip_chkbox, \"checked\")\n        type(self).y_flip = bind(self.y_flip_chkbox, \"checked\")\n        type(self).rotation = bind(self.rotations_chkbox, \"checked\")\n        type(self).model_depth = bind(self.model_depth_spin, \"value\")\n        type(self).num_conv_filters = bind(self.num_conv_filters_spin, \"value\")\n        type(self).indi_channels = bind(self.indi_channels_chkbox, \"checked\")\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration object from UI elements.\"\"\"\n        self.configuration.experiment_name = self.experiment_name\n\n        self.configuration.val_percentage = self.val_percentage\n        self.configuration.val_minimum_split = self.val_split\n\n        if isinstance(self.configuration.algorithm_config.model, UNetModel):\n            self.configuration.algorithm_config.model.depth = self.model_depth\n            self.configuration.algorithm_config.model.num_channels_init = (\n                self.num_conv_filters\n            )\n\n            self.configuration.algorithm_config.model.independent_channels = (\n                self.indi_channels\n            )\n\n        # update augmentations\n        augs: list[Union[XYFlipModel, XYRandomRotate90Model]] = []\n        if self.x_flip or self.y_flip:\n            augs.append(XYFlipModel(flip_x=self.x_flip, flip_y=self.y_flip, p=0.5))\n        if self.rotation:\n            augs.append(XYRandomRotate90Model(p=0.5))\n        self.configuration.data_config.transforms = augs  # type: ignore\n        # update advanced config as well\n        if self.advanced_configuration is not None:\n            self.advanced_configuration.x_flip = self.x_flip\n            self.advanced_configuration.y_flip = self.y_flip\n            self.advanced_configuration.rotations = self.rotation\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.__init__","title":"<code>__init__(parent, careamics_config, algorithm_config=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>Parent widget.</p> required <code>careamics_config</code> <code>BaseConfig</code> <p>The configuration for the CAREamics algorithm.</p> required Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def __init__(\n    self,\n    parent: QWidget | None,\n    careamics_config: BaseConfig,\n    algorithm_config: AdvancedConfig | None = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    parent : QWidget\n        Parent widget.\n    careamics_config : BaseConfig\n        The configuration for the CAREamics algorithm.\n    \"\"\"\n    super().__init__(parent)\n    self.setWindowModality(Qt.ApplicationModal)  # type: ignore\n\n    self.configuration = careamics_config\n    self.advanced_configuration = algorithm_config\n\n    self.tabs = QTabWidget()\n    self.add_common_tab()\n\n    self.save_button = QPushButton(\"Save\")\n    self.save_button.setMaximumWidth(120)\n    self.save_button.clicked.connect(self.save)\n\n    vbox = QVBoxLayout()\n    vbox.addWidget(self.tabs)\n    vbox.addWidget(self.save_button)\n    self.setLayout(vbox)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.add_algorithm_specific_tab","title":"<code>add_algorithm_specific_tab()</code>","text":"<p>Add algorithm specific advanced settings tab.</p> Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def add_algorithm_specific_tab(self) -&gt; None:\n    \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n    raise NotImplementedError(\"Should be implemented by subclasses.\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.add_common_tab","title":"<code>add_common_tab()</code>","text":"<p>Add common advanced setting tab.</p> Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def add_common_tab(self) -&gt; None:\n    \"\"\"Add common advanced setting tab.\"\"\"\n    # tab widget\n    tab_widget = QWidget()\n\n    # experiment name text box\n    label = QLabel(\"Experiment Name:\")\n    self.experiment_txtbox = QLineEdit()\n    self.experiment_txtbox.setText(self.configuration.experiment_name)\n    self.experiment_txtbox.setToolTip(\n        \"Name of the experiment. It will be used to save the model\\n\"\n        \"and the training history.\"\n    )\n\n    # validation\n    val_grpbox = QGroupBox(\"Validation\")\n    self.val_perc_spin = create_double_spinbox(\n        0.01, 1, self.configuration.val_percentage, 0.01, n_decimal=2\n    )\n    self.val_perc_spin.setToolTip(\n        \"Percentage of the training data used for validation.\"\n    )\n    self.val_split_spin = create_int_spinbox(\n        1, 100, self.configuration.val_minimum_split, 1\n    )\n    self.val_split_spin.setToolTip(\n        \"Minimum number of patches or images in the validation set.\"\n    )\n\n    # augmentations group box, with x_flip, y_flip and rotations\n    _x_flip = True\n    _y_flip = True\n    _rotations = True\n    if self.advanced_configuration is not None:\n        _x_flip = self.advanced_configuration.x_flip\n        _y_flip = self.advanced_configuration.y_flip\n        _rotations = self.advanced_configuration.rotations\n\n    augment_grpbox = QGroupBox(\"Augmentations\")\n    self.x_flip_chkbox = QCheckBox(\"X Flip\")\n    self.x_flip_chkbox.setChecked(_x_flip)\n    self.x_flip_chkbox.setToolTip(\n        \"Check to add augmentation that flips the image\\nalong the x-axis\"\n    )\n    self.y_flip_chkbox = QCheckBox(\"Y Flip\")\n    self.y_flip_chkbox.setChecked(_y_flip)\n    self.y_flip_chkbox.setToolTip(\n        \"Check to add augmentation that flips the image\\nalong the y-axis\"\n    )\n    self.rotations_chkbox = QCheckBox(\"90 Rotations\")\n    self.rotations_chkbox.setChecked(_rotations)\n    self.rotations_chkbox.setToolTip(\n        \"Check to add augmentation that rotates the image\\n\"\n        \"in 90 degree increments in XY\"\n    )\n\n    # model params\n    _depth = 2\n    _num_filters = 32\n    _indi_channels = True\n    if isinstance(self.configuration.algorithm_config.model, UNetModel):\n        _depth = self.configuration.algorithm_config.model.depth\n        _num_filters = self.configuration.algorithm_config.model.num_channels_init\n        _indi_channels = (\n            self.configuration.algorithm_config.model.independent_channels\n        )\n\n    model_grpbox = QGroupBox(\"UNet parameters\")\n    self.model_depth_spin = create_int_spinbox(2, 5, _depth, 1)\n    self.model_depth_spin.setToolTip(\"Depth of the U-Net model.\")\n    self.num_conv_filters_spin = create_int_spinbox(8, 1024, _num_filters, 8)\n    self.num_conv_filters_spin.setToolTip(\n        \"Number of convolutional filters in the first layer.\"\n    )\n\n    # independent channels checkbox\n    self.indi_channels_chkbox = QCheckBox(\"Independent Channels\")\n    self.indi_channels_chkbox.setChecked(_indi_channels)\n    self.indi_channels_chkbox.setToolTip(\n        \"Check to treat the channels independently during\\ntraining.\"\n    )\n    self.indi_channels_chkbox.setEnabled(\n        \"C\" in self.configuration.data_config.axes  # type: ignore\n    )\n\n    # layout\n    layout = QVBoxLayout()\n    hbox = QHBoxLayout()\n    hbox.addWidget(label)\n    hbox.addWidget(self.experiment_txtbox)\n    layout.addLayout(hbox)\n    layout.addSpacing(10)\n\n    validation_layout = QFormLayout()\n    validation_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    validation_layout.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    validation_layout.addRow(\"Percentage\", self.val_perc_spin)\n    validation_layout.addRow(\"Minimum split\", self.val_split_spin)\n    val_grpbox.setLayout(validation_layout)\n    layout.addWidget(val_grpbox)\n    layout.addSpacing(10)\n\n    augmentations_layout = QFormLayout()\n    augmentations_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    augmentations_layout.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    augmentations_layout.addRow(self.x_flip_chkbox)\n    augmentations_layout.addRow(self.y_flip_chkbox)\n    augmentations_layout.addRow(self.rotations_chkbox)\n    augment_grpbox.setLayout(augmentations_layout)\n    layout.addWidget(augment_grpbox)\n    layout.addSpacing(10)\n\n    model_params_layout = QFormLayout()\n    model_params_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    model_params_layout.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    model_params_layout.addRow(\"Depth\", self.model_depth_spin)\n    model_params_layout.addRow(\"# Filters\", self.num_conv_filters_spin)\n    model_grpbox.setLayout(model_params_layout)\n    layout.addWidget(model_grpbox)\n    layout.addSpacing(10)\n\n    layout.addWidget(self.indi_channels_chkbox)\n\n    tab_widget.setLayout(layout)\n    self.tabs.addTab(tab_widget, \"Common\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.bind_properties","title":"<code>bind_properties()</code>","text":"<p>Create and bind class properties to the UI elements.</p> Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def bind_properties(self) -&gt; None:\n    \"\"\"Create and bind class properties to the UI elements.\"\"\"\n    # type(self) returns the class of the instance, so we are adding\n    # properties to the class itself, not the instance.\n    type(self).experiment_name = bind(self.experiment_txtbox, \"text\")\n    type(self).val_percentage = bind(self.val_perc_spin, \"value\")\n    type(self).val_split = bind(self.val_split_spin, \"value\")\n    type(self).x_flip = bind(self.x_flip_chkbox, \"checked\")\n    type(self).y_flip = bind(self.y_flip_chkbox, \"checked\")\n    type(self).rotation = bind(self.rotations_chkbox, \"checked\")\n    type(self).model_depth = bind(self.model_depth_spin, \"value\")\n    type(self).num_conv_filters = bind(self.num_conv_filters_spin, \"value\")\n    type(self).indi_channels = bind(self.indi_channels_chkbox, \"checked\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.save","title":"<code>save()</code>","text":"<p>Save and close the window.</p> Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save and close the window.\"\"\"\n    self.update_config()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/advanced_config/#careamics_napari.widgets.advanced_config.AdvancedConfigurationWindow.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration object from UI elements.</p> Source code in <code>src/careamics_napari/widgets/advanced_config.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration object from UI elements.\"\"\"\n    self.configuration.experiment_name = self.experiment_name\n\n    self.configuration.val_percentage = self.val_percentage\n    self.configuration.val_minimum_split = self.val_split\n\n    if isinstance(self.configuration.algorithm_config.model, UNetModel):\n        self.configuration.algorithm_config.model.depth = self.model_depth\n        self.configuration.algorithm_config.model.num_channels_init = (\n            self.num_conv_filters\n        )\n\n        self.configuration.algorithm_config.model.independent_channels = (\n            self.indi_channels\n        )\n\n    # update augmentations\n    augs: list[Union[XYFlipModel, XYRandomRotate90Model]] = []\n    if self.x_flip or self.y_flip:\n        augs.append(XYFlipModel(flip_x=self.x_flip, flip_y=self.y_flip, p=0.5))\n    if self.rotation:\n        augs.append(XYRandomRotate90Model(p=0.5))\n    self.configuration.data_config.transforms = augs  # type: ignore\n    # update advanced config as well\n    if self.advanced_configuration is not None:\n        self.advanced_configuration.x_flip = self.x_flip\n        self.advanced_configuration.y_flip = self.y_flip\n        self.advanced_configuration.rotations = self.rotation\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/","title":"axes_widget","text":"<p>Widget for specifying axes order.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget","title":"<code>AxesWidget</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>A widget allowing users to specify axes.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class AxesWidget(QWidget):\n    \"\"\"A widget allowing users to specify axes.\n\n    Parameters\n    ----------\n        careamics_config : Configuration\n            careamics configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        careamics_config: BaseConfig,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n            training_signal : BaseConfig\n                A careamics configuration object.\n        \"\"\"\n        super().__init__()\n        self.configuration = careamics_config\n        self.is_text_valid = True\n\n        # layout\n        layout = QHBoxLayout()\n        layout.setSpacing(0)\n        layout.setContentsMargins(0, 0, 0, 0)\n\n        # text field\n        self.label = QLabel(\"Axes\")\n        self.text_field = QLineEdit(self.configuration.data_config.axes)  # type: ignore\n        self.text_field.setMaxLength(6)\n        self.text_field.setValidator(LettersValidator(REF_AXES))\n        self.text_field.textChanged.connect(self._validate_axes)  # type: ignore\n        self.text_field.setToolTip(\n            \"Enter the axes order as they are in your images, e.g. SZYX.\\n\"\n            \"Accepted axes are S(ample), T(ime), C(hannel), Z, Y, and X. Red\\n\"\n            \"color highlighting means that a character is not recognized,\\n\"\n            \"orange means that the axes order is not allowed. YX axes are\\n\"\n            \"mandatory.\"\n        )\n        # layout\n        layout.addWidget(self.label)\n        layout.addWidget(self.text_field)\n        self.setLayout(layout)\n        # validate text\n        self._validate_axes(self.text_field.text())\n\n        # create and bind properties to ui\n        type(self).axes = bind(\n            self.text_field,\n            \"text\",\n            default_value=self.configuration.data_config.axes,  # type: ignore\n            validation_fn=self._validate_axes,\n        )\n\n    def update_config(self: Self) -&gt; None:\n        \"\"\"Update the axes in the configuration if it's valid.\"\"\"\n        if self.is_text_valid and isinstance(self.configuration.data_config, DataConfig):\n            self.configuration.data_config.axes = self.axes\n\n    def _validate_axes(self: Self, axes: str | None = None) -&gt; bool:\n        \"\"\"Validate the input text in the text field.\"\"\"\n        if axes is None:\n            axes = self.text_field.text()\n        # change text color according to axes validation\n        if are_axes_valid(axes):\n            self._set_text_color(Highlight.VALID)\n            self.is_text_valid = True\n            # if axes.upper() in filter_dimensions(self.n_axes, self.is_3D):\n            #     self._set_text_color(Highlight.VALID)\n            # else:\n            #     self._set_text_color(Highlight.NOT_ACCEPTED)\n        else:\n            self._set_text_color(Highlight.UNRECOGNIZED)\n            self.is_text_valid = False\n\n        return self.is_text_valid\n\n    def _set_text_color(self: Self, highlight: Highlight) -&gt; None:\n        \"\"\"Set the text color according to the highlight type.\n\n        Parameters\n        ----------\n        highlight : Highlight\n            Highlight type.\n        \"\"\"\n        if highlight == Highlight.UNRECOGNIZED:\n            self.text_field.setStyleSheet(\"color: red;\")\n        elif highlight == Highlight.NOT_ACCEPTED:\n            self.text_field.setStyleSheet(\"color: orange;\")\n        else:  # VALID\n            self.text_field.setStyleSheet(\"color: white;\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.__init__","title":"<code>__init__(careamics_config)</code>","text":"<p>Initialize the widget.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def __init__(\n    self,\n    careamics_config: BaseConfig,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n        training_signal : BaseConfig\n            A careamics configuration object.\n    \"\"\"\n    super().__init__()\n    self.configuration = careamics_config\n    self.is_text_valid = True\n\n    # layout\n    layout = QHBoxLayout()\n    layout.setSpacing(0)\n    layout.setContentsMargins(0, 0, 0, 0)\n\n    # text field\n    self.label = QLabel(\"Axes\")\n    self.text_field = QLineEdit(self.configuration.data_config.axes)  # type: ignore\n    self.text_field.setMaxLength(6)\n    self.text_field.setValidator(LettersValidator(REF_AXES))\n    self.text_field.textChanged.connect(self._validate_axes)  # type: ignore\n    self.text_field.setToolTip(\n        \"Enter the axes order as they are in your images, e.g. SZYX.\\n\"\n        \"Accepted axes are S(ample), T(ime), C(hannel), Z, Y, and X. Red\\n\"\n        \"color highlighting means that a character is not recognized,\\n\"\n        \"orange means that the axes order is not allowed. YX axes are\\n\"\n        \"mandatory.\"\n    )\n    # layout\n    layout.addWidget(self.label)\n    layout.addWidget(self.text_field)\n    self.setLayout(layout)\n    # validate text\n    self._validate_axes(self.text_field.text())\n\n    # create and bind properties to ui\n    type(self).axes = bind(\n        self.text_field,\n        \"text\",\n        default_value=self.configuration.data_config.axes,  # type: ignore\n        validation_fn=self._validate_axes,\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.update_config","title":"<code>update_config()</code>","text":"<p>Update the axes in the configuration if it's valid.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def update_config(self: Self) -&gt; None:\n    \"\"\"Update the axes in the configuration if it's valid.\"\"\"\n    if self.is_text_valid and isinstance(self.configuration.data_config, DataConfig):\n        self.configuration.data_config.axes = self.axes\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight","title":"<code>Highlight</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Axes highlight types.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class Highlight(Enum):\n    \"\"\"Axes highlight types.\"\"\"\n\n    VALID = 0\n    \"\"\"Valid axes.\"\"\"\n\n    UNRECOGNIZED = 1\n    \"\"\"Unrecognized axes.\"\"\"\n\n    NOT_ACCEPTED = 2\n    \"\"\"Axes not accepted.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.NOT_ACCEPTED","title":"<code>NOT_ACCEPTED = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Axes not accepted.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.UNRECOGNIZED","title":"<code>UNRECOGNIZED = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Unrecognized axes.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.VALID","title":"<code>VALID = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Valid axes.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator","title":"<code>LettersValidator</code>","text":"<p>               Bases: <code>QValidator</code></p> <p>Custom validator.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>str</code> <p>Allowed characters.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class LettersValidator(QtGui.QValidator):\n    \"\"\"Custom validator.\n\n    Parameters\n    ----------\n    options : str\n        Allowed characters.\n    *args : Any\n        Variable length argument list.\n    **kwargs : Any\n        Arbitrary keyword arguments.\n    \"\"\"\n\n    def __init__(self: Self, options: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the validator.\n\n        Parameters\n        ----------\n        options : str\n            Allowed characters.\n        *args : Any\n            Variable length argument list.\n        **kwargs : Any\n            Arbitrary keyword arguments.\n        \"\"\"\n        QtGui.QValidator.__init__(self, *args, **kwargs)\n        self._options = options\n\n    def validate(\n        self: Self, value: str, pos: int\n    ) -&gt; tuple[QtGui.QValidator.State, str, int]:\n        \"\"\"Validate the input.\n\n        Parameters\n        ----------\n        value : str\n            Input value.\n        pos : int\n            Position of the cursor.\n\n        Returns\n        -------\n        (QtGui.QValidator.State, str, int)\n            Validation state, value, and position.\n        \"\"\"\n        if len(value) &gt; 0:\n            if value[-1] in self._options:\n                return QtGui.QValidator.Acceptable, value, pos  # type: ignore\n        else:\n            if value == \"\":\n                return QtGui.QValidator.Intermediate, value, pos  # type: ignore\n        return QtGui.QValidator.Invalid, value, pos  # type: ignore\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator.__init__","title":"<code>__init__(options, *args, **kwargs)</code>","text":"<p>Initialize the validator.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>str</code> <p>Allowed characters.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def __init__(self: Self, options: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the validator.\n\n    Parameters\n    ----------\n    options : str\n        Allowed characters.\n    *args : Any\n        Variable length argument list.\n    **kwargs : Any\n        Arbitrary keyword arguments.\n    \"\"\"\n    QtGui.QValidator.__init__(self, *args, **kwargs)\n    self._options = options\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator.validate","title":"<code>validate(value, pos)</code>","text":"<p>Validate the input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Input value.</p> required <code>pos</code> <code>int</code> <p>Position of the cursor.</p> required <p>Returns:</p> Type Description <code>(State, str, int)</code> <p>Validation state, value, and position.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def validate(\n    self: Self, value: str, pos: int\n) -&gt; tuple[QtGui.QValidator.State, str, int]:\n    \"\"\"Validate the input.\n\n    Parameters\n    ----------\n    value : str\n        Input value.\n    pos : int\n        Position of the cursor.\n\n    Returns\n    -------\n    (QtGui.QValidator.State, str, int)\n        Validation state, value, and position.\n    \"\"\"\n    if len(value) &gt; 0:\n        if value[-1] in self._options:\n            return QtGui.QValidator.Acceptable, value, pos  # type: ignore\n    else:\n        if value == \"\":\n            return QtGui.QValidator.Intermediate, value, pos  # type: ignore\n    return QtGui.QValidator.Invalid, value, pos  # type: ignore\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/","title":"banner_widget","text":"<p>A banner widget with CAREamics logo, and links to Github and documentation.</p>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget.CAREamicsBanner","title":"<code>CAREamicsBanner</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>Banner widget with CAREamics logo, and links to Github and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the banner.</p> required <code>short_desc</code> <code>str</code> <p>Short description of the banner.</p> required Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>class CAREamicsBanner(QWidget):\n    \"\"\"Banner widget with CAREamics logo, and links to Github and documentation.\n\n    Parameters\n    ----------\n    title : str\n        Title of the banner.\n    short_desc : str\n        Short description of the banner.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        title: str,\n        short_desc: str,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        title : str\n            Title of the banner.\n        short_desc : str\n            Short description of the banner.\n        \"\"\"\n        super().__init__()\n\n        self.setMinimumSize(250, 200)\n\n        layout = QHBoxLayout()\n        # layout.setContentsMargins(0, 0, 0, 0)\n        self.setLayout(layout)\n\n        # bg_color = self.palette().color(QtGui.QPalette.ColorRole.Background).name()\n\n        # logo\n        icon = QPixmap(ICON_CAREAMICS)\n        img_widget = QLabel()\n        img_widget.setPixmap(icon)\n        img_widget.setFixedSize(94, 128)\n\n        # right panel\n        right_layout = QVBoxLayout()\n        right_widget = QWidget()\n        right_widget.setLayout(right_layout)\n\n        # title\n        title_label = QLabel(title)\n        title_label.setStyleSheet(\"font-weight: bold;\")\n\n        # description\n        description_widget = QPlainTextEdit()\n        description_widget.setReadOnly(True)\n        description_widget.setPlainText(short_desc)\n        description_widget.setFixedSize(200, 50)\n        # description_widget.setStyleSheet(\n        #     f\"background-color: {bg_color};\"\n        #     f\"border: 2px solid {bg_color};\"\n        # )\n\n        # bottom widget\n        bottom_widget = QWidget()\n        bottom_widget.setLayout(QHBoxLayout())\n\n        # github logo\n        gh_icon = QPixmap(ICON_GITHUB)\n        gh_widget = QLabel()\n        gh_widget.setPixmap(gh_icon)\n        gh_widget.mousePressEvent = _open_link(GH_LINK)\n        gh_widget.setCursor(QCursor(QtCore.Qt.CursorShape.PointingHandCursor))\n        gh_widget.setToolTip(\"Report issues\")\n\n        # add widgets\n        bottom_widget.layout().addWidget(_create_link(DOC_LINK, \"Documentation\"))\n        bottom_widget.layout().addWidget(gh_widget)\n\n        right_widget.layout().addWidget(title_label)\n        right_widget.layout().addWidget(description_widget)\n        right_widget.layout().addWidget(bottom_widget)\n\n        # add widgets\n        layout.addWidget(img_widget)\n        layout.addWidget(right_widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget.CAREamicsBanner.__init__","title":"<code>__init__(title, short_desc)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the banner.</p> required <code>short_desc</code> <code>str</code> <p>Short description of the banner.</p> required Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    title: str,\n    short_desc: str,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    title : str\n        Title of the banner.\n    short_desc : str\n        Short description of the banner.\n    \"\"\"\n    super().__init__()\n\n    self.setMinimumSize(250, 200)\n\n    layout = QHBoxLayout()\n    # layout.setContentsMargins(0, 0, 0, 0)\n    self.setLayout(layout)\n\n    # bg_color = self.palette().color(QtGui.QPalette.ColorRole.Background).name()\n\n    # logo\n    icon = QPixmap(ICON_CAREAMICS)\n    img_widget = QLabel()\n    img_widget.setPixmap(icon)\n    img_widget.setFixedSize(94, 128)\n\n    # right panel\n    right_layout = QVBoxLayout()\n    right_widget = QWidget()\n    right_widget.setLayout(right_layout)\n\n    # title\n    title_label = QLabel(title)\n    title_label.setStyleSheet(\"font-weight: bold;\")\n\n    # description\n    description_widget = QPlainTextEdit()\n    description_widget.setReadOnly(True)\n    description_widget.setPlainText(short_desc)\n    description_widget.setFixedSize(200, 50)\n    # description_widget.setStyleSheet(\n    #     f\"background-color: {bg_color};\"\n    #     f\"border: 2px solid {bg_color};\"\n    # )\n\n    # bottom widget\n    bottom_widget = QWidget()\n    bottom_widget.setLayout(QHBoxLayout())\n\n    # github logo\n    gh_icon = QPixmap(ICON_GITHUB)\n    gh_widget = QLabel()\n    gh_widget.setPixmap(gh_icon)\n    gh_widget.mousePressEvent = _open_link(GH_LINK)\n    gh_widget.setCursor(QCursor(QtCore.Qt.CursorShape.PointingHandCursor))\n    gh_widget.setToolTip(\"Report issues\")\n\n    # add widgets\n    bottom_widget.layout().addWidget(_create_link(DOC_LINK, \"Documentation\"))\n    bottom_widget.layout().addWidget(gh_widget)\n\n    right_widget.layout().addWidget(title_label)\n    right_widget.layout().addWidget(description_widget)\n    right_widget.layout().addWidget(bottom_widget)\n\n    # add widgets\n    layout.addWidget(img_widget)\n    layout.addWidget(right_widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/","title":"care_config_window","text":""},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow","title":"<code>CAREConfigurationWindow</code>","text":"<p>               Bases: <code>AdvancedConfigurationWindow</code></p> <p>A dialog widget for modifying CARE advanced settings.</p> Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>class CAREConfigurationWindow(AdvancedConfigurationWindow):\n    \"\"\"A dialog widget for modifying CARE advanced settings.\"\"\"\n\n    def __init__(\n        self,\n        parent: QWidget | None,\n        careamics_config: BaseConfig,\n        algorithm_config: CAREAdvancedConfig,\n    ) -&gt; None:\n        \"\"\"Initialize the window.\n\n        Parameters\n        ----------\n        parent : QWidget | None\n            Parent widget.\n        careamics_config : BaseConfig\n            Careamics configuration object.\n        algorithm_config : CAREAdvancedConfig\n            CARE advanced configuration object.\n        \"\"\"\n        super().__init__(parent, careamics_config, algorithm_config)\n\n        self.advanced_configuration = algorithm_config\n\n        self.add_algorithm_specific_tab()\n\n        self.bind_properties()\n\n    def add_algorithm_specific_tab(self) -&gt; None:\n        \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n        # tab widget\n        tab_widget = QWidget()\n\n        # number of input channels\n        _n_channels = self.advanced_configuration.n_channels_in or 1\n        self.num_channels_in_spin = create_int_spinbox(1, 10, _n_channels, 1)\n        self.num_channels_in_spin.setEnabled(\n            \"C\" in self.configuration.data_config.axes  # type: ignore\n        )\n        self.num_channels_in_spin.setToolTip(\n            \"Number of input channels of the input image (C must be in axes).\"\n        )\n\n        # number of output channels\n        _n_channels = self.advanced_configuration.n_channels_out or 1\n        self.num_channels_out_spin = create_int_spinbox(1, 10, _n_channels, 1)\n        # self.num_channels_out_spin.setEnabled(\n        #     \"C\" in self.configuration.data_config.axes  # type: ignore\n        # )\n        self.num_channels_out_spin.setToolTip(\"Number of output channels.\")\n\n        # layout\n        layout = QVBoxLayout()\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        form.addRow(\"Number of Input Channels:\", self.num_channels_in_spin)\n        form.addRow(\"Number of Output Channels:\", self.num_channels_out_spin)\n        layout.addLayout(form)\n\n        tab_widget.setLayout(layout)\n        self.tabs.addTab(tab_widget, \"CARE\")\n\n    def save(self) -&gt; None:\n        \"\"\"Save the current state of the UI into configurations.\"\"\"\n        super().update_config()\n        self.update_config()\n        self.close()\n\n    def bind_properties(self) -&gt; None:\n        \"\"\"Create and bind the properties to the UI elements.\"\"\"\n        # bind the properties from the base class first\n        super().bind_properties()\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        type(self).in_channels = bind(self.num_channels_in_spin, \"value\")\n        type(self).out_channels = bind(self.num_channels_out_spin, \"value\")\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration object from UI elements.\"\"\"\n        self.advanced_configuration.n_channels_in = self.in_channels\n        self.advanced_configuration.n_channels_out = self.out_channels\n\n        if isinstance(self.configuration.algorithm_config, CAREAlgorithm):\n            self.configuration.algorithm_config.model.in_channels = self.in_channels\n            self.configuration.algorithm_config.model.num_classes = self.out_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow.__init__","title":"<code>__init__(parent, careamics_config, algorithm_config)</code>","text":"<p>Initialize the window.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget | None</code> <p>Parent widget.</p> required <code>careamics_config</code> <code>BaseConfig</code> <p>Careamics configuration object.</p> required <code>algorithm_config</code> <code>CAREAdvancedConfig</code> <p>CARE advanced configuration object.</p> required Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>def __init__(\n    self,\n    parent: QWidget | None,\n    careamics_config: BaseConfig,\n    algorithm_config: CAREAdvancedConfig,\n) -&gt; None:\n    \"\"\"Initialize the window.\n\n    Parameters\n    ----------\n    parent : QWidget | None\n        Parent widget.\n    careamics_config : BaseConfig\n        Careamics configuration object.\n    algorithm_config : CAREAdvancedConfig\n        CARE advanced configuration object.\n    \"\"\"\n    super().__init__(parent, careamics_config, algorithm_config)\n\n    self.advanced_configuration = algorithm_config\n\n    self.add_algorithm_specific_tab()\n\n    self.bind_properties()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow.add_algorithm_specific_tab","title":"<code>add_algorithm_specific_tab()</code>","text":"<p>Add algorithm specific advanced settings tab.</p> Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>def add_algorithm_specific_tab(self) -&gt; None:\n    \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n    # tab widget\n    tab_widget = QWidget()\n\n    # number of input channels\n    _n_channels = self.advanced_configuration.n_channels_in or 1\n    self.num_channels_in_spin = create_int_spinbox(1, 10, _n_channels, 1)\n    self.num_channels_in_spin.setEnabled(\n        \"C\" in self.configuration.data_config.axes  # type: ignore\n    )\n    self.num_channels_in_spin.setToolTip(\n        \"Number of input channels of the input image (C must be in axes).\"\n    )\n\n    # number of output channels\n    _n_channels = self.advanced_configuration.n_channels_out or 1\n    self.num_channels_out_spin = create_int_spinbox(1, 10, _n_channels, 1)\n    # self.num_channels_out_spin.setEnabled(\n    #     \"C\" in self.configuration.data_config.axes  # type: ignore\n    # )\n    self.num_channels_out_spin.setToolTip(\"Number of output channels.\")\n\n    # layout\n    layout = QVBoxLayout()\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    form.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    form.addRow(\"Number of Input Channels:\", self.num_channels_in_spin)\n    form.addRow(\"Number of Output Channels:\", self.num_channels_out_spin)\n    layout.addLayout(form)\n\n    tab_widget.setLayout(layout)\n    self.tabs.addTab(tab_widget, \"CARE\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow.bind_properties","title":"<code>bind_properties()</code>","text":"<p>Create and bind the properties to the UI elements.</p> Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>def bind_properties(self) -&gt; None:\n    \"\"\"Create and bind the properties to the UI elements.\"\"\"\n    # bind the properties from the base class first\n    super().bind_properties()\n    # type(self) returns the class of the instance, so we are adding\n    # properties to the class itself, not the instance.\n    type(self).in_channels = bind(self.num_channels_in_spin, \"value\")\n    type(self).out_channels = bind(self.num_channels_out_spin, \"value\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow.save","title":"<code>save()</code>","text":"<p>Save the current state of the UI into configurations.</p> Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save the current state of the UI into configurations.\"\"\"\n    super().update_config()\n    self.update_config()\n    self.close()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/care_config_window/#careamics_napari.widgets.care_config_window.CAREConfigurationWindow.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration object from UI elements.</p> Source code in <code>src/careamics_napari/widgets/care_config_window.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration object from UI elements.\"\"\"\n    self.advanced_configuration.n_channels_in = self.in_channels\n    self.advanced_configuration.n_channels_out = self.out_channels\n\n    if isinstance(self.configuration.algorithm_config, CAREAlgorithm):\n        self.configuration.algorithm_config.model.in_channels = self.in_channels\n        self.configuration.algorithm_config.model.num_classes = self.out_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/","title":"folder_widget","text":"<p>A widget used for selecting an existing folder.</p>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget","title":"<code>FolderWidget</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>A widget used for selecting an existing folder.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text displayed on the button.</p> required Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>class FolderWidget(QWidget):\n    \"\"\"A widget used for selecting an existing folder.\n\n    Parameters\n    ----------\n    text : str\n        Text displayed on the button.\n    \"\"\"\n\n    def __init__(self: Self, text: str) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        text : str\n            Text displayed on the button.\n        \"\"\"\n        super().__init__()\n\n        hbox = QHBoxLayout()\n        hbox.setSpacing(0)\n        hbox.setContentsMargins(0, 0, 0, 0)\n\n        # text field\n        self.text_field = QLineEdit(\"\")\n        self.text_field.setReadOnly(True)\n        hbox.addWidget(self.text_field)\n\n        # folder selection button\n        self.button = QPushButton(text)\n        hbox.addWidget(self.button)\n        self.button.clicked.connect(self._open_dialog)\n\n        self.setLayout(hbox)\n\n    def _open_dialog(self: Self) -&gt; None:\n        \"\"\"Open a dialog to select a folder.\"\"\"\n        path = QFileDialog.getExistingDirectory(self, \"Select Folder\")\n        print(path)\n\n        # set text in the text field\n        self.text_field.setText(path)\n\n    def get_folder(self: Self) -&gt; str:\n        \"\"\"Get the selected folder.\n\n        Returns\n        -------\n        str\n            The selected folder as read out from the text field.\n        \"\"\"\n        return self.text_field.text()\n\n    def get_text_widget(self: Self) -&gt; QLineEdit:\n        \"\"\"Get the text widget.\n\n        Returns\n        -------\n        QLineEdit\n            The text widget.\n        \"\"\"\n        return self.text_field\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.__init__","title":"<code>__init__(text)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text displayed on the button.</p> required Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def __init__(self: Self, text: str) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    text : str\n        Text displayed on the button.\n    \"\"\"\n    super().__init__()\n\n    hbox = QHBoxLayout()\n    hbox.setSpacing(0)\n    hbox.setContentsMargins(0, 0, 0, 0)\n\n    # text field\n    self.text_field = QLineEdit(\"\")\n    self.text_field.setReadOnly(True)\n    hbox.addWidget(self.text_field)\n\n    # folder selection button\n    self.button = QPushButton(text)\n    hbox.addWidget(self.button)\n    self.button.clicked.connect(self._open_dialog)\n\n    self.setLayout(hbox)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.get_folder","title":"<code>get_folder()</code>","text":"<p>Get the selected folder.</p> <p>Returns:</p> Type Description <code>str</code> <p>The selected folder as read out from the text field.</p> Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def get_folder(self: Self) -&gt; str:\n    \"\"\"Get the selected folder.\n\n    Returns\n    -------\n    str\n        The selected folder as read out from the text field.\n    \"\"\"\n    return self.text_field.text()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.get_text_widget","title":"<code>get_text_widget()</code>","text":"<p>Get the text widget.</p> <p>Returns:</p> Type Description <code>QLineEdit</code> <p>The text widget.</p> Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def get_text_widget(self: Self) -&gt; QLineEdit:\n    \"\"\"Get the text widget.\n\n    Returns\n    -------\n    QLineEdit\n        The text widget.\n    \"\"\"\n    return self.text_field\n</code></pre>"},{"location":"reference/careamics_napari/widgets/gpu_widget/","title":"gpu_widget","text":"<p>A label indicating whether GPU is available to torch.</p>"},{"location":"reference/careamics_napari/widgets/gpu_widget/#careamics_napari.widgets.gpu_widget.create_gpu_label","title":"<code>create_gpu_label()</code>","text":"<p>A label widget indicating whether GPU or CPU is available with torch.</p> <p>Returns:</p> Type Description <code>QLabel</code> <p>GPU label widget.</p> Source code in <code>src/careamics_napari/widgets/gpu_widget.py</code> <pre><code>def create_gpu_label() -&gt; QLabel:\n    \"\"\"A label widget indicating whether GPU or CPU is available with torch.\n\n    Returns\n    -------\n    QLabel\n        GPU label widget.\n    \"\"\"\n    if is_gpu_available():\n        text = \"GPU\"\n        color = \"ADC2A9\"  # green\n    else:\n        text = \"CPU\"\n        color = \"FFDBA4\"  # yellow\n\n    gpu_label = QLabel(text)\n    font_color = gpu_label.palette().color(gpu_label.foregroundRole()).name()[1:]\n    gpu_label.setStyleSheet(\n        f\"\"\"\n            QLabel {{\n                font-weight: bold; color: #{color};\n            }}\n            QToolTip {{\n                color: {font_color};\n            }}\n        \"\"\"\n    )\n    gpu_label.setToolTip(\n        \"Indicates whether PyTorch has access to a GPU.\\n\"\n        \"If your machine has GPU and this label indicates\\n\"\n        \"CPU, please check your PyTorch installation.\"\n    )\n\n    return gpu_label\n</code></pre>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/","title":"magicgui_widgets","text":"<p>Magicgui widgets.</p>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/#careamics_napari.widgets.magicgui_widgets.layer_choice","title":"<code>layer_choice(annotation=Image, **kwargs)</code>","text":"<p>Create a widget to select a layer from the napari viewer.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>Any or None</code> <p>The annotation type to filter the layers.</p> <code>Image</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the widget.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Widget</code> <p>The widget to select a layer from the napari viewer.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If napari is not installed.</p> Source code in <code>src/careamics_napari/widgets/magicgui_widgets.py</code> <pre><code>def layer_choice(annotation: Optional[Any] = Image, **kwargs: Any) -&gt; Widget:\n    \"\"\"Create a widget to select a layer from the napari viewer.\n\n    Parameters\n    ----------\n    annotation : Any or None, default=Image\n        The annotation type to filter the layers.\n    **kwargs : Any\n        Additional keyword arguments to pass to the widget.\n\n    Returns\n    -------\n    Widget\n        The widget to select a layer from the napari viewer.\n\n    Raises\n    ------\n    ImportError\n        If napari is not installed.\n    \"\"\"\n    if not _has_napari:\n        raise ImportError(\"napari is not installed.\")\n\n    widget: Widget = create_widget(annotation=annotation, **kwargs)\n    widget.reset_choices()  # type: ignore\n    viewer = current_viewer()\n\n    # connect to napari events\n    if viewer is not None:\n        viewer.layers.events.inserted.connect(widget.reset_choices)  # type: ignore\n        viewer.layers.events.removed.connect(widget.reset_choices)  # type: ignore\n        viewer.layers.events.changed.connect(widget.reset_choices)  # type: ignore\n\n    return widget\n</code></pre>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/#careamics_napari.widgets.magicgui_widgets.load_button","title":"<code>load_button(Model)</code>","text":"<p>A button to load model files.</p> <p>Parameters:</p> Name Type Description Default <code>Model</code> <code>Path</code> <p>The path to the model file.</p> required Source code in <code>src/careamics_napari/widgets/magicgui_widgets.py</code> <pre><code>@magic_factory(auto_call=True, Model={\"mode\": \"r\", \"filter\": \"*.ckpt *.zip\"})\ndef load_button(Model: Path):\n    \"\"\"A button to load model files.\n\n    Parameters\n    ----------\n    Model : pathlib.Path\n        The path to the model file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/","title":"n2n_config_window","text":""},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow","title":"<code>N2NConfigurationWindow</code>","text":"<p>               Bases: <code>AdvancedConfigurationWindow</code></p> <p>A dialog widget for modifying N2N advanced settings.</p> Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>class N2NConfigurationWindow(AdvancedConfigurationWindow):\n    \"\"\"A dialog widget for modifying N2N advanced settings.\"\"\"\n\n    def __init__(\n        self,\n        parent: QWidget | None,\n        careamics_config: BaseConfig,\n        algorithm_config: N2NAdvancedConfig,\n    ) -&gt; None:\n        \"\"\"Initialize the window.\n\n        Parameters\n        ----------\n        parent : QWidget | None\n            Parent widget.\n        careamics_config : BaseConfig\n            Careamics configuration object.\n        algorithm_config : N2NAdvancedConfig\n            N2N advanced configuration object.\n        \"\"\"\n        super().__init__(parent, careamics_config, algorithm_config)\n\n        self.advanced_configuration = algorithm_config\n\n        self.add_algorithm_specific_tab()\n\n        self.bind_properties()\n\n    def add_algorithm_specific_tab(self) -&gt; None:\n        \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n        # tab widget\n        tab_widget = QWidget()\n\n        # number of input channels\n        _n_channels = self.advanced_configuration.n_channels_in or 1\n        self.num_channels_in_spin = create_int_spinbox(1, 10, _n_channels, 1)\n        self.num_channels_in_spin.setEnabled(\n            \"C\" in self.configuration.data_config.axes  # type: ignore\n        )\n        self.num_channels_in_spin.setToolTip(\n            \"Number of input channels of the input image (C must be in axes).\"\n        )\n\n        # number of output channels\n        _n_channels = self.advanced_configuration.n_channels_out or 1\n        self.num_channels_out_spin = create_int_spinbox(1, 10, _n_channels, 1)\n        # self.num_channels_out_spin.setEnabled(\n        #     \"C\" in self.configuration.data_config.axes  # type: ignore\n        # )\n        self.num_channels_out_spin.setToolTip(\"Number of output channels.\")\n\n        # layout\n        layout = QVBoxLayout()\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        form.addRow(\"Number of Input Channels:\", self.num_channels_in_spin)\n        form.addRow(\"Number of Output Channels:\", self.num_channels_out_spin)\n        layout.addLayout(form)\n\n        tab_widget.setLayout(layout)\n        self.tabs.addTab(tab_widget, \"N2N\")\n\n    def save(self) -&gt; None:\n        \"\"\"Save the current state of the UI into configurations.\"\"\"\n        super().update_config()\n        self.update_config()\n        self.close()\n\n    def bind_properties(self) -&gt; None:\n        \"\"\"Create and bind the properties to the UI elements.\"\"\"\n        # bind the properties from the base class first\n        super().bind_properties()\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        type(self).in_channels = bind(self.num_channels_in_spin, \"value\")\n        type(self).out_channels = bind(self.num_channels_out_spin, \"value\")\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration object from UI elements.\"\"\"\n        self.advanced_configuration.n_channels_in = self.in_channels\n        self.advanced_configuration.n_channels_out = self.out_channels\n\n        if isinstance(self.configuration.algorithm_config, CAREAlgorithm):\n            self.configuration.algorithm_config.model.in_channels = self.in_channels\n            self.configuration.algorithm_config.model.num_classes = self.out_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow.__init__","title":"<code>__init__(parent, careamics_config, algorithm_config)</code>","text":"<p>Initialize the window.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget | None</code> <p>Parent widget.</p> required <code>careamics_config</code> <code>BaseConfig</code> <p>Careamics configuration object.</p> required <code>algorithm_config</code> <code>N2NAdvancedConfig</code> <p>N2N advanced configuration object.</p> required Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>def __init__(\n    self,\n    parent: QWidget | None,\n    careamics_config: BaseConfig,\n    algorithm_config: N2NAdvancedConfig,\n) -&gt; None:\n    \"\"\"Initialize the window.\n\n    Parameters\n    ----------\n    parent : QWidget | None\n        Parent widget.\n    careamics_config : BaseConfig\n        Careamics configuration object.\n    algorithm_config : N2NAdvancedConfig\n        N2N advanced configuration object.\n    \"\"\"\n    super().__init__(parent, careamics_config, algorithm_config)\n\n    self.advanced_configuration = algorithm_config\n\n    self.add_algorithm_specific_tab()\n\n    self.bind_properties()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow.add_algorithm_specific_tab","title":"<code>add_algorithm_specific_tab()</code>","text":"<p>Add algorithm specific advanced settings tab.</p> Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>def add_algorithm_specific_tab(self) -&gt; None:\n    \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n    # tab widget\n    tab_widget = QWidget()\n\n    # number of input channels\n    _n_channels = self.advanced_configuration.n_channels_in or 1\n    self.num_channels_in_spin = create_int_spinbox(1, 10, _n_channels, 1)\n    self.num_channels_in_spin.setEnabled(\n        \"C\" in self.configuration.data_config.axes  # type: ignore\n    )\n    self.num_channels_in_spin.setToolTip(\n        \"Number of input channels of the input image (C must be in axes).\"\n    )\n\n    # number of output channels\n    _n_channels = self.advanced_configuration.n_channels_out or 1\n    self.num_channels_out_spin = create_int_spinbox(1, 10, _n_channels, 1)\n    # self.num_channels_out_spin.setEnabled(\n    #     \"C\" in self.configuration.data_config.axes  # type: ignore\n    # )\n    self.num_channels_out_spin.setToolTip(\"Number of output channels.\")\n\n    # layout\n    layout = QVBoxLayout()\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    form.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    form.addRow(\"Number of Input Channels:\", self.num_channels_in_spin)\n    form.addRow(\"Number of Output Channels:\", self.num_channels_out_spin)\n    layout.addLayout(form)\n\n    tab_widget.setLayout(layout)\n    self.tabs.addTab(tab_widget, \"N2N\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow.bind_properties","title":"<code>bind_properties()</code>","text":"<p>Create and bind the properties to the UI elements.</p> Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>def bind_properties(self) -&gt; None:\n    \"\"\"Create and bind the properties to the UI elements.\"\"\"\n    # bind the properties from the base class first\n    super().bind_properties()\n    # type(self) returns the class of the instance, so we are adding\n    # properties to the class itself, not the instance.\n    type(self).in_channels = bind(self.num_channels_in_spin, \"value\")\n    type(self).out_channels = bind(self.num_channels_out_spin, \"value\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow.save","title":"<code>save()</code>","text":"<p>Save the current state of the UI into configurations.</p> Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save the current state of the UI into configurations.\"\"\"\n    super().update_config()\n    self.update_config()\n    self.close()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2n_config_window/#careamics_napari.widgets.n2n_config_window.N2NConfigurationWindow.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration object from UI elements.</p> Source code in <code>src/careamics_napari/widgets/n2n_config_window.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration object from UI elements.\"\"\"\n    self.advanced_configuration.n_channels_in = self.in_channels\n    self.advanced_configuration.n_channels_out = self.out_channels\n\n    if isinstance(self.configuration.algorithm_config, CAREAlgorithm):\n        self.configuration.algorithm_config.model.in_channels = self.in_channels\n        self.configuration.algorithm_config.model.num_classes = self.out_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/","title":"n2v_config_window","text":""},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow","title":"<code>N2VConfigurationWindow</code>","text":"<p>               Bases: <code>AdvancedConfigurationWindow</code></p> <p>A dialog widget for modifying N2V advanced settings.</p> Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>class N2VConfigurationWindow(AdvancedConfigurationWindow):\n    \"\"\"A dialog widget for modifying N2V advanced settings.\"\"\"\n\n    def __init__(\n        self,\n        parent: QWidget | None,\n        careamics_config: BaseConfig,\n        algorithm_config: N2VAdvancedConfig,\n    ) -&gt; None:\n        \"\"\"Initialize the window.\n\n        Parameters\n        ----------\n        parent : QWidget | None\n            Parent widget.\n        careamics_config : BaseConfig\n            Careamics configuration object.\n        algorithm_config : N2VAdvancedConfig\n            N2V advanced configuration object.\n        \"\"\"\n        super().__init__(parent, careamics_config, algorithm_config)\n\n        self.advanced_configuration = algorithm_config\n\n        self.add_algorithm_specific_tab()\n\n        self.bind_properties()\n\n    def add_algorithm_specific_tab(self) -&gt; None:\n        \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n        # tab widget\n        tab_widget = QWidget()\n\n        # use n2v2 checkbox\n        self.n2v2_chkbox = QCheckBox(\"Use N2V2\")\n        self.n2v2_chkbox.setChecked(self.advanced_configuration.use_n2v2)\n        self.n2v2_chkbox.setToolTip(\"If checked, will use N2V2 instead of N2V\")\n\n        # roi size spin box\n        self.roi_spin = create_int_spinbox(\n            1, 101, self.advanced_configuration.roi_size, 2\n        )\n        self.roi_spin.setToolTip(\n            \"The size of the area around each pixel \"\n            \"that will be manipulated by algorithm (must be an odd number).\"\n        )\n\n        # masked pixel percentage double spin box\n        self.masked_percentage_spin = create_double_spinbox(\n            0.0,\n            1.0,\n            self.advanced_configuration.masked_pixel_percentage,\n            0.01,\n            n_decimal=2,\n        )\n        self.masked_percentage_spin.setToolTip(\n            \"Percentage of pixels that per patch that will be manipulated.\"\n        )\n\n        # number of channels\n        _n_channels = self.advanced_configuration.n_channels or 1\n        self.num_channels_spin = create_int_spinbox(1, 10, _n_channels, 1)\n        self.num_channels_spin.setEnabled(\n            \"C\" in self.configuration.data_config.axes  # type: ignore\n        )\n        self.num_channels_spin.setToolTip(\n            \"Number of channels in the input image (C must be in axes).\"\n        )\n\n        # layout\n        layout = QVBoxLayout()\n        layout.addWidget(self.n2v2_chkbox)\n        layout.addSpacing(15)\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        form.addRow(\"ROI Size:\", self.roi_spin)\n        form.addRow(\"Masked Pixel Percentage:\", self.masked_percentage_spin)\n        form.addRow(\"Number of Channels:\", self.num_channels_spin)\n        layout.addLayout(form)\n\n        tab_widget.setLayout(layout)\n        self.tabs.addTab(tab_widget, \"N2V\")\n\n    def save(self) -&gt; None:\n        \"\"\"Save the current state of the UI into configurations.\"\"\"\n        super().update_config()\n        self.update_config()\n        self.close()\n\n    def bind_properties(self) -&gt; None:\n        \"\"\"Create and bind the properties to the UI elements.\"\"\"\n        # bind the properties from the base class first\n        super().bind_properties()\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        # e.g. when self.n2v2_chkbox is changed,\n        # self.use_n2v2 will be updated automatically.\n        type(self).use_n2v2 = bind(self.n2v2_chkbox, \"checked\")\n        type(self).roi_size = bind(self.roi_spin, \"value\")\n        type(self).masked_pixel_percentage = bind(self.masked_percentage_spin, \"value\")\n        type(self).n_channels = bind(self.num_channels_spin, \"value\")\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration object from UI elements.\"\"\"\n        self.advanced_configuration.use_n2v2 = self.use_n2v2\n        self.configuration.algorithm_config.set_n2v2(self.use_n2v2)  # type: ignore\n\n        self.advanced_configuration.roi_size = self.roi_size\n        self.advanced_configuration.masked_pixel_percentage = self.masked_pixel_percentage\n        self.advanced_configuration.n_channels = self.n_channels\n\n        if isinstance(self.configuration.algorithm_config, N2VAlgorithm):\n            self.configuration.algorithm_config.n2v_config.roi_size = self.roi_size\n            self.configuration.algorithm_config.n2v_config.masked_pixel_percentage = (\n                self.masked_pixel_percentage\n            )\n            self.configuration.algorithm_config.model.in_channels = self.n_channels\n            self.configuration.algorithm_config.model.num_classes = self.n_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow.__init__","title":"<code>__init__(parent, careamics_config, algorithm_config)</code>","text":"<p>Initialize the window.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget | None</code> <p>Parent widget.</p> required <code>careamics_config</code> <code>BaseConfig</code> <p>Careamics configuration object.</p> required <code>algorithm_config</code> <code>N2VAdvancedConfig</code> <p>N2V advanced configuration object.</p> required Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>def __init__(\n    self,\n    parent: QWidget | None,\n    careamics_config: BaseConfig,\n    algorithm_config: N2VAdvancedConfig,\n) -&gt; None:\n    \"\"\"Initialize the window.\n\n    Parameters\n    ----------\n    parent : QWidget | None\n        Parent widget.\n    careamics_config : BaseConfig\n        Careamics configuration object.\n    algorithm_config : N2VAdvancedConfig\n        N2V advanced configuration object.\n    \"\"\"\n    super().__init__(parent, careamics_config, algorithm_config)\n\n    self.advanced_configuration = algorithm_config\n\n    self.add_algorithm_specific_tab()\n\n    self.bind_properties()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow.add_algorithm_specific_tab","title":"<code>add_algorithm_specific_tab()</code>","text":"<p>Add algorithm specific advanced settings tab.</p> Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>def add_algorithm_specific_tab(self) -&gt; None:\n    \"\"\"Add algorithm specific advanced settings tab.\"\"\"\n    # tab widget\n    tab_widget = QWidget()\n\n    # use n2v2 checkbox\n    self.n2v2_chkbox = QCheckBox(\"Use N2V2\")\n    self.n2v2_chkbox.setChecked(self.advanced_configuration.use_n2v2)\n    self.n2v2_chkbox.setToolTip(\"If checked, will use N2V2 instead of N2V\")\n\n    # roi size spin box\n    self.roi_spin = create_int_spinbox(\n        1, 101, self.advanced_configuration.roi_size, 2\n    )\n    self.roi_spin.setToolTip(\n        \"The size of the area around each pixel \"\n        \"that will be manipulated by algorithm (must be an odd number).\"\n    )\n\n    # masked pixel percentage double spin box\n    self.masked_percentage_spin = create_double_spinbox(\n        0.0,\n        1.0,\n        self.advanced_configuration.masked_pixel_percentage,\n        0.01,\n        n_decimal=2,\n    )\n    self.masked_percentage_spin.setToolTip(\n        \"Percentage of pixels that per patch that will be manipulated.\"\n    )\n\n    # number of channels\n    _n_channels = self.advanced_configuration.n_channels or 1\n    self.num_channels_spin = create_int_spinbox(1, 10, _n_channels, 1)\n    self.num_channels_spin.setEnabled(\n        \"C\" in self.configuration.data_config.axes  # type: ignore\n    )\n    self.num_channels_spin.setToolTip(\n        \"Number of channels in the input image (C must be in axes).\"\n    )\n\n    # layout\n    layout = QVBoxLayout()\n    layout.addWidget(self.n2v2_chkbox)\n    layout.addSpacing(15)\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    form.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    form.addRow(\"ROI Size:\", self.roi_spin)\n    form.addRow(\"Masked Pixel Percentage:\", self.masked_percentage_spin)\n    form.addRow(\"Number of Channels:\", self.num_channels_spin)\n    layout.addLayout(form)\n\n    tab_widget.setLayout(layout)\n    self.tabs.addTab(tab_widget, \"N2V\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow.bind_properties","title":"<code>bind_properties()</code>","text":"<p>Create and bind the properties to the UI elements.</p> Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>def bind_properties(self) -&gt; None:\n    \"\"\"Create and bind the properties to the UI elements.\"\"\"\n    # bind the properties from the base class first\n    super().bind_properties()\n    # type(self) returns the class of the instance, so we are adding\n    # properties to the class itself, not the instance.\n    # e.g. when self.n2v2_chkbox is changed,\n    # self.use_n2v2 will be updated automatically.\n    type(self).use_n2v2 = bind(self.n2v2_chkbox, \"checked\")\n    type(self).roi_size = bind(self.roi_spin, \"value\")\n    type(self).masked_pixel_percentage = bind(self.masked_percentage_spin, \"value\")\n    type(self).n_channels = bind(self.num_channels_spin, \"value\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow.save","title":"<code>save()</code>","text":"<p>Save the current state of the UI into configurations.</p> Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Save the current state of the UI into configurations.\"\"\"\n    super().update_config()\n    self.update_config()\n    self.close()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/n2v_config_window/#careamics_napari.widgets.n2v_config_window.N2VConfigurationWindow.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration object from UI elements.</p> Source code in <code>src/careamics_napari/widgets/n2v_config_window.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration object from UI elements.\"\"\"\n    self.advanced_configuration.use_n2v2 = self.use_n2v2\n    self.configuration.algorithm_config.set_n2v2(self.use_n2v2)  # type: ignore\n\n    self.advanced_configuration.roi_size = self.roi_size\n    self.advanced_configuration.masked_pixel_percentage = self.masked_pixel_percentage\n    self.advanced_configuration.n_channels = self.n_channels\n\n    if isinstance(self.configuration.algorithm_config, N2VAlgorithm):\n        self.configuration.algorithm_config.n2v_config.roi_size = self.roi_size\n        self.configuration.algorithm_config.n2v_config.masked_pixel_percentage = (\n            self.masked_pixel_percentage\n        )\n        self.configuration.algorithm_config.model.in_channels = self.n_channels\n        self.configuration.algorithm_config.model.num_classes = self.n_channels\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/","title":"predict_data_widget","text":"<p>A widget used to select a path or layer for prediction.</p>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget","title":"<code>PredictDataWidget</code>","text":"<p>               Bases: <code>QTabWidget</code></p> <p>A widget offering to select a layer from napari or a path from disk.</p> Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>class PredictDataWidget(QTabWidget):\n    \"\"\"A widget offering to select a layer from napari or a path from disk.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the widget.\"\"\"\n        super().__init__()\n\n        # QTabs\n        layer_tab = QWidget()\n        disk_tab = QWidget()\n\n        # add tabs\n        _tab_idx = 0\n        if _has_napari and napari.current_viewer() is not None:\n            # tab for selecting data from napari layers\n            self.addTab(layer_tab, \"From layers\")\n            self.setTabToolTip(_tab_idx, \"Use images from napari layers\")\n            # add tab contents\n            self._set_layer_tab(layer_tab)\n            _tab_idx += 1\n        # tab for selecting data from disk\n        self.addTab(disk_tab, \"From disk\")\n        self.setTabToolTip(_tab_idx, \"Use patches saved on the disk\")\n        self._set_disk_tab(disk_tab)\n\n    def get_data_sources(self) -&gt; str | np.ndarray | None:\n        \"\"\"Get the selected data sources.\"\"\"\n        if (\n            self.img_pred.value is None  # type: ignore\n            and len(self.pred_images_folder.get_folder()) == 0\n        ):\n            # no prediction data has been selected\n            return None\n\n        if self.currentIndex() == 0:\n            # data is expected from napari layers\n            pred_data = self.img_pred.value.data  # type: ignore\n        else:\n            # data is expected from disk\n            pred_data = self.pred_images_folder.get_folder()\n\n        return pred_data\n\n    def _set_layer_tab(\n        self,\n        layer_tab: QWidget,\n    ) -&gt; None:\n        \"\"\"Set up the layer tab.\n\n        Parameters\n        ----------\n        layer_tab : QWidget\n            The layer tab.\n        \"\"\"\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n        form.setContentsMargins(12, 12, 0, 0)\n\n        self.img_pred = layer_choice()\n        self.img_pred.native.setToolTip(\"Select the prediction layer.\")\n        form.addRow(\"Predict\", self.img_pred.native)\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(form)\n        layer_tab.setLayout(vbox)\n\n    def _set_disk_tab(self, disk_tab: QWidget) -&gt; None:\n        \"\"\"Set up the disk tab.\n\n        Parameters\n        ----------\n        disk_tab : QWidget\n            The disk tab.\n        \"\"\"\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n        form.setContentsMargins(12, 12, 0, 0)\n\n        self.pred_images_folder = FolderWidget(\"Choose\")\n        self.pred_images_folder.setToolTip(\"Select a folder containing images.\")\n        form.addRow(\"Predict\", self.pred_images_folder)\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(form)\n        disk_tab.setLayout(vbox)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the widget.</p> Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the widget.\"\"\"\n    super().__init__()\n\n    # QTabs\n    layer_tab = QWidget()\n    disk_tab = QWidget()\n\n    # add tabs\n    _tab_idx = 0\n    if _has_napari and napari.current_viewer() is not None:\n        # tab for selecting data from napari layers\n        self.addTab(layer_tab, \"From layers\")\n        self.setTabToolTip(_tab_idx, \"Use images from napari layers\")\n        # add tab contents\n        self._set_layer_tab(layer_tab)\n        _tab_idx += 1\n    # tab for selecting data from disk\n    self.addTab(disk_tab, \"From disk\")\n    self.setTabToolTip(_tab_idx, \"Use patches saved on the disk\")\n    self._set_disk_tab(disk_tab)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget.get_data_sources","title":"<code>get_data_sources()</code>","text":"<p>Get the selected data sources.</p> Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def get_data_sources(self) -&gt; str | np.ndarray | None:\n    \"\"\"Get the selected data sources.\"\"\"\n    if (\n        self.img_pred.value is None  # type: ignore\n        and len(self.pred_images_folder.get_folder()) == 0\n    ):\n        # no prediction data has been selected\n        return None\n\n    if self.currentIndex() == 0:\n        # data is expected from napari layers\n        pred_data = self.img_pred.value.data  # type: ignore\n    else:\n        # data is expected from disk\n        pred_data = self.pred_images_folder.get_folder()\n\n    return pred_data\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/","title":"prediction_widget","text":"<p>Widget used to run prediction from the Training plugin.</p>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget","title":"<code>PredictionWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget to run prediction on images from within the Training plugin.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>BaseConfig</code> <pre><code>The configuration for the CAREamics algorithm.\n</code></pre> required <code>train_status</code> <code>TrainingStatus or None</code> <p>The training status signal.</p> <code>None</code> <code>pred_status</code> <code>PredictionStatus or None</code> <p>The prediction status signal.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>class PredictionWidget(QGroupBox):\n    \"\"\"A widget to run prediction on images from within the Training plugin.\n\n    Parameters\n    ----------\n    careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n    train_status : TrainingStatus or None, default=None\n        The training status signal.\n    pred_status : PredictionStatus or None, default=None\n        The prediction status signal.\n    \"\"\"\n\n    # set a signal to send a careamist object\n    # when it's loaded from disk.\n    careamist_loaded = Signal(CAREamist)\n    # signal for model selection changed\n    model_from_disk = Signal(bool)\n\n    def __init__(\n        self,\n        careamics_config: BaseConfig,\n        train_status: TrainingStatus | None = None,\n        pred_status: PredictionStatus | None = None,\n        prediction_queue: Queue | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n        train_status : TrainingStatus or None, default=None\n            The training status signal.\n        pred_status : PredictionStatus or None, default=None\n            The prediction status signal.\n        prediction_queue : Queue or None, default=None\n            The prediction queue.\n        \"\"\"\n        super().__init__()\n\n        self.configuration = careamics_config\n        self.train_status = (\n            TrainingStatus() if train_status is None else train_status  # type: ignore\n        )\n        self.pred_status = (\n            PredictionStatus() if pred_status is None else pred_status  # type: ignore\n        )\n        self.prediction_queue = (\n            Queue(10) if prediction_queue is None else prediction_queue\n        )\n\n        self.setTitle(\"Prediction\")\n\n        # model selection\n        self.from_train_radiobutton = QRadioButton(\"From the trained model\")\n        self.from_train_radiobutton.setChecked(True)\n        self.from_disk_radiobutton = QRadioButton(\"Load model from disk\")\n        self.model_textbox = QLineEdit()\n        self.model_textbox.setReadOnly(True)\n        self.model_textbox.setEnabled(False)\n        self.load_button = QPushButton(\"Load...\")\n        self.load_button.setEnabled(False)\n\n        # data selection\n        self.predict_data_widget = PredictDataWidget()\n\n        # checkbox\n        self.tiling_cbox = QCheckBox(\"Tile prediction\")\n        self.tiling_cbox.setChecked(True)\n        self.tiling_cbox.setToolTip(\n            \"Select to predict the image by tiles, allowing to predict on large images.\"\n        )\n\n        # tiling spinboxes\n        self.tile_size_xy_spin = PowerOfTwoSpinBox(64, 1024, 64)\n        self.tile_size_xy_spin.setToolTip(\"Tile size in the xy dimension.\")\n        # self.tile_size_xy.setEnabled(False)\n\n        self.tile_size_z_spin = PowerOfTwoSpinBox(4, 32, 8)\n        self.tile_size_z_spin.setToolTip(\"Tile size in the z dimension.\")\n        self.tile_size_z_spin.setEnabled(self.configuration.is_3D)\n\n        # batch size spinbox\n        self.batch_size_spin = create_int_spinbox(1, 512, 1, 1)\n        self.batch_size_spin.setToolTip(\n            \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n        )\n        # self.batch_size_spin.setEnabled(False)\n\n        # prediction progress bar\n        self.pb_prediction = create_progressbar(\n            max_value=20, text_format=\"Prediction ?/?\"\n        )\n        self.pb_prediction.setToolTip(\"Show the progress of the prediction\")\n\n        # predict button\n        self.predict_button = QPushButton(\"Predict\", self)\n        self.predict_button.setMinimumWidth(120)\n        self.predict_button.setEnabled(False)\n        self.predict_button.setToolTip(\"Run the trained model on the images\")\n        # stop button\n        self.stop_button = QPushButton(\"Stop\", self)\n        self.stop_button.setMinimumWidth(120)\n        self.stop_button.setEnabled(False)\n        self.stop_button.setToolTip(\"Stop the prediction\")\n\n        # layout\n        vbox = QVBoxLayout()\n        model_vbox = QVBoxLayout()\n        model_vbox.addWidget(self.from_train_radiobutton)\n        model_vbox.addWidget(self.from_disk_radiobutton)\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.model_textbox)\n        hbox.addWidget(self.load_button)\n        model_vbox.addLayout(hbox)\n        vbox.addLayout(model_vbox)\n        vbox.addWidget(self.predict_data_widget)\n        vbox.addWidget(self.tiling_cbox)\n        tiling_form = QFormLayout()\n        tiling_form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        tiling_form.setFieldGrowthPolicy(\n            QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n        )\n        tiling_form.addRow(\"XY tile size\", self.tile_size_xy_spin)\n        tiling_form.addRow(\"Z tile size\", self.tile_size_z_spin)\n        tiling_form.addRow(\"Batch size\", self.batch_size_spin)\n        vbox.addLayout(tiling_form)\n        vbox.addWidget(self.pb_prediction)\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.predict_button, alignment=Qt.AlignLeft)  # type: ignore\n        hbox.addWidget(self.stop_button, alignment=Qt.AlignRight)  # type: ignore\n        vbox.addLayout(hbox)\n        self.setLayout(vbox)\n\n        # actions\n        self.from_train_radiobutton.clicked.connect(self._model_selection_changed)\n        self.from_disk_radiobutton.clicked.connect(self._model_selection_changed)\n        self.load_button.clicked.connect(self._select_model_checkpoint)\n        self.tiling_cbox.clicked.connect(self._update_tilings)\n        self.predict_button.clicked.connect(self._predict_button_clicked)\n        self.stop_button.clicked.connect(self._stop_button_clicked)\n\n        self.pred_status.events.state.connect(self._update_button_from_pred)\n        self.pred_status.events.sample_idx.connect(self._update_sample_idx)\n        self.pred_status.events.max_samples.connect(self._update_max_sample)\n\n        # bind properties\n        self._bind_properties()\n\n    def set_3d(self, state: bool) -&gt; None:\n        \"\"\"Enable the z tile size spinbox if the data is 3D.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the 3D checkbox.\n        \"\"\"\n        # this method can be used by the parent plugin when the train config is updated.\n        self.configuration.is_3D = state\n        self.tile_size_z_spin.setEnabled(self.do_tiling and state)\n\n    def update_button_from_train(self, state: TrainingState) -&gt; None:\n        \"\"\"Update the predict button based on the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            The new state of the training plugin.\n        \"\"\"\n        if state == TrainingState.DONE:\n            self.predict_button.setEnabled(True)\n            self.stop_button.setEnabled(False)\n        else:\n            self.predict_button.setEnabled(False)\n            self.stop_button.setEnabled(False)\n\n    def get_data_source(self) -&gt; str | np.ndarray | None:\n        \"\"\"Get the selected data sources from the predict data widget.\"\"\"\n        return self.predict_data_widget.get_data_sources()\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the prediction configuration from the UI element.\"\"\"\n        # tile size\n        self.configuration.tile_size = None\n        if self.do_tiling:\n            _tile_size = [self.tile_size_xy, self.tile_size_xy]\n            if self.configuration.is_3D:\n                _tile_size.insert(0, self.tile_size_z)\n            self.configuration.tile_size = tuple(_tile_size)\n\n        # batch size\n        self.configuration.pred_batch_size = self.batch_size\n\n    def _bind_properties(self) -&gt; None:\n        \"\"\"Create and bind the properties to the UI elements.\"\"\"\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        # to check if should use a loaded model\n        type(self).load_from_disk = bind(self.from_disk_radiobutton, \"checked\", False)\n        # tiling\n        type(self).do_tiling = bind(self.tiling_cbox, \"checked\", True)\n        # tile size in xy\n        type(self).tile_size_xy = bind(self.tile_size_xy_spin, \"value\", 64)\n        # tile size in z\n        type(self).tile_size_z = bind(self.tile_size_z_spin, \"value\", 8)\n        # batch size\n        type(self).batch_size = bind(self.batch_size_spin, \"value\", 1)\n        # for example when self.batch_size_spin value is changed,\n        # self.batch_size will be updated automatically.\n\n    def _model_selection_changed(self) -&gt; None:\n        \"\"\"Update model selection ui.\"\"\"\n        # load_from_disk = self.from_disk_radiobutton.isChecked()\n        self.model_textbox.setEnabled(self.load_from_disk)\n        self.load_button.setEnabled(self.load_from_disk)\n        self.model_from_disk.emit(self.load_from_disk)\n\n    def _select_model_checkpoint(self) -&gt; None:\n        \"\"\"Load a selected CAREamics model.\"\"\"\n        selected_file, _filter = QFileDialog.getOpenFileName(\n            self, \"CAREamics\", \".\", \"CAREamics Model(*.ckpt *.zip)\"\n        )\n        if selected_file is not None and len(selected_file) &gt; 0:\n            careamist = self._load_model(selected_file)\n            if careamist is None:\n                print(f\"Error loading the model: {selected_file}\")\n                # if _has_napari:\n                #     ntf.show_error(f\"Error loading the model: {selected_file}\")\n                return\n            # sent the careamist to the parent window / plugin\n            self.careamist_loaded.emit(careamist)\n            self.model_textbox.setText(selected_file)\n            self.predict_button.setEnabled(True)\n            self.stop_button.setEnabled(False)\n\n    def _load_model(self, model_path: str) -&gt; CAREamist | None:\n        \"\"\"Load a CAREamics model.\n\n        Parameters\n        ----------\n        model_path : str\n            Path to the model checkpoint.\n\n        Returns\n        -------\n        careamist : CAREamist or None\n            CAREamist instance or None if the model could not be loaded.\n        \"\"\"\n        try:\n            # make a training queue\n            training_queue = Queue(10)\n            # careamist: carefully load the model among the mist! :)\n            careamist = CAREamist(\n                model_path,\n                work_dir=self.configuration.work_dir,\n                callbacks=[\n                    UpdaterCallBack(training_queue, self.prediction_queue),\n                    StopPredictionCallback(self.pred_status),\n                ],\n            )\n\n            # check the loaded model algorithm\n            # to be compatible with the current configuration\n            model_algo = careamist.cfg.get_algorithm_friendly_name()\n            config_algo = self.configuration.get_algorithm_friendly_name()\n            if model_algo != config_algo:\n                err_msg = (\n                    f\"The loaded model ({model_algo}) does not match \"\n                    f\"the current configuration ({config_algo}).\"\n                )\n                if _has_napari:\n                    ntf.show_error(err_msg)\n                raise ValueError(err_msg)\n\n            return careamist\n\n        except Exception as e:\n            print(f\"Error loading the model:\\n{e}\")\n            return None\n\n    def _update_tilings(self, state: bool) -&gt; None:\n        \"\"\"Update the widgets and the signal tiling parameter.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the tiling checkbox.\n        \"\"\"\n        # self.do_tiling = state\n        self.tile_size_xy_spin.setEnabled(state)\n        self.batch_size_spin.setEnabled(state)\n        self.tile_size_z_spin.setEnabled(state and self.configuration.is_3D)\n\n    def _update_3d_tiles(self, state: bool) -&gt; None:\n        \"\"\"Enable the z tile size spinbox if the data is 3D and tiled.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the 3D checkbox.\n        \"\"\"\n        if self.pred_signal.tiled:\n            self.tile_size_z_spin.setEnabled(state)\n\n    def _update_max_sample(self, max_sample: int) -&gt; None:\n        \"\"\"Update the maximum value of the progress bar.\n\n        Parameters\n        ----------\n        max_sample : int\n            The new maximum value of the progress bar.\n        \"\"\"\n        self.pb_prediction.setMaximum(max_sample)\n\n    def _update_sample_idx(self, sample: int) -&gt; None:\n        \"\"\"Update the value of the progress bar.\n\n        Parameters\n        ----------\n        sample : int\n            The new value of the progress bar.\n        \"\"\"\n        self.pb_prediction.setValue(sample + 1)\n        self.pb_prediction.setFormat(\n            f\"Sample {sample + 1}/{self.pred_status.max_samples}\"\n        )\n\n    def _predict_button_clicked(self) -&gt; None:\n        \"\"\"Run the prediction on the images.\"\"\"\n        if self.pred_status is not None:\n            if self.pred_status.state != PredictionState.PREDICTING:\n                self.predict_button.setEnabled(False)\n                self.stop_button.setEnabled(True)\n                self.pred_status.state = PredictionState.PREDICTING\n\n    def _stop_button_clicked(self) -&gt; None:\n        \"\"\"Stop the prediction.\"\"\"\n        if self.pred_status.state == PredictionState.PREDICTING:\n            self.stop_button.setEnabled(False)\n            self.pred_status.state = PredictionState.STOPPED\n\n    def _update_button_from_pred(self, state: PredictionState) -&gt; None:\n        \"\"\"Update the predict button based on the prediction state.\n\n        Parameters\n        ----------\n        state : PredictionState\n            The new state of the prediction plugin.\n        \"\"\"\n        if (\n            state == PredictionState.DONE\n            or state == PredictionState.CRASHED\n            or state == PredictionState.STOPPED\n        ):\n            self.predict_button.setEnabled(True)\n            self.stop_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.__init__","title":"<code>__init__(careamics_config, train_status=None, pred_status=None, prediction_queue=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>BaseConfig</code> <p>The configuration for the CAREamics algorithm.</p> required <code>train_status</code> <code>TrainingStatus or None</code> <p>The training status signal.</p> <code>None</code> <code>pred_status</code> <code>PredictionStatus or None</code> <p>The prediction status signal.</p> <code>None</code> <code>prediction_queue</code> <code>Queue or None</code> <p>The prediction queue.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def __init__(\n    self,\n    careamics_config: BaseConfig,\n    train_status: TrainingStatus | None = None,\n    pred_status: PredictionStatus | None = None,\n    prediction_queue: Queue | None = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    careamics_config : BaseConfig\n        The configuration for the CAREamics algorithm.\n    train_status : TrainingStatus or None, default=None\n        The training status signal.\n    pred_status : PredictionStatus or None, default=None\n        The prediction status signal.\n    prediction_queue : Queue or None, default=None\n        The prediction queue.\n    \"\"\"\n    super().__init__()\n\n    self.configuration = careamics_config\n    self.train_status = (\n        TrainingStatus() if train_status is None else train_status  # type: ignore\n    )\n    self.pred_status = (\n        PredictionStatus() if pred_status is None else pred_status  # type: ignore\n    )\n    self.prediction_queue = (\n        Queue(10) if prediction_queue is None else prediction_queue\n    )\n\n    self.setTitle(\"Prediction\")\n\n    # model selection\n    self.from_train_radiobutton = QRadioButton(\"From the trained model\")\n    self.from_train_radiobutton.setChecked(True)\n    self.from_disk_radiobutton = QRadioButton(\"Load model from disk\")\n    self.model_textbox = QLineEdit()\n    self.model_textbox.setReadOnly(True)\n    self.model_textbox.setEnabled(False)\n    self.load_button = QPushButton(\"Load...\")\n    self.load_button.setEnabled(False)\n\n    # data selection\n    self.predict_data_widget = PredictDataWidget()\n\n    # checkbox\n    self.tiling_cbox = QCheckBox(\"Tile prediction\")\n    self.tiling_cbox.setChecked(True)\n    self.tiling_cbox.setToolTip(\n        \"Select to predict the image by tiles, allowing to predict on large images.\"\n    )\n\n    # tiling spinboxes\n    self.tile_size_xy_spin = PowerOfTwoSpinBox(64, 1024, 64)\n    self.tile_size_xy_spin.setToolTip(\"Tile size in the xy dimension.\")\n    # self.tile_size_xy.setEnabled(False)\n\n    self.tile_size_z_spin = PowerOfTwoSpinBox(4, 32, 8)\n    self.tile_size_z_spin.setToolTip(\"Tile size in the z dimension.\")\n    self.tile_size_z_spin.setEnabled(self.configuration.is_3D)\n\n    # batch size spinbox\n    self.batch_size_spin = create_int_spinbox(1, 512, 1, 1)\n    self.batch_size_spin.setToolTip(\n        \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n    )\n    # self.batch_size_spin.setEnabled(False)\n\n    # prediction progress bar\n    self.pb_prediction = create_progressbar(\n        max_value=20, text_format=\"Prediction ?/?\"\n    )\n    self.pb_prediction.setToolTip(\"Show the progress of the prediction\")\n\n    # predict button\n    self.predict_button = QPushButton(\"Predict\", self)\n    self.predict_button.setMinimumWidth(120)\n    self.predict_button.setEnabled(False)\n    self.predict_button.setToolTip(\"Run the trained model on the images\")\n    # stop button\n    self.stop_button = QPushButton(\"Stop\", self)\n    self.stop_button.setMinimumWidth(120)\n    self.stop_button.setEnabled(False)\n    self.stop_button.setToolTip(\"Stop the prediction\")\n\n    # layout\n    vbox = QVBoxLayout()\n    model_vbox = QVBoxLayout()\n    model_vbox.addWidget(self.from_train_radiobutton)\n    model_vbox.addWidget(self.from_disk_radiobutton)\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.model_textbox)\n    hbox.addWidget(self.load_button)\n    model_vbox.addLayout(hbox)\n    vbox.addLayout(model_vbox)\n    vbox.addWidget(self.predict_data_widget)\n    vbox.addWidget(self.tiling_cbox)\n    tiling_form = QFormLayout()\n    tiling_form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    tiling_form.setFieldGrowthPolicy(\n        QFormLayout.AllNonFixedFieldsGrow  # type: ignore\n    )\n    tiling_form.addRow(\"XY tile size\", self.tile_size_xy_spin)\n    tiling_form.addRow(\"Z tile size\", self.tile_size_z_spin)\n    tiling_form.addRow(\"Batch size\", self.batch_size_spin)\n    vbox.addLayout(tiling_form)\n    vbox.addWidget(self.pb_prediction)\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.predict_button, alignment=Qt.AlignLeft)  # type: ignore\n    hbox.addWidget(self.stop_button, alignment=Qt.AlignRight)  # type: ignore\n    vbox.addLayout(hbox)\n    self.setLayout(vbox)\n\n    # actions\n    self.from_train_radiobutton.clicked.connect(self._model_selection_changed)\n    self.from_disk_radiobutton.clicked.connect(self._model_selection_changed)\n    self.load_button.clicked.connect(self._select_model_checkpoint)\n    self.tiling_cbox.clicked.connect(self._update_tilings)\n    self.predict_button.clicked.connect(self._predict_button_clicked)\n    self.stop_button.clicked.connect(self._stop_button_clicked)\n\n    self.pred_status.events.state.connect(self._update_button_from_pred)\n    self.pred_status.events.sample_idx.connect(self._update_sample_idx)\n    self.pred_status.events.max_samples.connect(self._update_max_sample)\n\n    # bind properties\n    self._bind_properties()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.get_data_source","title":"<code>get_data_source()</code>","text":"<p>Get the selected data sources from the predict data widget.</p> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def get_data_source(self) -&gt; str | np.ndarray | None:\n    \"\"\"Get the selected data sources from the predict data widget.\"\"\"\n    return self.predict_data_widget.get_data_sources()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.set_3d","title":"<code>set_3d(state)</code>","text":"<p>Enable the z tile size spinbox if the data is 3D.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>bool</code> <p>The new state of the 3D checkbox.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def set_3d(self, state: bool) -&gt; None:\n    \"\"\"Enable the z tile size spinbox if the data is 3D.\n\n    Parameters\n    ----------\n    state : bool\n        The new state of the 3D checkbox.\n    \"\"\"\n    # this method can be used by the parent plugin when the train config is updated.\n    self.configuration.is_3D = state\n    self.tile_size_z_spin.setEnabled(self.do_tiling and state)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.update_button_from_train","title":"<code>update_button_from_train(state)</code>","text":"<p>Update the predict button based on the training state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>The new state of the training plugin.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def update_button_from_train(self, state: TrainingState) -&gt; None:\n    \"\"\"Update the predict button based on the training state.\n\n    Parameters\n    ----------\n    state : TrainingState\n        The new state of the training plugin.\n    \"\"\"\n    if state == TrainingState.DONE:\n        self.predict_button.setEnabled(True)\n        self.stop_button.setEnabled(False)\n    else:\n        self.predict_button.setEnabled(False)\n        self.stop_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.update_config","title":"<code>update_config()</code>","text":"<p>Update the prediction configuration from the UI element.</p> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the prediction configuration from the UI element.\"\"\"\n    # tile size\n    self.configuration.tile_size = None\n    if self.do_tiling:\n        _tile_size = [self.tile_size_xy, self.tile_size_xy]\n        if self.configuration.is_3D:\n            _tile_size.insert(0, self.tile_size_z)\n        self.configuration.tile_size = tuple(_tile_size)\n\n    # batch size\n    self.configuration.pred_batch_size = self.batch_size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/","title":"qt_widgets","text":"<p>Various pure Qt widgets.</p>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.DoubleSpinBox","title":"<code>DoubleSpinBox</code>","text":"<p>               Bases: <code>QDoubleSpinBox</code></p> <p>A double spin box that ignores wheel events.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class DoubleSpinBox(QDoubleSpinBox):\n    \"\"\"A double spin box that ignores wheel events.\"\"\"\n\n    def wheelEvent(self: Self, event: Any) -&gt; None:\n        \"\"\"Ignore wheel events.\n\n        Parameters\n        ----------\n        event : Any\n            The wheel event.\n        \"\"\"\n        event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.DoubleSpinBox.wheelEvent","title":"<code>wheelEvent(event)</code>","text":"<p>Ignore wheel events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The wheel event.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def wheelEvent(self: Self, event: Any) -&gt; None:\n    \"\"\"Ignore wheel events.\n\n    Parameters\n    ----------\n    event : Any\n        The wheel event.\n    \"\"\"\n    event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox","title":"<code>PowerOfTwoSpinBox</code>","text":"<p>               Bases: <code>QSpinBox</code></p> <p>A spin box that only accepts power of two values.</p> <p>Parameters:</p> Name Type Description Default <code>min_val</code> <code>int</code> <p>Minimum value.</p> required <code>max_val</code> <code>int</code> <p>Maximum value.</p> required <code>default</code> <code>int</code> <p>Default value.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class PowerOfTwoSpinBox(QSpinBox):\n    \"\"\"A spin box that only accepts power of two values.\n\n    Parameters\n    ----------\n    min_val : int\n        Minimum value.\n    max_val : int\n        Maximum value.\n    default : int\n        Default value.\n    *args : Any\n        Additional arguments.\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self: Self, min_val: int, max_val: int, default: int, *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        min_val : int\n            Minimum value.\n        max_val : int\n            Maximum value.\n        default : int\n            Default value.\n        *args : Any\n            Additional arguments.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Raises\n        ------\n        ValueError\n            If min or max are not power of 2.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # min or max are not power of 2\n        if min_val &amp; (min_val - 1) != 0:\n            raise ValueError(f\"Minimum value must be a power of 2, got {min_val}.\")\n\n        if max_val &amp; (max_val - 1) != 0:\n            raise ValueError(f\"Maximum value must be a power of 2, got {max_val}.\")\n\n        self.setRange(min_val, max_val)\n        self.setSingleStep(1)\n        self.setValue(default)\n\n    def stepBy(self: Self, steps: int) -&gt; None:\n        \"\"\"Step the value by a given number of steps.\n\n        Parameters\n        ----------\n        steps : int\n            Number of steps to step the value by.\n        \"\"\"\n        current_value = self.value()\n        current_power = self._get_base_2_log(current_value)\n\n        # Step up or down by adjusting the power of two\n        new_power = current_power + steps\n        new_value = 2**new_power\n        self.setValue(new_value)\n\n    def _get_base_2_log(self: Self, value: int) -&gt; int:\n        \"\"\"Get base-2 logarithm.\n\n        Parameters\n        ----------\n        value : int\n            The value to get the power of two for.\n\n        Returns\n        -------\n        int\n            The power of two of the given value.\n        \"\"\"\n        return int(math.log2(value))\n\n    def textFromValue(self: Self, value: int) -&gt; str:\n        \"\"\"Get the text representation of the value.\n\n        Parameters\n        ----------\n        value : int\n            The value.\n\n        Returns\n        -------\n        str\n            The text representation of the value.\n        \"\"\"\n        return str(value)\n\n    def valueFromText(self: Self, text: str) -&gt; int:\n        \"\"\"Get the value from the text representation.\n\n        Parameters\n        ----------\n        text : str\n            The text representation.\n\n        Returns\n        -------\n        int\n            The value.\n        \"\"\"\n        return int(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.__init__","title":"<code>__init__(min_val, max_val, default, *args, **kwargs)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>min_val</code> <code>int</code> <p>Minimum value.</p> required <code>max_val</code> <code>int</code> <p>Maximum value.</p> required <code>default</code> <code>int</code> <p>Default value.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min or max are not power of 2.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def __init__(\n    self: Self, min_val: int, max_val: int, default: int, *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    min_val : int\n        Minimum value.\n    max_val : int\n        Maximum value.\n    default : int\n        Default value.\n    *args : Any\n        Additional arguments.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Raises\n    ------\n    ValueError\n        If min or max are not power of 2.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n\n    # min or max are not power of 2\n    if min_val &amp; (min_val - 1) != 0:\n        raise ValueError(f\"Minimum value must be a power of 2, got {min_val}.\")\n\n    if max_val &amp; (max_val - 1) != 0:\n        raise ValueError(f\"Maximum value must be a power of 2, got {max_val}.\")\n\n    self.setRange(min_val, max_val)\n    self.setSingleStep(1)\n    self.setValue(default)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.stepBy","title":"<code>stepBy(steps)</code>","text":"<p>Step the value by a given number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to step the value by.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def stepBy(self: Self, steps: int) -&gt; None:\n    \"\"\"Step the value by a given number of steps.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to step the value by.\n    \"\"\"\n    current_value = self.value()\n    current_power = self._get_base_2_log(current_value)\n\n    # Step up or down by adjusting the power of two\n    new_power = current_power + steps\n    new_value = 2**new_power\n    self.setValue(new_value)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.textFromValue","title":"<code>textFromValue(value)</code>","text":"<p>Get the text representation of the value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text representation of the value.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def textFromValue(self: Self, value: int) -&gt; str:\n    \"\"\"Get the text representation of the value.\n\n    Parameters\n    ----------\n    value : int\n        The value.\n\n    Returns\n    -------\n    str\n        The text representation of the value.\n    \"\"\"\n    return str(value)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.valueFromText","title":"<code>valueFromText(text)</code>","text":"<p>Get the value from the text representation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text representation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The value.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def valueFromText(self: Self, text: str) -&gt; int:\n    \"\"\"Get the value from the text representation.\n\n    Parameters\n    ----------\n    text : str\n        The text representation.\n\n    Returns\n    -------\n    int\n        The value.\n    \"\"\"\n    return int(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.SpinBox","title":"<code>SpinBox</code>","text":"<p>               Bases: <code>QSpinBox</code></p> <p>A spin box that ignores wheel events.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class SpinBox(QSpinBox):\n    \"\"\"A spin box that ignores wheel events.\"\"\"\n\n    def wheelEvent(self: Self, event: Any) -&gt; None:\n        \"\"\"Ignore wheel events.\n\n        Parameters\n        ----------\n        event : Any\n            The wheel event.\n        \"\"\"\n        event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.SpinBox.wheelEvent","title":"<code>wheelEvent(event)</code>","text":"<p>Ignore wheel events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The wheel event.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def wheelEvent(self: Self, event: Any) -&gt; None:\n    \"\"\"Ignore wheel events.\n\n    Parameters\n    ----------\n    event : Any\n        The wheel event.\n    \"\"\"\n    event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_double_spinbox","title":"<code>create_double_spinbox(min_value=0, max_value=1, value=0.5, step=0.1, visible=True, tooltip=None, n_decimal=1)</code>","text":"<p>Create a double-typed spin box.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum value.</p> <code>0</code> <code>max_value</code> <code>float</code> <p>Maximum value.</p> <code>1</code> <code>value</code> <code>float</code> <p>Default value.</p> <code>0.5</code> <code>step</code> <code>float</code> <p>Step value.</p> <code>0.1</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <code>n_decimal</code> <code>int</code> <p>Number of decimal places.</p> <code>1</code> <p>Returns:</p> Type Description <code>DoubleSpinBox</code> <p>The double spin box.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_double_spinbox(\n    min_value: float = 0,\n    max_value: float = 1,\n    value: float = 0.5,\n    step: float = 0.1,\n    visible: bool = True,\n    tooltip: str | None = None,\n    n_decimal: int = 1,\n) -&gt; DoubleSpinBox:\n    \"\"\"Create a double-typed spin box.\n\n    Parameters\n    ----------\n    min_value : float, default=0\n        Minimum value.\n    max_value : float, default=1\n        Maximum value.\n    value : float, default=0.5\n        Default value.\n    step : float, default=0.1\n        Step value.\n    visible : bool, default=True\n        Visibility.\n    tooltip : str or None, default=None\n        Tooltip text.\n    n_decimal : int, default=1\n        Number of decimal places.\n\n    Returns\n    -------\n    DoubleSpinBox\n        The double spin box.\n    \"\"\"\n    spin_box = DoubleSpinBox()\n    spin_box.setDecimals(n_decimal)\n    spin_box.setMinimum(min_value)\n    spin_box.setMaximum(max_value)\n    spin_box.setSingleStep(step)\n    spin_box.setValue(value)\n    spin_box.setVisible(visible)\n    spin_box.setToolTip(tooltip)\n    spin_box.setMinimumHeight(25)\n    # spin_box.setContentsMargins(0, 3, 0, 3)\n    return spin_box\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_int_spinbox","title":"<code>create_int_spinbox(min_value=1, max_value=1000, value=2, step=1, visible=True, tooltip=None)</code>","text":"<p>Create an integer-typed spin box.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>int</code> <p>Minimum value.</p> <code>1</code> <code>max_value</code> <code>int</code> <p>Maximum value.</p> <code>1000</code> <code>value</code> <code>int</code> <p>Default value.</p> <code>2</code> <code>step</code> <code>int</code> <p>Step value.</p> <code>1</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <p>Returns:</p> Type Description <code>SpinBox</code> <p>The integer spin box.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_int_spinbox(\n    min_value: int = 1,\n    max_value: int = 1000,\n    value: int = 2,\n    step: int = 1,\n    visible: bool = True,\n    tooltip: str | None = None,\n) -&gt; SpinBox:\n    \"\"\"Create an integer-typed spin box.\n\n    Parameters\n    ----------\n    min_value : int, default=1\n        Minimum value.\n    max_value : int, default=1000\n        Maximum value.\n    value : int, default=2\n        Default value.\n    step : int, default=1\n        Step value.\n    visible : bool, default=True\n        Visibility.\n    tooltip : str or None, default=None\n        Tooltip text.\n\n    Returns\n    -------\n    SpinBox\n        The integer spin box.\n    \"\"\"\n    spin_box = SpinBox()\n    spin_box.setMinimum(min_value)\n    spin_box.setMaximum(max_value)\n    spin_box.setSingleStep(step)\n    spin_box.setValue(value)\n    spin_box.setVisible(visible)\n    spin_box.setToolTip(tooltip)\n    spin_box.setMinimumHeight(25)\n    # spin_box.setContentsMargins(0, 3, 0, 3)\n\n    return spin_box\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_progressbar","title":"<code>create_progressbar(min_value=0, max_value=100, value=0, text_visible=True, visible=True, text_format=f'Epoch ?/{100}', tooltip=None)</code>","text":"<p>Create a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>int</code> <p>Minimum value.</p> <code>0</code> <code>max_value</code> <code>int</code> <p>Maximum value.</p> <code>100</code> <code>value</code> <code>int</code> <p>Default value.</p> <code>0</code> <code>text_visible</code> <code>bool</code> <p>Visibility of the text.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>text_format</code> <code>str</code> <p>Text format.</p> <code>\"Epoch ?/{100}\"</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <p>Returns:</p> Type Description <code>QProgressBar</code> <p>The progress bar.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_progressbar(\n    min_value: int = 0,\n    max_value: int = 100,\n    value: int = 0,\n    text_visible: bool = True,\n    visible: bool = True,\n    text_format: str = f\"Epoch ?/{100}\",\n    tooltip: str | None = None,\n) -&gt; QProgressBar:\n    \"\"\"Create a progress bar.\n\n    Parameters\n    ----------\n    min_value : int, default=0\n        Minimum value.\n    max_value : int, default=100\n        Maximum value.\n    value : int, default=0\n        Default value.\n    text_visible : bool, default=True\n        Visibility of the text.\n    visible : bool, default=True\n        Visibility.\n    text_format : str, default=\"Epoch ?/{100}\"\n        Text format.\n    tooltip : str or None, default=None\n        Tooltip text.\n\n    Returns\n    -------\n    QProgressBar\n        The progress bar.\n    \"\"\"\n    progress_bar = QProgressBar()\n    progress_bar.setMinimum(min_value)\n    progress_bar.setMaximum(max_value)\n    progress_bar.setValue(value)\n    progress_bar.setVisible(visible)\n    progress_bar.setTextVisible(text_visible)\n    progress_bar.setFormat(text_format)\n    progress_bar.setToolTip(tooltip)\n    progress_bar.setMinimumHeight(30)\n\n    return progress_bar\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/","title":"saving_widget","text":"<p>A widget allowing users to select a model type and a path.</p>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget","title":"<code>SavingWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget allowing users to export and save a model.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>BaseConfig</code> <p>The configuration for the CAREamics algorithm.</p> required <code>careamist</code> <code>CAREamist</code> <pre><code>Instance of CAREamist.\n</code></pre> required <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal containing training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>class SavingWidget(QGroupBox):\n    \"\"\"A widget allowing users to export and save a model.\n\n    Parameters\n    ----------\n    careamics_config : BaseConfig\n        The configuration for the CAREamics algorithm.\n    careamist : CAREamist\n            Instance of CAREamist.\n    train_status : TrainingStatus or None, default=None\n        Signal containing training parameters.\n    \"\"\"\n\n    export_model = Signal(Path, str)\n\n    def __init__(\n        self,\n        careamics_config: BaseConfig,\n        # careamist: CAREamist | None = None,\n        train_status: TrainingStatus | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        careamics_config : BaseConfig\n            The configuration for the CAREamics algorithm.\n        careamist : CAREamist\n            Instance of CAREamist.\n        train_status : TrainingStatus or None, default=None\n            Signal containing training parameters.\n        \"\"\"\n        super().__init__()\n\n        self.configuration = careamics_config\n        # self.careamist = careamist\n        self.train_status = train_status\n\n        self.setTitle(\"Export\")\n\n        # format combobox\n        self.save_choice = QComboBox()\n        self.save_choice.addItems(ExportType.list())\n        self.save_choice.setToolTip(\"Output format\")\n\n        self.save_button = QPushButton(\"Export Model\")\n        self.save_button.setMinimumWidth(120)\n        self.save_button.setEnabled(False)\n        self.save_button.setToolTip(\"Save the model weights and configuration.\")\n\n        # layout\n        vbox = QVBoxLayout()\n        vbox.addWidget(self.save_choice)\n        vbox.addWidget(self.save_button, alignment=Qt.AlignLeft)  # type: ignore\n        self.setLayout(vbox)\n\n        # actions\n        if self.train_status is not None:\n            # updates from signals\n            self.train_status.events.state.connect(self._update_training_state)\n            # when clicking the save button\n            self.save_button.clicked.connect(self._save_model)\n\n    def _update_training_state(self, state: TrainingState) -&gt; None:\n        \"\"\"Update the widget state based on the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            Current training state.\n        \"\"\"\n        if state == TrainingState.DONE or state == TrainingState.STOPPED:\n            self.save_button.setEnabled(True)\n        elif state == TrainingState.IDLE:\n            self.save_button.setEnabled(False)\n\n    def _save_model(self) -&gt; None:\n        \"\"\"Ask user for the destination folder and export the model.\"\"\"\n        destination = QFileDialog.getExistingDirectory(caption=\"Export Model\")\n        if len(destination) &gt; 0:\n            destination = Path(destination)\n            export_type = self.save_choice.currentText()\n            # emit export\n            self.export_model.emit(destination, export_type)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget.__init__","title":"<code>__init__(careamics_config, train_status=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>BaseConfig</code> <p>The configuration for the CAREamics algorithm.</p> required <code>careamist</code> <code>CAREamist</code> <p>Instance of CAREamist.</p> required <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal containing training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>def __init__(\n    self,\n    careamics_config: BaseConfig,\n    # careamist: CAREamist | None = None,\n    train_status: TrainingStatus | None = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    careamics_config : BaseConfig\n        The configuration for the CAREamics algorithm.\n    careamist : CAREamist\n        Instance of CAREamist.\n    train_status : TrainingStatus or None, default=None\n        Signal containing training parameters.\n    \"\"\"\n    super().__init__()\n\n    self.configuration = careamics_config\n    # self.careamist = careamist\n    self.train_status = train_status\n\n    self.setTitle(\"Export\")\n\n    # format combobox\n    self.save_choice = QComboBox()\n    self.save_choice.addItems(ExportType.list())\n    self.save_choice.setToolTip(\"Output format\")\n\n    self.save_button = QPushButton(\"Export Model\")\n    self.save_button.setMinimumWidth(120)\n    self.save_button.setEnabled(False)\n    self.save_button.setToolTip(\"Save the model weights and configuration.\")\n\n    # layout\n    vbox = QVBoxLayout()\n    vbox.addWidget(self.save_choice)\n    vbox.addWidget(self.save_button, alignment=Qt.AlignLeft)  # type: ignore\n    self.setLayout(vbox)\n\n    # actions\n    if self.train_status is not None:\n        # updates from signals\n        self.train_status.events.state.connect(self._update_training_state)\n        # when clicking the save button\n        self.save_button.clicked.connect(self._save_model)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/","title":"scroll_wrapper","text":"<p>Wrap a widget in a scroll area.</p>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/#careamics_napari.widgets.scroll_wrapper.ScrollWidgetWrapper","title":"<code>ScrollWidgetWrapper</code>","text":"<p>               Bases: <code>QScrollArea</code></p> <p>Wrap a widget in a scroll area.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>QWidget</code> <p>Widget to wrap.</p> required Source code in <code>src/careamics_napari/widgets/scroll_wrapper.py</code> <pre><code>class ScrollWidgetWrapper(QScrollArea):\n    \"\"\"Wrap a widget in a scroll area.\n\n    Parameters\n    ----------\n    widget : QWidget\n        Widget to wrap.\n    \"\"\"\n\n    def __init__(self, widget: QWidget) -&gt; None:\n        \"\"\"Wrap a widget in a scroll area.\n\n        Parameters\n        ----------\n        widget : QWidget\n            Widget to wrap.\n        \"\"\"\n        super().__init__()\n        self.setVerticalScrollBarPolicy(\n            Qt.ScrollBarPolicy.ScrollBarAlwaysOn\n        )  # ScrollBarAsNeeded\n        self.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)\n        self.setWidgetResizable(True)\n        self.setWidget(widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/#careamics_napari.widgets.scroll_wrapper.ScrollWidgetWrapper.__init__","title":"<code>__init__(widget)</code>","text":"<p>Wrap a widget in a scroll area.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>QWidget</code> <p>Widget to wrap.</p> required Source code in <code>src/careamics_napari/widgets/scroll_wrapper.py</code> <pre><code>def __init__(self, widget: QWidget) -&gt; None:\n    \"\"\"Wrap a widget in a scroll area.\n\n    Parameters\n    ----------\n    widget : QWidget\n        Widget to wrap.\n    \"\"\"\n    super().__init__()\n    self.setVerticalScrollBarPolicy(\n        Qt.ScrollBarPolicy.ScrollBarAlwaysOn\n    )  # ScrollBarAsNeeded\n    self.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)\n    self.setWidgetResizable(True)\n    self.setWidget(widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/","title":"tbplot_widget","text":"<p>A widget displaying losses and a button to open TensorBoard in the browser.</p>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget","title":"<code>TBPlotWidget</code>","text":"<p>               Bases: <code>Container</code></p> <p>A widget displaying losses and a button to open TensorBoard in the browser.</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int or None</code> <p>Minimum width of the widget.</p> <code>None</code> <code>min_height</code> <code>int or None</code> <p>Minimum height of the widget.</p> <code>None</code> <code>max_width</code> <code>int or None</code> <p>Maximum width of the widget.</p> <code>None</code> <code>max_height</code> <code>int or None</code> <p>Maximum height of the widget.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>class TBPlotWidget(Container):\n    \"\"\"A widget displaying losses and a button to open TensorBoard in the browser.\n\n    Parameters\n    ----------\n    min_width : int or None, default=None\n        Minimum width of the widget.\n    min_height : int or None, default=None\n        Minimum height of the widget.\n    max_width : int or None, default=None\n        Maximum width of the widget.\n    max_height : int or None, default=None\n        Maximum height of the widget.\n    \"\"\"\n\n    # TODO what is this method used for?\n    def __setitem__(self: Self, key: Any, value: Any) -&gt; None:\n        \"\"\"Ignore set item.\n\n        Parameters\n        ----------\n        key : Any\n            Ignored.\n        value : Any\n            Ignored.\n        \"\"\"\n        pass\n\n    def __init__(\n        self: Self,\n        min_width: int | None = None,\n        min_height: int | None = None,\n        max_width: int | None = None,\n        max_height: int | None = None,\n        work_dir: Path | None = None,\n    ):\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        min_width : int or None, default=None\n            Minimum width of the widget.\n        min_height : int or None, default=None\n            Minimum height of the widget.\n        max_width : int or None, default=None\n            Maximum width of the widget.\n        max_height : int or None, default=None\n            Maximum height of the widget.\n        \"\"\"\n        super().__init__()\n\n        # self.train_signal = train_signal\n        self.work_dir = work_dir\n\n        if max_width:\n            self.native.setMaximumWidth(max_width)\n        if max_height:\n            self.native.setMaximumHeight(max_height)\n        if min_width:\n            self.native.setMinimumWidth(min_width)\n        if min_height:\n            self.native.setMinimumHeight(min_height)\n\n        self.graphics_widget = pg.GraphicsLayoutWidget()\n        self.graphics_widget.setBackground(None)\n        self.native.layout().addWidget(self.graphics_widget)\n\n        # plot widget\n        self.plot = self.graphics_widget.addPlot()\n        self.plot.setLabel(\"bottom\", \"epoch\")\n        self.plot.setLabel(\"left\", \"loss\")\n        self.plot.addLegend(offset=(125, -50))\n\n        # tensorboard button\n        tb_button = QPushButton(\"Open in TensorBoard\")\n        tb_button.setToolTip(\"Open TensorBoard in your browser\")\n        tb_button.setIcon(QIcon(QPixmap(ICON_TF)))\n        tb_button.setLayoutDirection(Qt.LeftToRight)  # type: ignore\n        tb_button.setIconSize(QSize(32, 29))\n        tb_button.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))\n        tb_button.clicked.connect(self.open_tb)\n\n        # add to layout on the bottom left\n        hbox = QHBoxLayout()\n        hbox.addWidget(tb_button)\n        hbox.addWidget(QLabel(\"\"))\n        button_widget = QWidget()\n        button_widget.setLayout(hbox)\n        self.native.layout().addWidget(button_widget)\n\n        # set empty references\n        self.epochs: list[int] = []\n        self.train_loss: list[float] = []\n        self.val_loss: list[float] = []\n        self.url: str | None = None\n        self.tb = None\n\n    def stop_tb(self: Self) -&gt; None:\n        \"\"\"Stop the TensorBoard process.\n\n        Currently not implemented.\n        \"\"\"\n        # haven't found any good way to stop the tb process, there's currently no API\n        # for it\n        pass\n\n    def open_tb(self: Self) -&gt; None:\n        \"\"\"Open TensorBoard in the browser.\"\"\"\n        if self.tb is None and self.work_dir is not None:\n            from tensorboard import program\n\n            self.tb = program.TensorBoard()\n\n            path = str(self.work_dir / \"logs\" / \"lightning_logs\")\n            self.tb.configure(argv=[None, \"--logdir\", path])  # type: ignore\n            self.url = self.tb.launch()  # type: ignore\n\n            if self.url is not None:\n                webbrowser.open(self.url)\n        else:\n            if self.url is not None:\n                webbrowser.open(self.url)\n\n    def update_plot(self: Self, epoch: int, train_loss: float, val_loss: float) -&gt; None:\n        \"\"\"Update the plot with new data.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n        train_loss : float\n            Training loss.\n        val_loss : float\n            Validation loss.\n        \"\"\"\n        # clear the plot\n        self.plot.clear()\n\n        # add the new points\n        self.epochs.append(epoch)\n        self.train_loss.append(train_loss)\n        self.val_loss.append(val_loss)\n\n        # replot\n        self.plot.plot(\n            self.epochs,\n            self.train_loss,\n            pen=pg.mkPen(color=(204, 221, 255)),\n            symbol=\"o\",\n            symbolSize=2,\n            name=\"Train\",\n        )\n        self.plot.plot(\n            self.epochs,\n            self.val_loss,\n            pen=pg.mkPen(color=(244, 173, 173)),\n            symbol=\"o\",\n            symbolSize=2,\n            name=\"Val\",\n        )\n\n    def clear_plot(self: Self) -&gt; None:\n        \"\"\"Clear the plot.\"\"\"\n        self.plot.clear()\n        self.epochs = []\n        self.train_loss = []\n        self.val_loss = []\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.__init__","title":"<code>__init__(min_width=None, min_height=None, max_width=None, max_height=None, work_dir=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int or None</code> <p>Minimum width of the widget.</p> <code>None</code> <code>min_height</code> <code>int or None</code> <p>Minimum height of the widget.</p> <code>None</code> <code>max_width</code> <code>int or None</code> <p>Maximum width of the widget.</p> <code>None</code> <code>max_height</code> <code>int or None</code> <p>Maximum height of the widget.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    min_width: int | None = None,\n    min_height: int | None = None,\n    max_width: int | None = None,\n    max_height: int | None = None,\n    work_dir: Path | None = None,\n):\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    min_width : int or None, default=None\n        Minimum width of the widget.\n    min_height : int or None, default=None\n        Minimum height of the widget.\n    max_width : int or None, default=None\n        Maximum width of the widget.\n    max_height : int or None, default=None\n        Maximum height of the widget.\n    \"\"\"\n    super().__init__()\n\n    # self.train_signal = train_signal\n    self.work_dir = work_dir\n\n    if max_width:\n        self.native.setMaximumWidth(max_width)\n    if max_height:\n        self.native.setMaximumHeight(max_height)\n    if min_width:\n        self.native.setMinimumWidth(min_width)\n    if min_height:\n        self.native.setMinimumHeight(min_height)\n\n    self.graphics_widget = pg.GraphicsLayoutWidget()\n    self.graphics_widget.setBackground(None)\n    self.native.layout().addWidget(self.graphics_widget)\n\n    # plot widget\n    self.plot = self.graphics_widget.addPlot()\n    self.plot.setLabel(\"bottom\", \"epoch\")\n    self.plot.setLabel(\"left\", \"loss\")\n    self.plot.addLegend(offset=(125, -50))\n\n    # tensorboard button\n    tb_button = QPushButton(\"Open in TensorBoard\")\n    tb_button.setToolTip(\"Open TensorBoard in your browser\")\n    tb_button.setIcon(QIcon(QPixmap(ICON_TF)))\n    tb_button.setLayoutDirection(Qt.LeftToRight)  # type: ignore\n    tb_button.setIconSize(QSize(32, 29))\n    tb_button.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))\n    tb_button.clicked.connect(self.open_tb)\n\n    # add to layout on the bottom left\n    hbox = QHBoxLayout()\n    hbox.addWidget(tb_button)\n    hbox.addWidget(QLabel(\"\"))\n    button_widget = QWidget()\n    button_widget.setLayout(hbox)\n    self.native.layout().addWidget(button_widget)\n\n    # set empty references\n    self.epochs: list[int] = []\n    self.train_loss: list[float] = []\n    self.val_loss: list[float] = []\n    self.url: str | None = None\n    self.tb = None\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Ignore set item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>Ignored.</p> required <code>value</code> <code>Any</code> <p>Ignored.</p> required Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def __setitem__(self: Self, key: Any, value: Any) -&gt; None:\n    \"\"\"Ignore set item.\n\n    Parameters\n    ----------\n    key : Any\n        Ignored.\n    value : Any\n        Ignored.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.clear_plot","title":"<code>clear_plot()</code>","text":"<p>Clear the plot.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def clear_plot(self: Self) -&gt; None:\n    \"\"\"Clear the plot.\"\"\"\n    self.plot.clear()\n    self.epochs = []\n    self.train_loss = []\n    self.val_loss = []\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.open_tb","title":"<code>open_tb()</code>","text":"<p>Open TensorBoard in the browser.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def open_tb(self: Self) -&gt; None:\n    \"\"\"Open TensorBoard in the browser.\"\"\"\n    if self.tb is None and self.work_dir is not None:\n        from tensorboard import program\n\n        self.tb = program.TensorBoard()\n\n        path = str(self.work_dir / \"logs\" / \"lightning_logs\")\n        self.tb.configure(argv=[None, \"--logdir\", path])  # type: ignore\n        self.url = self.tb.launch()  # type: ignore\n\n        if self.url is not None:\n            webbrowser.open(self.url)\n    else:\n        if self.url is not None:\n            webbrowser.open(self.url)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.stop_tb","title":"<code>stop_tb()</code>","text":"<p>Stop the TensorBoard process.</p> <p>Currently not implemented.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def stop_tb(self: Self) -&gt; None:\n    \"\"\"Stop the TensorBoard process.\n\n    Currently not implemented.\n    \"\"\"\n    # haven't found any good way to stop the tb process, there's currently no API\n    # for it\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.update_plot","title":"<code>update_plot(epoch, train_loss, val_loss)</code>","text":"<p>Update the plot with new data.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required <code>train_loss</code> <code>float</code> <p>Training loss.</p> required <code>val_loss</code> <code>float</code> <p>Validation loss.</p> required Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def update_plot(self: Self, epoch: int, train_loss: float, val_loss: float) -&gt; None:\n    \"\"\"Update the plot with new data.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n    train_loss : float\n        Training loss.\n    val_loss : float\n        Validation loss.\n    \"\"\"\n    # clear the plot\n    self.plot.clear()\n\n    # add the new points\n    self.epochs.append(epoch)\n    self.train_loss.append(train_loss)\n    self.val_loss.append(val_loss)\n\n    # replot\n    self.plot.plot(\n        self.epochs,\n        self.train_loss,\n        pen=pg.mkPen(color=(204, 221, 255)),\n        symbol=\"o\",\n        symbolSize=2,\n        name=\"Train\",\n    )\n    self.plot.plot(\n        self.epochs,\n        self.val_loss,\n        pen=pg.mkPen(color=(244, 173, 173)),\n        symbol=\"o\",\n        symbolSize=2,\n        name=\"Val\",\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/","title":"train_data_widget","text":"<p>A widget allowing users to select data source for the training.</p>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget","title":"<code>TrainDataWidget</code>","text":"<p>               Bases: <code>QTabWidget</code></p> <p>A widget offering to select layers from napari or paths from disk.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <pre><code>careamics configuration object.\n</code></pre> required <code>use_target</code> <code>bool</code> <p>Whether to target fields.</p> <code>False</code> Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>class TrainDataWidget(QTabWidget):\n    \"\"\"A widget offering to select layers from napari or paths from disk.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n            careamics configuration object.\n    use_target : bool, default=False\n        Whether to target fields.\n    \"\"\"\n\n    def __init__(\n        self,\n        careamics_config: BaseConfig,\n        use_target: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        careamics_config : Configuration\n            careamics configuration object.\n        use_target : bool, default=False\n            Whether to target fields.\n        \"\"\"\n        super().__init__()\n        self.configuration = careamics_config\n        self.use_target = use_target\n\n        # QTabs\n        layer_tab = QWidget()\n        disk_tab = QWidget()\n\n        # add tabs\n        _tab_idx = 0\n        if _has_napari and napari.current_viewer() is not None:\n            # tab for selecting data from napari layers\n            self.addTab(layer_tab, \"From layers\")\n            self.setTabToolTip(_tab_idx, \"Use images from napari layers\")\n            # add tab contents\n            self._set_layer_tab(layer_tab)\n            _tab_idx += 1\n        # tab for selecting data from disk\n        self.addTab(disk_tab, \"From disk\")\n        self.setTabToolTip(_tab_idx, \"Use patches saved on the disk\")\n        self._set_disk_tab(disk_tab)\n\n    def get_data_sources(self) -&gt; dict[str, list] | None:\n        \"\"\"Get the selected data sources.\"\"\"\n        if (\n            self.img_train.value is None  # type: ignore\n            and len(self.train_images_folder.get_folder()) == 0\n        ):\n            # no training data has been selected\n            return None\n\n        if self.currentIndex() == 0:\n            # data is expected from napari layers\n            train_data = [self.img_train.value.data]  # type: ignore\n            val_data = [self.img_val.value.data]  # type: ignore\n            if self.use_target:\n                train_data.append(self.target_train.value.data)  # type: ignore\n                val_data.append(self.target_val.value.data)  # type: ignore\n\n        else:\n            # data is expected from disk\n            train_data = [self.train_images_folder.get_folder()]\n            val_data = [self.val_images_folder.get_folder()]\n            if self.use_target:\n                train_data.append(self.train_target_folder.get_folder())\n                val_data.append(self.val_target_folder.get_folder())\n\n        return {\"train\": train_data, \"val\": val_data}\n\n    def _set_layer_tab(self, layer_tab: QWidget) -&gt; None:\n        \"\"\"Set up the layer tab.\n\n        Parameters\n        ----------\n        layer_tab : QWidget\n            Layer tab widget.\n        \"\"\"\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n        form.setContentsMargins(12, 12, 0, 0)\n\n        self.img_train = layer_choice()\n        self.img_train.native.setToolTip(\"Select the training layer.\")\n\n        self.img_val = layer_choice()\n        self.img_train.native.setToolTip(\"Select the validation layer.\")\n\n        form.addRow(\"Train\", self.img_train.native)\n        form.addRow(\"Val\", self.img_val.native)\n\n        if self.use_target:\n            # get the target layers\n            self.target_train = layer_choice()\n            self.target_val = layer_choice()\n\n            # tool tips\n            self.target_train.native.setToolTip(\"Select a training target layer.\")\n            self.target_val.native.setToolTip(\"Select a validation target layer.\")\n\n            form.addRow(\"Train target\", self.target_train.native)\n            form.addRow(\"Val target\", self.target_val.native)\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(form)\n        layer_tab.setLayout(vbox)\n\n    def _set_disk_tab(self, disk_tab: QWidget) -&gt; None:\n        \"\"\"Set up the disk tab.\n\n        Parameters\n        ----------\n        disk_tab : QWidget\n            Disk tab widget.\n        \"\"\"\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n        form.setContentsMargins(12, 12, 0, 0)\n\n        self.train_images_folder = FolderWidget(\"Choose\")\n        self.val_images_folder = FolderWidget(\"Choose\")\n        form.addRow(\"Train\", self.train_images_folder)\n        form.addRow(\"Val\", self.val_images_folder)\n\n        if self.use_target:\n            self.train_target_folder = FolderWidget(\"Choose\")\n            self.val_target_folder = FolderWidget(\"Choose\")\n\n            form.addRow(\"Train target\", self.train_target_folder)\n            form.addRow(\"Val target\", self.val_target_folder)\n\n            self.train_target_folder.setToolTip(\n                \"Select a folder containing the training\\ntarget.\"\n            )\n            self.val_target_folder.setToolTip(\n                \"Select a folder containing the validation\\ntarget.\"\n            )\n            self.train_images_folder.setToolTip(\n                \"Select a folder containing the training\\nimages.\"\n            )\n            self.val_images_folder.setToolTip(\n                \"Select a folder containing the validation\\nimages.\"\n            )\n\n        else:\n            self.train_images_folder.setToolTip(\n                \"Select a folder containing the training\\nimages.\"\n            )\n            self.val_images_folder.setToolTip(\n                \"Select a folder containing the validation\\n\"\n                \"images, if you select the same folder as\\n\"\n                \"for training, the validation patches will\\n\"\n                \"be extracted from the training data.\"\n            )\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(form)\n        disk_tab.setLayout(vbox)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget.__init__","title":"<code>__init__(careamics_config, use_target=False)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <p>careamics configuration object.</p> required <code>use_target</code> <code>bool</code> <p>Whether to target fields.</p> <code>False</code> Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def __init__(\n    self,\n    careamics_config: BaseConfig,\n    use_target: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n        careamics configuration object.\n    use_target : bool, default=False\n        Whether to target fields.\n    \"\"\"\n    super().__init__()\n    self.configuration = careamics_config\n    self.use_target = use_target\n\n    # QTabs\n    layer_tab = QWidget()\n    disk_tab = QWidget()\n\n    # add tabs\n    _tab_idx = 0\n    if _has_napari and napari.current_viewer() is not None:\n        # tab for selecting data from napari layers\n        self.addTab(layer_tab, \"From layers\")\n        self.setTabToolTip(_tab_idx, \"Use images from napari layers\")\n        # add tab contents\n        self._set_layer_tab(layer_tab)\n        _tab_idx += 1\n    # tab for selecting data from disk\n    self.addTab(disk_tab, \"From disk\")\n    self.setTabToolTip(_tab_idx, \"Use patches saved on the disk\")\n    self._set_disk_tab(disk_tab)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget.get_data_sources","title":"<code>get_data_sources()</code>","text":"<p>Get the selected data sources.</p> Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def get_data_sources(self) -&gt; dict[str, list] | None:\n    \"\"\"Get the selected data sources.\"\"\"\n    if (\n        self.img_train.value is None  # type: ignore\n        and len(self.train_images_folder.get_folder()) == 0\n    ):\n        # no training data has been selected\n        return None\n\n    if self.currentIndex() == 0:\n        # data is expected from napari layers\n        train_data = [self.img_train.value.data]  # type: ignore\n        val_data = [self.img_val.value.data]  # type: ignore\n        if self.use_target:\n            train_data.append(self.target_train.value.data)  # type: ignore\n            val_data.append(self.target_val.value.data)  # type: ignore\n\n    else:\n        # data is expected from disk\n        train_data = [self.train_images_folder.get_folder()]\n        val_data = [self.val_images_folder.get_folder()]\n        if self.use_target:\n            train_data.append(self.train_target_folder.get_folder())\n            val_data.append(self.val_target_folder.get_folder())\n\n    return {\"train\": train_data, \"val\": val_data}\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/","title":"train_progress_widget","text":"<p>A widget displaying the training progress using two progress bars.</p>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget","title":"<code>TrainProgressWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget displaying the training progress using two progress bars.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <pre><code>careamics configuration object.\n</code></pre> required <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal representing the training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>class TrainProgressWidget(QGroupBox):\n    \"\"\"A widget displaying the training progress using two progress bars.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n            careamics configuration object.\n    train_status : TrainingStatus or None, default=None\n        Signal representing the training status.\n    \"\"\"\n\n    def __init__(\n        self,\n        careamics_config: BaseConfig,\n        train_status: TrainingStatus | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        careamics_config : Configuration\n            careamics configuration object.\n        train_status : TrainingStatus or None, default=None\n            Signal representing the training status.\n        \"\"\"\n        super().__init__()\n\n        self.configuration = careamics_config\n        self.train_status = (\n            train_status\n            if train_status is not None  # for typing purposes\n            else TrainingStatus()  # type: ignore\n        )\n\n        self.setTitle(\"Training Progress\")\n        layout = QVBoxLayout()\n        layout.setContentsMargins(20, 20, 20, 0)\n\n        # progress bars\n        self.pb_epochs = create_progressbar(\n            max_value=self.train_status.max_epochs,\n            text_format=f\"Epoch ?/{self.train_status.max_epochs}\",\n            value=0,\n        )\n\n        self.pb_batch = create_progressbar(\n            max_value=self.train_status.max_batches,\n            text_format=f\"Batch ?/{self.train_status.max_batches}\",\n            value=0,\n        )\n\n        # plot widget\n        self.plot = TBPlotWidget(\n            max_width=300,\n            max_height=300,\n            min_height=250,\n            work_dir=self.configuration.work_dir,\n        )\n\n        layout.addWidget(self.pb_epochs)\n        layout.addWidget(self.pb_batch)\n        layout.addWidget(self.plot.native)\n        self.setLayout(layout)\n\n        # set actions based on the training status\n        self.train_status.events.state.connect(self._update_training_state)\n        self.train_status.events.epoch_idx.connect(self._update_epoch)\n        self.train_status.events.max_epochs.connect(self._update_max_epoch)\n        self.train_status.events.batch_idx.connect(self._update_batch)\n        self.train_status.events.max_batches.connect(self._update_max_batch)\n        self.train_status.events.val_loss.connect(self._update_loss)\n\n    def _update_training_state(self, state: TrainingState) -&gt; None:\n        \"\"\"Update the widget according to the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            Training state.\n        \"\"\"\n        if state == TrainingState.IDLE or state == TrainingState.TRAINING:\n            self.plot.clear_plot()\n\n    def _update_max_epoch(self, max_epoch: int):\n        \"\"\"Update the maximum number of epochs in the progress bar.\n\n        Parameters\n        ----------\n        max_epoch : int\n            Maximum number of epochs.\n        \"\"\"\n        self.pb_epochs.setMaximum(max_epoch)\n\n    def _update_epoch(self, epoch: int) -&gt; None:\n        \"\"\"Update the epoch progress bar.\n\n        Parameters\n        ----------\n        epoch : int\n            Current epoch.\n        \"\"\"\n        self.pb_epochs.setValue(epoch + 1)\n        self.pb_epochs.setFormat(f\"Epoch {epoch + 1}/{self.train_status.max_epochs}\")\n\n    def _update_max_batch(self, max_batches: int) -&gt; None:\n        \"\"\"Update the maximum number of batches in the progress bar.\n\n        Parameters\n        ----------\n        max_batches : int\n            Maximum number of batches.\n        \"\"\"\n        self.pb_batch.setMaximum(max_batches)\n\n    def _update_batch(self) -&gt; None:\n        \"\"\"Update the batch progress bar.\"\"\"\n        self.pb_batch.setValue(self.train_status.batch_idx + 1)\n        self.pb_batch.setFormat(\n            f\"Batch {self.train_status.batch_idx + 1}/{self.train_status.max_batches}\"\n        )\n\n    def _update_loss(self) -&gt; None:\n        \"\"\"Update the loss plot.\"\"\"\n        self.plot.update_plot(\n            epoch=self.train_status.epoch_idx,\n            train_loss=self.train_status.loss,\n            val_loss=self.train_status.val_loss,\n        )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget.__init__","title":"<code>__init__(careamics_config, train_status=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <p>careamics configuration object.</p> required <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal representing the training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def __init__(\n    self,\n    careamics_config: BaseConfig,\n    train_status: TrainingStatus | None = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n        careamics configuration object.\n    train_status : TrainingStatus or None, default=None\n        Signal representing the training status.\n    \"\"\"\n    super().__init__()\n\n    self.configuration = careamics_config\n    self.train_status = (\n        train_status\n        if train_status is not None  # for typing purposes\n        else TrainingStatus()  # type: ignore\n    )\n\n    self.setTitle(\"Training Progress\")\n    layout = QVBoxLayout()\n    layout.setContentsMargins(20, 20, 20, 0)\n\n    # progress bars\n    self.pb_epochs = create_progressbar(\n        max_value=self.train_status.max_epochs,\n        text_format=f\"Epoch ?/{self.train_status.max_epochs}\",\n        value=0,\n    )\n\n    self.pb_batch = create_progressbar(\n        max_value=self.train_status.max_batches,\n        text_format=f\"Batch ?/{self.train_status.max_batches}\",\n        value=0,\n    )\n\n    # plot widget\n    self.plot = TBPlotWidget(\n        max_width=300,\n        max_height=300,\n        min_height=250,\n        work_dir=self.configuration.work_dir,\n    )\n\n    layout.addWidget(self.pb_epochs)\n    layout.addWidget(self.pb_batch)\n    layout.addWidget(self.plot.native)\n    self.setLayout(layout)\n\n    # set actions based on the training status\n    self.train_status.events.state.connect(self._update_training_state)\n    self.train_status.events.epoch_idx.connect(self._update_epoch)\n    self.train_status.events.max_epochs.connect(self._update_max_epoch)\n    self.train_status.events.batch_idx.connect(self._update_batch)\n    self.train_status.events.max_batches.connect(self._update_max_batch)\n    self.train_status.events.val_loss.connect(self._update_loss)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/","title":"training_configuration_widget","text":"<p>A widget allowing the creation of a CAREamics configuration.</p>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget","title":"<code>ConfigurationWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget allowing the creation of a CAREamics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <p>careamics configuration object.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>class ConfigurationWidget(QGroupBox):\n    \"\"\"A widget allowing the creation of a CAREamics configuration.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n        careamics configuration object.\n    \"\"\"\n\n    # signal to show algorithm advanced configuration window.\n    show_advanced_config = Signal()\n\n    def __init__(self, careamics_config: BaseConfig) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        careamics_config : Configuration\n            careamics configuration object.\n        \"\"\"\n        super().__init__()\n\n        self.configuration = careamics_config\n\n        self.setTitle(\"Training Parameters\")\n        self.setMinimumWidth(200)\n\n        # advanced settings\n        icon = QtGui.QIcon(ICON_GEAR)\n        self.training_expert_btn = QPushButton(icon, \"\")\n        self.training_expert_btn.setFixedSize(35, 35)\n        self.training_expert_btn.setToolTip(\"Open the advanced settings window.\")\n        self.training_expert_btn.clicked.connect(lambda: self.show_advanced_config.emit())\n\n        # 3D checkbox\n        self.enable_3d_chkbox = QCheckBox()\n        self.enable_3d_chkbox.setToolTip(\"Use a 3D network\")\n        self.enable_3d_chkbox.clicked.connect(self._enable_3d_changed)\n\n        # axes\n        self.axes_widget = AxesWidget(careamics_config=self.configuration)\n\n        # number of epochs\n        _n_epochs = 30\n        if self.configuration.training_config.lightning_trainer_config is not None:\n            _n_epochs = self.configuration.training_config.lightning_trainer_config[\n                \"max_epochs\"\n            ]\n        self.n_epochs_spin = create_int_spinbox(\n            1, 1000, _n_epochs, tooltip=\"Number of epochs\"\n        )\n\n        # batch size\n        self.batch_size_spin = create_int_spinbox(1, 512, 16, 1)\n        self.batch_size_spin.setToolTip(\n            \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n        )\n\n        # patch size XY\n        self.patch_xy_spin = PowerOfTwoSpinBox(16, 512, 64)\n        self.patch_xy_spin.setToolTip(\"Dimension of the patches in XY.\")\n        # patch size Z\n        self.patch_z_spin = PowerOfTwoSpinBox(8, 512, 8)\n        self.patch_z_spin.setToolTip(\"Dimension of the patches in Z.\")\n        self.patch_z_spin.setEnabled(self.configuration.is_3D)\n\n        # layout\n        formLayout = QFormLayout()\n        formLayout.setContentsMargins(0, 0, 0, 0)\n        formLayout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n        formLayout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n        formLayout.addRow(\"Enable 3D\", self.enable_3d_chkbox)\n        formLayout.addRow(self.axes_widget.label.text(), self.axes_widget.text_field)\n        formLayout.addRow(\"# Epochs\", self.n_epochs_spin)\n        formLayout.addRow(\"Batch size\", self.batch_size_spin)\n        formLayout.addRow(\"Patch XY\", self.patch_xy_spin)\n        formLayout.addRow(\"Patch Z\", self.patch_z_spin)\n        formLayout.minimumSize()\n\n        vbox = QVBoxLayout()\n        vbox.setContentsMargins(5, 20, 5, 10)\n        vbox.addWidget(\n            self.training_expert_btn,\n            alignment=Qt.AlignRight | Qt.AlignVCenter,  # type: ignore\n        )\n        vbox.addLayout(formLayout)\n        self.setLayout(vbox)\n\n        # create and bind properties to ui\n        self._bind_properties()\n\n    def update_config(self) -&gt; None:\n        \"\"\"Update the configuration from the UI element.\"\"\"\n        # update config axes (from axes widget)\n        self.axes_widget.update_config()\n        # is 3D\n        self.configuration.is_3D = self.is_3D\n\n        # num epochs\n        if self.configuration.training_config.lightning_trainer_config is not None:\n            self.configuration.training_config.lightning_trainer_config[\"max_epochs\"] = (\n                self.num_epochs\n            )\n\n        if isinstance(self.configuration.data_config, DataConfig):\n            # batch size\n            self.configuration.data_config.batch_size = self.batch_size\n            # patch size\n            _patch_size = [self.patch_xy_size, self.patch_xy_size]\n            if self.is_3D:\n                _patch_size.insert(0, self.patch_z_size)\n            self.configuration.data_config.patch_size = _patch_size\n            self.configuration.set_3D(\n                self.is_3D, self.configuration.data_config.axes, _patch_size\n            )  # maybe not necessary, but let's have it to be sure.\n\n    def _enable_3d_changed(self, state: bool) -&gt; None:\n        \"\"\"Update the signal 3D state.\n\n        Parameters\n        ----------\n        state : bool\n            3D state.\n        \"\"\"\n        self.patch_z_spin.setEnabled(state)\n\n    def _bind_properties(self) -&gt; None:\n        \"\"\"Create and bind the properties to the UI elements.\"\"\"\n        # type(self) returns the class of the instance, so we are adding\n        # properties to the class itself, not the instance.\n        # is 3D\n        type(self).is_3D = bind(self.enable_3d_chkbox, \"checked\")\n        # number of epochs\n        if self.configuration.training_config.lightning_trainer_config is not None:\n            type(self).num_epochs = bind(self.n_epochs_spin, \"value\")\n\n        if isinstance(self.configuration.data_config, DataConfig):\n            # batch size\n            type(self).batch_size = bind(self.batch_size_spin, \"value\")\n            # XY patch size\n            type(self).patch_xy_size = bind(self.patch_xy_spin, \"value\")\n            # Z patch size\n            type(self).patch_z_size = bind(self.patch_z_spin, \"value\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget.__init__","title":"<code>__init__(careamics_config)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>careamics_config</code> <code>Configuration</code> <p>careamics configuration object.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def __init__(self, careamics_config: BaseConfig) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    careamics_config : Configuration\n        careamics configuration object.\n    \"\"\"\n    super().__init__()\n\n    self.configuration = careamics_config\n\n    self.setTitle(\"Training Parameters\")\n    self.setMinimumWidth(200)\n\n    # advanced settings\n    icon = QtGui.QIcon(ICON_GEAR)\n    self.training_expert_btn = QPushButton(icon, \"\")\n    self.training_expert_btn.setFixedSize(35, 35)\n    self.training_expert_btn.setToolTip(\"Open the advanced settings window.\")\n    self.training_expert_btn.clicked.connect(lambda: self.show_advanced_config.emit())\n\n    # 3D checkbox\n    self.enable_3d_chkbox = QCheckBox()\n    self.enable_3d_chkbox.setToolTip(\"Use a 3D network\")\n    self.enable_3d_chkbox.clicked.connect(self._enable_3d_changed)\n\n    # axes\n    self.axes_widget = AxesWidget(careamics_config=self.configuration)\n\n    # number of epochs\n    _n_epochs = 30\n    if self.configuration.training_config.lightning_trainer_config is not None:\n        _n_epochs = self.configuration.training_config.lightning_trainer_config[\n            \"max_epochs\"\n        ]\n    self.n_epochs_spin = create_int_spinbox(\n        1, 1000, _n_epochs, tooltip=\"Number of epochs\"\n    )\n\n    # batch size\n    self.batch_size_spin = create_int_spinbox(1, 512, 16, 1)\n    self.batch_size_spin.setToolTip(\n        \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n    )\n\n    # patch size XY\n    self.patch_xy_spin = PowerOfTwoSpinBox(16, 512, 64)\n    self.patch_xy_spin.setToolTip(\"Dimension of the patches in XY.\")\n    # patch size Z\n    self.patch_z_spin = PowerOfTwoSpinBox(8, 512, 8)\n    self.patch_z_spin.setToolTip(\"Dimension of the patches in Z.\")\n    self.patch_z_spin.setEnabled(self.configuration.is_3D)\n\n    # layout\n    formLayout = QFormLayout()\n    formLayout.setContentsMargins(0, 0, 0, 0)\n    formLayout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)  # type: ignore\n    formLayout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)  # type: ignore\n    formLayout.addRow(\"Enable 3D\", self.enable_3d_chkbox)\n    formLayout.addRow(self.axes_widget.label.text(), self.axes_widget.text_field)\n    formLayout.addRow(\"# Epochs\", self.n_epochs_spin)\n    formLayout.addRow(\"Batch size\", self.batch_size_spin)\n    formLayout.addRow(\"Patch XY\", self.patch_xy_spin)\n    formLayout.addRow(\"Patch Z\", self.patch_z_spin)\n    formLayout.minimumSize()\n\n    vbox = QVBoxLayout()\n    vbox.setContentsMargins(5, 20, 5, 10)\n    vbox.addWidget(\n        self.training_expert_btn,\n        alignment=Qt.AlignRight | Qt.AlignVCenter,  # type: ignore\n    )\n    vbox.addLayout(formLayout)\n    self.setLayout(vbox)\n\n    # create and bind properties to ui\n    self._bind_properties()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget.update_config","title":"<code>update_config()</code>","text":"<p>Update the configuration from the UI element.</p> Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def update_config(self) -&gt; None:\n    \"\"\"Update the configuration from the UI element.\"\"\"\n    # update config axes (from axes widget)\n    self.axes_widget.update_config()\n    # is 3D\n    self.configuration.is_3D = self.is_3D\n\n    # num epochs\n    if self.configuration.training_config.lightning_trainer_config is not None:\n        self.configuration.training_config.lightning_trainer_config[\"max_epochs\"] = (\n            self.num_epochs\n        )\n\n    if isinstance(self.configuration.data_config, DataConfig):\n        # batch size\n        self.configuration.data_config.batch_size = self.batch_size\n        # patch size\n        _patch_size = [self.patch_xy_size, self.patch_xy_size]\n        if self.is_3D:\n            _patch_size.insert(0, self.patch_z_size)\n        self.configuration.data_config.patch_size = _patch_size\n        self.configuration.set_3D(\n            self.is_3D, self.configuration.data_config.axes, _patch_size\n        )  # maybe not necessary, but let's have it to be sure.\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/","title":"training_widget","text":"<p>Training widget.</p>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget","title":"<code>TrainingWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>Training widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>class TrainingWidget(QGroupBox):\n    \"\"\"Training widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Training status.\n    \"\"\"\n\n    def __init__(self, train_status: TrainingStatus | None = None) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        train_status : TrainingStatus or None, default=None\n            Training status.\n        \"\"\"\n        super().__init__()\n\n        self.train_status = (\n            TrainingStatus() if train_status is None else train_status  # type: ignore\n        )\n\n        # TODO add val percentage and val minimum ?\n\n        # train button\n        self.train_button = QPushButton(\"Train\", self)\n        self.train_button.setMinimumWidth(120)\n        self.train_button.clicked.connect(self._train_stop_clicked)\n\n        # reset button\n        self.reset_model_button = QPushButton(\"Reset\", self)\n        self.reset_model_button.setMinimumWidth(120)\n        self.reset_model_button.setEnabled(False)\n        self.reset_model_button.setToolTip(\n            \"Reset the weights of the model (forget the training)\"\n        )\n        self.reset_model_button.clicked.connect(self._reset_clicked)\n\n        hbox = QHBoxLayout()\n        hbox.addWidget(self.train_button, alignment=Qt.AlignLeft)  # type: ignore\n        hbox.addWidget(self.reset_model_button, alignment=Qt.AlignLeft)  # type: ignore\n\n        vbox = QVBoxLayout()\n        vbox.addLayout(hbox)\n        self.setLayout(vbox)\n        self.setTitle(\"Train\")\n\n        # listening to the training status events\n        self.train_status.events.state.connect(self._update_button)\n\n    def _train_stop_clicked(self) -&gt; None:\n        \"\"\"Update the UI and training status when the train button is clicked.\"\"\"\n        if self.train_status is not None:\n            if (\n                self.train_status.state == TrainingState.IDLE\n                or self.train_status.state == TrainingState.DONE\n            ):\n                # important to do it before state change\n                self.train_button.setText(\"Stop\")\n                self.reset_model_button.setEnabled(False)\n                self.train_status.state = TrainingState.TRAINING\n\n            elif self.train_status.state == TrainingState.TRAINING:\n                self.train_button.setText(\"Train\")\n                self.reset_model_button.setEnabled(True)\n                self.train_status.state = TrainingState.STOPPED\n\n            elif self.train_status.state == TrainingState.STOPPED:\n                self.train_button.setText(\"Stop\")\n                self.train_status.state = TrainingState.TRAINING\n\n    def _reset_clicked(self) -&gt; None:\n        \"\"\"Update the UI and training status when the reset button is clicked.\"\"\"\n        if self.train_status is not None:\n            if self.train_status.state != TrainingState.TRAINING:\n                self.train_status.state = TrainingState.IDLE\n                self.train_button.setText(\"Train\")\n                self.reset_model_button.setEnabled(False)\n\n    def _update_button(self, new_state: TrainingState) -&gt; None:\n        \"\"\"Update the button text based on the training state.\n\n        Parameters\n        ----------\n        new_state : TrainingState\n            New training state.\n        \"\"\"\n        if new_state == TrainingState.DONE or new_state == TrainingState.STOPPED:\n            self.train_button.setText(\"Train\")\n            self.reset_model_button.setEnabled(True)\n        elif new_state == TrainingState.CRASHED:\n            self._reset_clicked()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget.__init__","title":"<code>__init__(train_status=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>def __init__(self, train_status: TrainingStatus | None = None) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Training status.\n    \"\"\"\n    super().__init__()\n\n    self.train_status = (\n        TrainingStatus() if train_status is None else train_status  # type: ignore\n    )\n\n    # TODO add val percentage and val minimum ?\n\n    # train button\n    self.train_button = QPushButton(\"Train\", self)\n    self.train_button.setMinimumWidth(120)\n    self.train_button.clicked.connect(self._train_stop_clicked)\n\n    # reset button\n    self.reset_model_button = QPushButton(\"Reset\", self)\n    self.reset_model_button.setMinimumWidth(120)\n    self.reset_model_button.setEnabled(False)\n    self.reset_model_button.setToolTip(\n        \"Reset the weights of the model (forget the training)\"\n    )\n    self.reset_model_button.clicked.connect(self._reset_clicked)\n\n    hbox = QHBoxLayout()\n    hbox.addWidget(self.train_button, alignment=Qt.AlignLeft)  # type: ignore\n    hbox.addWidget(self.reset_model_button, alignment=Qt.AlignLeft)  # type: ignore\n\n    vbox = QVBoxLayout()\n    vbox.addLayout(hbox)\n    self.setLayout(vbox)\n    self.setTitle(\"Train\")\n\n    # listening to the training status events\n    self.train_status.events.state.connect(self._update_button)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/utils/","title":"utils","text":""},{"location":"reference/careamics_napari/widgets/utils/#careamics_napari.widgets.utils.Win","title":"<code>Win</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>docstring for Win</p> Source code in <code>src/careamics_napari/widgets/utils.py</code> <pre><code>class Win(QWidget):\n    \"\"\"docstring for Win\"\"\" \"\"\n\n    def __init__(self):\n        super().__init__()\n        line_edit = QLineEdit()\n        # line_edit.setText(config.data_config.axes)\n        btn = QPushButton(\"Test\")\n        btn.clicked.connect(lambda: print(self.axes))\n        vbox = QVBoxLayout()\n        vbox.addWidget(line_edit)\n        vbox.addWidget(btn)\n        self.setLayout(vbox)\n\n        type(self).axes = bind(\n            line_edit,\n            \"text\",\n            default_value=config.data_config.axes,  # type: ignore\n            validation_fn=are_axes_valid,\n        )\n        self.axes = config.data_config.axes  # type: ignore\n</code></pre>"},{"location":"reference/careamics_napari/widgets/utils/#careamics_napari.widgets.utils.bind","title":"<code>bind(widget, prop_name, default_value=None, validation_fn=None)</code>","text":"<p>Returns a property bound to the given widget.</p> <p>This can be used as a general way to bind a widget property to a class property. So that when the class property is accessed, it gets the value from the widget, and when the class property is set, it sets the value in the widget. In this way, we don't need to watch for widget's value changed signals to update the class attributes.</p> <p>Returns:</p> Type Description <code>    property: A property</code> Source code in <code>src/careamics_napari/widgets/utils.py</code> <pre><code>def bind(\n    widget: QWidget,\n    prop_name: str,\n    default_value: Any | None = None,\n    validation_fn: Callable | None = None,\n) -&gt; property:\n    \"\"\"Returns a property bound to the given widget.\n\n    This can be used as a general way to bind a widget property to a class property.\n    So that when the class property is accessed, it gets the value from the widget,\n    and when the class property is set, it sets the value in the widget.\n    In this way, we don't need to watch for widget's value changed signals\n    to update the class attributes.\n\n    Parameters\n    ----------\n        widget: QWidget\n            The widget whose property we want to bind to\n        prop_name: str\n            The name of the property in the widget (e.g. text)\n        default_value: Any (optional)\n            The default value to be used when the widget value is not valid.\n            Defaults to None.\n        validation_fn: Callable (optional)\n        The validation function to check if the widget value is valid\n        (must return a boolean). Defaults to None.\n\n    Returns\n    -------\n        property: A property\n    \"\"\"\n\n    def getter(self):\n        ui_value = widget.property(prop_name)\n        if validation_fn:\n            if not validation_fn(ui_value):\n                print(f\"Invalid input: {ui_value}\")\n                ui_value = default_value\n\n        return ui_value\n\n    def setter(self, value):\n        widget.setProperty(prop_name, value)\n\n    return property(fget=getter, fset=setter)\n</code></pre>"},{"location":"reference/careamics_napari/workers/prediction_worker/","title":"prediction_worker","text":"<p>A thread worker function running CAREamics prediction.</p>"},{"location":"reference/careamics_napari/workers/prediction_worker/#careamics_napari.workers.prediction_worker.predict_worker","title":"<code>predict_worker(careamist, pred_data, configuration, update_queue)</code>","text":"<p>Model prediction worker.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamist instance.</p> required <code>pred_data</code> <code>NDArray | str</code> <p>Prediction data source.</p> required <code>configuration</code> <code>BaseConfig</code> <p>careamics configuration.</p> required <code>update_queue</code> <code>Queue</code> <p>Queue used to send updates to the UI.</p> required <p>Yields:</p> Type Description <code>Generator[PredictionUpdate, None, None]</code> <p>Updates.</p> Source code in <code>src/careamics_napari/workers/prediction_worker.py</code> <pre><code>@thread_worker\ndef predict_worker(\n    careamist: CAREamist,\n    pred_data: NDArray | str,\n    configuration: BaseConfig,\n    update_queue: Queue,\n) -&gt; Generator[PredictionUpdate, None, None]:\n    \"\"\"Model prediction worker.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamist instance.\n    pred_data : NDArray | str\n        Prediction data source.\n    configuration : BaseConfig\n        careamics configuration.\n    update_queue : Queue\n        Queue used to send updates to the UI.\n\n    Yields\n    ------\n    Generator[PredictionUpdate, None, None]\n        Updates.\n    \"\"\"\n    # start prediction thread\n    prediction = Thread(\n        target=_predict,\n        args=(\n            careamist,\n            pred_data,\n            configuration,\n            update_queue,\n        ),\n    )\n    prediction.start()\n\n    # look for updates\n    while True:\n        update: PredictionUpdate = update_queue.get(block=True)\n\n        yield update\n\n        if (\n            update.type == PredictionUpdateType.STATE\n            or update.type == PredictionUpdateType.EXCEPTION\n        ):\n            break\n</code></pre>"},{"location":"reference/careamics_napari/workers/training_worker/","title":"training_worker","text":"<p>A thread worker function running CAREamics training.</p>"},{"location":"reference/careamics_napari/workers/training_worker/#careamics_napari.workers.training_worker.train_worker","title":"<code>train_worker(configuration, data_sources, training_queue, predict_queue, careamist=None, pred_status=None)</code>","text":"<p>Model training worker.</p> <p>Parameters:</p> Name Type Description Default <code>configuration</code> <code>BaseConfig</code> <p>careamics configuration.</p> required <code>data_sources</code> <code>dict[str, list]</code> <p>Train and validation data sources.</p> required <code>training_queue</code> <code>Queue</code> <p>Training update queue.</p> required <code>predict_queue</code> <code>Queue</code> <p>Prediction update queue.</p> required <code>careamist</code> <code>CAREamist or None</code> <p>CAREamist instance.</p> <code>None</code> <code>pred_status</code> <code>PredictionStatus or None</code> <p>Prediction status for stop callback.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[TrainUpdate, None, None]</code> <p>Updates.</p> Source code in <code>src/careamics_napari/workers/training_worker.py</code> <pre><code>@thread_worker\ndef train_worker(\n    configuration: BaseConfig,\n    data_sources: dict[str, list],\n    training_queue: Queue,\n    predict_queue: Queue,\n    careamist: CAREamist | None = None,\n    pred_status: PredictionStatus | None = None,\n) -&gt; Generator[TrainUpdate, None, None]:\n    \"\"\"Model training worker.\n\n    Parameters\n    ----------\n    configuration : BaseConfig\n        careamics configuration.\n    data_sources : dict[str, list]\n        Train and validation data sources.\n    training_queue : Queue\n        Training update queue.\n    predict_queue : Queue\n        Prediction update queue.\n    careamist : CAREamist or None, default=None\n        CAREamist instance.\n    pred_status : PredictionStatus or None, default=None\n        Prediction status for stop callback.\n\n    Yields\n    ------\n    Generator[TrainUpdate, None, None]\n        Updates.\n    \"\"\"\n    # start training thread\n    training = Thread(\n        target=_train,\n        args=(\n            configuration,\n            data_sources,\n            training_queue,\n            predict_queue,\n            careamist,\n            pred_status,\n        ),\n    )\n    training.start()\n\n    # look for updates\n    while True:\n        update: TrainUpdate = training_queue.get(block=True)\n\n        yield update\n\n        if (\n            update.type == TrainUpdateType.STATE and update.value == TrainingState.DONE\n        ) or (update.type == TrainUpdateType.EXCEPTION):\n            break\n\n    # wait for the other thread to finish\n    training.join()\n</code></pre>"}]}