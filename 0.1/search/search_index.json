{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#documentation","title":"Documentation","text":"<p>Documentation for CAREamics v0.0.9.</p> <p>CAREamics is a PyTorch library aimed at simplifying the use of Noise2Void and its many variants and cousins (CARE, Noise2Noise, N2V2, P(P)N2V, HDN, muSplit etc.).</p>"},{"location":"#getting-started","title":"Getting Started","text":"Installation <p>                                         Get started with CAREamics installation.                                     </p> Current State <p>                                         Check out where we stand and where we want to go.                                     </p> Guides <p>                                         In-depth guides on CAREamics usage and features.                                     </p> Applications <p>                                         Examples of CAREamics in action on various datasets.                                     </p> Algorithms <p>                                         Dive into the various CAREamics algorithms.                                     </p> Code Reference <p>                                         Code documentation for all CAREamics libraries.                                     </p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you are having trouble using the library or the napari plugin, contact us via the  Image.sc forum.</p> <p>We are always welcoming feedback on what to improve of what features could be useful, therefore do not hesitate to open an issue on the Github repository!</p>"},{"location":"current_state/","title":"Current state","text":"<p>CAREamics is an on-going project and will include new algorithms in the next releases.  We are currently reaching Milestone 1. Here is a list of the current features:</p>"},{"location":"current_state/#algorithms","title":"Algorithms","text":"<ul> <li> Noise2Void 2D/3D/Channels</li> <li> structN2V 2D/3D/Channels</li> <li> N2V2 2D/3D/Channels</li> <li> CARE 2D/3D/Channels</li> <li> Noise2Noise 2D/3D/Channels</li> <li> CryoCARE 2D/3D</li> <li> PN2V 2D/3D/Channels</li> <li> PPN2V 2D/3D/Channels</li> <li> DivNoising/HDN 2D/3D/Channels</li> <li> muSplit 2D/3D</li> <li> denoiSplit 2D/3D</li> <li> EmbedSeg</li> </ul>"},{"location":"current_state/#features","title":"Features","text":"<ul> <li> Pydantic configuration</li> <li> TIFF dataloader</li> <li> In memory dataset</li> <li> Training/prediction</li> <li> Tiled prediction</li> <li> Checkpoint saving/loading</li> <li> Save/load bioimage.io format</li> <li> Zarr dataset</li> <li> Automated Mixed-Precision</li> <li> torch.compile</li> <li> napari plugin</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>CAREamics is a deep-learning library and we therefore recommend having GPU support as training the algorithms on the CPU can be very slow.</p> <p>We recommend using mamba (miniforge)  to install all packages in a virtual environment. As an alternative, you can use conda  (miniconda) with the same commands (replacing <code>mamba</code> by <code>conda</code>). </p> <p>For macOS silicon-acceleration, please refer to the specific sections!</p>"},{"location":"installation/#careamics-library-step-by-step","title":"CAREamics library step-by-step","text":"<p>This section install CAREamics for use in your own library or tool, via jupyter notebook or as scripts. For the napari plugin, refer to the next section.</p> Linux and WindowsmacOS (no GPU) <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Verify that the GPU is available:</p> <pre><code>python -c \"import torch; print([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\"\n</code></pre> <p>This should show a list of available GPUs. If the list is empty, then you will need to change the <code>pytorch</code> and <code>pytorch-cuda</code> versions to match your hardware (linux and windows).</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol> <p>These instructions were tested on a linux virtual machine (RedHat 8.6) with a  NVIDIA A40-8Q GPU.</p> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch::pytorch torchvision -c pytorch\n</code></pre> <p> Note that this will probably not install silicon GPU acceleration. If you want GPU acceleration, please refer to the relevant section to ensure that you install packages for the correct platform.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol>"},{"location":"installation/#extra-dependencies","title":"Extra dependencies","text":"<p>CAREamics extra dependencies can be installed by specifying them in brackets. In the previous section we installed <code>careamics[examples]</code>. You can add other extra dependencies, for instance <code>wandb</code> by doing:</p> <pre><code>pip install \"careamics[examples, wandb]\"\n</code></pre> <p>Here is a list of the extra dependencies:</p> <ul> <li><code>examples</code>: Dependencies required to run the example notebooks.</li> <li><code>wandb</code>: Dependencies to use WandB as a logger.</li> <li><code>tensorboard</code>: Dependencies to use TensorBoard as a logger.</li> <li><code>dev</code>: Dependencies required to run all the tooling necessary to develop with CAREamics.</li> </ul>"},{"location":"installation/#macos-silicon-gpu","title":"MacOS silicon GPU","text":"condamamba <ol> <li>Open the terminal and type <code>conda</code> to verify that conda is available.</li> <li> <p>Create a new environment:</p> <pre><code>CONDA_SUBDIR=osx-arm64 conda create -n careamics python=3.10\nconda activate careamics\nconda config --env --set subdir osx-arm64\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10 --platform osx-64\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics. We have several extra options (<code>dev</code>, <code>examples</code>, <code>wandb</code>     and <code>tensorboard</code>). If you wish to run the example notebooks,     we recommend the following:</p> <pre><code>pip install \"careamics[examples]\"\n</code></pre> </li> </ol>"},{"location":"installation/#quickstart","title":"Quickstart","text":"<p>Once you have installed CAREamics, the easiest way to get started is to look at the applications for full examples and the  guides for in-depth tweaking.</p>"},{"location":"installation/#careamics-napari-plugin","title":"CAREamics napari plugin","text":"Linux and WindowsmacOS (no GPU) <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch torchvision pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> </li> <li> <p>Verify that the GPU is available:</p> <pre><code>python -c \"import torch; print([torch.cuda.get_device_properties(i) for i in range(torch.cuda.device_count())])\"\n</code></pre> <p>This should show a list of available GPUs. If the list is empty, then you will need to change the <code>pytorch</code> and <code>pytorch-cuda</code> versions to match your hardware (linux and windows).</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol> <p>These instructions were tested on a linux virtual machine (RedHat 8.6) with a  NVIDIA A40-8Q GPU.</p> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>mamba install pytorch::pytorch torchvision -c pytorch\n</code></pre> <p> Note that this will probably not install silicon GPU acceleration. If you want GPU acceleration, please refer to the relevant section to ensure that you install packages for the correct platform.</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol>"},{"location":"installation/#macos-silicon-gpu_1","title":"MacOS silicon GPU","text":"condamamba <ol> <li>Open the terminal and type <code>conda</code> to verify that conda is available.</li> <li> <p>Create a new environment:</p> <pre><code>CONDA_SUBDIR=osx-arm64 conda create -n careamics python=3.10\nconda activate careamics\nconda config --env --set subdir osx-arm64\n</code></pre> </li> <li> <p>Install PyTorch following the official      instructions</p> <p>As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Verify that GPU is available:     <pre><code>python -c \"import torch; import platform; print((platform.processor() in ('arm', 'arm64') and torch.backends.mps.is_available()))\"\n</code></pre></p> <p>If this prints <code>False</code>, make sure that you do have an M1, M2 or M3 chip, and that the <code>conda</code>/<code>mamba</code> macOS-arm64 release was installed correctly.</p> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol> <ol> <li>Open the terminal and type <code>mamba</code> to verify that mamba is available.</li> <li> <p>Create a new environment:</p> <pre><code>mamba create -n careamics python=3.10 --platform osx-64\nmamba activate careamics\n</code></pre> </li> <li> <p>Install PyTorch following the official instructions     while specifying the platform. As an example, our test machine requires:</p> <pre><code>pip3 install torch torchvision\n</code></pre> </li> <li> <p>Install CAREamics napari plugin and napari:</p> <pre><code>pip install careamics-napari \"napari[all]\"\n</code></pre> </li> </ol>"},{"location":"algorithms/","title":"Algorithms","text":"<p>Work in progress</p> <p>These pages are still under construction and we expect a lot more details  descriptions of each algorithm in the near future.</p> <p>In these pages, you will find explanations and illustrations of how the various algorithms used in CAREamics work. These algorithms are divided into different  sections and a few keywords help you to understand the main characteristics of each algorithm. </p>"},{"location":"algorithms/#keywords","title":"Keywords","text":"<ul> <li>no ground-truth: The algorithm trains without clean images.</li> <li>single image: The algorithm can train on a single image.</li> <li>pairs of noisy images: The algorithm requires pairs of noisy images.</li> <li>ground-truth: The algorithm requires pairs of clean and noisy images.</li> </ul>"},{"location":"algorithms/#self-supervised-restoration","title":"Self-supervised restoration","text":"Noise2Void <p>                                         A self-supervised denoising algorithm based on a                                          pixel masking scheme.                                     </p> no ground-truth single image N2V2 <p>                                         A variant of Noise2Void capable of removing                                          checkboard artefacts.                                     </p> no ground-truth single image StructN2V <p>                                         A variant of Noise2Void that uses an enhanced mask                                         to remove structured noise.                                     </p> no ground-truth single image"},{"location":"algorithms/#noise-models","title":"Noise Models","text":"Noise Models <p>                                         Methods to estimate the noise models of                                         microscopes.                                     </p> no ground-truth"},{"location":"algorithms/#supervised-restoration-without-ground-truth","title":"Supervised restoration without ground-truth","text":"Noise2Noise <p>                                         A supervised methods that can denoise images without                                         corresponding clean data.                                     </p> no ground-truth pairs of noisy images"},{"location":"algorithms/#supervised-restoration","title":"Supervised restoration","text":"CARE <p>                                         The original supervised method to restore microscopy                                         images.                                     </p> ground-truth"},{"location":"algorithms/N2V2/","title":"N2V2","text":"<p>Fig 3.: Max pool vs max blur pool downsampling. An input (left) is passed through a max pool layer (center) and through a max blur pool layer (right) for comparison. The blur pool layer introduces blurring, avoiding aliasing-related artefacts. CC-BY.</p> <p>Fig 4.: Noise2Void vs N2V2 manipulation. In both algorithms, randomly selected pixels in the input patch (left) are selected and their value replaced. Noise2Void (center) replaces the values by the value of one of the neighboring pixel, while N2V2 (right) uses the median of the neighborhood. CC-BY.</p>"},{"location":"algorithms/N2V2/#n2v2","title":"N2V2\u00b6","text":""},{"location":"algorithms/N2V2/#overview","title":"Overview\u00b6","text":"<p>N2V2 [1] is a self-supervised denoising method based on Noise2Void [2,3]. The method was developed to remove pixel-noises from images without leaving unwanted checkerboard artefacts which can occasionally happen with Noise2Void.</p> <p>It retains the same training scheme (i.e. training on noisy images by randomly masking pixels), and differs from Noise2Void by the network architecture and pixel masking procedure. More specifically, N2V2 removes the first skip connection and introduces max blurpool layers in the UNet model [4], and the N2V2 masking scheme uses the median of each local image patch as replacement/masking value, rather than a randomly chosen neighboring pixel intensity.</p>"},{"location":"algorithms/N2V2/#checkerboard-artefacts","title":"Checkerboard artefacts\u00b6","text":"<p>Checkerboard artefacts [1] sometimes occur with Noise2Void. This effect is most prominent in the presence of salt and pepper noise or hot pixels in sCMOS cameras. They appear as little crosses, as illustrated in the figure below.</p> <p> </p> <p>Fig 1.: Checkerboard artefacts. The figure shows a noisy image crop (left), a noisy close-up (center) and the corresponding Noise2Void prediction (right). While the denoising was effective, a distinctive checkerboard pattern appears nonetheless in the Noise2Void prediction. CC-BY.</p>"},{"location":"algorithms/N2V2/#noise2void-vs-n2v2","title":"Noise2Void vs N2V2\u00b6","text":""},{"location":"algorithms/N2V2/#changes-to-the-architecture","title":"Changes to the architecture\u00b6","text":"<p>The model architecture used in Noise2Void and its siblings (N2V2, structN2V, P(P)N2V etc.) is a UNet model [4], a fully convolutional architecture.  Its main parts are an encoder, which downsamples and compresses the information into a bottleneck, and a decoder, which upsamples the information back to the original image size (see figure 2).</p> <p>In order to compress the information, the UNet architecture can perform multiple downsampling operations, defining multiple levels of spatial resolution. For each downsampling operation in the encoder, there is a corresponding upsampling operation in the decoder.</p> <p>One of the trick of the UNet architecture is to use skip connections between the encoder and the decoder. These skip connections allow the decoder to access the information from the encoder at the same spatial resolution. This is often useful to recover fine details of the image.</p> <p>N2V2 changes the architecture used in Noise2Void in two ways: the top skip connection is removed, and the downsampling is done using a different operation.</p> <p> </p> <p>Fig 2.: UNet architecture as used by Noise2Void. CC-BY.</p>"},{"location":"algorithms/N2V2/#skip-connections","title":"Skip connections\u00b6","text":"<p>In N2V2, the first skip connection is removed to constrain the among of high-frequency information that can be passed from the encoder to the decoder. Since the checkerboard artefacts are high-frequency artefacts, this change is expected to reduce their occurrence.</p>"},{"location":"algorithms/N2V2/#max-blur-pooling-layers","title":"Max blur pooling layers\u00b6","text":"<p>The second change is the introduction of max blur pooling as downsampling layers in the encoder to avoid aliasing-related artefacts, as opposed to the max pool layer used in Noise2Void. As with Noise2Void, the downsampling step starts from computing the maximum value in the patch (i.e. the region over which the filter is applied for each pixel), before applying a blur kernel (which is a convolutional operation) [5]. Since the output is smoothed, sharp transitions between pixel values are avoided and the aliasing artefacts are reduced.</p>"},{"location":"algorithms/N2V2/#changes-to-the-masking-scheme","title":"Changes to the masking scheme\u00b6","text":"<p>In Noise2Void, the masking scheme consists in replacing central pixels by a randomly chosen neighboring pixel (see the example). N2V2 changes the strategy for pixel selection by replacing the central pixel with the median of the neighborhood. This leads to a masking with a lower difference between masked pixels and their surrounding, preventing aliasing artefacts arising from sharp transitions between masked and unmasked pixels.</p>"},{"location":"algorithms/N2V2/#comparison","title":"Comparison\u00b6","text":"<p>These changes allow suppression of the checkerboard artefacts, as illustrated in the figure below. N2V2 does not take longer than Noise2Void to train, and you can easily compare the results of both methods.</p> <p>To train Noise2Void and N2V2 on the same dataset, refer to their respective examples: SEM Noise2Void and SEM N2V2.</p> <p> </p> <p>Fig 5.: Noise2Void vs N2V2 in the presence of checkerboard artefacts. The figure shows a noisy image crop (left), the Noise2Void prediction of the region delimited by a red square (center) and the corresponding N2V2 prediction (right). The cherckerboard artefacts are clearly visible in the Noise2Void prediction (center), while they are much less severe with N2V2 (left). CC-BY.</p>"},{"location":"algorithms/N2V2/#limitations","title":"Limitations\u00b6","text":"<p>Beyond the checkerboard artefacts, N2V2 suffers from the same limitations as Noise2Void regarding pixel-wise independent noise.</p>"},{"location":"algorithms/N2V2/#references","title":"References\u00b6","text":"<p>[1] Eva H\u00f6ck, Tim-Oliver Buchholz, Anselm Brachmann, Florian Jug, and Alexander Freytag. \"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture.\" ECCV, 2022. link</p> <p>[2] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[3] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p> <p>[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" MICCAI, 2015. link</p> <p>[5] Richard Zhang. \"Making convolutional networks shift-invariant again.\" MLR, 2019. link</p>"},{"location":"algorithms/Noise2Void/","title":"Noise2Void","text":"<p>Fig 1.: Noise2Void masking scheme. Top row: a toy example showcasing the pixel value replacement of masked pixels (left), and the resulting mask used for supervision (right). The mask contains the original values of the masked pixels. Bottom row: realistic example with the default masking parameters (data from 3). CC-BY.</p> <p>Fig 3.: Structured noise. Top row: image with uncorrelated noise (left) and its autocorrelation (close-up on the zero-shift component). Bottom row: same images with correlations introduced in the noise component. CC-BY.</p>"},{"location":"algorithms/Noise2Void/#noise2void","title":"Noise2Void\u00b6","text":""},{"location":"algorithms/Noise2Void/#overview","title":"Overview\u00b6","text":"<p>Noise2Void [1, 2] is a self-supervised denoising method. It trains by randomly masking pixels in the input image and predicting their masked value from the surrounding pixels.</p> <p>N2V relies on two fundamental hypotheses:</p> <ul> <li>The underlying structures are smooths (i.e. continuous)</li> <li>The noise is pixel-wise independent (i.e. the noise in one pixel is not correlated with the noise in a neighboring pixel)</li> </ul> <p>The corollary from these hypotheses is that if we consider the value of a pixel, $x$, as being the sum of the true signal, $s$, and a ceertain amount of noise, $\\epsilon$, such that $x = s + \\epsilon$, then:</p> <ul> <li>The true signal value $s$ can be estimated from the surrounding pixels</li> <li>The noise $\\epsilon$ cannot be estimated from the surrounding pixels</li> </ul> <p>Therefore, in cases where the hypotheses hold, if a pixel is masked, its noise cannot be estimated from the surrounding pixels, while the true signal can. This is exactly how N2V is trained!</p>"},{"location":"algorithms/Noise2Void/#training-noise2void","title":"Training Noise2Void\u00b6","text":"<p>Noise2Void is self-supervised, meaning that it trains on the data itself. It relies on a classical UNet architecture [4], and applies a special augmentation called <code>N2V manipulate</code> that replaces the value of randomly-selected pixels with the value of one of their neighbours (see the left column in the next figure).</p> <p>During training, the output of the network is compared to an image that is only comprised of the original pixels that were obfuscated during the augmentation. The loss is only computed on these obfuscated pixels, which means that the network is trained specifically to predict the masked pixels values!</p> <p>Below you can see two examples of the input to the network (left) and the masked pixels used to compute the loss (right). The first example is a toy image that allows visualizing the pixel replacement (it masks $2\\%$ of the pixels). The second example is a realistic image with the default N2V pixel manipulation parameters (e.g. $0.2\\%$ of masked pixels).</p>"},{"location":"algorithms/Noise2Void/#interpreting-the-loss","title":"Interpreting the loss\u00b6","text":"<p>As described above, the loss is computed by calculating the mean squared error over the masked pixels:</p> <p>$$loss = \\dfrac{1}{N_{masked}}\\sum_{i, masked}(x'_{i}-x_{i})^{2}$$</p> <p>where $N_{masked}$ is the total number of masked pixels, $x'_{i}$ is the network prediction for the masked pixel $i$, and $x_{i}$ is the original noisy value of pixel $i$.</p> <p>As opposed to supervised approaches, where the loss is computed with respect to a known ground truth, N2V computes the loss based on a noisy signal. At the beginning of the training the network usually learns within a few epochs an approximation of the structures in the image and the loss decreases sharply. Then, it rapidly reaches a plateau and oscillates around a particular loss value.</p> <p>Because the loss is computed between prediction and noisy signal, its absolute value is not informative for whether the network is properly trained. Likewise, oscillation on the plateau does not indicate that it does not learn anymore.</p> <p>The best way to assess the quality of the training is to look at the denoised images, potentially at different points during training, using the checkpoints. In practice, we often simply train long enough for the image to look properly denoised!</p> <p>If the training loss is not too informative, what about the validation loss? It is basically the same. If your validation loss increases, however, that does mean you are overfitting and might need to add more images to the training data.</p>"},{"location":"algorithms/Noise2Void/#predicting-on-the-training-set","title":"Predicting on the training set\u00b6","text":"<p>We are usually told that training and validation images should not be used to assess the performance (even qualitatively) of the network, because it was trained specifically to perform well on them.</p> <p>With Noise2Void, however, the network is trained without biasing it towards a ground-truth as it is trained on noisy pixels only. Therefore, it is perfectly fine to predict on the training images.</p>"},{"location":"algorithms/Noise2Void/#re-using-a-trained-model","title":"Re-using a trained model\u00b6","text":"<p>Noise2Void learns both the noise distribution in the image and a structural prior, that is to say how the structures in the image look. As with most deep learning approaches, if you try applying a trained neural network on images that are different from the training set, the network will most likely fail and produce results of lesser quality.</p> <p>This will happen with N2V, for example, if the noise distribution is different in the new images, or if they contain different structures. Fortunately, N2V is quick to train, so you can train a single network for each new experiment!</p>"},{"location":"algorithms/Noise2Void/#why-do-n2v-predictions-sometimes-look-blurry","title":"Why do N2V predictions sometimes look blurry?\u00b6","text":"<p>The absence of noise can by itself make images look slightly blurry. If some regions of your data look unreasonably blurry, you might be encountering the a case of regression to the mean.</p> <p>For each noisy image, we often say that there is a whole distribution of possible denoised images. In particular, for strongly degraded images, even different structures can produce the same noisy patch.</p> <p>Because of this, a network trained with the mean squared error, such as N2V, will tend to predict the average of all possible denoised images. This is why the prediction can often look blurry and washed out. For approaches that predict single instances from the distribution of possible denoised images, check out DivNoising or HDN.</p>"},{"location":"algorithms/Noise2Void/#assessing-quality-of-results","title":"Assessing quality of results\u00b6","text":"<p>Denoising is a difficult task to assess quantitatively. The most common and more accurate way to estimate the performances of a network is to compare its output with a ground truth image. That means that you need to acquire images with and without noise. This can be done by acquiring with more laser power, longer exposure time, or by averaging multiple images. That is, however, cumbersome and often impossible.</p> <p>There are other ways perform controls on the quality of the denoising.</p>"},{"location":"algorithms/Noise2Void/#quality-assessment-by-inspecting-the-residuals","title":"Quality assessment by inspecting the residuals\u00b6","text":"<p>The residuals are the pixel intensity removed from the original image by the network:</p> <p>$$res = image - pred$$</p> <p>They correspond to the subtraction of the prediction from the original image. If the residual show details of the image structure, then the training went wrong or the network did not train for long enough.</p> <p> </p> <p>Fig 2.: The residuals are a good indicator of successful training. Top row: input image (left), prediction (center) and corresponding residuals (right) after one training epoch. The residuals show structures. Bottom row: same after 50 epochs, there is no visible structure that is not pure noise. CC-BY.</p> <p>Note that in the presence of Poisson noise (a.k.a shot noise), the amplitude of noise scales with the square root of the signal. Therefore, the residuals can still be modulated by the structures in the image, especially the bright ones. Please note that a property of the residuals is that the sum of its pixels should always be close to zero: even if some areas of the residuals look brighter, on closer inspection, you should see a lot of very dark pixels in this very region.</p>"},{"location":"algorithms/Noise2Void/#assessing-quality-by-training-multiple-networks","title":"Assessing quality by training multiple networks\u00b6","text":"<p>As Noise2Void training is stochastic, in order to assess whether structures are really in your image, you can perform different types of experiments:</p> <ul> <li>Split your images in several subsets and train independent networks on each of them. Compare the results.</li> <li>Train multiple randomly initialized networks on your training data and compare the results.</li> </ul>"},{"location":"algorithms/Noise2Void/#limitations","title":"Limitations\u00b6","text":""},{"location":"algorithms/Noise2Void/#pixel-wise-independent-noise","title":"Pixel-wise independent noise\u00b6","text":"<p>It might happen that the noise in your images is not pixel-wise independent. In this case, the network can learn the amount of noise in the masked pixels from its neighboring pixels. Noise2Void might then introduce small artefacts or simply reinforce the correlated pattern in the denoised image.</p> <p>Correlations in the noise can sometimes be observed with sCMOS camera or point-scanning microscopy methods. To estimate whether the noise is correlated, one can perform image autocorrelation and inspect the shape of the central peak in the distribution. The next figure shows two examples of the same image with artificial Gaussian noise. In the second one, we introduced a correlation in the Gaussian noise, leading to an horizontal line in the autocorrelation image.</p>"},{"location":"algorithms/Noise2Void/#checkerboard-artefacts","title":"Checkerboard artefacts\u00b6","text":"<p>Checkerboard artefacts [5] sometimes occur with Noise2Void. This effect is most prominent in the presence of salt and pepper noise or hot pixels in sCMOS cameras. They are characterized by small cross patterns in the denoised image (see figure 4). To reduce the presence of these artefacts, you can use N2V2.</p> <p> </p> <p>Fig 4.: Checkerboard artefacts. The figure shows a noisy image crop (left), a noisy close-up (center) and the corresponding Noise2Void prediction (right). While the denoising was effective, a distinctive checkerboard pattern nonetheless appears in the Noise2Void prediction. CC-BY.</p>"},{"location":"algorithms/Noise2Void/#references","title":"References\u00b6","text":"<p>[1] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[2] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p> <p>[3] Tim-Oliver Buchholz, Mangal Prakash, Deborah Schmidt, Alexander Krull, and Florian Jug. \"Denoiseg: joint denoising and segmentation.\" ECCV, 2020. link</p> <p>[4] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" MICCAI, 2015. link</p> <p>[5] Eva H\u00f6ck, Tim-Oliver Buchholz, Anselm Brachmann, Florian Jug, and Alexander Freytag. \"N2V2 - Fixing Noise2Void Checkerboard Artifacts with Modified Sampling Strategies and a Tweaked Network Architecture.\" ECCV, 2022. link</p>"},{"location":"algorithms/NoiseModels/","title":"Noise Models","text":""},{"location":"algorithms/NoiseModels/#noise-models","title":"Noise Models\u00b6","text":"<p>Noise models are an important aspect of denoising images using P(P)N2V, HDN or microSplit.</p> <p> </p>"},{"location":"algorithms/structN2V/","title":"structN2V","text":"<p>Fig 1.: Structured noise. Top row: image with uncorrelated noise (left) and its autocorrelation (close-up on the zero-shift component). Bottom row: same images with correlations introduced in the noise component. CC-BY.</p> <p>Fig 2.: Noise2Void vs structN2V manipulation, with a toy example (top row) and a real-world one (bottom row). In both algorithms, randomly selected pixels in the input patch (left) are selected and their value replaced. Noise2Void (center) replaces the values by the value of one of the neighboring pixel, while structN2V (right) additionally masks neighboring pixels using a uniformly sampled value comprised between the min and max patch value to prevent the network from learning from the noise correlations. CC-BY.</p>"},{"location":"algorithms/structN2V/#structn2v","title":"structN2V\u00b6","text":""},{"location":"algorithms/structN2V/#overview","title":"Overview\u00b6","text":"<p>structN2V [1] is a self-supervised denoising method based on Noise2Void [2,3]. The method was developed to remove pixel-noises from images, even in the presence of spatial noise correlations.</p> <p>It retains the same training scheme (i.e. training on noisy images by randomly masking pixels), and differs from Noise2Void by the use of an additional mask covering the extent of the spatial noise correlation.</p>"},{"location":"algorithms/structN2V/#pixel-wise-independent-noise","title":"Pixel-wise independent noise\u00b6","text":"<p>It might happen that the noise in your images is not pixel-wise independent. In this case, the network can learn the amount of noise in the masked pixels from its neighboring pixels. Noise2Void might then introduce small artefacts or simply reinforce the correlated pattern in the denoised image.</p> <p>Correlations in the noise can sometimes be observed with sCMOS camera or point-scanning microscopy methods. To estimate whether the noise is correlated, one can perform image autocorrelation and inspect the shape of the central peak in the distribution. The next figure shows two examples of the same image with artificial Gaussian noise. In the second one, we introduced a correlation in the Gaussian noise, leading to an horizontal line in the autocorrelation image.</p>"},{"location":"algorithms/structN2V/#n2v-vs-structn2v-masking","title":"N2V vs structN2V masking\u00b6","text":"<p>In order to obfuscate the pixel values whose noise might be correlated, structN2V introduces a mask spanning the extent of the correlation. In order to estimate the size and direction of the correlation, one simply needs to examine the autocorrelation image (see section above).</p> <p>However, as in Noise2Void, masking pixels by setting their values to zero will confuse the network by introducing unrealistic values. To circumvent this issue, structN2V replaces the masked pixels by values uniformly sampled between the minimum and maximum pixel values of the patch.</p>"},{"location":"algorithms/structN2V/#references","title":"References\u00b6","text":"<p>[1] Coleman Broaddus, Alexander Krull, Martin Weigert, Uwe Schmidt, and Gene Myers. \" Removing structured noise with self-supervised blind-spot networks.\" ISBI, 2020. link</p> <p>[2] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. \"Noise2Void - learning denoising from single noisy images.\" CVPR, 2019. link</p> <p>[3] Joshua Batson, and Loic Royer. \"Noise2Self: Blind denoising by self-supervision.\" MLR, 2019. link</p>"},{"location":"applications/","title":"Applications","text":"<p>Click on your algorithm of choice to explore various applications. We collected the  algorithms based on the type of training data they require! </p>"},{"location":"applications/#keywords","title":"Keywords","text":"<ul> <li>no ground-truth: The algorithm trains without clean images.</li> <li>single image: The algorithm can train on a single image.</li> <li>pairs of noisy images: The algorithm requires pairs of noisy images.</li> <li>ground-truth: The algorithm requires pairs of clean and noisy images.</li> </ul>"},{"location":"applications/#denoising-noisy-images-without-clean-data","title":"Denoising noisy images without clean data","text":"<p>You have noisy images and no clean images? No problem! These algorithms can help you, as they do not require any ground-truth data. You can also train on a single image of reasonable size.</p> Noise2Void <p>                                         A self-supervised denoising algorithm based on a                                          pixel masking scheme.                                     </p> no ground-truth single image N2V2 <p>                                         A variant of Noise2Void capable of removing                                          checkboard artefacts.                                     </p> no ground-truth single image StructN2V <p>                                         A variant of Noise2Void that uses an enhanced mask                                         to remove structured noise.                                     </p> no ground-truth single image <p>If you have multiple noisy instances of the same structure (e.g. a noisy time-lapse),  then Noise2Noise might be the right choice for you.</p> Noise2Noise <p>                                         A supervised methods that can denoise images without                                         corresponding clean data.                                     </p> no ground-truth pairs of noisy images"},{"location":"applications/#supervised-restoration-with-clean-images","title":"Supervised restoration with clean images","text":"<p>If you have pairs of clean (e.g. high SNR, long exposure or high laser power) and noisy images, then CARE might be the right choice for you.</p> <p>Note that CARE can be used for a variety of tasks, such as denoising, deconvolution, isotropic resolution restoration or projection.</p> CARE <p>                                         The original supervised method to restore microscopy                                         images.                                     </p> ground-truth"},{"location":"applications/#using-the-lightning-api","title":"Using the Lightning API","text":"<p>If you need more control on the algorithm training, for instance to implement or replace features, you can use the Lightning API. </p> <p>It uses PyTorch Lightning and the  CAREamics Lightning components.</p> Lightning API <p>                                         Get full control of the training and prediction                                         pipelines by using CAREamics Lightning components.                                     </p>"},{"location":"applications/SUMMARY/","title":"SUMMARY","text":"<ul> <li>N2V2<ul> <li>BSD68</li> <li>SEM</li> </ul> </li> <li>Noise2Noise<ul> <li>SEM</li> </ul> </li> <li>structN2V<ul> <li>Convallaria</li> </ul> </li> <li>Lightning_API<ul> <li>BSD68 N2V</li> </ul> </li> <li>CARE<ul> <li>denoising U2OS</li> </ul> </li> <li>Noise2Void<ul> <li>Mouse Nuclei</li> <li>JUMP</li> <li>Flywing</li> <li>SUPPORT</li> <li>BSD68</li> <li>W2S</li> <li>SEM</li> <li>Hagen</li> </ul> </li> </ul>"},{"location":"applications/CARE/","title":"CARE","text":"<p>For more details on the algorithm, check out its description.</p> denoising U2OS <p>                                   Supervised denoising of nuclei imaged in 2D fluorescence with CARE.                               </p> 2D fluorescence nuclei <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/CARE/denoising_U2OS/","title":"denoising U2OS","text":"<p> Find me on Github </p> <p>The U2OS dataset is composed of pairs of noisy and high SNR nuclei images acquired in fluorescence microscopy. They were originally used in Weigert et al (2018) to showcase CARE denoising.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_care_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_care_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.CARE_U2OS.download(root_path)\n\n# path to the training data\nroot_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\"\ntrain_path = root_path / \"train\" / \"low\"\ntarget_path = root_path / \"train\" / \"GT\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.CARE_U2OS.download(root_path)  # path to the training data root_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\" train_path = root_path / \"train\" / \"low\" target_path = root_path / \"train\" / \"GT\" In\u00a0[3]: Copied! <pre># load training image and target, and show them side by side\ntrain_files = list(train_path.rglob(\"*.tif\"))\ntrain_files.sort()\n\ntarget_files = list(target_path.rglob(\"*.tif\"))\ntarget_files.sort()\n\n# select random example\nind = np.random.randint(len(train_files))\ntrain_image = tifffile.imread(train_files[ind])\ntrain_target = tifffile.imread(target_files[ind])\n\n# plot the two images and a crop\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[00].set_title(\"Training image\")\n\nax[1].imshow(train_target, cmap=\"gray\")\nax[1].set_title(\"Target image\")\n</pre> # load training image and target, and show them side by side train_files = list(train_path.rglob(\"*.tif\")) train_files.sort()  target_files = list(target_path.rglob(\"*.tif\")) target_files.sort()  # select random example ind = np.random.randint(len(train_files)) train_image = tifffile.imread(train_files[ind]) train_target = tifffile.imread(target_files[ind])  # plot the two images and a crop fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[00].set_title(\"Training image\")  ax[1].imshow(train_target, cmap=\"gray\") ax[1].set_title(\"Target image\") Out[3]: <pre>Text(0.5, 1.0, 'Target image')</pre> In\u00a0[4]: Copied! <pre>config = create_care_configuration(\n    experiment_name=\"care_U20S\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=(128, 128),\n    batch_size=32,\n    num_epochs=100,\n)\n\nprint(config)\n</pre> config = create_care_configuration(     experiment_name=\"care_U20S\",     data_type=\"tiff\",     axes=\"YX\",     patch_size=(128, 128),     batch_size=32,     num_epochs=100, )  print(config) <pre>{'algorithm_config': {'algorithm': 'care',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': False,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'tiff',\n                 'patch_size': [128, 128],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}]},\n 'experiment_name': 'care_U20S',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 100},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    train_target=target_path,\n    val_percentage=0.01,\n    val_minimum_split=20,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     train_target=target_path,     val_percentage=0.01,     val_minimum_split=20, ) In\u00a0[\u00a0]: remove_output Copied! <pre>test_path = root_path / \"test\" / \"low\"\n\nprediction = careamist.predict(source=test_path)\n</pre> test_path = root_path / \"test\" / \"low\"  prediction = careamist.predict(source=test_path) In\u00a0[7]: Copied! <pre># Show two images\ntest_GT_path = root_path / \"test\" / \"GT\"\ntest_GT_files = list(test_GT_path.rglob(\"*.tif\"))\ntest_GT_files.sort()\n\ntest_low_path = root_path / \"test\" / \"low\"\ntest_low_files = list(test_low_path.rglob(\"*.tif\"))\ntest_low_files.sort()\n\ntest_GT = [tifffile.imread(f) for f in test_GT_files]\ntest_low = [tifffile.imread(f) for f in test_low_files]\n\n# images to show\nimages = np.random.choice(range(len(test_GT)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 17))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n\n    psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])\n    psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)\n\n    ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images test_GT_path = root_path / \"test\" / \"GT\" test_GT_files = list(test_GT_path.rglob(\"*.tif\")) test_GT_files.sort()  test_low_path = root_path / \"test\" / \"low\" test_low_files = list(test_low_path.rglob(\"*.tif\")) test_low_files.sort()  test_GT = [tifffile.imread(f) for f in test_GT_files] test_low = [tifffile.imread(f) for f in test_low_files]  # images to show images = np.random.choice(range(len(test_GT)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 17)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()      psnr_noisy = scale_invariant_psnr(test_GT[images[i]], test_low[images[i]])     psnr_result = scale_invariant_psnr(test_GT[images[i]], pred_image)      ax[i, 0].imshow(test_low[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(test_GT[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[8]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, test_GT)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, test_GT)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") <pre>PSNR: 31.53 +/- 3.71\n</pre> In\u00a0[9]: Copied! <pre># create a cover image\nim_idx = 17\ncv_image_noisy = test_low[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros_like(cv_image_noisy, dtype=np.float32)\nwidth = cover.shape[1]\n\n# # normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : width // 2] = norm_noise[:, : width // 2]\ncover[:, width // 2 :] = norm_pred[:, width // 2 :]\n\n# plot the single image\nplt.imshow(cover)\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"U2OS_CARE.jpeg\")\n</pre> # create a cover image im_idx = 17 cv_image_noisy = test_low[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros_like(cv_image_noisy, dtype=np.float32) width = cover.shape[1]  # # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : width // 2] = norm_noise[:, : width // 2] cover[:, width // 2 :] = norm_pred[:, width // 2 :]  # plot the single image plt.imshow(cover)  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"U2OS_CARE.jpeg\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"applications/CARE/denoising_U2OS/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The CARE U2OS dataset is composed of thousands of examples organized in train and test:</p> <ul> <li>train<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> <li>test<ul> <li>low: low SNR data</li> <li>GT: high SNR data</li> </ul> </li> </ul>"},{"location":"applications/CARE/denoising_U2OS/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/CARE/denoising_U2OS/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/CARE/denoising_U2OS/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/CARE/denoising_U2OS/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training, and we can use the test set.</p>"},{"location":"applications/CARE/denoising_U2OS/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/CARE/denoising_U2OS/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Lightning_API/","title":"Lightning_API","text":"BSD68 N2V <p>                                   Example of Noise2Void using the Lightning API with 2D natural images and synthetic noise.                               </p> 2D benchmark N2V <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/","title":"BSD68 N2V","text":"<p> Find me on Github </p> <p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of Noise2Void using the Lightning API of CAREamics.</p> <p>This API gives you more freedom to customize the training by using wrappers around the main elements of CAREamics: the datasets and the lightning module.</p> In\u00a0[4]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom PIL import Image\nfrom careamics.lightning import (\n    create_careamics_module,\n    create_predict_datamodule,\n    create_train_datamodule,\n)\nfrom careamics.config.support import SupportedTransform\nfrom careamics.prediction_utils import convert_outputs\nfrom careamics.utils.metrics import psnr\nfrom careamics_portfolio import PortfolioManager\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from PIL import Image from careamics.lightning import (     create_careamics_module,     create_predict_datamodule,     create_train_datamodule, ) from careamics.config.support import SupportedTransform from careamics.prediction_utils import convert_outputs from careamics.utils.metrics import psnr from careamics_portfolio import PortfolioManager from pytorch_lightning import Trainer from pytorch_lightning.callbacks import ModelCheckpoint In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[6]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[6]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[7]: Copied! <pre>model = create_careamics_module(\n    algorithm=\"n2v\",\n    loss=\"n2v\",\n    architecture=\"UNet\",\n    model_parameters={\"n2v2\": False},\n)\n</pre> model = create_careamics_module(     algorithm=\"n2v\",     loss=\"n2v\",     architecture=\"UNet\",     model_parameters={\"n2v2\": False}, ) In\u00a0[10]: Copied! <pre>train_data_module = create_train_datamodule(\n    train_data=train_path,\n    val_data=val_path,\n    data_type=\"tiff\",\n    patch_size=(64, 64),\n    axes=\"SYX\",\n    batch_size=64,\n    transforms=[\n        { # you can delete a transform here to not apply it\n            \"name\": SupportedTransform.XY_FLIP.value,\n            \"flip_x\": True, # you can set parameters\n            \"flip_y\": True,\n        },\n        {\n            \"name\": SupportedTransform.XY_RANDOM_ROTATE90.value,\n        },\n        {\n            \"name\": SupportedTransform.N2V_MANIPULATE.value, # mandatory to run N2V\n            # here you can modify the N2V manipulate parameters\n        },\n    ],\n)\n</pre> train_data_module = create_train_datamodule(     train_data=train_path,     val_data=val_path,     data_type=\"tiff\",     patch_size=(64, 64),     axes=\"SYX\",     batch_size=64,     transforms=[         { # you can delete a transform here to not apply it             \"name\": SupportedTransform.XY_FLIP.value,             \"flip_x\": True, # you can set parameters             \"flip_y\": True,         },         {             \"name\": SupportedTransform.XY_RANDOM_ROTATE90.value,         },         {             \"name\": SupportedTransform.N2V_MANIPULATE.value, # mandatory to run N2V             # here you can modify the N2V manipulate parameters         },     ], ) In\u00a0[\u00a0]: remove_output Copied! <pre># Create Callbacks\nroot = Path(\"bsd68_n2v\")\ncallbacks = [\n    ModelCheckpoint(\n        dirpath=root / \"checkpoints\",\n        filename=\"bsd68_lightning_api\",\n        save_last=True,\n    )\n]\n\n# Create a Lightning Trainer\ntrainer = Trainer(max_epochs=100, default_root_dir=root, callbacks=callbacks)\n\n# Train the model\ntrainer.fit(model, datamodule=train_data_module)\n</pre> # Create Callbacks root = Path(\"bsd68_n2v\") callbacks = [     ModelCheckpoint(         dirpath=root / \"checkpoints\",         filename=\"bsd68_lightning_api\",         save_last=True,     ) ]  # Create a Lightning Trainer trainer = Trainer(max_epochs=100, default_root_dir=root, callbacks=callbacks)  # Train the model trainer.fit(model, datamodule=train_data_module) In\u00a0[\u00a0]: remove_output Copied! <pre>means, stds = train_data_module.get_data_statistics()\npred_data_module = create_predict_datamodule(\n    pred_data=test_path,\n    data_type=\"tiff\",\n    axes=\"YX\",\n    batch_size=1,\n    tta_transforms=True,\n    image_means=means,\n    image_stds=stds,\n    tile_size=(128, 128),\n    tile_overlap=(32, 32),\n)\n</pre> means, stds = train_data_module.get_data_statistics() pred_data_module = create_predict_datamodule(     pred_data=test_path,     data_type=\"tiff\",     axes=\"YX\",     batch_size=1,     tta_transforms=True,     image_means=means,     image_stds=stds,     tile_size=(128, 128),     tile_overlap=(32, 32), ) In\u00a0[\u00a0]: remove_output Copied! <pre># Predict\nprediction = trainer.predict(model, datamodule=pred_data_module)\n\n# Convert the outputs to the original format, mostly useful if tiling is used\nprediction = convert_outputs(prediction, tiled=True)\n</pre> # Predict prediction = trainer.predict(model, datamodule=pred_data_module)  # Convert the outputs to the original format, mostly useful if tiling is used prediction = convert_outputs(prediction, tiled=True) In\u00a0[14]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    psnr_noisy = psnr(gts[images[i]], noises[images[i]])\n    psnr_result = psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()     psnr_noisy = psnr(gts[images[i]], noises[images[i]])     psnr_result = psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[15]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 27.16 +/- 2.91\nReported PSNR: 27.71\n</pre> In\u00a0[16]: Copied! <pre># create a cover image\nim_idx = 3\ncv_image_noisy = noises[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))   \n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min())\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())\n\n# fill in halves\ncover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2]\ncover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"BSD68_Noise2Void_lightning_api.jpeg\")\n</pre> # create a cover image im_idx = 3 cv_image_noisy = noises[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256))    (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min()) norm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())  # fill in halves cover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2] cover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"BSD68_Noise2Void_lightning_api.jpeg\")"},{"location":"applications/Lightning_API/BSD68_N2V/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#train-with-the-careamics-lightning-api","title":"Train with the CAREamics Lightning API\u00b6","text":"<p>Using the Lightning API of CAREamics, you need to instantiate the lightning module, the data module and the trainer yourself.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-lightning-module","title":"Create the Lightning module\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-data-module","title":"Create the data module\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-the-trainer","title":"Create the trainer\u00b6","text":"<p>Note that here we modify the prediction loop, but this will be  changed in the near future.</p>"},{"location":"applications/Lightning_API/BSD68_N2V/#predict-with-careamics-lightning-api","title":"Predict with CAREamics Lightning API\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#define-the-prediction-datamodule","title":"Define the prediction datamodule\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Lightning_API/BSD68_N2V/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/N2V2/","title":"N2V2","text":"<p>For more details on the algorithm, check out its description.</p> SEM <p>                                   Denoising of 2D scanning electron microscopy with N2V2.                               </p> 2D electron microscopy BSD68 <p>                                   Benchmark N2V2 in 2D using natural images and synthetic noise.                               </p> 2D benchmark <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/N2V2/BSD68/","title":"BSD68","text":"<p> Find me on Github </p> <p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of N2V2, an extension of Noise2Void.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[3]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[4]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"bsd68_n2v\",\n    data_type=\"tiff\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=100,\n    use_n2v2=True,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"bsd68_n2v\",     data_type=\"tiff\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=100,     use_n2v2=True, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': True,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 64,\n                 'data_type': 'tiff',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'median',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'bsd68_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 100,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    val_source=val_path,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     val_source=val_path, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=test_path,\n    axes=\"YX\",\n    tile_size=(256, 256),\n    tile_overlap=(48, 48),\n)\n</pre> prediction = careamist.predict(     source=test_path,     axes=\"YX\",     tile_size=(256, 256),     tile_overlap=(48, 48), ) In\u00a0[7]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])\n    psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()     psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])     psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[8]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 26.89 +/- 2.45\nReported PSNR: 27.71\n</pre> In\u00a0[9]: Copied! <pre># create a cover image\nim_idx = 3\ncv_image_noisy = noises[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))\n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : 256 // 2] = norm_noise[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2\n]\ncover[:, 256 // 2 :] = norm_pred[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"BSD68_N2V2.jpeg\")\n</pre> # create a cover image im_idx = 3 cv_image_noisy = noises[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256)) (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : 256 // 2] = norm_noise[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2 ] cover[:, 256 // 2 :] = norm_pred[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2 ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"BSD68_N2V2.jpeg\") In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"bsd68_n2v2_model.zip\",\n    friendly_model_name=\"BSD68_N2V2\",\n    input_array=noises[im_idx][np.newaxis, :256, :256],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"bsd68_n2v2_model.zip\",     friendly_model_name=\"BSD68_N2V2\",     input_array=noises[im_idx][np.newaxis, :256, :256],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}], )"},{"location":"applications/N2V2/BSD68/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/N2V2/BSD68/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/N2V2/BSD68/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/N2V2/BSD68/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p> <p>Here, the switch between Noise2Void and N2V2 is done by changing the <code>use_n2v2</code> parameter.</p>"},{"location":"applications/N2V2/BSD68/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/N2V2/BSD68/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training.</p>"},{"location":"applications/N2V2/BSD68/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/N2V2/BSD68/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/N2V2/BSD68/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/N2V2/SEM/","title":"SEM","text":"<p> Find me on Github </p> <p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of N2V2, an extension of Noise2Void, on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_SEM.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_SEM.download(root_path) In\u00a0[3]: Copied! <pre>portfolio.denoising.N2V_SEM.description\n</pre> portfolio.denoising.N2V_SEM.description Out[3]: <pre>'Cropped images from a SEM dataset from T.-O. Buchholz et al (Methods Cell Biol, 2020).'</pre> In\u00a0[4]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(files[0])\nval_image = tifffile.imread(files[1])\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(files[0]) val_image = tifffile.imread(files[1])  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[4]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[5]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"sem_n2v2\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=30,\n    use_n2v2=True,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"sem_n2v2\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=30,     use_n2v2=True, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': True,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'median',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'sem_n2v2',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 30,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_source=val_image,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_source=val_image, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[8]: Copied! <pre># Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x7fae61127d30&gt;</pre> In\u00a0[9]: Copied! <pre># create a cover image\nx_start, width = 500, 512\ny_start, height = 1400, 512\n\n# create image\ncover = np.zeros((height, width))\n\n# normalize train and prediction\nnorm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())\n\npred = prediction[0].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, : width // 2] = norm_train[\n    y_start : y_start + height, x_start : x_start + width // 2\n]\ncover[:, width // 2 :] = norm_pred[\n    y_start : y_start + height, x_start + width // 2 : x_start + width\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"SEM_N2V2.jpeg\")\n</pre> # create a cover image x_start, width = 500, 512 y_start, height = 1400, 512  # create image cover = np.zeros((height, width))  # normalize train and prediction norm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())  pred = prediction[0].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, : width // 2] = norm_train[     y_start : y_start + height, x_start : x_start + width // 2 ] cover[:, width // 2 :] = norm_pred[     y_start : y_start + height, x_start + width // 2 : x_start + width ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"SEM_N2V2.jpeg\") In\u00a0[10]: Copied! <pre>general_description = (\n    \"This model is a UNet trained using the N2V2 algorithm to denoise images. N2V2 is \"\n    \"a variant of Noise2Void that addresses the checkerboard artifacts sometimes \"\n    \"produced by models trained with Noise2Void. The training data consists of crops \"\n    \"from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The \"\n    \"notebook used to train this model is available on the CAREamics documentation \"\n    \"website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/N2V2/SEM/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained using the N2V2 algorithm to denoise images. N2V2 is \"     \"a variant of Noise2Void that addresses the checkerboard artifacts sometimes \"     \"produced by models trained with Noise2Void. The training data consists of crops \"     \"from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The \"     \"notebook used to train this model is available on the CAREamics documentation \"     \"website at the following link: \"     \"https://careamics.github.io/0.1/applications/N2V2/SEM/.\" ) print(general_description) <pre>This model is a UNet trained using the N2V2 algorithm to denoise images. N2V2 is a variant of Noise2Void that addresses the checkerboard artifacts sometimes produced by models trained with Noise2Void. The training data consists of crops from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/N2V2/SEM/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\",\ncareamist.export_to_bmz(\n    path_to_archive=\"sem_n2v2_model.zip\",\n    friendly_model_name=\"SEM_N2V2\",\n    input_array=train_image[1400:1656, 500:756],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=portfolio.denoising.N2V_SEM.description\n)\n</pre> # Export the model\", careamist.export_to_bmz(     path_to_archive=\"sem_n2v2_model.zip\",     friendly_model_name=\"SEM_N2V2\",     input_array=train_image[1400:1656, 500:756],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=portfolio.denoising.N2V_SEM.description )"},{"location":"applications/N2V2/SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/N2V2/SEM/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/N2V2/SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/N2V2/SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/N2V2/SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p> <p>There the switch between Noise2Void and N2V2 is done by changing the <code>use_n2v2</code> parameter.</p>"},{"location":"applications/N2V2/SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/N2V2/SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/N2V2/SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/N2V2/SEM/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"applications/Noise2Noise/","title":"Noise2Noise","text":"<p>For more details on the algorithm, check out its description.</p> SEM <p>                                   Denoising of 2D scanning electron microscopy with Noise2Noise.                               </p> 2D electron microscopy <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Noise2Noise/SEM/","title":"SEM","text":"<p> Find me on Github </p> <p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Noise on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport tifffile\nimport numpy as np\nfrom PIL import Image\n\nfrom careamics import CAREamist\nfrom careamics.config import create_n2n_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import tifffile import numpy as np from PIL import Image  from careamics import CAREamist from careamics.config import create_n2n_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\ndownload = portfolio.denoising.N2N_SEM.download(root_path)\nfiles = [f for f in download if f.endswith(\"tif\")]\nfiles.sort()\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") download = portfolio.denoising.N2N_SEM.download(root_path) files = [f for f in download if f.endswith(\"tif\")] files.sort() In\u00a0[3]: Copied! <pre># load training and target image and show them side by side\ntrain_stack = tifffile.imread(files[1])\n\n# use the 1 us scan time to perform Noise2Noise\ntrain_image = train_stack[2]\ntrain_target = train_stack[3]\n\n# plot the two images and a crop\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].set_title(\"Training image (1 us)\")\n\nax[0, 1].imshow(train_target, cmap=\"gray\")\nax[0, 1].set_title(\"Target image (1 us)\")\n\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 0].set_title(\"Training crop (1 us)\")\n\nax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].set_title(\"Target crop (1 us)\")\n</pre> # load training and target image and show them side by side train_stack = tifffile.imread(files[1])  # use the 1 us scan time to perform Noise2Noise train_image = train_stack[2] train_target = train_stack[3]  # plot the two images and a crop fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].set_title(\"Training image (1 us)\")  ax[0, 1].imshow(train_target, cmap=\"gray\") ax[0, 1].set_title(\"Target image (1 us)\")  x_start, x_end = 600, 850 y_start, y_end = 200, 450 ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 0].set_title(\"Training crop (1 us)\")  ax[1, 1].imshow(train_target[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].set_title(\"Target crop (1 us)\") Out[3]: <pre>Text(0.5, 1.0, 'Target crop (1 us)')</pre> In\u00a0[4]: Copied! <pre>config = create_n2n_configuration(\n    experiment_name=\"sem_n2n\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=50,\n)\n\nprint(config)\n</pre> config = create_n2n_configuration(     experiment_name=\"sem_n2n\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=50, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2n',\n                      'loss': 'mae',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': False,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 64,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5}]},\n 'experiment_name': 'sem_n2n',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 50},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    train_target=train_target,\n    val_minimum_split=5,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     train_target=train_target,     val_minimum_split=5, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[7]: Copied! <pre># get pseudo ground-truth from the 5 us averaged scan time\npseudo_gt = train_stack[-1]\npsnr_noisy = scale_invariant_psnr(pseudo_gt, train_image)\npsnr_pred = scale_invariant_psnr(pseudo_gt, prediction[0].squeeze())\n\n# Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 3, figsize=(15, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")\n\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")\n\nax[0, 2].imshow(pseudo_gt, cmap=\"gray\")\nax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")\n\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # get pseudo ground-truth from the 5 us averaged scan time pseudo_gt = train_stack[-1] psnr_noisy = scale_invariant_psnr(pseudo_gt, train_image) psnr_pred = scale_invariant_psnr(pseudo_gt, prediction[0].squeeze())  # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 3, figsize=(15, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].title.set_text(f\"Training image (1 us)\\nPSNR: {psnr_noisy:.2f}\")  ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[0, 1].title.set_text(f\"Prediction (1 us)\\nPSNR: {psnr_pred:.2f}\")  ax[0, 2].imshow(pseudo_gt, cmap=\"gray\") ax[0, 2].title.set_text(\"Pseudo GT (5 us averaged)\")  ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 2].imshow(pseudo_gt[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[7]: <pre>&lt;matplotlib.image.AxesImage at 0x7ff8d0325f90&gt;</pre> In\u00a0[8]: Copied! <pre># create a cover image\nx_start, width = 500, 512\ny_start, height = 1400, 512\n\n# create image\ncover = np.zeros((height, width))   \n\n# normalize train and prediction\nnorm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())\n\npred = prediction[0].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2]\ncover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"SEM_Noise2Noise.jpeg\")\n</pre> # create a cover image x_start, width = 500, 512 y_start, height = 1400, 512  # create image cover = np.zeros((height, width))     # normalize train and prediction norm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())  pred = prediction[0].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2] cover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"SEM_Noise2Noise.jpeg\")"},{"location":"applications/Noise2Noise/SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p> <p>The N2N SEM dataset consists of EM images with 7 different levels of noise:</p> <ul> <li>Image 0 is recorded with 0.2 us scan time</li> <li>Image 1 is recorded with 0.5 us scan time</li> <li>Image 2 is recorded with 1 us scan time</li> <li>Image 3 is recorded with 1 us scan time</li> <li>Image 4 is recorded with 2.1 us scan time</li> <li>Image 5 is recorded with 5.0 us scan time</li> <li>Image 6 is recorded with 5.0 us scan time and is the avg. of 4 images</li> </ul>"},{"location":"applications/Noise2Noise/SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Noise/SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Noise/SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Noise/SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Noise/SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Noise/SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Noise/SEM/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/","title":"Noise2Void","text":"<p>For more details on the algorithm, check out its description.</p> Mouse Nuclei <p>                                   Denoising of 2D fluorescence images of nuclei with Noise2Void.                               </p> 2D fluorescence SEM <p>                                   Denoising of 2D scanning electron microscopy with Noise2Void.                               </p> 2D electron microscopy Hagen <p>                                   N2V on the AI4Life Denoising Challenge with structured noise.                               </p> 2D fluorescence structured Flywing <p>                                   Denoising of 3D tissue using Noise2Void.                               </p> 3D fluorescence tissue SUPPORT <p>                                   N2V on the AI4Life Denoising Challenge with structured noise.                               </p> 3D fluorescence structured JUMP <p>                                   N2V on the AI4Life Denoising Challenge with unstructured noise.                               </p> 2D channels fluorescence W2S <p>                                   N2V on the AI4Life Denoising Challenge with unstructured noise.                               </p> 2D channels fluorescence BSD68 <p>                                   Benchmark Noise2Void in 2D using natural images and synthetic noise.                               </p> 2D benchmark <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/Noise2Void/BSD68/","title":"BSD68","text":"<p> Find me on Github </p> <p>The BSD68 dataset was adapted from K. Zhang et al (TIP, 2017) and is composed of natural images. The noise was artificially added, allowing for quantitative comparisons with the ground truth, one of the benchmark used in many denoising publications. Here, we check the performances of Noise2Void.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_BSD68.download(root_path)\n\n# create paths for the data\ndata_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\")\ntrain_path = data_path / \"train\"\nval_path = data_path / \"val\"\ntest_path = data_path / \"test\" / \"images\"\ngt_path = data_path / \"test\" / \"gt\"\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_BSD68.download(root_path)  # create paths for the data data_path = Path(root_path / \"denoising-N2V_BSD68.unzip/BSD68_reproducibility_data\") train_path = data_path / \"train\" val_path = data_path / \"val\" test_path = data_path / \"test\" / \"images\" gt_path = data_path / \"test\" / \"gt\" In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\nsingle_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0]\nsingle_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(single_train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(single_val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side single_train_image = tifffile.imread(next(iter(train_path.rglob(\"*.tiff\"))))[0] single_val_image = tifffile.imread(next(iter(val_path.rglob(\"*.tiff\"))))[0]  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(single_train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(single_val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[3]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[4]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"bsd68_n2v\",\n    data_type=\"tiff\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=64,\n    num_epochs=100,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"bsd68_n2v\",     data_type=\"tiff\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=64,     num_epochs=100, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 64,\n                 'data_type': 'tiff',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'bsd68_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 100,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_path,\n    val_source=val_path,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_path,     val_source=val_path, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=test_path,\n    axes=\"YX\",\n    tile_size=(128, 128),\n    tile_overlap=(48, 48),\n    batch_size=1,\n)\n</pre> prediction = careamist.predict(     source=test_path,     axes=\"YX\",     tile_size=(128, 128),     tile_overlap=(48, 48),     batch_size=1, ) In\u00a0[7]: Copied! <pre># Show two images\nnoises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))]\ngts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]\n\n# images to show\nimages = np.random.choice(range(len(noises)), 3)\n\nfig, ax = plt.subplots(3, 3, figsize=(15, 15))\nfig.tight_layout()\n\nfor i in range(3):\n    pred_image = prediction[images[i]].squeeze()\n    psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])\n    psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)\n\n    ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")\n    ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")\n\n    ax[i, 1].imshow(pred_image, cmap=\"gray\")\n    ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")\n\n    ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")\n    ax[i, 2].title.set_text(\"Ground-truth\")\n</pre> # Show two images noises = [tifffile.imread(f) for f in sorted(test_path.glob(\"*.tiff\"))] gts = [tifffile.imread(f) for f in sorted(gt_path.glob(\"*.tiff\"))]  # images to show images = np.random.choice(range(len(noises)), 3)  fig, ax = plt.subplots(3, 3, figsize=(15, 15)) fig.tight_layout()  for i in range(3):     pred_image = prediction[images[i]].squeeze()     psnr_noisy = scale_invariant_psnr(gts[images[i]], noises[images[i]])     psnr_result = scale_invariant_psnr(gts[images[i]], pred_image)      ax[i, 0].imshow(noises[images[i]], cmap=\"gray\")     ax[i, 0].title.set_text(f\"Noisy\\nPSNR: {psnr_noisy:.2f}\")      ax[i, 1].imshow(pred_image, cmap=\"gray\")     ax[i, 1].title.set_text(f\"Prediction\\nPSNR: {psnr_result:.2f}\")      ax[i, 2].imshow(gts[images[i]], cmap=\"gray\")     ax[i, 2].title.set_text(\"Ground-truth\") In\u00a0[8]: Copied! <pre>psnrs = np.zeros((len(prediction), 1))\n\nfor i, (pred, gt) in enumerate(zip(prediction, gts)):\n    psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\nprint(\"Reported PSNR: 27.71\")\n</pre> psnrs = np.zeros((len(prediction), 1))  for i, (pred, gt) in enumerate(zip(prediction, gts)):     psnrs[i] = scale_invariant_psnr(gt, pred.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") print(\"Reported PSNR: 27.71\") <pre>PSNR: 26.39 +/- 2.59\nReported PSNR: 27.71\n</pre> In\u00a0[9]: Copied! <pre># create a cover image\nim_idx = 3\ncv_image_noisy = noises[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))\n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : 256 // 2] = norm_noise[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2\n]\ncover[:, 256 // 2 :] = norm_pred[\n    height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"BSD68_Noise2Void.jpeg\")\n</pre> # create a cover image im_idx = 3 cv_image_noisy = noises[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256)) (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : 256 // 2] = norm_noise[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 - 256 // 2 : width // 2 ] cover[:, 256 // 2 :] = norm_pred[     height // 2 - 256 // 2 : height // 2 + 256 // 2, width // 2 : width // 2 + 256 // 2 ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"BSD68_Noise2Void.jpeg\") In\u00a0[11]: Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"bsd68_n2v_model.zip\",\n    friendly_model_name=\"BSD68_N2V\",\n    input_array=noises[im_idx][np.newaxis, ..., :256, :256],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"bsd68_n2v_model.zip\",     friendly_model_name=\"BSD68_N2V\",     input_array=noises[im_idx][np.newaxis, ..., :256, :256],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}], ) <pre>LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/localscratch/miniforge3/envs/careamics/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n</pre> <pre>Predicting: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>computing SHA256 of inputs.npy (result: 4e99b89c03663aafbcdd7b1a92d0db4e3421c05ea63f68163e7a5d61775095c8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 1180.05it/s]\ncomputing SHA256 of outputs.npy (result: a79cf7edbaee8320b4c19d755cc2a4d7945681e8e6522f45a9c056c54b6a47ef): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 1695.35it/s]\ncomputing SHA256 of environment.yml (result: d743faa28b30f054d6caa699271006e27d3cda48e07c02cf092b4e0732830574): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 867.31it/s] \ncomputing SHA256 of weights.pth (result: ca2b5bafccb5d5d4f35b557d17c973a2a0cce8c51515d4867092970e278ae849): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;00:00, 5814.32it/s]\ncomputing SHA256 of config.yml (result: f57e31e3392743a721fd489d29628c89a23687563470a5bf46fb81825ee80b9e): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 912.60it/s] \n2024-09-19 16:15:00.647 | Level 30 | bioimageio.spec._internal.field_warning:issue_warning:149 - documentation: No '# Validation' (sub)section found in /home/melisande.croft/.careamics/README.md.\n2024-09-19 16:15:00.707 | INFO     | bioimageio.core._resource_tests:_test_model_inference:147 - starting 'Reproduce test outputs from test inputs (pytorch_state_dict)'\n/localscratch/miniforge3/envs/careamics/lib/python3.10/site-packages/bioimageio/core/model_adapters/_pytorch_model_adapter.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state: Any = torch.load(\n2024-09-19 16:15:00.946 | INFO     | bioimageio.core._resource_tests:_test_model_inference_parametrized:242 - Testing inference with 2 different input tensor sizes\n/localscratch/miniforge3/envs/careamics/lib/python3.10/site-packages/bioimageio/core/model_adapters/_pytorch_model_adapter.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state: Any = torch.load(\ncomputing SHA256 of config.yml (result: f57e31e3392743a721fd489d29628c89a23687563470a5bf46fb81825ee80b9e): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 299.21it/s]\ncomputing SHA256 of inputs.npy (result: 4e99b89c03663aafbcdd7b1a92d0db4e3421c05ea63f68163e7a5d61775095c8): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 719.27it/s]\ncomputing SHA256 of outputs.npy (result: a79cf7edbaee8320b4c19d755cc2a4d7945681e8e6522f45a9c056c54b6a47ef): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 748.40it/s]\ncomputing SHA256 of environment.yml (result: d743faa28b30f054d6caa699271006e27d3cda48e07c02cf092b4e0732830574): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 243.50it/s]\ncomputing SHA256 of weights.pth (result: ca2b5bafccb5d5d4f35b557d17c973a2a0cce8c51515d4867092970e278ae849): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00&lt;00:00, 3131.98it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"applications/Noise2Void/BSD68/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/BSD68/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/BSD68/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/BSD68/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/BSD68/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training.</p>"},{"location":"applications/Noise2Void/BSD68/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Void/BSD68/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/","title":"Flywing","text":"<p> Find me on Github </p> <p>The Flywing dataset is composed of a single 3D fluorescence microscopy stack. Here, we demonstrate the performances of Noise2Void on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils import autocorrelation\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils import autocorrelation from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfile = portfolio.denoising.Flywing.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") file = portfolio.denoising.Flywing.download(root_path) In\u00a0[3]: Copied! <pre># load stack\ntrain_image = tifffile.imread(file[0])\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(train_image[10])\nax[0].set_title(\"Slice 10\")\nax[1].imshow(train_image[20])\nax[1].set_title(\"Slice 20\")\nax[2].imshow(train_image[30])\nax[2].set_title(\"Slice 30\")\n</pre> # load stack train_image = tifffile.imread(file[0]) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].imshow(train_image[10]) ax[0].set_title(\"Slice 10\") ax[1].imshow(train_image[20]) ax[1].set_title(\"Slice 20\") ax[2].imshow(train_image[30]) ax[2].set_title(\"Slice 30\") <pre>Image shape: (35, 520, 692)\n</pre> Out[3]: <pre>Text(0.5, 1.0, 'Slice 30')</pre> In\u00a0[4]: Copied! <pre>slice_idx = 10\nautocorr = autocorrelation(train_image[slice_idx])\n\n# crop the correlation around (0, 0)\nmidpoint = train_image[slice_idx].shape[0] // 2\ncrop_size = 20\nslices = (\n    slice(midpoint - crop_size, midpoint + crop_size),\n    slice(midpoint - crop_size, midpoint + crop_size),\n)\n# plot autocorrelation\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nax[0].imshow(train_image[slice_idx])\nax[0].set_title(\"Image 1\")\nax[1].imshow(autocorr[slices], cmap=\"gray\")\nax[1].set_title(\"Autocorrelation\")\n</pre> slice_idx = 10 autocorr = autocorrelation(train_image[slice_idx])  # crop the correlation around (0, 0) midpoint = train_image[slice_idx].shape[0] // 2 crop_size = 20 slices = (     slice(midpoint - crop_size, midpoint + crop_size),     slice(midpoint - crop_size, midpoint + crop_size), ) # plot autocorrelation fig, ax = plt.subplots(1, 2, figsize=(15, 5)) ax[0].imshow(train_image[slice_idx]) ax[0].set_title(\"Image 1\") ax[1].imshow(autocorr[slices], cmap=\"gray\") ax[1].set_title(\"Autocorrelation\") Out[4]: <pre>Text(0.5, 1.0, 'Autocorrelation')</pre> In\u00a0[5]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"flywing_n2v\",\n    data_type=\"array\",\n    axes=\"ZYX\",\n    patch_size=(16, 64, 64),\n    batch_size=2,\n    num_epochs=50,\n    use_augmentations=False,  # remove augmentations\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"flywing_n2v\",     data_type=\"array\",     axes=\"ZYX\",     patch_size=(16, 64, 64),     batch_size=2,     num_epochs=50,     use_augmentations=False,  # remove augmentations )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 3,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'ZYX',\n                 'batch_size': 2,\n                 'data_type': 'array',\n                 'patch_size': [16, 64, 64],\n                 'transforms': [{'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'flywing_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 50},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(32, 128, 128),\n    tile_overlap=(8, 48, 48),\n    batch_size=1,\n    tta=False,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(32, 128, 128),     tile_overlap=(8, 48, 48),     batch_size=1,     tta=False, ) In\u00a0[8]: Copied! <pre>pred_folder = Path(\"results_n2v\")\npred_folder.mkdir(exist_ok=True, parents=True)\n\ntifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0])\n</pre> pred_folder = Path(\"results_n2v\") pred_folder.mkdir(exist_ok=True, parents=True)  tifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0]) In\u00a0[9]: Copied! <pre># Show multiple slices\nzs = [5, 10, 15, 20, 25, 30]\n\nfig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs)))\nfor i, z in enumerate(zs):\n    ax[i, 0].imshow(train_image[z])\n    ax[i, 0].set_title(f\"Noisy - Plane {z}\")\n\n    ax[i, 1].imshow(prediction[0].squeeze()[z])\n    ax[i, 1].set_title(f\"Prediction - Plane {z}\")\n</pre> # Show multiple slices zs = [5, 10, 15, 20, 25, 30]  fig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs))) for i, z in enumerate(zs):     ax[i, 0].imshow(train_image[z])     ax[i, 0].set_title(f\"Noisy - Plane {z}\")      ax[i, 1].imshow(prediction[0].squeeze()[z])     ax[i, 1].set_title(f\"Prediction - Plane {z}\") In\u00a0[44]: Copied! <pre># create a cover image\nim_idx = 20\nx_min = 0\ny_min = 0\nsize = 256\n\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[0].squeeze()[im_idx]\n\n# create image\ncover = np.zeros((size, size))\n(height, width) = cv_image_noisy.shape\nassert height &gt; size\nassert width &gt; size\nassert y_min + size &lt; height\nassert x_min + size &lt; width\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (\n    cv_image_noisy.max() - cv_image_noisy.min()\n)\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (\n    cv_image_pred.max() - cv_image_pred.min()\n)\n\n# fill in halves\ncover[:, : size // 2] = norm_noise[y_min : y_min + size, x_min : x_min + size // 2]\ncover[:, size // 2 :] = norm_pred[y_min : y_min + size, x_min : x_min + size // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image (saturate for rendering)\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\n\nim.save(\"Flywing_N2V.jpeg\")\n</pre> # create a cover image im_idx = 20 x_min = 0 y_min = 0 size = 256  cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[0].squeeze()[im_idx]  # create image cover = np.zeros((size, size)) (height, width) = cv_image_noisy.shape assert height &gt; size assert width &gt; size assert y_min + size &lt; height assert x_min + size &lt; width  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (     cv_image_noisy.max() - cv_image_noisy.min() ) norm_pred = (cv_image_pred - cv_image_pred.min()) / (     cv_image_pred.max() - cv_image_pred.min() )  # fill in halves cover[:, : size // 2] = norm_noise[y_min : y_min + size, x_min : x_min + size // 2] cover[:, size // 2 :] = norm_pred[y_min : y_min + size, x_min : x_min + size // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image (saturate for rendering) im = Image.fromarray(cover * 255) im = im.convert(\"L\")  im.save(\"Flywing_N2V.jpeg\")"},{"location":"applications/Noise2Void/Flywing/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/Flywing/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#compute-autocorrelation","title":"Compute autocorrelation\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/Flywing/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/Flywing/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/Flywing/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/Flywing/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/Flywing/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/","title":"Hagen dataset","text":"<p> Find me on Github </p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pooch\nimport tifffile\nfrom PIL import Image\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils import autocorrelation\n\n# use n2v2\nuse_n2v2 = False\n\n# folder in which to save all the data\nroot = Path(\"hagen\")\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import pooch import tifffile from PIL import Image from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils import autocorrelation  # use n2v2 use_n2v2 = False  # folder in which to save all the data root = Path(\"hagen\") In\u00a0[\u00a0]: remove_output Copied! <pre># download the data using pooch\ndata_root = root / \"data\"\ndataset_url = \"https://zenodo.org/records/10925855/files/noisy.tiff?download=1\"\n\nfile = pooch.retrieve(\n    url=dataset_url,\n    known_hash=\"ff12ee5566f443d58976757c037ecef8bf53a00794fa822fe7bcd0dd776a9c0f\",\n    path=data_root,\n)\n</pre> # download the data using pooch data_root = root / \"data\" dataset_url = \"https://zenodo.org/records/10925855/files/noisy.tiff?download=1\"  file = pooch.retrieve(     url=dataset_url,     known_hash=\"ff12ee5566f443d58976757c037ecef8bf53a00794fa822fe7bcd0dd776a9c0f\",     path=data_root, ) In\u00a0[3]: Copied! <pre>data_description = (\n    \"Laser scanning fluorescence microscopy imaging of actin in bovine pulmonary \"\n    \"artery endothelial cells, acquired with a low signal-to-noise ratio. This dataset \"\n    \"is a subset of Dataset 4, as described in the paper by Hagen, G.M. et al., 2021 \"\n    \"(Fluorescence Microscopy Datasets for Training Deep Neural Networks, \"\n    \"GigaScience). The data is available on Zenodo at \"\n    \"https://zenodo.org/records/10925855.\"\n)\nprint(data_description)\n</pre> data_description = (     \"Laser scanning fluorescence microscopy imaging of actin in bovine pulmonary \"     \"artery endothelial cells, acquired with a low signal-to-noise ratio. This dataset \"     \"is a subset of Dataset 4, as described in the paper by Hagen, G.M. et al., 2021 \"     \"(Fluorescence Microscopy Datasets for Training Deep Neural Networks, \"     \"GigaScience). The data is available on Zenodo at \"     \"https://zenodo.org/records/10925855.\" ) print(data_description) <pre>Laser scanning fluorescence microscopy imaging of actin in bovine pulmonary artery endothelial cells, acquired with a low signal-to-noise ratio. This dataset is a subset of Dataset 4, as described in the paper by Hagen, G.M. et al., 2021 (Fluorescence Microscopy Datasets for Training Deep Neural Networks, GigaScience). The data is available on Zenodo at https://zenodo.org/records/10925855.\n</pre> In\u00a0[4]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(file)\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(\n    train_image[0], \n    cmap=\"gray\", \n    vmin=np.percentile(train_image[0], 1), \n    vmax=np.percentile(train_image[0], 99),\n)\nax[0].set_title(\"Image 1\")\nax[1].imshow(\n    train_image[1], \n    cmap=\"gray\", \n    vmin=np.percentile(train_image[1], 1), \n    vmax=np.percentile(train_image[1], 99),\n)\nax[1].set_title(\"Image 2\")\nax[2].imshow(\n    train_image[2], \n    cmap=\"gray\", \n    vmin=np.percentile(train_image[2], 1), \n    vmax=np.percentile(train_image[2], 99),\n)\nax[2].set_title(\"Image 3\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(file) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].imshow(     train_image[0],      cmap=\"gray\",      vmin=np.percentile(train_image[0], 1),      vmax=np.percentile(train_image[0], 99), ) ax[0].set_title(\"Image 1\") ax[1].imshow(     train_image[1],      cmap=\"gray\",      vmin=np.percentile(train_image[1], 1),      vmax=np.percentile(train_image[1], 99), ) ax[1].set_title(\"Image 2\") ax[2].imshow(     train_image[2],      cmap=\"gray\",      vmin=np.percentile(train_image[2], 1),      vmax=np.percentile(train_image[2], 99), ) ax[2].set_title(\"Image 3\") <pre>Image shape: (79, 1024, 1024)\n</pre> Out[4]: <pre>Text(0.5, 1.0, 'Image 3')</pre> In\u00a0[5]: Copied! <pre>autocorr = autocorrelation(train_image[0])\n\n# crop the correlation around (0, 0)\nmidpoint = train_image[0].shape[0] // 2\ncrop_size = 20\nslices = (\n    slice(midpoint - crop_size, midpoint + crop_size),\n    slice(midpoint - crop_size, midpoint + crop_size),\n)\n# plot autocorrelation\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nax[0].imshow(train_image[0], cmap=\"gray\")\nax[0].set_title(\"Image 1\")\nax[1].imshow(autocorr[slices], cmap=\"gray\")\nax[1].set_title(\"Autocorrelation\")\n</pre> autocorr = autocorrelation(train_image[0])  # crop the correlation around (0, 0) midpoint = train_image[0].shape[0] // 2 crop_size = 20 slices = (     slice(midpoint - crop_size, midpoint + crop_size),     slice(midpoint - crop_size, midpoint + crop_size), ) # plot autocorrelation fig, ax = plt.subplots(1, 2, figsize=(15, 5)) ax[0].imshow(train_image[0], cmap=\"gray\") ax[0].set_title(\"Image 1\") ax[1].imshow(autocorr[slices], cmap=\"gray\") ax[1].set_title(\"Autocorrelation\") Out[5]: <pre>Text(0.5, 1.0, 'Autocorrelation')</pre> In\u00a0[6]: Copied! <pre># create configuration\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=\"hagen_\" + algo,\n    data_type=\"array\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=10,\n    use_n2v2=use_n2v2,\n)\n\n# change augmentations\nconfig.data_config.transforms[0].flip_y = False  # do not flip y\nconfig.data_config.transforms.pop(1)  # remove 90 degree rotations\n\nprint(config)\n</pre> # create configuration algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=\"hagen_\" + algo,     data_type=\"array\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=10,     use_n2v2=use_n2v2, )  # change augmentations config.data_config.transforms[0].flip_y = False  # do not flip y config.data_config.transforms.pop(1)  # remove 90 degree rotations  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': False,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'hagen_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 10,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(\n    source=config,\n    work_dir=root / algo,\n)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(     source=config,     work_dir=root / algo, )  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(256, 256),\n    tile_overlap=(48, 48),\n    batch_size=1,\n    tta=False,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(256, 256),     tile_overlap=(48, 48),     batch_size=1,     tta=False, ) In\u00a0[9]: Copied! <pre>pred_folder = root / (\"results_\" + algo)\npred_folder.mkdir(exist_ok=True, parents=True)\n\nfinal_data = np.concatenate(prediction)\ntifffile.imwrite(pred_folder / \"prediction.tiff\", final_data)\n</pre> pred_folder = root / (\"results_\" + algo) pred_folder.mkdir(exist_ok=True, parents=True)  final_data = np.concatenate(prediction) tifffile.imwrite(pred_folder / \"prediction.tiff\", final_data) In\u00a0[10]: Copied! <pre>n = 5\n\nfig, ax = plt.subplots(n, 2, figsize=(10, n * 5))\nfor i in range(n):\n    ax[i, 0].imshow(\n        train_image[i], \n        cmap=\"gray\", \n        vmin=np.percentile(train_image[i], 1), \n        vmax=np.percentile(train_image[i], 99),\n    )\n    ax[i, 0].set_title(f\"Noisy - Image {i}\")\n    ax[i, 1].imshow(\n        prediction[i].squeeze(), \n        cmap=\"gray\", \n        vmin=np.percentile(prediction[i], 1), \n        vmax=np.percentile(prediction[i], 99),\n    )\n    ax[i, 1].set_title(f\"Prediction - Prediction {i}\")\n</pre> n = 5  fig, ax = plt.subplots(n, 2, figsize=(10, n * 5)) for i in range(n):     ax[i, 0].imshow(         train_image[i],          cmap=\"gray\",          vmin=np.percentile(train_image[i], 1),          vmax=np.percentile(train_image[i], 99),     )     ax[i, 0].set_title(f\"Noisy - Image {i}\")     ax[i, 1].imshow(         prediction[i].squeeze(),          cmap=\"gray\",          vmin=np.percentile(prediction[i], 1),          vmax=np.percentile(prediction[i], 99),     )     ax[i, 1].set_title(f\"Prediction - Prediction {i}\") In\u00a0[11]: Copied! <pre># create a cover image\nim_idx = 0\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((256, 256))   \n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min())\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())\n\n# fill in halves\ncover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2]\ncover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"Hagen_N2V.jpeg\")\n</pre> # create a cover image im_idx = 0 cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((256, 256))    (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min()) norm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())  # fill in halves cover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2] cover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"Hagen_N2V.jpeg\") In\u00a0[12]: Copied! <pre>general_description = (\n    \"This model is a UNet trained with the Noise2Void algorithm to denoise images. The \"\n    \"training data consists of laser scanning confocal microscopy images of actin in \"\n    \"bovine pulmonary artery endothelial cells. The notebook used to train this model \"\n    \"is available on the CAREamics documentation website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/Noise2Void/Hagen/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained with the Noise2Void algorithm to denoise images. The \"     \"training data consists of laser scanning confocal microscopy images of actin in \"     \"bovine pulmonary artery endothelial cells. The notebook used to train this model \"     \"is available on the CAREamics documentation website at the following link: \"     \"https://careamics.github.io/0.1/applications/Noise2Void/Hagen/.\" ) print(general_description) <pre>This model is a UNet trained with the Noise2Void algorithm to denoise images. The training data consists of laser scanning confocal microscopy images of actin in bovine pulmonary artery endothelial cells. The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/Noise2Void/Hagen/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"hagen_n2v_model.zip\",\n    friendly_model_name=\"Hagen_N2V\",\n    input_array=train_image[[im_idx], 384:640, 256:512].astype(np.float32),\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=data_description,\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"hagen_n2v_model.zip\",     friendly_model_name=\"Hagen_N2V\",     input_array=train_image[[im_idx], 384:640, 256:512].astype(np.float32),     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=data_description, )"},{"location":"applications/Noise2Void/Hagen/#hagen-dataset","title":"Hagen dataset\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#import-the-dataset","title":"Import the dataset\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#compute-autocorrelation","title":"Compute autocorrelation\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#train-with-careamics","title":"Train with CAREamics\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#create-configuration","title":"Create configuration\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#train","title":"Train\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/Hagen/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/","title":"JUMP dataset","text":"<p> Find me on Github </p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pooch\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# use n2v2\nuse_n2v2 = False\n\n# folder in which to save all the data\nroot = Path(\"jump\")\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import pooch import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration  # use n2v2 use_n2v2 = False  # folder in which to save all the data root = Path(\"jump\") In\u00a0[\u00a0]: remove_output Copied! <pre># download the data using pooch\ndata_root = root / \"data\"\ndataset_url = \"https://zenodo.org/records/10912386/files/noisy.tiff\"\n\nfile = pooch.retrieve(\n    url=dataset_url,\n    known_hash=\"394541cd10b5f10cc929e8083e42f757e47eb0a318cf78448a9e4622179c3069\",\n    path=data_root,\n)\n</pre> # download the data using pooch data_root = root / \"data\" dataset_url = \"https://zenodo.org/records/10912386/files/noisy.tiff\"  file = pooch.retrieve(     url=dataset_url,     known_hash=\"394541cd10b5f10cc929e8083e42f757e47eb0a318cf78448a9e4622179c3069\",     path=data_root, ) In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(file)\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 4, figsize=(20, 5))\nax[0].imshow(train_image[0][0], cmap=\"gray\")\nax[0].set_title(\"Channel 1\")\nax[1].imshow(train_image[0][1], cmap=\"gray\")\nax[1].set_title(\"Channel 2\")\nax[2].imshow(train_image[0][2], cmap=\"gray\")\nax[2].set_title(\"Channel 3\")\nax[3].imshow(train_image[0][3], cmap=\"gray\")\nax[3].set_title(\"Channel 4\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(file) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 4, figsize=(20, 5)) ax[0].imshow(train_image[0][0], cmap=\"gray\") ax[0].set_title(\"Channel 1\") ax[1].imshow(train_image[0][1], cmap=\"gray\") ax[1].set_title(\"Channel 2\") ax[2].imshow(train_image[0][2], cmap=\"gray\") ax[2].set_title(\"Channel 3\") ax[3].imshow(train_image[0][3], cmap=\"gray\") ax[3].set_title(\"Channel 4\") <pre>Image shape: (517, 4, 540, 540)\n</pre> Out[3]: <pre>Text(0.5, 1.0, 'Channel 4')</pre> In\u00a0[4]: Copied! <pre># create configuration\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=\"jump_\" + algo,\n    data_type=\"array\",\n    axes=\"SCYX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=15,\n    n_channels=4,\n    use_n2v2=use_n2v2,\n)\n\nprint(config)\n</pre> # create configuration algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=\"jump_\" + algo,     data_type=\"array\",     axes=\"SCYX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=15,     n_channels=4,     use_n2v2=use_n2v2, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 4,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 4},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SCYX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'jump_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 15},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(\n    source=config,\n    work_dir=root / algo,\n)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(     source=config,     work_dir=root / algo, )  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(256, 256),\n    tile_overlap=(48, 48),\n    batch_size=1,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(256, 256),     tile_overlap=(48, 48),     batch_size=1, ) In\u00a0[7]: Copied! <pre>pred_folder = root / (\"results_\" + algo)\npred_folder.mkdir(exist_ok=True, parents=True)\n\nfinal_data = np.concatenate(prediction)\ntifffile.imwrite(pred_folder / \"prediction.tiff\", final_data)\n</pre> pred_folder = root / (\"results_\" + algo) pred_folder.mkdir(exist_ok=True, parents=True)  final_data = np.concatenate(prediction) tifffile.imwrite(pred_folder / \"prediction.tiff\", final_data) In\u00a0[10]: Copied! <pre>n = 5\n\nfig, ax = plt.subplots(4 * n, 2, figsize=(10, 4 * n * 5))\nfor i in range(n):\n    row = 4 * i\n\n    # channel 1\n    ax[row, 0].imshow(train_image[i, 0], cmap=\"gray\")\n    ax[row, 0].set_title(\"Noisy - Channel 1\")\n    ax[row, 1].imshow(prediction[i].squeeze()[0], cmap=\"gray\")\n    ax[row, 1].set_title(\"Prediction - Channel 1\")\n\n    # channel 2\n    ax[row + 1, 0].imshow(train_image[i, 1], cmap=\"gray\")\n    ax[row + 1, 0].set_title(\"Noisy - Channel 2\")\n    ax[row + 1, 1].imshow(prediction[i].squeeze()[1], cmap=\"gray\")\n    ax[row + 1, 1].set_title(\"Prediction - Channel 2\")\n\n    # channel 3\n    ax[row + 2, 0].imshow(train_image[i, 2], cmap=\"gray\")\n    ax[row + 2, 0].set_title(\"Noisy - Channel 3\")\n    ax[row + 2, 1].imshow(prediction[i].squeeze()[2], cmap=\"gray\")\n    ax[row + 2, 1].set_title(\"Prediction - Channel 3\")\n\n    # channel 4\n    ax[row + 3, 0].imshow(train_image[i, 3], cmap=\"gray\")\n    ax[row + 3, 0].set_title(\"Noisy - Channel 4\")\n    ax[row + 3, 1].imshow(prediction[i].squeeze()[3], cmap=\"gray\")\n    ax[row + 3, 1].set_title(\"Prediction - Channel 4\")\n</pre> n = 5  fig, ax = plt.subplots(4 * n, 2, figsize=(10, 4 * n * 5)) for i in range(n):     row = 4 * i      # channel 1     ax[row, 0].imshow(train_image[i, 0], cmap=\"gray\")     ax[row, 0].set_title(\"Noisy - Channel 1\")     ax[row, 1].imshow(prediction[i].squeeze()[0], cmap=\"gray\")     ax[row, 1].set_title(\"Prediction - Channel 1\")      # channel 2     ax[row + 1, 0].imshow(train_image[i, 1], cmap=\"gray\")     ax[row + 1, 0].set_title(\"Noisy - Channel 2\")     ax[row + 1, 1].imshow(prediction[i].squeeze()[1], cmap=\"gray\")     ax[row + 1, 1].set_title(\"Prediction - Channel 2\")      # channel 3     ax[row + 2, 0].imshow(train_image[i, 2], cmap=\"gray\")     ax[row + 2, 0].set_title(\"Noisy - Channel 3\")     ax[row + 2, 1].imshow(prediction[i].squeeze()[2], cmap=\"gray\")     ax[row + 2, 1].set_title(\"Prediction - Channel 3\")      # channel 4     ax[row + 3, 0].imshow(train_image[i, 3], cmap=\"gray\")     ax[row + 3, 0].set_title(\"Noisy - Channel 4\")     ax[row + 3, 1].imshow(prediction[i].squeeze()[3], cmap=\"gray\")     ax[row + 3, 1].set_title(\"Prediction - Channel 4\") In\u00a0[11]: Copied! <pre># create a cover image\nim_idx = 9\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((4, 256, 256))\n(_, height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# get min and max and reshape them so that they can be broadcasted with the images\nnoise_min = np.array(np.min(cv_image_noisy, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_noisy.ndim - 1))\n]\nnoise_max = np.array(np.max(cv_image_noisy, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_noisy.ndim - 1))\n]\npred_min = np.array(np.min(cv_image_pred, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_pred.ndim - 1))\n]\npred_max = np.array(np.max(cv_image_pred, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_pred.ndim - 1))\n]\n\n# normalize train and prediction per channel\nnorm_noise = (cv_image_noisy - noise_min) / (noise_max - noise_min)\nnorm_pred = (cv_image_pred - pred_min) / (pred_max - pred_min)\n\n# fill in halves\ncover[:, :, : 256 // 2] = norm_noise[\n    :,\n    height // 2 - 256 // 2 : height // 2 + 256 // 2,\n    width // 2 - 256 // 2 : width // 2,\n]\ncover[:, :, 256 // 2 :] = norm_pred[\n    :,\n    height // 2 - 256 // 2 : height // 2 + 256 // 2,\n    width // 2 : width // 2 + 256 // 2,\n]\n\n# move C axis at the end\ncover = np.moveaxis(cover, 0, -1)\n\n# plot the single image\nplt.imshow(cover)\n\n# save the image\nplt.imsave(\"JUMP_N2V.jpeg\", cover)\n</pre> # create a cover image im_idx = 9 cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((4, 256, 256)) (_, height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # get min and max and reshape them so that they can be broadcasted with the images noise_min = np.array(np.min(cv_image_noisy, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_noisy.ndim - 1)) ] noise_max = np.array(np.max(cv_image_noisy, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_noisy.ndim - 1)) ] pred_min = np.array(np.min(cv_image_pred, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_pred.ndim - 1)) ] pred_max = np.array(np.max(cv_image_pred, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_pred.ndim - 1)) ]  # normalize train and prediction per channel norm_noise = (cv_image_noisy - noise_min) / (noise_max - noise_min) norm_pred = (cv_image_pred - pred_min) / (pred_max - pred_min)  # fill in halves cover[:, :, : 256 // 2] = norm_noise[     :,     height // 2 - 256 // 2 : height // 2 + 256 // 2,     width // 2 - 256 // 2 : width // 2, ] cover[:, :, 256 // 2 :] = norm_pred[     :,     height // 2 - 256 // 2 : height // 2 + 256 // 2,     width // 2 : width // 2 + 256 // 2, ]  # move C axis at the end cover = np.moveaxis(cover, 0, -1)  # plot the single image plt.imshow(cover)  # save the image plt.imsave(\"JUMP_N2V.jpeg\", cover)"},{"location":"applications/Noise2Void/JUMP/#jump-dataset","title":"JUMP dataset\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#import-the-dataset","title":"Import the dataset\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#train-with-careamics","title":"Train with CAREamics\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#create-configuration","title":"Create configuration\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#train","title":"Train\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/JUMP/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/","title":"Mouse Nuclei","text":"<p> Find me on Github </p> <p>The mouse nuclei dataset is composed of a train and test dataset.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom careamics_portfolio import PortfolioManager\nfrom PIL import Image\n</pre> # Imports necessary to execute the code  import matplotlib.pyplot as plt import numpy as np from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils.metrics import scale_invariant_psnr from careamics_portfolio import PortfolioManager from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># download file\nportfolio = PortfolioManager()\nfiles = portfolio.denoiseg.MouseNuclei_n20.download()\nfiles.sort()\n\n# load images\ntrain_data = np.load(files[1])[\"X_train\"]\nprint(f\"Train data shape: {train_data.shape}\")\n</pre> # download file portfolio = PortfolioManager() files = portfolio.denoiseg.MouseNuclei_n20.download() files.sort()  # load images train_data = np.load(files[1])[\"X_train\"] print(f\"Train data shape: {train_data.shape}\") In\u00a0[3]: Copied! <pre>portfolio.denoiseg.MouseNuclei_n20.description\n</pre> portfolio.denoiseg.MouseNuclei_n20.description Out[3]: <pre>'A dataset depicting diverse and non-uniformly clustered nuclei in the mouse skull, consisting of 908 training and 160 validation patches. The test set counts 67 additional images'</pre> In\u00a0[4]: Copied! <pre>indices = [34, 293, 571, 783]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].imshow(train_data[indices[0]], cmap=\"gray\")\nax[0, 0].set_title(f\"Image {indices[0]}\")\nax[0, 0].set_xticks([])\nax[0, 0].set_yticks([])\n\nax[0, 1].imshow(train_data[indices[1]], cmap=\"gray\")\nax[0, 1].set_title(f\"Image {indices[1]}\")\nax[0, 1].set_xticks([])\nax[0, 1].set_yticks([])\n\nax[1, 0].imshow(train_data[indices[2]], cmap=\"gray\")\nax[1, 0].set_title(f\"Image {indices[2]}\")\nax[1, 0].set_xticks([])\nax[1, 0].set_yticks([])\n\nax[1, 1].imshow(train_data[indices[3]], cmap=\"gray\")\nax[1, 1].set_title(f\"Image {indices[3]}\")\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\n\nplt.show()\n</pre> indices = [34, 293, 571, 783]  fig, ax = plt.subplots(2, 2, figsize=(8, 8)) ax[0, 0].imshow(train_data[indices[0]], cmap=\"gray\") ax[0, 0].set_title(f\"Image {indices[0]}\") ax[0, 0].set_xticks([]) ax[0, 0].set_yticks([])  ax[0, 1].imshow(train_data[indices[1]], cmap=\"gray\") ax[0, 1].set_title(f\"Image {indices[1]}\") ax[0, 1].set_xticks([]) ax[0, 1].set_yticks([])  ax[1, 0].imshow(train_data[indices[2]], cmap=\"gray\") ax[1, 0].set_title(f\"Image {indices[2]}\") ax[1, 0].set_xticks([]) ax[1, 0].set_yticks([])  ax[1, 1].imshow(train_data[indices[3]], cmap=\"gray\") ax[1, 1].set_title(f\"Image {indices[3]}\") ax[1, 1].set_xticks([]) ax[1, 1].set_yticks([])  plt.show() In\u00a0[5]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"mouse_nuclei_n2v\",\n    data_type=\"array\",\n    axes=\"SYX\",\n    patch_size=(64, 64),\n    batch_size=16,\n    num_epochs=10,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"mouse_nuclei_n2v\",     data_type=\"array\",     axes=\"SYX\",     patch_size=(64, 64),     batch_size=16,     num_epochs=10, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SYX',\n                 'batch_size': 16,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'mouse_nuclei_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 10,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_data,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_data, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_data)\n</pre> prediction = careamist.predict(source=train_data) In\u00a0[8]: Copied! <pre># download ground truth\nfiles = portfolio.denoiseg.MouseNuclei_n0.download()\nfiles.sort()\n\n# load images\ngt_data = np.load(files[1])[\"X_train\"]\nprint(f\"GT data shape: {gt_data.shape}\")\n</pre> # download ground truth files = portfolio.denoiseg.MouseNuclei_n0.download() files.sort()  # load images gt_data = np.load(files[1])[\"X_train\"] print(f\"GT data shape: {gt_data.shape}\") <pre>GT data shape: (908, 128, 128)\n</pre> In\u00a0[9]: Copied! <pre>indices = [389, 621]\n\nfor i in indices:\n    # compute psnr\n    psnr_noisy = scale_invariant_psnr(gt_data[i], train_data[i])\n    psnr_denoised = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())\n\n    # plot images\n    fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n    ax[0].imshow(train_data[i], cmap=\"gray\")\n    ax[0].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}\")\n    ax[0].set_xticks([])\n    ax[0].set_yticks([])\n\n    ax[1].imshow(prediction[i].squeeze(), cmap=\"gray\")\n    ax[1].set_title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}\")\n    ax[1].set_xticks([])\n    ax[1].set_yticks([])\n\n    ax[2].imshow(gt_data[i], cmap=\"gray\")\n    ax[2].set_title(\"GT Image\")\n    ax[2].set_xticks([])\n    ax[2].set_yticks([])\n\n    plt.show()\n</pre> indices = [389, 621]  for i in indices:     # compute psnr     psnr_noisy = scale_invariant_psnr(gt_data[i], train_data[i])     psnr_denoised = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())      # plot images     fig, ax = plt.subplots(1, 3, figsize=(10, 10))     ax[0].imshow(train_data[i], cmap=\"gray\")     ax[0].set_title(f\"Noisy Image\\nPSNR: {psnr_noisy:.2f}\")     ax[0].set_xticks([])     ax[0].set_yticks([])      ax[1].imshow(prediction[i].squeeze(), cmap=\"gray\")     ax[1].set_title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}\")     ax[1].set_xticks([])     ax[1].set_yticks([])      ax[2].imshow(gt_data[i], cmap=\"gray\")     ax[2].set_title(\"GT Image\")     ax[2].set_xticks([])     ax[2].set_yticks([])      plt.show() In\u00a0[10]: Copied! <pre>psnrs = np.zeros(gt_data.shape[0])\n\nfor i in range(gt_data.shape[0]):\n    psnrs[i] = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())\n\nprint(f\"PSNR: {np.mean(psnrs):.2f} \u00b1 {np.std(psnrs):.2f}\")\n</pre> psnrs = np.zeros(gt_data.shape[0])  for i in range(gt_data.shape[0]):     psnrs[i] = scale_invariant_psnr(gt_data[i], prediction[i].squeeze())  print(f\"PSNR: {np.mean(psnrs):.2f} \u00b1 {np.std(psnrs):.2f}\") <pre>PSNR: 32.45 \u00b1 1.91\n</pre> In\u00a0[11]: Copied! <pre># create a cover image\nindex_cover = 5\nheight, width = 128, 128\n\n# create image\ncover = np.zeros((height, width))\n\n# normalize train and prediction\nnorm_train = (train_data[index_cover] - train_data[index_cover].min()) / (\n    train_data[index_cover].max() - train_data[index_cover].min()\n)\n\npred = prediction[index_cover].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, : width // 2] = norm_train[:height, : width // 2]\ncover[:, width // 2 :] = norm_pred[:height, width // 2 :]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"MouseNuclei_Noise2Void.jpeg\")\n</pre> # create a cover image index_cover = 5 height, width = 128, 128  # create image cover = np.zeros((height, width))  # normalize train and prediction norm_train = (train_data[index_cover] - train_data[index_cover].min()) / (     train_data[index_cover].max() - train_data[index_cover].min() )  pred = prediction[index_cover].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, : width // 2] = norm_train[:height, : width // 2] cover[:, width // 2 :] = norm_pred[:height, width // 2 :]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"MouseNuclei_Noise2Void.jpeg\") In\u00a0[12]: Copied! <pre>general_description = (\n    \"This model is a UNet trained with the Noise2Void algorithm to denoise images. \"\n    \"The training data consists of images of non-uniformly clustered nuclei in the \"\n    \"mouse skull. The notebook used to train this model is available on the CAREamics \"\n    \"documentation website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained with the Noise2Void algorithm to denoise images. \"     \"The training data consists of images of non-uniformly clustered nuclei in the \"     \"mouse skull. The notebook used to train this model is available on the CAREamics \"     \"documentation website at the following link: \"     \"https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\" ) print(general_description) <pre>This model is a UNet trained with the Noise2Void algorithm to denoise images. The training data consists of images of non-uniformly clustered nuclei in the mouse skull. The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/Noise2Void/Mouse_Nuclei/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"mousenuclei_n2v_model.zip\",\n    friendly_model_name=\"MouseNuclei_N2V\",\n    input_array=train_data[[indices[0]]].astype(np.float32),\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    data_description=portfolio.denoiseg.MouseNuclei_n20.description,\n    general_description=general_description\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"mousenuclei_n2v_model.zip\",     friendly_model_name=\"MouseNuclei_N2V\",     input_array=train_data[[indices[0]]].astype(np.float32),     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     data_description=portfolio.denoiseg.MouseNuclei_n20.description,     general_description=general_description )"},{"location":"applications/Noise2Void/Mouse_Nuclei/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/Mouse_Nuclei/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#compute-metrics","title":"Compute metrics\u00b6","text":""},{"location":"applications/Noise2Void/Mouse_Nuclei/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"applications/Noise2Void/SEM/","title":"SEM","text":"<p> Find me on Github </p> <p>The SEM dataset is composed of a training and a validation images acquired on a scanning electron microscopy (SEM). They were originally used in Buchholtz et al (2019) to showcase CARE denoising. Here, we demonstrate the performances of Noise2Void on this particular dataset!</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport tifffile\nimport numpy as np\nfrom PIL import Image\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics_portfolio import PortfolioManager\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import tifffile import numpy as np from PIL import Image from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics_portfolio import PortfolioManager In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate data portfolio manage\nportfolio = PortfolioManager()\n\n# and download the data\nroot_path = Path(\"./data\")\nfiles = portfolio.denoising.N2V_SEM.download(root_path)\n</pre> # instantiate data portfolio manage portfolio = PortfolioManager()  # and download the data root_path = Path(\"./data\") files = portfolio.denoising.N2V_SEM.download(root_path) In\u00a0[3]: Copied! <pre>portfolio.denoising.N2V_SEM.description\n</pre> portfolio.denoising.N2V_SEM.description Out[3]: <pre>'Cropped images from a SEM dataset from T.-O. Buchholz et al (Methods Cell Biol, 2020).'</pre> In\u00a0[4]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(files[0])\nval_image = tifffile.imread(files[1])\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(train_image, cmap=\"gray\")\nax[0].set_title(\"Training Image\")\nax[1].imshow(val_image, cmap=\"gray\")\nax[1].set_title(\"Validation Image\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(files[0]) val_image = tifffile.imread(files[1])  fig, ax = plt.subplots(1, 2, figsize=(10, 5)) ax[0].imshow(train_image, cmap=\"gray\") ax[0].set_title(\"Training Image\") ax[1].imshow(val_image, cmap=\"gray\") ax[1].set_title(\"Validation Image\") Out[4]: <pre>Text(0.5, 1.0, 'Validation Image')</pre> In\u00a0[5]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"sem_n2v\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=30,\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"sem_n2v\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=30, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'sem_n2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 30,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_source=val_image,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image,     val_source=val_image, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[8]: Copied! <pre># Show the full image and crops\nx_start, x_end = 600, 850\ny_start, y_end = 200, 450\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\")\nax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\")\n</pre> # Show the full image and crops x_start, x_end = 600, 850 y_start, y_end = 200, 450  fig, ax = plt.subplots(2, 2, figsize=(10, 10)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[1, 0].imshow(train_image[y_start:y_end, x_start:x_end], cmap=\"gray\") ax[1, 1].imshow(prediction[0].squeeze()[y_start:y_end, x_start:x_end], cmap=\"gray\") Out[8]: <pre>&lt;matplotlib.image.AxesImage at 0x7fb508551270&gt;</pre> In\u00a0[9]: Copied! <pre># create a cover image\nx_start, width = 500, 512\ny_start, height = 1400, 512\n\n# create image\ncover = np.zeros((height, width))   \n\n# normalize train and prediction\nnorm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())\n\npred = prediction[0].squeeze()\nnorm_pred = (pred - pred.min()) / (pred.max() - pred.min())\n\n# fill in halves\ncover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2]\ncover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"SEM_Noise2Void.jpeg\")\n</pre> # create a cover image x_start, width = 500, 512 y_start, height = 1400, 512  # create image cover = np.zeros((height, width))     # normalize train and prediction norm_train = (train_image - train_image.min()) / (train_image.max() - train_image.min())  pred = prediction[0].squeeze() norm_pred = (pred - pred.min()) / (pred.max() - pred.min())  # fill in halves cover[:, :width // 2] = norm_train[y_start:y_start + height, x_start:x_start + width // 2] cover[:, width // 2:] = norm_pred[y_start:y_start + height, x_start + width // 2:x_start + width]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"SEM_Noise2Void.jpeg\")  In\u00a0[10]: Copied! <pre>general_description = (\n    \"This model is a UNet trained using the Noise2Void algorithm to denoise \"\n    \"images. The training data consists of crops from an SEM dataset \"\n    \"(T.-O. Buchholz et al., Methods Cell Biol, 2020). The notebook used to \"\n    \"train this model is available on the CAREamics documentation website; \"\n    \"find it at the following link: \"\n    \"https://careamics.github.io/0.1/applications/Noise2Void/SEM/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained using the Noise2Void algorithm to denoise \"     \"images. The training data consists of crops from an SEM dataset \"     \"(T.-O. Buchholz et al., Methods Cell Biol, 2020). The notebook used to \"     \"train this model is available on the CAREamics documentation website; \"     \"find it at the following link: \"     \"https://careamics.github.io/0.1/applications/Noise2Void/SEM/.\" ) print(general_description) <pre>This model is a UNet trained using the Noise2Void algorithm to denoise images. The training data consists of crops from an SEM dataset (T.-O. Buchholz et al., Methods Cell Biol, 2020). The notebook used to train this model is available on the CAREamics documentation website; find it at the following link: https://careamics.github.io/0.1/applications/Noise2Void/SEM/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"sem_n2v_model.zip\",\n    friendly_model_name=\"SEM_N2V\",\n    input_array=train_image[1400:1656, 500:756],\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=portfolio.denoising.N2V_SEM.description\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"sem_n2v_model.zip\",     friendly_model_name=\"SEM_N2V\",     input_array=train_image[1400:1656, 500:756],     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=portfolio.denoising.N2V_SEM.description )"},{"location":"applications/Noise2Void/SEM/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/Noise2Void/SEM/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/Noise2Void/SEM/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p>"},{"location":"applications/Noise2Void/SEM/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/Noise2Void/SEM/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/Noise2Void/SEM/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/SEM/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"applications/Noise2Void/SUPPORT/","title":"SUPPORT dataset","text":"<p> Find me on Github </p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pooch\nimport tifffile\nimport numpy as np\nfrom PIL import Image\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.utils import autocorrelation\n\n# use n2v2\nuse_n2v2 = False\n\n# folder in which to save all the data\nroot = Path(\"support\")\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import pooch import tifffile import numpy as np from PIL import Image from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.utils import autocorrelation  # use n2v2 use_n2v2 = False  # folder in which to save all the data root = Path(\"support\") In\u00a0[\u00a0]: remove_output Copied! <pre># download the data using pooch\ndata_root = root / \"data\"\ndataset_url = \"https://zenodo.org/records/10925939/files/noisy.tiff?download=1\"\n\nfile = pooch.retrieve(\n    url=dataset_url,\n    known_hash=\"c09a0748a67a9364f257e0aff9502a283b8794f35381577f5dfea6ac1bd01e03\",\n    path=data_root,\n)\n</pre> # download the data using pooch data_root = root / \"data\" dataset_url = \"https://zenodo.org/records/10925939/files/noisy.tiff?download=1\"  file = pooch.retrieve(     url=dataset_url,     known_hash=\"c09a0748a67a9364f257e0aff9502a283b8794f35381577f5dfea6ac1bd01e03\",     path=data_root, ) In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(file)\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 4, figsize=(20, 5))\nax[0].imshow(train_image[100])\nax[0].set_title(\"Slice 100\")\nax[1].imshow(train_image[400])\nax[1].set_title(\"Slice 400\")\nax[2].imshow(train_image[700])\nax[2].set_title(\"Slice 700\")\nax[3].imshow(train_image[900])\nax[3].set_title(\"Slice 900\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(file) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 4, figsize=(20, 5)) ax[0].imshow(train_image[100]) ax[0].set_title(\"Slice 100\") ax[1].imshow(train_image[400]) ax[1].set_title(\"Slice 400\") ax[2].imshow(train_image[700]) ax[2].set_title(\"Slice 700\") ax[3].imshow(train_image[900]) ax[3].set_title(\"Slice 900\") <pre>Image shape: (1001, 1024, 1024)\n</pre> Out[3]: <pre>Text(0.5, 1.0, 'Slice 900')</pre> In\u00a0[4]: Copied! <pre>autocorr = autocorrelation(train_image[400])\n\n# crop the correlation around (0, 0)\nmidpoint = train_image[400].shape[0] // 2\ncrop_size = 20\nslices = (\n    slice(midpoint - crop_size, midpoint + crop_size),\n    slice(midpoint - crop_size, midpoint + crop_size),\n)\n# plot autocorrelation\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nax[0].imshow(train_image[400])\nax[0].set_title(\"Image 1\")\nax[1].imshow(autocorr[slices], cmap=\"gray\")\nax[1].set_title(\"Autocorrelation\")\n</pre> autocorr = autocorrelation(train_image[400])  # crop the correlation around (0, 0) midpoint = train_image[400].shape[0] // 2 crop_size = 20 slices = (     slice(midpoint - crop_size, midpoint + crop_size),     slice(midpoint - crop_size, midpoint + crop_size), ) # plot autocorrelation fig, ax = plt.subplots(1, 2, figsize=(15, 5)) ax[0].imshow(train_image[400]) ax[0].set_title(\"Image 1\") ax[1].imshow(autocorr[slices], cmap=\"gray\") ax[1].set_title(\"Autocorrelation\") Out[4]: <pre>Text(0.5, 1.0, 'Autocorrelation')</pre> In\u00a0[5]: Copied! <pre># create configuration\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=\"support_\" + algo,\n    data_type=\"array\",\n    axes=\"ZYX\",\n    patch_size=(16, 64, 64),\n    batch_size=2,\n    num_epochs=20,\n    use_n2v2=use_n2v2,\n)\n\n# change augmentations\nconfig.data_config.transforms[0].flip_y = False  # do not flip y\nconfig.data_config.transforms.pop(1)  # remove 90 degree rotations\n\nprint(config)\n</pre> # create configuration algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=\"support_\" + algo,     data_type=\"array\",     axes=\"ZYX\",     patch_size=(16, 64, 64),     batch_size=2,     num_epochs=20,     use_n2v2=use_n2v2, )  # change augmentations config.data_config.transforms[0].flip_y = False  # do not flip y config.data_config.transforms.pop(1)  # remove 90 degree rotations  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 3,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'ZYX',\n                 'batch_size': 2,\n                 'data_type': 'array',\n                 'patch_size': [16, 64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': False,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'support_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 20},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(\n    source=config,\n    work_dir=root / algo,\n)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(     source=config,     work_dir=root / algo, )  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(32, 128, 128),\n    tile_overlap=(8, 48, 48),\n    batch_size=1,\n    tta=False,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(32, 128, 128),     tile_overlap=(8, 48, 48),     batch_size=1,     tta=False, ) In\u00a0[8]: Copied! <pre>pred_folder = root / (\"results_\" + algo)\npred_folder.mkdir(exist_ok=True, parents=True)\n\ntifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0])\n</pre> pred_folder = root / (\"results_\" + algo) pred_folder.mkdir(exist_ok=True, parents=True)  tifffile.imwrite(pred_folder / \"prediction.tiff\", prediction[0]) In\u00a0[9]: Copied! <pre>zs = [100, 250, 500, 750, 900]\n\nfig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs)))\nfor i, z in enumerate(zs):\n    ax[i, 0].imshow(train_image[z])\n    ax[i, 0].set_title(f\"Noisy - Plane {z}\")\n\n    ax[i, 1].imshow(prediction[0].squeeze()[z])\n    ax[i, 1].set_title(f\"Prediction - Plane {z}\")\n</pre> zs = [100, 250, 500, 750, 900]  fig, ax = plt.subplots(len(zs), 2, figsize=(10, 5 * len(zs))) for i, z in enumerate(zs):     ax[i, 0].imshow(train_image[z])     ax[i, 0].set_title(f\"Noisy - Plane {z}\")      ax[i, 1].imshow(prediction[0].squeeze()[z])     ax[i, 1].set_title(f\"Prediction - Plane {z}\") In\u00a0[10]: Copied! <pre># create a cover image\nim_idx = 500\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[0].squeeze()[im_idx]\n\n# create image\ncover = np.zeros((256, 256))   \n(height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# normalize train and prediction\nnorm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min())\nnorm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())\n\n# fill in halves\ncover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2]\ncover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert('L')\nim.save(\"SUPPORT_N2V.jpeg\")\n</pre> # create a cover image im_idx = 500 cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[0].squeeze()[im_idx]  # create image cover = np.zeros((256, 256))    (height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # normalize train and prediction norm_noise = (cv_image_noisy - cv_image_noisy.min()) / (cv_image_noisy.max() - cv_image_noisy.min()) norm_pred = (cv_image_pred - cv_image_pred.min()) / (cv_image_pred.max() - cv_image_pred.min())  # fill in halves cover[:, :256 // 2] = norm_noise[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2 - 256 // 2:width // 2] cover[:, 256 // 2:] = norm_pred[height // 2 - 256 // 2:height // 2 + 256 // 2, width // 2:width // 2 + 256 // 2]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert('L') im.save(\"SUPPORT_N2V.jpeg\")"},{"location":"applications/Noise2Void/SUPPORT/#support-dataset","title":"SUPPORT dataset\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#import-the-dataset","title":"Import the dataset\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#compute-autocorrelation","title":"Compute autocorrelation\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#train-with-careamics","title":"Train with CAREamics\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#create-configuration","title":"Create configuration\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#train","title":"Train\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/SUPPORT/#create-cover","title":"Create cover\u00b6","text":""},{"location":"applications/Noise2Void/W2S/","title":"W2S dataset","text":"<p> Find me on Github </p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pooch\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# use n2v2\nuse_n2v2 = False\n\n# folder in which to save all the data\nroot = Path(\"w2s\")\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import pooch import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration  # use n2v2 use_n2v2 = False  # folder in which to save all the data root = Path(\"w2s\") In\u00a0[\u00a0]: remove_output Copied! <pre># download the data using pooch\ndata_root = root / \"data\"\ndataset_url = \"https://zenodo.org/records/10925783/files/noisy.tiff?download=1\"\n\nfile = pooch.retrieve(\n    url=dataset_url,\n    known_hash=\"b5cb1dbcb86ce72b8d6e0268498712974b9f38d2fc9e18e8a66673a34aa84215\",\n    path=data_root,\n)\n</pre> # download the data using pooch data_root = root / \"data\" dataset_url = \"https://zenodo.org/records/10925783/files/noisy.tiff?download=1\"  file = pooch.retrieve(     url=dataset_url,     known_hash=\"b5cb1dbcb86ce72b8d6e0268498712974b9f38d2fc9e18e8a66673a34aa84215\",     path=data_root, ) In\u00a0[3]: Copied! <pre># load training and validation image and show them side by side\ntrain_image = tifffile.imread(file)\nprint(f\"Image shape: {train_image.shape}\")\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(train_image[0][0], cmap=\"gray\")\nax[0].set_title(\"Channel 1\")\nax[1].imshow(train_image[0][1], cmap=\"gray\")\nax[1].set_title(\"Channel 2\")\nax[2].imshow(train_image[0][2], cmap=\"gray\")\nax[2].set_title(\"Channel 3\")\n</pre> # load training and validation image and show them side by side train_image = tifffile.imread(file) print(f\"Image shape: {train_image.shape}\")  fig, ax = plt.subplots(1, 3, figsize=(15, 5)) ax[0].imshow(train_image[0][0], cmap=\"gray\") ax[0].set_title(\"Channel 1\") ax[1].imshow(train_image[0][1], cmap=\"gray\") ax[1].set_title(\"Channel 2\") ax[2].imshow(train_image[0][2], cmap=\"gray\") ax[2].set_title(\"Channel 3\") <pre>Image shape: (120, 3, 512, 512)\n</pre> Out[3]: <pre>Text(0.5, 1.0, 'Channel 3')</pre> In\u00a0[4]: Copied! <pre># create configuration\nalgo = \"n2v2\" if use_n2v2 else \"n2v\"\n\nconfig = create_n2v_configuration(\n    experiment_name=\"w2s_\" + algo,\n    data_type=\"array\",\n    axes=\"SCYX\",\n    patch_size=(64, 64),\n    batch_size=32,\n    num_epochs=15,\n    n_channels=3,\n    use_n2v2=use_n2v2,\n)\n\nprint(config)\n</pre> # create configuration algo = \"n2v2\" if use_n2v2 else \"n2v\"  config = create_n2v_configuration(     experiment_name=\"w2s_\" + algo,     data_type=\"array\",     axes=\"SCYX\",     patch_size=(64, 64),     batch_size=32,     num_epochs=15,     n_channels=3,     use_n2v2=use_n2v2, )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 3,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 3},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'SCYX',\n                 'batch_size': 32,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': True,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'name': 'XYRandomRotate90', 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'none',\n                                 'struct_mask_span': 5}]},\n 'experiment_name': 'w2s_n2v',\n 'training_config': {'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'num_epochs': 15},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(\n    source=config,\n    work_dir=root / algo,\n)\n\n# train\ncareamist.train(\n    train_source=train_image,\n    val_percentage=0.0,\n    val_minimum_split=10,  # use 10 patches as validation\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(     source=config,     work_dir=root / algo, )  # train careamist.train(     train_source=train_image,     val_percentage=0.0,     val_minimum_split=10,  # use 10 patches as validation ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(\n    source=train_image,\n    tile_size=(256, 256),\n    tile_overlap=(48, 48),\n    batch_size=1,\n)\n</pre> prediction = careamist.predict(     source=train_image,     tile_size=(256, 256),     tile_overlap=(48, 48),     batch_size=1, ) In\u00a0[7]: Copied! <pre>pred_folder = root / (\"results_\" + algo)\npred_folder.mkdir(exist_ok=True, parents=True)\n\nfinal_data = np.concatenate(prediction)\ntifffile.imwrite(pred_folder / \"prediction.tiff\", final_data)\n</pre> pred_folder = root / (\"results_\" + algo) pred_folder.mkdir(exist_ok=True, parents=True)  final_data = np.concatenate(prediction) tifffile.imwrite(pred_folder / \"prediction.tiff\", final_data) In\u00a0[8]: Copied! <pre>n = 5\n\nfig, ax = plt.subplots(3 * n, 2, figsize=(10, 3 * n * 5))\nfor i in range(n):\n    row = 3 * i\n\n    # channel 1\n    ax[row, 0].imshow(train_image[i, 0], cmap=\"gray\")\n    ax[row, 0].set_title(\"Noisy - Channel 1\")\n    ax[row, 1].imshow(prediction[i].squeeze()[0], cmap=\"gray\")\n    ax[row, 1].set_title(\"Prediction - Channel 1\")\n\n    # channel 2\n    ax[row + 1, 0].imshow(train_image[i, 1], cmap=\"gray\")\n    ax[row + 1, 0].set_title(\"Noisy - Channel 2\")\n    ax[row + 1, 1].imshow(prediction[i].squeeze()[1], cmap=\"gray\")\n    ax[row + 1, 1].set_title(\"Prediction - Channel 2\")\n\n    # channel 3\n    ax[row + 2, 0].imshow(train_image[i, 2], cmap=\"gray\")\n    ax[row + 2, 0].set_title(\"Noisy - Channel 3\")\n    ax[row + 2, 1].imshow(prediction[i].squeeze()[2], cmap=\"gray\")\n    ax[row + 2, 1].set_title(\"Prediction - Channel 3\")\n</pre> n = 5  fig, ax = plt.subplots(3 * n, 2, figsize=(10, 3 * n * 5)) for i in range(n):     row = 3 * i      # channel 1     ax[row, 0].imshow(train_image[i, 0], cmap=\"gray\")     ax[row, 0].set_title(\"Noisy - Channel 1\")     ax[row, 1].imshow(prediction[i].squeeze()[0], cmap=\"gray\")     ax[row, 1].set_title(\"Prediction - Channel 1\")      # channel 2     ax[row + 1, 0].imshow(train_image[i, 1], cmap=\"gray\")     ax[row + 1, 0].set_title(\"Noisy - Channel 2\")     ax[row + 1, 1].imshow(prediction[i].squeeze()[1], cmap=\"gray\")     ax[row + 1, 1].set_title(\"Prediction - Channel 2\")      # channel 3     ax[row + 2, 0].imshow(train_image[i, 2], cmap=\"gray\")     ax[row + 2, 0].set_title(\"Noisy - Channel 3\")     ax[row + 2, 1].imshow(prediction[i].squeeze()[2], cmap=\"gray\")     ax[row + 2, 1].set_title(\"Prediction - Channel 3\") In\u00a0[9]: Copied! <pre># create a cover image\nim_idx = 8\ncv_image_noisy = train_image[im_idx]\ncv_image_pred = prediction[im_idx].squeeze()\n\n# create image\ncover = np.zeros((3, 256, 256))\n(_, height, width) = cv_image_noisy.shape\nassert height &gt; 256\nassert width &gt; 256\n\n# get min and max and reshape them so that they can be broadcasted with the images\nnoise_min = np.array(np.min(cv_image_noisy, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_noisy.ndim - 1))\n]\nnoise_max = np.array(np.max(cv_image_noisy, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_noisy.ndim - 1))\n]\npred_min = np.array(np.min(cv_image_pred, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_pred.ndim - 1))\n]\npred_max = np.array(np.max(cv_image_pred, axis=(1, 2)))[\n    (..., *[np.newaxis] * (cv_image_pred.ndim - 1))\n]\n\n# normalize train and prediction per channel\nnorm_noise = (cv_image_noisy - noise_min) / (noise_max - noise_min)\nnorm_pred = (cv_image_pred - pred_min) / (pred_max - pred_min)\n\n# fill in halves\ncover[:, :, : 256 // 2] = norm_noise[\n    :,\n    height // 2 - 256 // 2 : height // 2 + 256 // 2,\n    width // 2 - 256 // 2 : width // 2,\n]\ncover[:, :, 256 // 2 :] = norm_pred[\n    :,\n    height // 2 - 256 // 2 : height // 2 + 256 // 2,\n    width // 2 : width // 2 + 256 // 2,\n]\n\n# move C axis at the end\ncover = np.moveaxis(cover, 0, -1)\n\n# plot the single image\nplt.imshow(cover)\n\n# save the image\nplt.imsave(\"W2S_N2V.jpeg\", cover)\n</pre> # create a cover image im_idx = 8 cv_image_noisy = train_image[im_idx] cv_image_pred = prediction[im_idx].squeeze()  # create image cover = np.zeros((3, 256, 256)) (_, height, width) = cv_image_noisy.shape assert height &gt; 256 assert width &gt; 256  # get min and max and reshape them so that they can be broadcasted with the images noise_min = np.array(np.min(cv_image_noisy, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_noisy.ndim - 1)) ] noise_max = np.array(np.max(cv_image_noisy, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_noisy.ndim - 1)) ] pred_min = np.array(np.min(cv_image_pred, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_pred.ndim - 1)) ] pred_max = np.array(np.max(cv_image_pred, axis=(1, 2)))[     (..., *[np.newaxis] * (cv_image_pred.ndim - 1)) ]  # normalize train and prediction per channel norm_noise = (cv_image_noisy - noise_min) / (noise_max - noise_min) norm_pred = (cv_image_pred - pred_min) / (pred_max - pred_min)  # fill in halves cover[:, :, : 256 // 2] = norm_noise[     :,     height // 2 - 256 // 2 : height // 2 + 256 // 2,     width // 2 - 256 // 2 : width // 2, ] cover[:, :, 256 // 2 :] = norm_pred[     :,     height // 2 - 256 // 2 : height // 2 + 256 // 2,     width // 2 : width // 2 + 256 // 2, ]  # move C axis at the end cover = np.moveaxis(cover, 0, -1)  # plot the single image plt.imshow(cover)  # save the image plt.imsave(\"W2S_N2V.jpeg\", cover)"},{"location":"applications/Noise2Void/W2S/#w2s-dataset","title":"W2S dataset\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#import-the-dataset","title":"Import the dataset\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#train-with-careamics","title":"Train with CAREamics\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#create-configuration","title":"Create configuration\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#train","title":"Train\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#predict","title":"Predict\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#save-predictions","title":"Save predictions\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/Noise2Void/W2S/#cover","title":"Cover\u00b6","text":""},{"location":"applications/structN2V/","title":"structN2V","text":"<p>For more details on the algorithm, check out its description.</p> Convallaria <p>                                   An example of using structN2V on the convallaria dataset.                               </p> 2D fluorescence structN2V <p>To add notebooks to this section, refer to the guide.</p>"},{"location":"applications/structN2V/Convallaria/","title":"Convallaria","text":"<p> Find me on Github </p> <p>Convallaria (c. majalis) acquired by Britta Schroth-Diez of the MPI-CBG Light Microscopy Facility, denoised using structN2V. The data consists of 100 images of the same field of view, in order to show structN2V denoising capabilities, we only select a single image as to not overestimate the results by providing too much information to the network. The ground truth is obtained by averaging all the images together.</p> In\u00a0[1]: Copied! <pre># Imports necessary to execute the code\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pooch\nimport tifffile\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.config.transformations import XYFlipModel\nfrom careamics.utils import autocorrelation\nfrom careamics.utils.metrics import scale_invariant_psnr\nfrom PIL import Image\n</pre> # Imports necessary to execute the code from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import pooch import tifffile from careamics import CAREamist from careamics.config import create_n2v_configuration from careamics.config.transformations import XYFlipModel from careamics.utils import autocorrelation from careamics.utils.metrics import scale_invariant_psnr from PIL import Image In\u00a0[\u00a0]: remove_output Copied! <pre># # instantiate data portfolio manage\n# portfolio = PortfolioManager()\n\n# # and download the data\n# root_path = Path(\"./data\")\n# files = portfolio.denoising.Flower.download(root_path)\n\n# create a folder for our data.\n# TODO add it to the portfolio\npath = Path(\"data\")\nif not path.exists():\n    path.mkdir()\n\n# download file\npath_to_file = pooch.retrieve(\n    \"https://download.fht.org/jug/n2v/flower.tif\",\n    known_hash=\"08c2c77ae63d43d22631d69ae905ee695e8225cf9872183faf49bd0771e5b1a2\",\n    path=path,\n)\n</pre> # # instantiate data portfolio manage # portfolio = PortfolioManager()  # # and download the data # root_path = Path(\"./data\") # files = portfolio.denoising.Flower.download(root_path)  # create a folder for our data. # TODO add it to the portfolio path = Path(\"data\") if not path.exists():     path.mkdir()  # download file path_to_file = pooch.retrieve(     \"https://download.fht.org/jug/n2v/flower.tif\",     known_hash=\"08c2c77ae63d43d22631d69ae905ee695e8225cf9872183faf49bd0771e5b1a2\",     path=path, ) In\u00a0[3]: Copied! <pre>data_description = (\n    \"A single image of convallaria (*c. majalis*) acquired by Britta Schroth-Diez of \"\n    \"the MPI-CBG Light Microscopy Facility.\"\n)\n</pre> data_description = (     \"A single image of convallaria (*c. majalis*) acquired by Britta Schroth-Diez of \"     \"the MPI-CBG Light Microscopy Facility.\" ) In\u00a0[4]: Copied! <pre>x_min, x_max = 580, 630\ny_min, y_max = 380, 430\n\n# load training and validation image and show them side by side\ndata = tifffile.imread(path_to_file)\nprint(f\"Shape of the data {data.shape}\")\n\ntrain_image = data[50]\nprint(f\"Shape of the training data {train_image.shape}\")\n\ngt_image = data.mean(axis=0)\nprint(f\"Shape of the ground truth {gt_image.shape}\")\n\n# plot images\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\n\nax[0, 0].imshow(gt_image, cmap=\"gray\")\nax[0, 0].set_title(\"Ground truth\")\n\nax[0, 1].imshow(train_image, cmap=\"gray\")\nax[0, 1].set_title(\"Training image\")\n\nax[1, 0].imshow(gt_image[x_min:x_max, y_min:y_max], cmap=\"gray\")\n\nax[1, 1].imshow(train_image[x_min:x_max, y_min:y_max], cmap=\"gray\")\n</pre> x_min, x_max = 580, 630 y_min, y_max = 380, 430  # load training and validation image and show them side by side data = tifffile.imread(path_to_file) print(f\"Shape of the data {data.shape}\")  train_image = data[50] print(f\"Shape of the training data {train_image.shape}\")  gt_image = data.mean(axis=0) print(f\"Shape of the ground truth {gt_image.shape}\")  # plot images fig, ax = plt.subplots(2, 2, figsize=(10, 10))  ax[0, 0].imshow(gt_image, cmap=\"gray\") ax[0, 0].set_title(\"Ground truth\")  ax[0, 1].imshow(train_image, cmap=\"gray\") ax[0, 1].set_title(\"Training image\")  ax[1, 0].imshow(gt_image[x_min:x_max, y_min:y_max], cmap=\"gray\")  ax[1, 1].imshow(train_image[x_min:x_max, y_min:y_max], cmap=\"gray\") <pre>Shape of the data (100, 1024, 1024)\nShape of the training data (1024, 1024)\nShape of the ground truth (1024, 1024)\n</pre> Out[4]: <pre>&lt;matplotlib.image.AxesImage at 0x7f60ecca44f0&gt;</pre> In\u00a0[5]: Copied! <pre># compute autocorrelation\nautocorr = autocorrelation(train_image)\n\n# crop the correlation around (0, 0)\nmidpoint = train_image.shape[0] // 2\ncrop_size = 10\nslices = (\n    slice(midpoint - crop_size, midpoint + crop_size),\n    slice(midpoint - crop_size, midpoint + crop_size),\n)\n\n# plot image and autocorrelation\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nax[0].imshow(train_image, cmap=\"gray\")\nax[0].set_title(\"Training image\")\n\nax[1].imshow(autocorr[slices], cmap=\"gray\")\nax[1].set_title(\"Autocorrelation\")\n</pre> # compute autocorrelation autocorr = autocorrelation(train_image)  # crop the correlation around (0, 0) midpoint = train_image.shape[0] // 2 crop_size = 10 slices = (     slice(midpoint - crop_size, midpoint + crop_size),     slice(midpoint - crop_size, midpoint + crop_size), )  # plot image and autocorrelation fig, ax = plt.subplots(1, 2, figsize=(10, 5))  ax[0].imshow(train_image, cmap=\"gray\") ax[0].set_title(\"Training image\")  ax[1].imshow(autocorr[slices], cmap=\"gray\") ax[1].set_title(\"Autocorrelation\") Out[5]: <pre>Text(0.5, 1.0, 'Autocorrelation')</pre> In\u00a0[6]: Copied! <pre>config = create_n2v_configuration(\n    experiment_name=\"convallaria_structn2v\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=(64, 64),\n    batch_size=16,\n    num_epochs=50,\n    # structN2V parameters\n    struct_n2v_axis=\"horizontal\",\n    struct_n2v_span=11,\n    # disable augmentations because of the noise correlations\n    augmentations=[],\n)\n\n# only use x-axis flip in augmentations\nconfig.data_config.transforms.insert(\n    0,\n    XYFlipModel(flip_x=True, flip_y=False),\n)\n\nprint(config)\n</pre> config = create_n2v_configuration(     experiment_name=\"convallaria_structn2v\",     data_type=\"array\",     axes=\"YX\",     patch_size=(64, 64),     batch_size=16,     num_epochs=50,     # structN2V parameters     struct_n2v_axis=\"horizontal\",     struct_n2v_span=11,     # disable augmentations because of the noise correlations     augmentations=[], )  # only use x-axis flip in augmentations config.data_config.transforms.insert(     0,     XYFlipModel(flip_x=True, flip_y=False), )  print(config) <pre>{'algorithm_config': {'algorithm': 'n2v',\n                      'loss': 'n2v',\n                      'lr_scheduler': {'name': 'ReduceLROnPlateau',\n                                       'parameters': {}},\n                      'model': {'architecture': 'UNet',\n                                'conv_dims': 2,\n                                'depth': 2,\n                                'final_activation': 'None',\n                                'in_channels': 1,\n                                'independent_channels': True,\n                                'n2v2': False,\n                                'num_channels_init': 32,\n                                'num_classes': 1},\n                      'optimizer': {'name': 'Adam',\n                                    'parameters': {'lr': 0.0001}}},\n 'data_config': {'axes': 'YX',\n                 'batch_size': 16,\n                 'data_type': 'array',\n                 'patch_size': [64, 64],\n                 'transforms': [{'flip_x': True,\n                                 'flip_y': False,\n                                 'name': 'XYFlip',\n                                 'p': 0.5},\n                                {'masked_pixel_percentage': 0.2,\n                                 'name': 'N2VManipulate',\n                                 'roi_size': 11,\n                                 'strategy': 'uniform',\n                                 'struct_mask_axis': 'horizontal',\n                                 'struct_mask_span': 11}]},\n 'experiment_name': 'convallaria_structn2v',\n 'training_config': {'accumulate_grad_batches': 1,\n                     'check_val_every_n_epoch': 1,\n                     'checkpoint_callback': {'auto_insert_metric_name': False,\n                                             'mode': 'min',\n                                             'monitor': 'val_loss',\n                                             'save_last': True,\n                                             'save_top_k': 3,\n                                             'save_weights_only': False,\n                                             'verbose': False},\n                     'enable_progress_bar': True,\n                     'gradient_clip_algorithm': 'norm',\n                     'max_steps': -1,\n                     'num_epochs': 50,\n                     'precision': '32'},\n 'version': '0.1.0'}\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># instantiate a CAREamist\ncareamist = CAREamist(source=config)\n\n# train\ncareamist.train(\n    train_source=train_image,\n)\n</pre> # instantiate a CAREamist careamist = CAREamist(source=config)  # train careamist.train(     train_source=train_image, ) In\u00a0[\u00a0]: remove_output Copied! <pre>prediction = careamist.predict(source=train_image, tile_size=(256, 256))\n</pre> prediction = careamist.predict(source=train_image, tile_size=(256, 256)) In\u00a0[9]: Copied! <pre>x_min, x_max = 580, 630\ny_min, y_max = 380, 430\n\npsnr_noisy = scale_invariant_psnr(gt_image, train_image)\npnsr_pred = scale_invariant_psnr(gt_image, prediction[0].squeeze())\n\n\n# plot images\nfig, ax = plt.subplots(2, 3, figsize=(8, 5))\nax[0, 0].imshow(train_image, cmap=\"gray\")\nax[0, 0].set_title(f\"Training image\\npsnr={psnr_noisy:.2f}\")\nax[0, 0].set_xticks([])\nax[0, 0].set_yticks([])\n\nax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\")\nax[0, 1].set_title(f\"Prediction\\npsnr={pnsr_pred:.2f}\")\nax[0, 1].set_xticks([])\nax[0, 1].set_yticks([])\n\nax[0, 2].imshow(gt_image, cmap=\"gray\")\nax[0, 2].set_title(\"Ground truth\")\nax[0, 2].set_xticks([])\nax[0, 2].set_yticks([])\n\nax[1, 0].imshow(train_image[x_min:x_max, y_min:y_max], cmap=\"gray\")\nax[1, 0].set_xticks([])\nax[1, 0].set_yticks([])\n\nax[1, 1].imshow(prediction[0].squeeze()[x_min:x_max, y_min:y_max], cmap=\"gray\")\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\n\nax[1, 2].imshow(gt_image[x_min:x_max, y_min:y_max], cmap=\"gray\")\nax[1, 2].set_xticks([])\nax[1, 2].set_yticks([])\n\nfig.tight_layout()\nplt.show()\n</pre> x_min, x_max = 580, 630 y_min, y_max = 380, 430  psnr_noisy = scale_invariant_psnr(gt_image, train_image) pnsr_pred = scale_invariant_psnr(gt_image, prediction[0].squeeze())   # plot images fig, ax = plt.subplots(2, 3, figsize=(8, 5)) ax[0, 0].imshow(train_image, cmap=\"gray\") ax[0, 0].set_title(f\"Training image\\npsnr={psnr_noisy:.2f}\") ax[0, 0].set_xticks([]) ax[0, 0].set_yticks([])  ax[0, 1].imshow(prediction[0].squeeze(), cmap=\"gray\") ax[0, 1].set_title(f\"Prediction\\npsnr={pnsr_pred:.2f}\") ax[0, 1].set_xticks([]) ax[0, 1].set_yticks([])  ax[0, 2].imshow(gt_image, cmap=\"gray\") ax[0, 2].set_title(\"Ground truth\") ax[0, 2].set_xticks([]) ax[0, 2].set_yticks([])  ax[1, 0].imshow(train_image[x_min:x_max, y_min:y_max], cmap=\"gray\") ax[1, 0].set_xticks([]) ax[1, 0].set_yticks([])  ax[1, 1].imshow(prediction[0].squeeze()[x_min:x_max, y_min:y_max], cmap=\"gray\") ax[1, 1].set_xticks([]) ax[1, 1].set_yticks([])  ax[1, 2].imshow(gt_image[x_min:x_max, y_min:y_max], cmap=\"gray\") ax[1, 2].set_xticks([]) ax[1, 2].set_yticks([])  fig.tight_layout() plt.show() In\u00a0[\u00a0]: remove_output Copied! <pre>predictions = careamist.predict(source=data, tile_size=(256, 256), axes=\"SYX\")\n</pre> predictions = careamist.predict(source=data, tile_size=(256, 256), axes=\"SYX\") In\u00a0[11]: Copied! <pre>psnrs = np.zeros((len(predictions), 1))\n\nfor i, image in enumerate(predictions):\n    psnrs[i] = scale_invariant_psnr(gt_image, image.squeeze())\n\nprint(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\")\n</pre> psnrs = np.zeros((len(predictions), 1))  for i, image in enumerate(predictions):     psnrs[i] = scale_invariant_psnr(gt_image, image.squeeze())  print(f\"PSNR: {psnrs.mean():.2f} +/- {psnrs.std():.2f}\") <pre>PSNR: 28.38 +/- 0.02\n</pre> In\u00a0[12]: Copied! <pre># create a cover image\nx_start, width = 650, 256\ny_start, height = 450, 256\n\n# rotate images\nrot_train = np.rot90(train_image, 3)\nrot_pred = np.rot90(prediction[0].squeeze(), 3)\n\n# create image\ncover = np.zeros((height, width))\n\n# normalize train and prediction\nnorm_train = (rot_train - rot_train.min()) / (rot_train.max() - rot_train.min())\nnorm_pred = (rot_pred - rot_pred.min()) / (rot_pred.max() - rot_pred.min())\n\n# fill in halves\ncover[:, : width // 2] = norm_train[\n    y_start : y_start + height, x_start : x_start + width // 2\n]\ncover[:, width // 2 :] = norm_pred[\n    y_start : y_start + height, x_start + width // 2 : x_start + width\n]\n\n# plot the single image\nplt.imshow(cover, cmap=\"gray\")\n\n# save the image\nim = Image.fromarray(cover * 255)\nim = im.convert(\"L\")\nim.save(\"Convallaria_structN2V.jpeg\")\n</pre> # create a cover image x_start, width = 650, 256 y_start, height = 450, 256  # rotate images rot_train = np.rot90(train_image, 3) rot_pred = np.rot90(prediction[0].squeeze(), 3)  # create image cover = np.zeros((height, width))  # normalize train and prediction norm_train = (rot_train - rot_train.min()) / (rot_train.max() - rot_train.min()) norm_pred = (rot_pred - rot_pred.min()) / (rot_pred.max() - rot_pred.min())  # fill in halves cover[:, : width // 2] = norm_train[     y_start : y_start + height, x_start : x_start + width // 2 ] cover[:, width // 2 :] = norm_pred[     y_start : y_start + height, x_start + width // 2 : x_start + width ]  # plot the single image plt.imshow(cover, cmap=\"gray\")  # save the image im = Image.fromarray(cover * 255) im = im.convert(\"L\") im.save(\"Convallaria_structN2V.jpeg\") In\u00a0[13]: Copied! <pre>general_description = (\n    \"This model is a UNet trained using the StructN2V algorithm to denoise images. \"\n    \"StructN2V is a variant of Noise2Void that can remove 1D structured noise, the \"\n    \"main failure case of the original N2V. The training data consists of a single \"\n    \"image of convallaria (*c. majalis*). The notebook used to train this model is \"\n    \"available on the CAREamics documentation website at the following link: \"\n    \"https://careamics.github.io/0.1/applications/structN2V/Convallaria/.\"\n)\nprint(general_description)\n</pre> general_description = (     \"This model is a UNet trained using the StructN2V algorithm to denoise images. \"     \"StructN2V is a variant of Noise2Void that can remove 1D structured noise, the \"     \"main failure case of the original N2V. The training data consists of a single \"     \"image of convallaria (*c. majalis*). The notebook used to train this model is \"     \"available on the CAREamics documentation website at the following link: \"     \"https://careamics.github.io/0.1/applications/structN2V/Convallaria/.\" ) print(general_description) <pre>This model is a UNet trained using the StructN2V algorithm to denoise images. StructN2V is a variant of Noise2Void that can remove 1D structured noise, the main failure case of the original N2V. The training data consists of a single image of convallaria (*c. majalis*). The notebook used to train this model is available on the CAREamics documentation website at the following link: https://careamics.github.io/0.1/applications/structN2V/Convallaria/.\n</pre> In\u00a0[\u00a0]: remove_output Copied! <pre># Export the model\ncareamist.export_to_bmz(\n    path_to_archive=\"convallaria_structn2v_model.zip\",\n    friendly_model_name=\"Convallaria_StructN2V\",\n    input_array=train_image.astype(np.float32),\n    authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],\n    general_description=general_description,\n    data_description=data_description,\n)\n</pre> # Export the model careamist.export_to_bmz(     path_to_archive=\"convallaria_structn2v_model.zip\",     friendly_model_name=\"Convallaria_StructN2V\",     input_array=train_image.astype(np.float32),     authors=[{\"name\": \"CAREamics authors\", \"affiliation\": \"Human Technopole\"}],     general_description=general_description,     data_description=data_description, )"},{"location":"applications/structN2V/Convallaria/#import-the-dataset","title":"Import the dataset\u00b6","text":"<p>The dataset can be directly downloaded using the <code>careamics-portfolio</code> package, which uses <code>pooch</code> to download the data.</p>"},{"location":"applications/structN2V/Convallaria/#data-description","title":"Data description\u00b6","text":""},{"location":"applications/structN2V/Convallaria/#visualize-data-and-autocorrelation","title":"Visualize data and autocorrelation\u00b6","text":""},{"location":"applications/structN2V/Convallaria/#visualize-autocorrelation","title":"Visualize autocorrelation\u00b6","text":"<p>The autocorrelation shows a slight horizontal correlation.</p>"},{"location":"applications/structN2V/Convallaria/#train-with-careamics","title":"Train with CAREamics\u00b6","text":"<p>The easiest way to use CAREamics is to create a configuration and a <code>CAREamist</code>.</p>"},{"location":"applications/structN2V/Convallaria/#create-configuration","title":"Create configuration\u00b6","text":"<p>The configuration can be built from scratch, giving the user full control over the various parameters available in CAREamics. However, a straightforward way to create a configuration for a particular algorithm is to use one of the convenience functions.</p> <p>To use structN2V, we simply add the relevant parameters to the configuration.</p>"},{"location":"applications/structN2V/Convallaria/#train","title":"Train\u00b6","text":"<p>A <code>CAREamist</code> can be created using a configuration alone, and then be trained by using the data already loaded in memory.</p>"},{"location":"applications/structN2V/Convallaria/#predict-with-careamics","title":"Predict with CAREamics\u00b6","text":"<p>Prediction is done with the same <code>CAREamist</code> used for training. Because the image is large we predict using tiling.</p>"},{"location":"applications/structN2V/Convallaria/#visualize-the-prediction","title":"Visualize the prediction\u00b6","text":""},{"location":"applications/structN2V/Convallaria/#compute-metrics-over-the-whole-stack","title":"Compute metrics over the whole stack\u00b6","text":""},{"location":"applications/structN2V/Convallaria/#export-the-model","title":"Export the model\u00b6","text":"<p>The model is automatically saved during training (the so-called <code>checkpoints</code>) and can be loaded back easily, but you can also export the model to the BioImage Model Zoo format.</p>"},{"location":"guides/","title":"Guides","text":"<p>CAREamics can be used in two ways, the recommended way is to use the CAREamist API, as it made to be simple, intuitive and minimize potential errors. Advanced users might want to only use parts of CAREamics in their own pipeline, for this they should refer to the Lightning API.</p> <p>We also provide a command-line interface and developer resources!</p> CAREamist API <p>                                         The recommended way to use CAREamics in a few lines                                         of code to apply various methods in the most                                          comfortable way possible.                                     </p> Lightning API <p>                                         Advanced users can re-use parts of CAREamics in their                                         Lightning pipeline, with more customization potential                                         available.                                     </p> napari plugin <p>                                         A UI interface for CAREamics in the popular                                         napari image viewer, allowing training,                                          predicting and saving models on your                                          own data.                                     </p> Tutorials <p>                                         Thematic tutorials on various ways to use                                         and apply CAREamics algorithms, from data                                         inspection to using CAREamics in other                                         tools.                                     </p> Command-line interface <p>                                         Want to run CAREamics from the command line, on your                                         machine, remotely or on a cluster? Head this way!                                     </p> Developer resources <p>                                         More details on how the documentation is set up,                                         and guidelines for modifying and contributing to                                         CAREamics.                                     </p>"},{"location":"guides/careamist_api/","title":"CAREamist API","text":"<p>The CAREamist API is the recommended way to use CAREamics, it is a two stage process, in which users first define a configuration and then use a the <code>CAREamist</code> to run their  training and prediction.</p>"},{"location":"guides/careamist_api/#quick-start","title":"Quick start","text":"<p>The simplest way to use CAREamics is to create a configuration using the convenience functions. Checkout the applications section for real-world examples of the various algorithms.</p> Noise2VoidCARENoise2Noise <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256)).astype(np.float32)  # (2)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data)\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training!</p> </li> </ol> <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_care_configuration\n\n# create a configuration\nconfig = create_care_configuration(\n    experiment_name=\"care_2D\",\n    data_type=\"array\",\n    axes=\"SYX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (5, 256, 256)).astype(np.float32)  # (2)!\ntrain_target = np.random.randint(0, 255, (5, 256, 256)).astype(np.float32)\nval_data = np.random.randint(0, 255, (2, 256, 256)).astype(np.float32)\nval_target = np.random.randint(0, 255, (2, 256, 256)).astype(np.float32)\ncareamist.train(\n    train_source=train_data,\n    train_target=train_target,\n    val_source=val_data,\n    val_target=val_target,\n)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data, axes=\"YX\")\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training! Here, we added validation data as well.</p> </li> </ol> <pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2n_configuration\n\n# create a configuration\nconfig = create_n2n_configuration(\n    experiment_name=\"n2n_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256)).astype(np.float32)  # (2)!\ntrain_target = np.random.randint(0, 255, (256, 256)).astype(np.float32)\ncareamist.train(\n    train_source=train_data,\n    train_target=train_target,\n)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\nprediction = careamist.predict(source=pred_data)\n</code></pre> <ol> <li> <p>Obviously, choose a more realistic number of epochs for training.</p> </li> <li> <p>Use real data for training!</p> </li> </ol>"},{"location":"guides/careamist_api/#documentation","title":"Documentation","text":"<p>There are many features that can be useful for your application, explore the documentation to learn all the various aspects of CAREamics.</p> Configuration <p>                                         The configuration is at the heart of CAREamics, it                                          allow users to define how and which algorithm will be                                         trained.                                     </p> Using CAREAmics <p>                                         The CAREamist is the core element allowing training                                         and prediction using the model defined in the configuration.                                     </p>"},{"location":"guides/careamist_api/faq/","title":"Frequently asked questions","text":""},{"location":"guides/careamist_api/faq/#training","title":"Training","text":""},{"location":"guides/careamist_api/faq/#notimplementederror-the-operator-is-not-currently-implemented-for-the-mps-device","title":"<code>NotImplementedError: The operator [...] is not currently implemented for the MPS device</code>","text":"<p>Unfortunately, not all the operations in PyTorch are implemented for the Metal architecture  of recent Apple devices (MPS stands for Metal Performance Shaders). This error occurs when you are trying to run CAREamics on such a device without forcing training on CPU.</p> <pre><code>NotImplementedError: The operator 'aten::max_pool3d_with_indices' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n</code></pre> <p>Solution</p> <p>In your environment (e.g. terminal before you start the jupyter notebook), set the environment variable <code>PYTORCH_ENABLE_MPS_FALLBACK=1</code>. This will force the training to run on the CPU, which is slower but will work.</p> <p>This can be done for instance by running the following command in your terminal:</p> <pre><code>export PYTORCH_ENABLE_MPS_FALLBACK=1\n</code></pre>"},{"location":"guides/careamist_api/faq/#prediction","title":"Prediction","text":""},{"location":"guides/careamist_api/faq/#sizes-of-tensors-must-match","title":"Sizes of tensors must match","text":"<p>If you get an error similar to:</p> <pre><code>RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 320 but got size 321 for tensor number 1 in the list.\n</code></pre> <p>This is likely because the input data size is not compatible with the model (e.g. the model has layers that downsample the data, then upsample it, and this is only  compatible with dimensions that are power of 2!).</p> <p>Solution</p> <p>By tiling the prediction, you can ensure that every input to the model has the correct shape. Then upon stitching back the tiles, you recover your whole image!</p>"},{"location":"guides/careamist_api/faq/#bmz-model-export","title":"BMZ model export","text":""},{"location":"guides/careamist_api/faq/#pydantic_core_pydantic_corevalidationerror","title":"<code>pydantic_core._pydantic_core.ValidationError</code>","text":"<p>The BMZ format is also validated by the pydantic library. If some of the metadata given to the <code>export_to_bmz</code> function is not correct,  you might get such errors:</p> Code<pre><code>train_data = np.random.randint(0, 255, (256, 256)).\ncareamist.export_to_bmz( \n    path=\"n2v_models\", \n    name=\"n2v_model_example.bmz\", \n    input_array=train_data, \n    authors=[{ \"name\": \"nobody\", \"email\": \"nobody@nobody\", }]\n)\n</code></pre> Error message<pre><code>pydantic_core._pydantic_core.ValidationError: 2 validation errors for bioimage.io model specification\nname\n  Value error, 'n2v_model_example.bmz' is not restricted to 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_- ()' [type=value_error, input_value='n2v_model_example.bmz', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/value_error\nauthors.0.email\n  value is not a valid email address: The part after the @-sign is not valid. It should have a period. [type=value_error, input_value='nobody@nobody', input_type=str]\n</code></pre> <p>If we look closely at the error message, Pydantic is telling us that there are actually two errors: - <code>name</code> should only contain letters, numbers, hyphens, underscores, spaces and parentheses. - <code>authors.0.email</code> is not a valid email address, as it is missing a period after the @-sign.</p> <p>Solution</p> <p>Look at the input <code>name</code>, it contains a forbidden character: the dot. The <code>email</code> should have something like <code>.com</code> after the <code>@</code> sign. Fixing these two issues will solve the problem.</p>"},{"location":"guides/careamist_api/configuration/","title":"Configuration","text":"<p>The configuration summarizes all the parameters used internally by CAREamics. It is  used to create a <code>CAREamist</code> instance and is saved together with the checkpoints and  saved models.</p> <p>Configurations are created for specific algorithms using convenience functions. They are Python objects, but can be exported to a dictionary, and contain a hierarchy of parameters.</p> Configuration example <p>Here is an example of a configuration for the <code>Noise2Void</code> algorithm.</p> Noise2Void configuration<pre><code>version: 0.1.0\nexperiment_name: N2V 3D\nalgorithm_config:\n  algorithm: n2v\n  loss: n2v\n  model:\n    architecture: UNet\n    conv_dims: 3\n    num_classes: 1\n    in_channels: 1\n    depth: 2\n    num_channels_init: 32\n    final_activation: None\n    n2v2: false\n    independent_channels: true\n  optimizer:\n    name: Adam\n    parameters:\n      lr: 0.0001\n  lr_scheduler:\n    name: ReduceLROnPlateau\n    parameters: {}\n  n2v_config:\n    name: N2VManipulate\n    roi_size: 11\n    masked_pixel_percentage: 0.2\n    remove_center: true\n    strategy: uniform\n    struct_mask_axis: none\n    struct_mask_span: 5\ndata_config:\n  data_type: tiff\n  axes: ZYX\n  patch_size:\n  - 8\n  - 64\n  - 64\n  batch_size: 8\n  transforms:\n  - name: XYFlip\n    flip_x: true\n    flip_y: true\n    p: 0.5\n  - name: XYRandomRotate90\n    p: 0.5\n  train_dataloader_params:\n    shuffle: true\n  val_dataloader_params: {}\ntraining_config:\n  num_epochs: 20\n  precision: '32'\n  max_steps: -1\n  check_val_every_n_epoch: 1\n  enable_progress_bar: true\n  accumulate_grad_batches: 1\n  gradient_clip_algorithm: norm\n  checkpoint_callback:\n    monitor: val_loss\n    verbose: false\n    save_weights_only: false\n    save_last: true\n    save_top_k: 3\n    mode: min\n    auto_insert_metric_name: false\n</code></pre> <p>The number of parameters might appear overwhelming, but in practice users only call a function with few parameters. The configuration is designed to hide the complexity of the algorithms and provide a simple interface to the user.</p> <pre><code>from careamics.config import (\n    create_care_configuration,  # CARE\n    create_n2n_configuration,  # Noise2Noise\n    create_n2v_configuration,  # Noise2Void\n)\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_experiment\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=50,\n)\n</code></pre> <p>In the next sections, you can dive deeper on how to create a configuration and interact with the configuration with different levels of expertise.</p> <p> Beginner</p> <p> Intermediate</p> <p> Advanced</p> <ul> <li> Convenience functions</li> <li> Save and load configurations</li> <li> Custom types</li> <li> Understanding configuration errors</li> <li> Build the configuration from scratch</li> <li> Algorithm requirements</li> </ul>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/","title":"Algorithm requirements","text":"<p>In this section we detail the constraints of each algorithm on the configuration and their differences. </p>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#parent-configuration","title":"Parent configuration","text":"<p>The parent configuration is <code>Configuration</code> and it is the same for all algorithms. It  should not be used to train any of the algorithms as the child classes ensure coherence across the parameters.</p>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#noise2void-family","title":"Noise2Void family","text":"<p>Noise2Void algorithms (N2V, N2V2, structN2V) are configured using <code>N2VConfiguration</code>. It enforces the following constraints:</p> <ul> <li><code>algorithm_config</code>: must be a <code>N2VAlgorithm</code></li> <li><code>algorithm_config.algorithm=\"n2v\"</code></li> <li><code>algorithm_config.loss=\"n2v\"</code></li> <li><code>algorithm_config.model</code>: <ul> <li>must be a UNet (<code>architecture=\"UNet\"</code>)</li> <li><code>in_channels</code> and <code>num_classes</code> must be equal</li> </ul> </li> <li><code>data_config</code>: must be a <code>N2VDataConfiguration</code></li> <li><code>data_config.transforms</code>: must contain <code>N2VManipulateModel</code> as the last transform</li> </ul>"},{"location":"guides/careamist_api/configuration/algorithm_requirements/#care-and-noise2noise","title":"CARE and Noise2Noise","text":"<p>The two algorithms are very similar and therefore their constraints are sensibly the same. They are configured using <code>CAREConfiguration</code> and <code>N2NConfiguration</code> respectively.</p> <ul> <li><code>algorithm_config</code>: must be a <code>CAREAlgorithm</code> (CARE) or <code>N2NAlgorithm</code> (Noise2Noise)</li> <li><code>algorithm_config.algorithm</code>: <code>care</code> (CARE) or <code>n2n</code> (Noise2Noise)</li> <li><code>algorithm_config.loss</code>: <code>mae</code> or <code>mse</code></li> <li><code>algorithm_config.model</code>: must be a UNet (<code>architecture=\"UNet\"</code>)</li> <li><code>data_config</code>: must be a <code>DataConfiguration</code></li> <li><code>data_config.transforms</code>: must not contain <code>N2VManipulateModel</code></li> </ul>"},{"location":"guides/careamist_api/configuration/build_configuration/","title":"Build the configuration","text":"<p>Beginner vs Advanced</p> <p>This is an advanced level way to create CAREamics configuration. Do check the convenience functions if you are looking for a simpler way to create CAREanics configurations!</p> <p>CAREamics configuration is validated using Pydantic,  a library that allows you to define schemas and automatically check the types of the  input data you provide. </p> <p>In addition, it allows great flexibility in writing custom validators. In turns this ensures that the configuration is always valid and coherent, protecting against errors deep in the library.</p> <p>As shown in the introduction, a CAREamics configuration is composed of four main elements:</p> <ol> <li>Experiment name, a simple string</li> <li>Algorithm configuration, also a Pydantic model</li> <li>Data configuration, also a Pydantic model</li> <li>Training configuration, also a Pydantic model</li> </ol> <p>Each of the parameters and models are validated independently, and the configuration as a whole is validated at the end.</p> <p>There are two ways to build Pydantic models: by passing a dictionary that reproduces the model structure, or by calling all Pydantic models explicitly. While the second method is  more concise, the first approach is less error prone and allow you to explore all parameters available easily if you are using an IDE (e.g. VSCode, JupyterLab etc.).</p> <p>Each algorithm has a specific configuration that may or may not be similar to the others. For differences between algorithms, refer to the algorithm requirements section.</p> <p>Complete list of parameters</p> <p>A complete list of all parameters would be very long and difficult to showcase, as it also depends on the algorithm.</p> <p>The preferred way to check the parameters of the various configurations and subconfigurations is to read directly the code reference or the source code.</p>"},{"location":"guides/careamist_api/configuration/build_configuration/#using-pydantic-models","title":"Using Pydantic models","text":"<p>The preferred way to build the configuration is to call the Pydantic models directly. This allows you to explore the parameters via your IDE, but also to get the validation errors closer to the source of the error.</p> Building the configuration using Pydantic models<pre><code>from careamics.config import (  # (1)!\n    Configuration,\n    DataConfig,\n    N2VAlgorithm,\n    TrainingConfig,\n)\nfrom careamics.config.architectures import UNetModel\nfrom careamics.config.callback_model import CheckpointModel, EarlyStoppingModel\nfrom careamics.config.support import (\n    SupportedData,\n    SupportedLogger,\n)\nfrom careamics.config.transformations import XYFlipModel\n\nexperiment_name = \"N2V_example\"\n\n# build the model and algorithm configurations\nmodel = UNetModel(\n    architecture=\"UNet\",  # (2)!\n    num_channels_init=64,  # (3)!\n    depth=3,\n    # (4)!\n)\n\nalgorithm_model = N2VAlgorithm(  # (5)!\n    model=model,\n    # (6)!\n)\n\n# then the data configuration for N2V\ndata_model = DataConfig(  # (7)!\n    data_type=SupportedData.ARRAY.value,\n    patch_size=(256, 256),\n    batch_size=8,\n    axes=\"YX\",\n    transforms=[XYFlipModel(flip_y=False)],  # (8)!\n    dataloader_params={  # (9)!\n        \"num_workers\": 4,\n        \"shuffle\": True,\n    },\n)\n\n# then the TrainingConfig\nearlystopping = EarlyStoppingModel(\n    # (10)!\n)\n\ncheckpoints = CheckpointModel(every_n_epochs=10)  # (11)!\n\ntraining_model = TrainingConfig(\n    num_epochs=30,\n    logger=SupportedLogger.WANDB.value,\n    early_stopping_callback=earlystopping,\n    checkpoint_callback=checkpoints,\n    # (12)!\n)\n\n# finally, build the Configuration\nconfig = Configuration(  # (13)!\n    experiment_name=experiment_name,\n    algorithm_config=algorithm_model,\n    data_config=data_model,\n    training_config=training_model,\n)\n</code></pre> <ol> <li>The Pydantic models are imported from the <code>careamics.config</code>      submodules, and its various submodules.</li> <li>Because the <code>architecture</code> parameter is used to discriminate between different models     within CAREamics, there is no default value and it is therefore necessary to set it.</li> <li>You can change here the model parameters.</li> <li>There are more parameters available, but keep in mind that by creating the models     directly, you are responsible for setting the correct parameters and you might get     errors when assembling the final configuration. For instance, Noise2Void requires     the same number of input and output channels, which is not checked here.</li> <li>Noise2Void requires a specific algorithm configuration.</li> <li>Here once can modify also the N2V specific parameters related to the pixel replacement (see     N2V algorithm description).</li> <li>As opposed to CARE and Noise2Noise, Noise2Void requires a specific data configuration.</li> <li>You can decide on the augmentations and their parameters.</li> <li>The dataloader parameters are the general parameters that can be passed to PyTorch's     <code>DataLoader</code> class (see documentation). If     you modify these, you need to add the <code>shuffle</code> parameter to the dictionary. We strongly     recommend to keep the default value (<code>True</code>) for the shuffling.</li> <li>The early stopping callback has many parameters, which are those available in the     PyTorch Lightning callback (see documentation).</li> <li>Similarly, the <code>CheckpointModel</code> parameters are those of the corresponding PyTorch     Lightning callback (see documentation).</li> <li>The training configuration is general and has many parameters, checkout the code     reference or the code base for more information.</li> <li>Finally the Noise2Void configuration can be instantiated.</li> </ol>"},{"location":"guides/careamist_api/configuration/build_configuration/#using-nested-dictionaries","title":"Using nested dictionaries","text":"<p>An alternative to working with Pydantic models is to assemble the configuration using a dictionary. While this is neat, because you are dealing with nested dictionaries, it is easy to add the parameters at the wrong level and you need to constantly refer to the code documentation to know which parameters are available. Finally, because you are validating the configuration at once, you will get all the validation errors in one go.</p> <p>Here, we reproduce the same configuration as previously, but as a dictionary this time:</p> Building the configuration with a dictionary<pre><code>from careamics.config import Configuration\n\nconfig_dict = {\n    \"experiment_name\": \"N2V_example\",\n    \"algorithm_config\": {\n        \"algorithm\": \"n2v\",  # (1)!\n        \"loss\": \"n2v\",\n        \"model\": {\n            \"architecture\": \"UNet\",  # (2)!\n            \"num_channels_init\": 64,\n            \"depth\": 3,\n        },\n        # (3)!\n    },\n    \"data_config\": {\n        \"data_type\": \"array\",\n        \"patch_size\": [256, 256],\n        \"batch_size\": 8,\n        \"axes\": \"YX\",\n        \"transforms\": [\n            {\n                \"name\": \"XYFlip\",\n                \"flip_y\": False,\n            },\n        ],\n        \"dataloader_params\": {\n            \"num_workers\": 4,\n        },\n    },\n    \"training_config\": {\n        \"num_epochs\": 30,\n        \"logger\": \"wandb\",\n        \"early_stopping_callback\": {},  # (4)!\n        \"checkpoint_callback\": {\n            \"every_n_epochs\": 10,\n        },\n    },\n}\n\n# instantiate specific configuration\nconfig_as_dict = Configuration(**config_dict)  # (5)!\n</code></pre> <ol> <li>In order to correctly instantiate the N2V configuration via a dictionary, we have     to explicitely specify certain paramaters that otherwise have a default value when     using Pydantic. This is the case for the <code>algorithm</code> parameter.</li> <li>As previously, we also specify the architecture.</li> <li>In parctice, we do not change the <code>n2v_config</code>, but this is were one could. The <code>n2v_config</code>     is related to the parameters of the pixel replacement in N2V.</li> <li>Since many parameters have default values, we don't need to specify them but still     need to pass an empty dictionary to make sure that there is an early stopping     callback.</li> <li>Here we are using the Pydantic class with the unpacked dictionary.</li> </ol>"},{"location":"guides/careamist_api/configuration/configuration_errors/","title":"Configuration errors","text":"<p>Because the CAREamics configuration is validated using Pydantic, all the configuration errors are delegated to the validation library.</p> <p>Here we showcase a certain number of confguration errors that can appear when using the convenience functions. It is impossible to cover all possibilities, so these examples are here to highlight how to read Pydantic errors.</p> <p>Under construction</p> <p>This page is under construction and will be updated soon.</p>"},{"location":"guides/careamist_api/configuration/configuration_errors/#pydantic-error","title":"Pydantic error","text":"<pre><code>ValidationError: 1 validation errors for union[function-after[validate_n2v2(), function-after[validate_3D(), N2VConfiguration]],function-after[validate_3D(), N2NConfiguration],function-after[validate_3D(), CAREConfiguration]]\nfunction-after[validate_n2v2(), function-after[validate_3D(), N2VConfiguration]].data_config.data_type\n  Input should be 'array', 'tiff' or 'custom' [type=literal_error, input_value='arrray', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\n...\n</code></pre> <p>In this case, the input to <code>data_config.data_type</code> should be either <code>'array'</code>, <code>'tiff'</code> or <code>'custom'</code>. The error message is telling us that the input value <code>'arrray'</code> is not a valid option.</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/","title":"Convenience functions","text":"<p>As building a full CAREamics configuration requires a complete understanding of the  various parameters and experience with Pydantic, we provide convenience functions to create configurations with a only few parameters related to the algorithm users want to train.</p> <p>All convenience methods can be found in the <code>careamics.config</code> modules. CAREamics  currently supports Noise2Void and its variants,  CARE and Noise2Noise. </p> Import convenience functions<pre><code>from careamics.config import (\n    create_care_configuration,  # CARE\n    create_n2n_configuration,  # Noise2Noise\n    create_n2v_configuration,  # Noise2Void\n)\n</code></pre> <p>Each method does all the heavy lifting to make the configuration coherent. They share a certain numbers of mandatory parameters:</p> <ul> <li><code>experiment_name</code>: The name of the experiment, used to differentiate trained models.</li> <li><code>data_type</code>: One of the types supported by CAREamics (<code>array</code>, <code>tiff</code> or <code>custom</code>).</li> <li><code>axes</code>: Axes of the data (e.g. SYX), can only the following letters: <code>STCZYX</code>.</li> <li><code>patch_size</code>: Size of the patches along the spatial dimensions (e.g. [64, 64]).</li> <li><code>batch_size</code>: Batch size to use during training (e.g. 8). This parameter affects the     memory footprint on the GPU.</li> <li><code>num_epochs</code>: Number of epochs.</li> </ul> <p>Additional optional parameters can be passed to tweak the configuration. </p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#training-with-channels","title":"Training with channels","text":"<p>When training with multiple channels, the <code>axes</code> parameter should contain <code>C</code> (e.g. <code>YXC</code>). An error will be then thrown if the optional parameter <code>n_channels</code> (or <code>n_channel_in</code> for  CARE and Noise2Noise) is not specified! Likewise if <code>n_channels</code> is specified but <code>C</code> is not in <code>axes</code>.</p> <p>The correct way is to specify them both at the same time.</p> Noise2VoidCARENoise2Noise Configuration with multiple channels<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,  # (2)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> </ol> Configuration with multiple channels<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,  # (2)!\n    n_channels_out=2,  # (3)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> <li>Depending on the CARE task, you also see to set <code>n_channels_out</code> (optional).</li> </ol> Configuration with multiple channels<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,  # (2)!\n    n_channels_out=2,  # (3)!\n)\n</code></pre> <ol> <li>The axes contain the letter <code>C</code>.</li> <li>The number of channels is specified.</li> <li>Depending on the CARE task, you also see to set <code>n_channels_out</code> (optional).</li> </ol> <p>Independent channels</p> <p>By default, the channels are trained independently: that means that they have no influence on each other. As they might have completely different noise models, this can lead to better results.</p> <p>However, in some cases, you might want to train the channels together to get more structural information.</p> <p>To control whether the channels are trained independently, you can use the  <code>independent_channels</code> parameter:</p> Noise2VoidCARENoise2Noise Training channels together<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol> Training channels together<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,\n    n_channels_out=2,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels_in</code>.</li> <li>This ensures that the channels are trained together!</li> </ol> Training channels together<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_mix_channels\",\n    data_type=\"tiff\",\n    axes=\"YXC\",  # (1)!\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    n_channels_in=3,\n    independent_channels=False,  # (2)!\n)\n</code></pre> <ol> <li>As previously, we specify the channels in <code>axes</code> and <code>n_channels</code>.</li> <li>This ensures that the channels are trained together!</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#augmentations","title":"Augmentations","text":"<p>By default CAREamics configuration uses augmentations that are specific to the algorithm (e.g. Noise2Void) and that are compatible with microscopy images (e.g. flip and 90 degrees rotations).</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#disable-augmentations","title":"Disable augmentations","text":"<p>However in certain cases, users might want to disable augmentations. For instance if you have structures that are always oriented in the same direction. To do so there is a single <code>augmentations</code> parameter:</p> Noise2VoidCARENoise2Noise Configuration without augmentations<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization and N2V pixel manipulation will still be added by CAREamics!).</li> </ol> Configuration without augmentations<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization will still be added!).</li> </ol> Configuration without augmentations<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[],  # (1)!\n)\n</code></pre> <ol> <li>Augmentations are disabled (but normalization will still be added!).</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#non-default-augmentations","title":"Non-default augmentations","text":"<p>Default augmentations apply a random flip along X or Y and a 90 degrees rotation (note that there is always for each patch and for each augmentation a 0.5 probability that no augmentation is applied). For samples that contain objects that are never flipped or rotated (e.g. objects with always the same orientation, or with patterns along a certain direction), it will be beneficial to apply non-default augmentations.</p> <p>For instance, in a case where the objects can only be flipped horizontally, we would only apply flipping along the <code>X</code> axis and not apply any rotation.</p> Noise2VoidCARENoise2Noise Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D_no_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_care_configuration(\n    experiment_name=\"care_2D_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> Configuration with non-default augmentations<pre><code>from careamics.config.transformations import XYFlipModel\n\nconfig = create_n2n_configuration(\n    experiment_name=\"n2n_2D_aug\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    augmentations=[XYFlipModel(flip_y=False)],  # (1)!\n)\n</code></pre> <ol> <li>Only flipping along the <code>X</code> axis is applied.</li> </ol> <p>Available augmentations</p> <p>The available augmentations are the following:</p> <ul> <li><code>XYFlipModel</code>, which can be along <code>X</code>, <code>Y</code> or both.</li> <li><code>XYRandomRotate90Model</code></li> </ul>"},{"location":"guides/careamist_api/configuration/convenience_functions/#choosing-a-logger","title":"Choosing a logger","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p> Noise2VoidCARENoise2Noise Configuration with WandB<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol> Configuration with WandB<pre><code>config = create_care_configuration(\n    experiment_name=\"care_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol> Configuration with WandB<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_2D_wandb\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    logger=\"wandb\",  # (1)!\n)\n</code></pre> <ol> <li><code>wandb</code> or <code>tensorboard</code></li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#advanced-passing-data-loader-parameters","title":"(Advanced) Passing data loader parameters","text":"<p>The convenience functions allow passing data loader parameters directly through the <code>train_dataloader_params</code> or <code>val_dataloader_params</code> parameters. These are the same parameters as those accepted by the <code>torch.utils.data.DataLoader</code> class (see PyTorch documentation).</p> Noise2VoidCARENoise2Noise Configuration with data loader parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol> Configuration with data loader parameters<pre><code>config = create_care_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol> Configuration with data loader parameters<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    train_dataloader_params={\n        \"num_workers\": 4,  # (1)!\n    },\n    val_dataloader_params={\n        \"num_workers\": 2,  # (2)!\n    },\n)\n</code></pre> <ol> <li>In practice this is the one parameter you might want to change.</li> <li>You can also set the parameters for the validation dataloader.</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#advanced-passing-model-specific-parameters","title":"(Advanced) Passing model specific parameters","text":"<p>By default, the convenience functions use the default UNet model parameters. But if  you are feeling brave, you can pass model specific parameters in the <code>model_params</code> dictionary. </p> Noise2VoidCARENoise2Noise Configuration with model specific parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> Configuration with model specific parameters<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> Configuration with model specific parameters<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    model_params={\n        \"depth\": 3,  # (1)!\n        \"num_channels_init\": 64,  # (2)!\n        # (3)!\n    },\n)\n</code></pre> <ol> <li>The depth of the UNet.</li> <li>The number of channels in the first layer.</li> <li>Add any other parameter specific to the model!</li> </ol> <p>Model parameters overwriting</p> <p>Some values of the model parameters are not compatible with certain algorithms.  Therefore, these are overwritten by the convenience functions. For instance, if you pass <code>in_channels</code> or <code>independent_channels</code> in the <code>model_kwargs</code> dictionary,  they will be ignored and replaced by the explicit parameters passed to the convenience function.</p> <p>Model parameters</p> <p>The model parameters are the following:</p> <ul> <li><code>conv_dims</code></li> <li><code>num_classes</code></li> <li><code>in_channels</code></li> <li><code>depth</code></li> <li><code>num_channels_init</code></li> <li><code>final_activation</code> </li> <li><code>n2v2</code></li> <li><code>independent_channels</code></li> </ul> <p>Description for each parameter can be found in the code reference.</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#noise2void-specific-parameters","title":"Noise2Void specific parameters","text":"<p>Noise2Void has a few additional parameters that can be set, including for using its  variants N2V2 and structN2V.</p> <p>Understanding Noise2Void and its variants</p> <p>Before deciding which variant to use, and how to modify the parameters, we recommend to die a little a bit on how each algorithm works!</p>"},{"location":"guides/careamist_api/configuration/convenience_functions/#noise2void-parameters","title":"Noise2Void parameters","text":"<p>There are two Noise2Void parameters that influence how the patches are manipulated during training:</p> <ul> <li><code>roi_size</code>: This parameter specifies the size of the area used to replace the masked pixel value.</li> <li><code>masked_pixel_percentage</code>: This parameter specifies how many pixels per patch will be manipulated.</li> </ul> <p>While the default values are usually fine, they can be tweaked to improve the training in certain cases.</p> Configuration with N2V parameters<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"tiff\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n    roi_size=7,\n    masked_pixel_percentage=0.5,\n)\n</code></pre>"},{"location":"guides/careamist_api/configuration/convenience_functions/#n2v2","title":"N2V2","text":"<p>To use N2V2, the <code>use_n2v2</code> parameter should simply be set to <code>True</code>.</p> Configuration with N2V2<pre><code>config = create_n2v_configuration(\n    experiment_name=\"n2v2_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    use_n2v2=True,  # (1)!\n)\n</code></pre> <ol> <li>What it does is modifying the architecture of the UNet model and the way the masked     pixels are replaced.</li> </ol>"},{"location":"guides/careamist_api/configuration/convenience_functions/#structn2v","title":"structN2V","text":"<p>StructN2V has two parameters that can be set:</p> <ul> <li><code>struct_n2v_axis</code>: The axis along which the structN2V mask will be applied. By default it     is set to <code>none</code> (structN2V is disabled), you can set it to either <code>horizontal</code> or <code>vertical</code>.</li> <li><code>struct_n2v_span</code>: The size of the structN2V mask.</li> </ul> Configuration with structN2V<pre><code>config = create_n2v_configuration(\n    experiment_name=\"structn2v_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    struct_n2v_axis=\"horizontal\",\n    struct_n2v_span=5,\n)\n</code></pre>"},{"location":"guides/careamist_api/configuration/convenience_functions/#care-and-noise2noise-parameters","title":"CARE and Noise2Noise parameters","text":""},{"location":"guides/careamist_api/configuration/convenience_functions/#using-another-loss-function","title":"Using another loss function","text":"<p>As opposed to Noise2Void, CARE and Noise2Noise  can be trained with different loss functions. This can be set using the <code>loss</code> parameter  (surprise, surprise!).</p> CARENoise2Noise Configuration with different loss<pre><code>config = create_care_configuration(\n    experiment_name=\"care_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol> Configuration with different loss<pre><code>config = create_n2n_configuration(\n    experiment_name=\"n2n_3D\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=[16, 64, 64],\n    batch_size=8,\n    num_epochs=20,\n    loss=\"mae\",  # (1)!\n)\n</code></pre> <ol> <li><code>mae</code> or <code>mse</code></li> </ol>"},{"location":"guides/careamist_api/configuration/custom_types/","title":"Custom types","text":"<p>The <code>data_type</code> parameter of the <code>DataConfig</code> class is a string that is used to choose the data loader within CAREamics. We currently only support <code>array</code> and <code>tiff</code> explicitly.</p> <p>However, users can set the <code>data_type</code> to <code>custom</code> and use their own read function.</p> Custom data type<pre><code>from careamics.config import DataConfig\n\ndata_config = DataConfig(\n    data_type=\"custom\",  # (1)!\n    axes=\"YX\",\n    patch_size=[128, 128],\n    batch_size=8,\n    num_epochs=20,\n)\n</code></pre> <ol> <li>As far as the configuration is concerned, you only set the <code>data_type</code> to <code>custom</code>. The     rest happens in the <code>CAREamist</code> instance.</li> </ol> <p>Full example in other sections</p> <p>A full example of the use of a custom data type is available in the CAREamist and Applications sections.</p>"},{"location":"guides/careamist_api/configuration/save_load/","title":"Save and load","text":"<p>CAREamics configurations can be saved to the disk as <code>.yml</code> file and loaded easily to start similar experiments.</p>"},{"location":"guides/careamist_api/configuration/save_load/#save-a-configuration","title":"Save a configuration","text":"Save a configuration<pre><code>from careamics import save_configuration\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"Config_to_save\",\n    data_type=\"tiff\",\n    axes=\"ZYX\",\n    patch_size=(8, 64, 64),\n    batch_size=8,\n    num_epochs=20,\n)\nsave_configuration(config, \"config.yml\")\n</code></pre> <p>In the resulting file, you can see all the parameters that are defaults and hidden from you.</p> resulting config.yml file <pre><code>version: 0.1.0\nexperiment_name: N2V 3D\nalgorithm_config:\n  algorithm: n2v\n  loss: n2v\n  model:\n    architecture: UNet\n    conv_dims: 3\n    num_classes: 1\n    in_channels: 1\n    depth: 2\n    num_channels_init: 32\n    final_activation: None\n    n2v2: false\n    independent_channels: true\n  optimizer:\n    name: Adam\n    parameters:\n      lr: 0.0001\n  lr_scheduler:\n    name: ReduceLROnPlateau\n    parameters: {}\n  n2v_config:\n    name: N2VManipulate\n    roi_size: 11\n    masked_pixel_percentage: 0.2\n    remove_center: true\n    strategy: uniform\n    struct_mask_axis: none\n    struct_mask_span: 5\ndata_config:\n  data_type: tiff\n  axes: ZYX\n  patch_size:\n  - 8\n  - 64\n  - 64\n  batch_size: 8\n  transforms:\n  - name: XYFlip\n    flip_x: true\n    flip_y: true\n    p: 0.5\n  - name: XYRandomRotate90\n    p: 0.5\n  train_dataloader_params:\n    shuffle: true\n  val_dataloader_params: {}\ntraining_config:\n  num_epochs: 20\n  precision: '32'\n  max_steps: -1\n  check_val_every_n_epoch: 1\n  enable_progress_bar: true\n  accumulate_grad_batches: 1\n  gradient_clip_algorithm: norm\n  checkpoint_callback:\n    monitor: val_loss\n    verbose: false\n    save_weights_only: false\n    save_last: true\n    save_top_k: 3\n    mode: min\n    auto_insert_metric_name: false\n</code></pre>"},{"location":"guides/careamist_api/configuration/save_load/#load-a-configuration","title":"Load a configuration","text":"Load a configuration<pre><code>from careamics import load_configuration\n\nconfig = load_configuration(\"config.yml\")\n</code></pre>"},{"location":"guides/careamist_api/usage/","title":"Using CAREamics","text":"<p>In this section, we will explore the many facets of the <code>CAREamist</code> class, which allors training and predicting using the various algorithms in CAREamics.</p> <p>The workflow in CAREamics has five steps: creating a configuration, instantiating a <code>CAREamist</code> object, training, prediction, and model export.</p> Basic CAREamics usage<pre><code>import numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\n# create a configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,  # (1)!\n)\n\n# instantiate a careamist\ncareamist = CAREamist(config)\n\n# train the model\ntrain_data = np.random.randint(0, 255, (256, 256))  # (2)!\ncareamist.train(train_source=train_data)\n\n# once trained, predict\npred_data = np.random.randint(0, 255, (128, 128)).astype(np.float32)\npredction = careamist.predict(source=pred_data)\n\n# export to BMZ format\ncareamist.export_to_bmz(\n    path_to_archive=\"n2v_model.zip\",\n    friendly_model_name=\"N2V 2D\",\n    input_array=pred_data,\n    authors=[{\"name\": \"CAREamics authors\"}],\n    general_description=\"This model was trained to denoise 2D images.\",\n    data_description=\"The data was acquired on a confocal microscope [...]\",\n)\n</code></pre> <ol> <li> <p>Obviously, one should choose a more realistic number of epochs for training.</p> </li> <li> <p>One should use real data for training!</p> </li> </ol>"},{"location":"guides/careamist_api/usage/careamist/","title":"CAREamist","text":"<p>The <code>CAREamist</code> is the central class in CAREamics, it provides the API to train, predict and save models. There are three ways to create a <code>CAREamist</code> object: with a configuration,  with a path to a configuration, or with a path to a trained model.</p>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-configuration","title":"Instantiating with a configuration","text":"<p>When passing a configuration to the <code>CAREamist</code> constructor, the model is initialized with random weights and prediction will not be possible until the model is trained.</p> Instantiating CAREamist with a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)  # (1)!\n\ncareamist = CAREamist(config)\n</code></pre> <ol> <li>Any valid configuration will do!</li> </ol> <p>Creating configurations</p> <p>Check out the configuration guide to learn how to create configurations.</p>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-path-to-a-configuration","title":"Instantiating with a path to a configuration","text":"<p>This is similar to the previous section, except that the configuration is loaded from a file on disk.</p> Instantiating CAREamist with a path to a configuration<pre><code>from careamics import CAREamist\nfrom careamics.config import create_n2v_configuration, save_configuration\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=8,\n    num_epochs=20,\n)\n\nsave_configuration(config, \"configuration_example.yml\")\n\ncareamist = CAREamist(\"configuration_example.yml\")\n</code></pre>"},{"location":"guides/careamist_api/usage/careamist/#instantiating-with-a-path-to-a-model","title":"Instantiating with a path to a model","text":"<p>There are two types of models exported from CAREamics. During training, the model is saved as checkpoints (<code>.ckpt</code>). After training, users can export the model to the  bioimage model zoo format (saved as a<code>.zip</code>). Both can be loaded into CAREamics to either retrain or predict. Alternatively, a checkpoint can be loaded in order to  export it as a bioimage model zoo model.</p> <p>In any case, both types of pre-trained models can be loaded into CAREamics by passing the path to the model file. The instantiated CAREamist is then ready to predict on new images!</p> Instantiating CAREamist with a path to a model<pre><code>from careamics import CAREamist\n\npath_to_model = \"model.zip\"  # (1)!\n\ncareamist = CAREamist(path_to_model)\n</code></pre> <ol> <li>Any valid path to a model, as a string or a <code>Path.path</code> object, will work.</li> </ol>"},{"location":"guides/careamist_api/usage/careamist/#setting-the-working-directory","title":"Setting the working directory","text":"<p>By default, CAREamics will save the checkpoints in the current working directory. When creating a new CAREamist, you can indicate a different working directory in which to save the logs and checkpoints during training.</p> Changing the working directory<pre><code>careamist = CAREamist(config, work_dir=\"work_dir\")\n</code></pre>"},{"location":"guides/careamist_api/usage/careamist/#custom-callbacks","title":"Custom callbacks","text":"<p>CAREamics uses two different callbacks from PyTorch Lightning:</p> <ul> <li><code>ModelCheckpoint</code>: to save the model at different points during the training.</li> <li><code>EarlyStopping</code>: to stop the training based on a few parameters.</li> </ul> <p>The parameters for the callbacks are the same as the ones from PyTorch Lightning, and can be set in the configuration.</p> <p>Custom callbacks can be passed to the <code>CAREamist</code> constructor. The callbacks must inherit from the PyTorch Lightning <code>Callback</code> class.</p> Custom callbacks<pre><code>from pytorch_lightning.callbacks import Callback\n\n\n# define a custom callback\nclass MyCallback(Callback):  # (1)!\n    def __init__(self):\n        super().__init__()\n\n        self.has_started = False\n        self.has_ended = False\n\n    def on_train_start(self, trainer, pl_module):\n        self.has_started = True  # (2)!\n\n    def on_train_end(self, trainer, pl_module):\n        self.has_ended = True\n\n\nmy_callback = MyCallback()  # (3)!\n\ncareamist = CAREamist(config, callbacks=[my_callback])  # (4)!\n</code></pre> <ol> <li> <p>The callbacks must inherit from the PyTorch Lightning <code>Callback</code> class.</p> </li> <li> <p>This is just an example to test that the callback was called!</p> </li> <li> <p>Create your callback.</p> </li> <li> <p>Pass the callback to the CAREamist constructor as a list.</p> </li> </ol>"},{"location":"guides/careamist_api/usage/datasets/","title":"(Intermediate) Datasets","text":"<p>Datasets are the internal classes providing the individual patches for training,  validation and prediction. In CAREamics, we provide a <code>TrainDataModule</code> class that  creates the datasets for training and validation (there is a class for prediction as well, which is simpler and shares some parameters with the training one). In most cases, it is created internally. In this section, we describe what it does and shed light on some of its parameters that are passed to the train methods.</p> <p>Datasets in practice</p> <p>This section contains descriptions of the internal working of CAREamics. In practice, most users will never have to instantiate the datasets themselves, as they are created from within the <code>careamist.train</code> or <code>careamist.predict</code> methods.</p>"},{"location":"guides/careamist_api/usage/datasets/#overview","title":"Overview","text":"<p>The <code>TrainDataModule</code> receives both data configuration and data itself. The data can be passed a path to a folder, to a file or as <code>numpy</code> array. </p> Simplest way to instantiate TrainDataModule<pre><code>import numpy as np\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\ntrain_array = np.random.rand(128, 128)\n\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"array\",\n    axes=\"YX\",\n    patch_size=[64, 64],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = TrainDataModule(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n</code></pre> <p>It has the following parameters:</p> <ul> <li><code>data_config</code>: data configuration</li> <li><code>train_data</code>: training data (array or path)</li> <li><code>(optional) val_data</code>: validation data, if not provided, the validation data is taken from the training data</li> <li><code>(optional) train_data_target</code>: target data for training (if applicable)</li> <li><code>(optional) val_data_target</code>: target data for validation (if applicable)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> <li><code>(optional) val_percentage</code>: percentage of validation data to extract from the training     (see splitting validation)</li> <li><code>(optional) val_minimum_split</code>: minimum validation split      (see splitting validation)</li> <li><code>(optional) use_in_memory</code>: whether to use in-memory dataset if possible (Default is <code>True</code>),      not applicable to mnumpy arrays.</li> </ul> <p>Depending on the type of the data, which is specified in the <code>data_config</code> and is compared to the type of <code>train_data</code>, the <code>TrainDataModule</code> will create the appropriate dataset for both training and validation data.</p> <p>In the absence of validation, validation data is extracted from training data (see splitting validation).</p>"},{"location":"guides/careamist_api/usage/datasets/#available-datasets","title":"Available datasets","text":"<p>CAREamics currently support two datasets:</p> <ul> <li>InMemoryDataset: used when the data fits in memory.</li> <li>IterableDataset: used when the data is too large to fit in memory.</li> </ul> <p>If the data is a <code>numpy</code> array, the <code>InMemoryDataset</code> is used automatically. Otherwise, we list the files contained in the path, compute the size of the data and instantiate an <code>InMemoryDataset</code> if the data is less than 80% of the total RAM size. If not, CAREamics instantiate an <code>IterableDataset</code>.</p> <p>Both datasets work differently, and the main differences can be summarized as follows:</p> Feature <code>InMemoryDataset</code> <code>IterableDataset</code> Used with arrays  Yes  No Patch extraction Sequential Random Data loading All in memory One file at a time <p>In the next sections, we describe the different steps they perform.</p>"},{"location":"guides/careamist_api/usage/datasets/#in-memory-dataset","title":"In-memory dataset","text":"<p>As the name implies, the in-memory dataset loads all the data in memory. It is used when the data on the disk seems to fit in memory, or when the data is already in memory and  passed as a numpy array. The advantage of the dataset is that is allows faster access to the patches, and therefore faster training time.</p> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/careamist_api/usage/datasets/#iterable-dataset","title":"Iterable dataset","text":"<p>The iterable dataset is used to load patches from a single file at a time, one file after another. This allows training on datasets that are too large to fit in memory. This dataset is exclusively used with files input (data passed as paths).</p> <p>Iterable dataset and splitting validation</p> <p>The iterable dataset does not split patches from the training data, but files!  (see splitting validation).</p> <p>What about supervised training?</p> <p>For supervised training, the steps are the same and are performed for the targets alongside the source.</p> <p>What if I have a time (<code>T</code>) axis?</p> <p><code>T</code> axes are accepted by the CAREamics configuration, but are treated as a sample dimension (<code>S</code>). If both <code>S</code> and <code>T</code> are present, the two axes are concatenated.</p>"},{"location":"guides/careamist_api/usage/datasets/#intermediate-transforms","title":"(Intermediate) Transforms","text":"<p>Transforms are augmentations and any operation applied to the patches before feeding them into the network. CAREamics supports the following transforms (see  configuration full spec for an example on how to configure them):</p> Transform Description Notes <code>Normalize</code> Normalize (zero mean, unit variance) Necessary <code>XYFlip</code> Flip the image along X and Y, one at a time Can flip a single axis, optional <code>XYRandomRotate90Model</code> Rotate by 90 degrees the XY axes Optional <code>N2VManipulateModel</code> N2V pixel manipulation Only for N2V, in which case it is necessary <p>The <code>Normalize</code> transform is always applied, and the rest are optional. The exception is <code>N2VManipulateModel</code>, which is only applied when training with N2V (see Noise2Void).</p> <p>When to turn off transforms?</p> <p>The configuration allows turning off transforms. In this case, only normalization (and potentially the <code>N2VManipulateModel</code> for N2V) is applied. This is useful when the structures in your sample are always in the same orientation, and flipping and rotation do not make sense.</p>"},{"location":"guides/careamist_api/usage/datasets/#advanced-custom-data-types","title":"(Advanced) Custom data types","text":"<p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You should also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.npy\") to filter the files extension using <code>extension_filter</code>.</p> Read custom data types<pre><code>from pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\n\ndef read_npy(  # (1)!\n    path: Path,  # (2)!\n    *args: Any,\n    **kwargs: Any,  # (3)!\n) -&gt; np.ndarray:\n    return np.load(path)  # (4)!\n\n\n# example data\ntrain_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"custom\",  # (5)!\n    axes=\"YX\",\n    patch_size=[32, 32],\n    batch_size=1,\n    num_epochs=1,\n)\n\ndata_module = TrainDataModule(\n    data_config=config.data_config,\n    train_data=\"train_array.npy\",  # (6)!\n    read_source_func=read_npy,  # (7)!\n    extension_filter=\"*.npy\",  # (8)!\n)\ndata_module.prepare_data()\ndata_module.setup()  # (9)!\n\n# check dataset output\ndataloader = data_module.train_dataloader()\nprint(dataloader.dataset[0][0].shape)  # (10)!\n</code></pre> <ol> <li> <p>We define a function that reads the custom data type.</p> </li> <li> <p>It takes a path as argument!</p> </li> <li> <p>But it also need to receive <code>*args</code> and <code>**kwargs</code> to be compatible with the <code>read_source_func</code> signature.</p> </li> <li> <p>It simply returns a <code>numpy</code> array.</p> </li> <li> <p>The data type must be <code>custom</code>!</p> </li> <li> <p>And we pass a <code>Path | str</code>.</p> </li> <li> <p>Simply pass the method by name.</p> </li> <li> <p>We also need to provide an extension filter that is compatible with <code>fnmatch</code> and <code>Path.rglob</code>.</p> </li> <li> <p>These two lines are necessary to instantiate the training dataset that we call at the end. They are     called automatically by PyTorch Lightning during training.</p> </li> <li> <p>The dataloader gives access to the dataset, we choose the first element, and since     we configured CAREamics to use N2V, the output is a tuple whose first element is our     first patch!</p> </li> </ol> <p>In practice, you should not access the dataloader directly (except for testing). Using  custom types for training should be done as follows:</p> <pre><code>from pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nfrom careamics import CAREamist\nfrom careamics.config import create_n2v_configuration\nfrom careamics.lightning import TrainDataModule\n\n\ndef read_npy(\n    path: Path,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return np.load(path)\n\n\n# example data\ntrain_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# configuration\nconfig = create_n2v_configuration(\n    experiment_name=\"n2v_2D\",\n    data_type=\"custom\",\n    axes=\"YX\",\n    patch_size=[32, 32],\n    batch_size=1,\n    num_epochs=1,\n)\n\n# Data module for custom types\ndata_module = TrainDataModule(\n    data_config=config.data_config,\n    train_data=\"train_array.npy\",\n    read_source_func=read_npy,\n    extension_filter=\"*.npy\",\n)\n\n# CAREamist\ncareamist = CAREamist(source=config)\n\n# Train\ncareamist.train(datamodule=data_module)\n</code></pre>"},{"location":"guides/careamist_api/usage/datasets/#prediction-datasets","title":"Prediction datasets","text":"<p>The prediction data module, <code>PredictDataModule</code> works similarly to <code>TrainDataModule</code>, albeit with different parameters:</p> <ul> <li><code>pred_config</code>: inference configuration</li> <li><code>pred_data</code>: prediction data (array or path)</li> <li><code>(optional) read_source_func</code>: function to read custom data types      (see custom data types)</li> <li><code>(optional) extension_filter</code>: filter to select custom types     (see custom data types)</li> </ul>"},{"location":"guides/careamist_api/usage/datasets/#advanced-subclass-traindatamodule","title":"(Advanced) Subclass TrainDataModule","text":"<p>The data module used in CAREamics have only a limited number of parameters, and they  make use of the CAREamics datasets. If you need to have a different dataset, then you can subclass <code>TrainDataModule</code> and override the <code>setup</code> method to use your own datasets.</p>"},{"location":"guides/careamist_api/usage/model_export/","title":"Export to BMZ","text":"<p>The BioImage Model Zoo is a zoo of models that can be run in  a variety of software thanks to the  BMZ format. CAREamics is compatible  with the BMZ format and can export and load (CAREamics) models in this format.</p> <p>To export a trained model, you can simply call <code>careamist.export_to_bmz</code>:</p> Export to BMZ format<pre><code>careamist.export_to_bmz(\n    path_to_archive=export_path / \"my_model.zip\",  # (1)!\n    friendly_model_name=\"CARE_mito\",  # (2)!\n    input_array=my_array,  # (3)!\n    authors=[\n        {\n            \"name\": \"Ignatius J. Reilly\",\n            \"affiliation\": \"Levy Pants\",\n            \"email\": \"ijr@levy.com\",\n        },\n        {\"name\": \"Myrna Minkoff\", \"orcid\": \"0000-0002-3291-8524\"},  # (4)!\n    ],\n    general_description=\"This model was trained to denoise 2D images of mitochondria.\",  # (5)!\n    data_description=\"The data was acquired on a confocal microscope [...]\",  # (6)!\n)\n</code></pre> <ol> <li> <p>The model export should be a <code>.zip</code> file, if not CAREamics will add the extension.</p> </li> <li> <p>Give the model a name that is informative! It should consist of letters, numbers, hyphens and underscores.</p> </li> <li> <p>We need an input array to verify the export, the input and the prediction will be  packaged in the BMZ model. They will also be used to create the cover of the model if it is uploaded to the BioImage Model Zoo.</p> </li> <li> <p>You can have multiple authors, and <code>affiliation</code>, <code>email</code>, <code>orcid</code> and  <code>github _user</code>.</p> </li> <li> <p>A README will automatically be generated by CAREamics, containing information on how the model was trained. The general description should be a short description of what the model is used for.</p> </li> <li> <p>The <code>data_description</code> should contain precise information on the type of data the  model was trained on, this can include the type of data (specimen, modality), the dimensions (physical, number of pixels) and the content (type of structures).</p> </li> </ol>"},{"location":"guides/careamist_api/usage/model_export/#optional-parameters","title":"Optional parameters","text":"<p>The <code>export_to_bmz</code> function has an optional parameter:</p> <pre><code>careamist.export_to_bmz(\n    path_to_archive=export_path / \"my_model.zip\",\n    friendly_model_name=\"CARE_mito\",\n    input_array=my_array,\n    authors=[\n        {\n            \"name\": \"Ignatius J. Reilly\",\n            \"affiliation\": \"Levy Pants\",\n            \"email\": \"ijr@levy.com\",\n        },\n        {\"name\": \"Myrna Minkoff\", \"orcid\": \"0000-0002-3291-8524\"},\n    ],\n    general_description=\"This model was trained to denoise 2D images of mitochondria.\",\n    data_description=\"The data was acquired on a confocal microscope [...]\",\n    channel_names=[\"mito\", \"nucleus\"],  # (1)!\n)\n</code></pre> <ol> <li>If your data has channels, then you should add their name!</li> </ol>"},{"location":"guides/careamist_api/usage/model_export/#examples-of-careamics-models","title":"Examples of CAREamics models","text":"<p>In progress</p> <p>This page is still in construction.</p>"},{"location":"guides/careamist_api/usage/prediction/","title":"Prediction","text":"<p>Prediction is done by calling <code>careamist.predict</code> on either path or arrays. By default, the prediction function will expect the same type of data (e.g. array or path) as it was trained on, but it is possible to predict on a different type of data.</p> <p>Prediction is performed using the current weights.</p>"},{"location":"guides/careamist_api/usage/prediction/#predict-on-arrays-or-paths","title":"Predict on arrays or paths","text":"On numpy arrays On Paths <pre><code>import numpy as np\n\narray = np.random.rand(256, 256)\n\ncareamist.predict(\n    source=array,\n)\n</code></pre> <pre><code>careamist.predict(\n    source=path_to_data,\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/prediction/#tiling","title":"Tiling","text":"<p>Often, an image will be too large to fit on the GPU memory, or will have dimensions that are incompatible with the model (e.g. odd dimensions). To solve this, the image can be tiled into smaller overlapping patches, predicted upon and ultimately recombined.</p> <p>In <code>careamist.predict</code>, this is done by passing two parameters:</p> <pre><code>careamist.predict(\n    source=array,\n    tile_size=[64, 64],  # (1)!\n    tile_overlap=[32, 32],  # (2)!\n)\n</code></pre> <ol> <li> <p>The tile sizes correspond to each spatial dimensions. A good start is the patch size used during training.</p> </li> <li> <p>The overlap is the number of pixels that each patch will overlap with its neighbors.</p> </li> </ol> <p>After prediction, each tile is cropped to half of the overlap in each of the directions it overlaps with a neighboring tile. The reason is to minimize edge artifacts.</p> <p>Tile and overlap model constraints</p> <p>Some models, e.g. UNet model, impose constraints on the tile and overlap sizes. This is a direct consequence of the model architecture.</p> <p>For instance, when using a UNet model, the pooling and upsampling operations are not compatible with any tile size:</p> <ul> <li>tile sizes must be equal to \\(k2^n\\), where \\(n\\) is the number of pooling layers (equal to the model depth) and \\(k\\) is an integer.</li> <li>overlaps must be even and larger than twice the receptive field.</li> </ul>"},{"location":"guides/careamist_api/usage/prediction/#test-time-augmentation","title":"Test time augmentation","text":"<p>Test-time augmentation applies augmentations to the prediction input and averages the  (de-augmented) predictions. This can improve the quality of the prediction. The TTA generates all possible flipped and rotated versions of the image.</p> <p>By default, test-time augmentation is applied by CAREamics. In order to deactivate TTA,  you can set <code>tta</code> to <code>False</code>.</p> <pre><code>careamist.predict(\n    source=array,\n    tta=False,  # (1)!\n)\n</code></pre> <ol> <li>By default, TTA is activated!</li> </ol> <p>Transforms and TTA</p> <p>If you have turned off transforms, or used the non default ones, then you should  turn off TTA, as it would otherwise create images that do not correspond to your training data.</p>"},{"location":"guides/careamist_api/usage/prediction/#using-batches","title":"Using batches","text":"<p>To potentially predict faster, you can predict on batches of images.</p> <pre><code>careamist.predict(\n    source=array,\n    batch_size=2,  # (1)!\n)\n</code></pre> <ol> <li>Each prediction step will be performed on 2 images or tiles.</li> </ol> <p>Batch and TTA</p> <p>Having <code>batch_size&gt;1</code> is compatible with the TTA.</p>"},{"location":"guides/careamist_api/usage/prediction/#changing-the-data-type","title":"Changing the data type","text":"<p>You can use a different type (in the sense <code>path</code> vs <code>array</code>) of data at prediction time by changing the <code>data_type</code> parameter.</p> <pre><code>careamist.predict(\n    source=path_to_data,\n    data_type=\"tiff\",  # (1)!\n)\n</code></pre> <ol> <li>As in the rest of CAREamics, the supported value are <code>tiff</code>, <code>array</code> and <code>custom</code>. </li> </ol>"},{"location":"guides/careamist_api/usage/prediction/#changing-the-axes","title":"Changing the axes","text":"<p>Similarly, if you want to predict on data that has different axes than the training data, as long as those have the same number of channels and spatial dimensions, then you can change the <code>axes</code> parameter.</p> <pre><code>careamist.predict(\n    source=other_array,\n    axes=\"SYX\",  # (1)!\n)\n</code></pre> <ol> <li>Obviously, this need to match <code>source</code>.</li> </ol>"},{"location":"guides/careamist_api/usage/prediction/#advanced-predict-on-custom-data-type","title":"(Advanced) Predict on custom data type","text":"<p>As for the training, one can predict on custom data types by providing a function that reads the data from a path and a function to filter the requested extension.</p> <pre><code>def read_npy(\n    path: Path,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    return np.load(path)\n\n\n# example data\npredict_array = np.random.rand(128, 128)\nnp.save(\"train_array.npy\", train_array)\n\n# Train\ncareamist.predict(\n    source=predict_array,\n    read_source_func=read_npy,\n    extension_filter=\"*.npy\",\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/training/","title":"Training","text":"<p>You can provide data in various way to train your model: as a <code>numpy</code> array, using a path to a folder or files, or by using CAREamics data module class for more control (advanced).</p> <p>The details of how CAREamics deals with the loading and patching is detailed in the dataset section.</p> <p>Data type</p> <p>The data type of the source and targets must be the same as the one specified in the configuration. That is to say <code>array</code> in the case of <code>np.ndarray</code>, and <code>tiff</code> in the case of paths.</p>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-an-array","title":"Training by passing an array","text":"<p>CAREamics can be trained by simply passing numpy arrays.</p> Training by passing an array<pre><code>import numpy as np\n\ntrain_array = np.random.rand(256, 256)\nval_array = np.random.rand(256, 256)\n\ncareamist.train(\n    train_source=train_array,  # (1)!\n    val_source=val_array,  # (2)!\n)\n</code></pre> <ol> <li>All parameters to the <code>train</code> method must be specified by keyword.</li> <li>If you don't provide a validation source, CAREamics will use a fraction of the training data    to validate the model.</li> </ol> <p>Supervised training</p> <p>If you are training a supervised model, you must provide the target data as well.</p> <pre><code>careamist_supervised.train(\n    train_source=train_array,\n    train_target=target_array,\n    val_source=val_array,\n    val_target=val_target_array,\n)\n</code></pre>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-a-path","title":"Training by passing a path","text":"<p>The same thing can be done by passing a path to a folder or files.</p> Training by passing a path<pre><code>careamist.train(\n    train_source=path_to_train_data,  # (1)!\n    val_source=path_to_val_data,\n)\n</code></pre> <ol> <li>The path can point to a single file, or contain multiple files.</li> </ol> <p>Training from path</p> <p>To train from a path, the data type must be set to <code>tiff</code> or <code>custom</code> in the  configuration.</p>"},{"location":"guides/careamist_api/usage/training/#splitting-validation-from-training-data","title":"Splitting validation from training data","text":"<p>If you only provide training data, CAREamics will extract the validation data directly from the training set. There are two parameters controlling that behaviour: <code>val_percentage</code> and <code>val_minimum_split</code>.</p> <p><code>val_percentage</code> is the fraction of the training data that will be used for validation, and <code>val_minimum_split</code> is the minimum number of images used. If the percentage leads to a  number of patches smaller than <code>val_minimum_split</code>, CAREamics will use <code>val_minimum_split</code>.</p> Splitting validation from training data<pre><code>careamist.train(\n    train_source=train_array,\n    val_percentage=0.1,  # (1)!\n    val_minimum_split=5,  # (2)!\n)\n</code></pre> <ol> <li>10% of the training data will be used for validation.</li> <li>If the number of images is less than 5, CAREamics will use 5 images for validation.</li> </ol> <p>Arrays vs files</p> <p>The behaviour of <code>val_percentage</code> and <code>val_minimum_split</code> is based different depending on whether the source data is an array or a path. If the source is an array, the split is done on the patches (<code>N</code> patches are used for validation). If the source is a path, the split is done on the files (<code>N</code> files are used for validation).</p>"},{"location":"guides/careamist_api/usage/training/#training-by-passing-a-traindatamodule-object","title":"Training by passing a <code>TrainDataModule</code> object","text":"<p>CAREamics provides a class to handle the data loading of custom data type. We will dive  in more details in the next section into what this class can be used for. Here is a  brief overview of how it is passed to the <code>train</code> method.</p> Training by passing a TrainDataModule object<pre><code>from careamics.lightning import TrainDataModule\n\ndata_module = TrainDataModule(  # (1)!\n    data_config=config.data_config, train_data=train_array\n)\n\ncareamist.train(datamodule=data_module)\n</code></pre> <ol> <li>Here this does the same thing as passing the <code>train_source</code> directly into the <code>train</code> method.     In the next section, we will see a more useful example.</li> </ol>"},{"location":"guides/careamist_api/usage/training/#logging-the-training","title":"Logging the training","text":"<p>By default, CAREamics simply log the training progress in the console. However, it is  possible to use either WandB or TensorBoard.</p> <p>To decide on the logger, check out the Configuration section.</p> <p>Loggers installation</p> <p>Using WandB or TensorBoard require the installation of <code>extra</code> dependencies. Check out the installation section to know more about it.</p>"},{"location":"guides/cli/","title":"Command-line interface","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <pre><code>$ careamics --help\nUsage: careamics [OPTIONS] COMMAND [ARGS]...\n\nRun CAREamics algorithms from the command line, including Noise2Void and its many variants and cousins                                                      \n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion  Install completion for the      \u2502 \n\u2502                       current shell.                  \u2502\n\u2502 --show-completion     Show completion for the current \u2502\n|                       shell, to copy it or customize  |\n|                       the installation.               \u2502\n\u2502 --help                Show this message and exit.     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 conf      Build and save CAREamics configuration files.                      \u2502\n\u2502 predict   Create and save predictions from CAREamics models.                 \u2502\n\u2502 train     Train CAREamics models.                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"guides/dev_resources/","title":"Developer resources","text":"<p>Work in progress</p> <p>These pages are still under construction.</p> <ul> <li>How to contribute to the project?</li> <li>How does the website work?</li> <li>CAREamics docstring conventions</li> <li>Conda-forge package maintenance</li> </ul>"},{"location":"guides/dev_resources/conda/","title":"Conda","text":"<p>CAREamics is available via conda-forge. Package maintenance is done through the careamics-feedstock.</p>"},{"location":"guides/dev_resources/conda/#new-careamics-version","title":"New CAREamics version","text":"<p>New version are automatically pulled by conda-forge from PyPi, and a PR is made to the feedstock repository.</p> <p>Typically, the PR updates the <code>version</code> number and the <code>hash</code> in the <code>meta.yaml</code> file.</p>"},{"location":"guides/dev_resources/conda/#updating-the-recipe","title":"Updating the recipe","text":"<p>When dependencies change, the conda recipe needs to be updated. This is done by making a PR (e.g. from careamics-feedstock fork).</p> <p>The best is to update the dependencies in the <code>meta.yaml</code> file, and push the changes to the automated PR on the official feedstock repository.</p>"},{"location":"guides/dev_resources/conda/#testing-the-recipe","title":"Testing the recipe","text":"<p>It may happen that the container build on the official feedstock repo fails, in which  case one needs to test locally what can go wrong. Here is how to do it:</p> <pre><code>cd careamics-feedstock\nconda create -n forge python=3.10\nconda activate forge\nconda install conda-build\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <p>Then create a local conda config:</p> <pre><code>vi recipe/conda_build_config.yaml\n</code></pre> <p>And add the following content:</p> <pre><code>python_min:\n  - \"3.8\"  # Set the minimum Python version required for your recipe\n</code></pre> <p>Finally, build the package:</p> <pre><code>conda build recipe\n</code></pre>"},{"location":"guides/dev_resources/contribute/","title":"Contribute to CAREamics","text":"<p>CAREamics is a free and open-source software and we are happy to receive contributions from the community! There are several ways to contribute to the project:</p> <ul> <li>Contribute applications</li> <li>Contribute methods and features</li> <li>Contribute to the documentation</li> </ul>"},{"location":"guides/dev_resources/contribute/#contribute-applications","title":"Contribute applications","text":"<p>Microscopy is a vast field and there are many different types of data and projects that can benefit from denoising. We only have applications that were brought to us or used in the publications.</p> <p>We would love to know more about different types of microscopy data, and whether  CAREamics helped analyse them or not. In the future, we want to illustrate how to scientifically apply the methods in CAREamics to research data and any example, working or failing, is of great help to the community!</p> <p>To contribute applications, open an issue so that we can discuss the problems you encountered or have a look at the results!</p> <p>Once an application is accepted, you can create a <code>jupyter notebook</code> and add it to the careamics-example repository. Finally, the website guide explains how to add the notebook to the  website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-methods-and-features","title":"Contribute methods and features","text":"<p>CAREamics is a growing project and we are always looking for new methods and features to better help the community. We are interested in any method that proved to be  valuable for microscopy data denoising and restoration.</p> <p>What about data formats?</p> <p>We currently only support <code>numpy arrays</code> and <code>tif</code> files. In the near future, we will focus on <code>zarr</code> data. We do not intend in maintaining support for other data formats.</p> <p>For particular data formats, we advise to use the custom data loading, which is described in the training guide.</p> <p>To contribute methods, open an issue so that we can discuss the method and its implementation. Same for new features!</p>"},{"location":"guides/dev_resources/contribute/#opening-a-pull-request","title":"Opening a pull request","text":"<p>Before opening a pull request, make sure that you installed the <code>dev</code> optional dependencies of CAREamics:</p> <pre><code>pip install careamics[dev]\n</code></pre> <p>In particular, make sure that you use pre-commit before committing changes:</p> <pre><code>pre-commit install\n</code></pre> <p>The PR need to pass the tests and the pre-commit checks! Make sure to also fill in the  PR template and make a PR to the documentation website.</p>"},{"location":"guides/dev_resources/contribute/#contribute-to-the-documentation","title":"Contribute to the documentation","text":"<p>If you find any typo, mistakes or missing information in the documentation, feel free to make a PR to the documentation repository.</p> <p>Read the website guide to know how to better understand the various mechanisms implemented in the website.</p>"},{"location":"guides/dev_resources/docstring/","title":"Docstring conventions","text":"<p>CAREamics follows the numpydoc  docstring conventions. The is enforced by the use of the <code>numpydoc</code> pre-commit hook.</p> <p>On top of the numpy conventions, we try to build a more human readable docstring by adapting principles from Pandas docstring.</p>"},{"location":"guides/dev_resources/docstring/#parameter-declaration","title":"Parameter declaration","text":"<pre><code>\"\"\"\n    param : Union[str, int] \u274c\n    param : str or int \u2705\n    param : str | int \u2705\n\n    choice : Literal[\"a\", \"b\", \"c\"] \u274c\n    choice : {\"a\", \"b\", \"c\"} \u2705\n\n    param : Tuple[int, int] \u274c\n    param : (int, int) \u2705\n\n    sequence: List[int] \u274c\n    sequence: list of int \u2705\n\n    param : int \n        The default is 1. \u274c\n    param : int, default=1 \u2705\n\n    param : Optional[int] \u274c\n    param : int, optional \u2705\n\"\"\"\n</code></pre>"},{"location":"guides/dev_resources/docstring/#third-party-types","title":"Third-party types","text":"<pre><code>\"\"\"\n    param : pandas.DataFrame\n    param : NDArray\n    param : torch.Tensor\n    param : tensorflow.Tensor\n\"\"\"\n</code></pre>"},{"location":"guides/dev_resources/website/","title":"Github pages","text":"<p>The Github pages are built using mkdocs, more specifically the  mkdocs-material theme. Modifications to the theme were greatly inspired from pydev-guide.</p> <p>In this page, we describe some of the technical details on how to maintain this website.</p>"},{"location":"guides/dev_resources/website/#environment","title":"Environment","text":"<p>The <code>requirements.txt</code> file contains all the packages used to generate this website.</p>"},{"location":"guides/dev_resources/website/#build-the-pages-locally","title":"Build the pages locally","text":"<p>In order to build the pages locally, follow these steps:</p> <ol> <li>Fork this repository and clone it.</li> <li>Create a new environment and install the dependencies:     <pre><code>conda create -n careamics-docs python=3.11\npip install -r requirements.txt\n</code></pre></li> <li>Run the following scripts (which are normally run by the CI):     <pre><code>sh scripts/check_out_repos.sh\nsh scripts/check_out_examples.sh\n</code></pre></li> <li>Build the pages:     <pre><code>mkdocs serve\n</code></pre></li> <li>Open the local link in your browser.</li> </ol> <p>Note: This will not show you the version mechanism. For this, check out the  Version release section.</p>"},{"location":"guides/dev_resources/website/#code-snippets","title":"Code snippets","text":"<p>Code snippets are all automatically tested in careamics-example and added automaticallt to the Github pages in the guides section.</p> <p>The script <code>scripts/check_out_examples.sh</code> clone the examples repository locally and upon building the pages, the code snippets are automatically added to the markdown by the PyMdown Snippets extension.</p> <p>It works as follows, first create a code snippet in a python file on  careamics-example:</p> careamics-example/some_example/some_file.py<pre><code># code necessary for running successfully the snippet\n# but not shown on the Github pages\nimport numpy as np\n\narray = np.ones((32, 32))\n\n# we add a snippet section:\nsum_array = np.sum(array) # snippets appearing on the website (can be multi-lines)\n\n# then more code or snippets sections\n...\n</code></pre> <p>Then, in the CAREamics Github pages source, the corresponding markdown file has the following content telling <code>PyMdown.Snippets</code> to  include the snippet section:</p> careamics.github.io/guides/some_example/some_file.py<pre><code>    To sum the array, simply do:\n\n    ```python\n    --8&lt;-- \"careamics-example/some_example/some_file.py:my_snippet\"\n    ```\n</code></pre> <p>The examples in the guide are automatically tested in the CI. To make sure it runs on the correct branches in the pull request, use the same branch name for the PR to the CAREamics source code and to the examples repository.</p>"},{"location":"guides/dev_resources/website/#jupyter-notebook-applications","title":"Jupyter notebook applications","text":"<p>The pages in the application section are automatically generated from the Jupyter notebooks in careamics-example  using mkdocs-jupyter. A bash script (<code>scripts/check_out_examples.sh</code>) checks out the repository and copies  all the notebooks referenced into the correct path in the application  folder. Finally, the script <code>scripts/gen_jupyter_nav.py</code> creates entries for each notebook  in the navigation file of mkdocs.</p>"},{"location":"guides/dev_resources/website/#adding-a-new-notebook","title":"Adding a new notebook","text":"<ol> <li>Add the notebook to <code>scripts/notebooks.json</code>. The structure is as follows:     <pre><code>{\n    \"applications\": [\n        ...,\n        {\n            \"name\": \"Name_of_the_Notebook\", // (1)!\n            \"description\": \"Some description of the notebook.\", // (2)!\n            \"cover\": \"File_name.jpeg\", // (3)!\n            \"source\": \"path/in/careamics-examples/repo/File_Name.ipynb\", // (4)!\n            \"destination\": \"Category_in_Applications\", // (5)!\n            \"tags\": \"3D, channels, fluorescence\" // (6)!\n        },\n        // (7)!\n    ]\n    ...\n}\n</code></pre><ol> <li><code>name</code> will be used as the name of the notebook, without the <code>_</code> when used as title and with <code>Name_of_the_Notebook.ipynb</code> as the file name after copy.</li> <li><code>description</code> will be shown in the cards corresponding to the notebook on the website.</li> <li><code>cover</code> is the name of the image file that will be used as the cover in the notebook card, the actual image should be placed in <code>docs/assets/notebook_covers</code>.</li> <li><code>source</code> is the path to the notebook in the <code>careamics-examples</code> repository.</li> <li><code>destination</code> is the category in the <code>applications</code> folder where the notebook will be copied to (e.g. all notebooks using <code>Noise2Void</code> are in the <code>Noise2Void</code> category).</li> <li><code>tags</code> are shown on the notebook card and inform on the characteristic of the data. These are comma separated values.</li> <li>You can add multiple notebooks!</li> </ol> </li> <li>You can test that the notebook was correctly added by running <code>sh scripts/notebooks.sh</code> then <code>mkdocs serve</code>.</li> </ol> <p>Cell tags</p> <p>By default, all notebook cell outputs are shown. To hide the output of a particular cell, add the tag <code>remove_output</code> to the cell. The <code>mkdocs.ynml</code> specifies that this  tag is used to hide cell outputs.</p> <p>For instance, this is useful for the training cell, which output hundreds of lines.</p>"},{"location":"guides/dev_resources/website/#code-reference","title":"Code reference","text":"<p>The code reference is generated using mkdocstrings,  the script <code>scripts/checkout_repos.sh</code> and the page building script <code>scripts/gen_ref_pages.py</code>.  To include a new package, simply add it to the <code>scripts/git_repositories.txt</code> file.</p> <pre><code>https://github.com/CAREamics/careamics\nhttps://github.com/CAREamics/careamics-portfolio\n&lt;new project here&gt;\n</code></pre>"},{"location":"guides/dev_resources/website/#updating-the-website-version","title":"Updating the website version","text":"<p>In principle, when a new release of CAREamics is made, the state of the documentation is saved into the corresponding version, and the documentation is tagged with the next (ongoing) version.</p> <p>For instance, the documentation is showing version <code>0.4</code>, upon release of version  <code>0.4</code>, the state of the documentation is saved. The latest documentation is then  tagged with version <code>0.5</code> (the next version) until this one is released.</p> <p>In order to keep track of versions, we use mike.  We apply the following procedure:</p> <ol> <li>Release version MAJOR.MINOR of CAREamics</li> <li>Tag the latest documentation with version MAJOR.(MINOR+1)   <pre><code>git tag MAJOR.(MINOR+1)\ngit push --tags\n</code></pre></li> </ol> <p>To visualize the pages with the versions, you can use:</p> <pre><code>mike serve\n</code></pre>"},{"location":"guides/dev_resources/website/#correcting-a-version-error","title":"Correcting a version error","text":"<p>All the versions are stored in the <code>gh-pages</code> branch. If you made a mistake in the version tagging, you can correct it by deleting the tag and pushing the changes.</p>"},{"location":"guides/lightning_api/","title":"Lightning API","text":"<p>The so-called \"Lightning API\" is how we refer to using the lightning modules  from CAREamics in a PyTorch Ligthning  pipeline. In our high-level API, these modules are  hidden from users and many checks, validations, error handling, and other  features are provided. However, if you want to have increased flexibility, for instance to use your own dataset, model or a different training loop, you can re-use many of  CAREamics modules in your own PyTorch Lightning pipeline.</p> Basic Usage<pre><code>import numpy as np\nfrom careamics.lightning import (  # (1)!\n    create_careamics_module,\n    create_predict_datamodule,\n    create_train_datamodule,\n)\nfrom careamics.prediction_utils import convert_outputs\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import (\n    ModelCheckpoint,\n)\n\n# training data\nrng = np.random.default_rng(42)\ntrain_array = rng.integers(0, 255, (32, 32)).astype(np.float32)\nval_array = rng.integers(0, 255, (32, 32)).astype(np.float32)\n\n# create lightning module\nmodel = create_careamics_module(  # (2)!\n    algorithm=\"n2v\",\n    loss=\"n2v\",\n    architecture=\"UNet\",\n)\n\n# create data module\ndata = create_train_datamodule(\n    train_data=train_array,\n    val_data=val_array,\n    data_type=\"array\",\n    patch_size=(16, 16),\n    axes=\"YX\",\n    batch_size=2,\n)\n\n# create trainer\ntrainer = Trainer(  # (3)!\n    max_epochs=1,\n    default_root_dir=mypath,\n    callbacks=[\n        ModelCheckpoint(  # (4)!\n            dirpath=mypath / \"checkpoints\",\n            filename=\"basic_usage_lightning_api\",\n        )\n    ],\n)\n\n# train\ntrainer.fit(model, datamodule=data)\n\n# predict\nmeans, stds = data.get_data_statistics()\npredict_data = create_predict_datamodule(\n    pred_data=val_array,\n    data_type=\"array\",\n    axes=\"YX\",\n    image_means=means,\n    image_stds=stds,\n    tile_size=(8, 8),  # (5)!\n    tile_overlap=(2, 2),\n)\n\n# predict\npredicted = trainer.predict(model, datamodule=predict_data)\npredicted_stitched = convert_outputs(predicted, tiled=True)  # (6)!\n</code></pre> <ol> <li> <p>We provide convenience functions to create the various Lightning modules.</p> </li> <li> <p>Each convenience function will have a set of algorithms. Often, these correspond  to the parameters in the CAREamics configuration. You can check the next pages for more details.</p> </li> <li> <p>As for any Lightning pipeline, you need to instantiate a <code>Trainer</code>.</p> </li> <li> <p>This way, you have all freedom to set your own callbacks.</p> </li> <li> <p>Our prediction Lightning data module has the possibility to break the images into overlapping tiles.</p> </li> <li> <p>If you predicted using tiled images, you need to recombine the tiles into images. We provide a general function to take care of this.</p> </li> </ol> <p>There are three types of Lightning modules in CAREamics:</p> <ul> <li>Lightning Module</li> <li>Training Lightning Datamodule</li> <li>Prediction Lightning Datamodule</li> </ul> <p>In the next pages, we give more details on the various parameters of the convenience  functions. For the rest, refer the the PyTorch Lightning documentation.</p>"},{"location":"guides/napari_plugin/","title":"napari plugin","text":"<p>Work in progress</p> <p>These pages are still under construction.</p>"},{"location":"guides/tutorials/","title":"Tutorials","text":"<p>Work in progress</p> <p>These pages are still under construction.</p>"},{"location":"reference/","title":"Code Reference","text":"CAREamics CAREamics napari CAREamics portfolio"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>index.md</li> <li>careamics<ul> <li>index.md</li> <li>careamist</li> <li>cli<ul> <li>conf</li> <li>main</li> <li>utils</li> </ul> </li> <li>config<ul> <li>algorithms<ul> <li>care_algorithm_model</li> <li>n2n_algorithm_model</li> <li>n2v_algorithm_model</li> <li>unet_algorithm_model</li> <li>vae_algorithm_model</li> </ul> </li> <li>architectures<ul> <li>architecture_model</li> <li>lvae_model</li> <li>unet_model</li> </ul> </li> <li>callback_model</li> <li>configuration</li> <li>configuration_factories</li> <li>configuration_io</li> <li>data<ul> <li>data_model</li> </ul> </li> <li>inference_model</li> <li>likelihood_model</li> <li>loss_model</li> <li>nm_model</li> <li>optimizer_models</li> <li>support<ul> <li>supported_activations</li> <li>supported_algorithms</li> <li>supported_architectures</li> <li>supported_data</li> <li>supported_loggers</li> <li>supported_losses</li> <li>supported_optimizers</li> <li>supported_pixel_manipulations</li> <li>supported_struct_axis</li> <li>supported_transforms</li> </ul> </li> <li>tile_information</li> <li>training_model</li> <li>transformations<ul> <li>n2v_manipulate_model</li> <li>normalize_model</li> <li>transform_model</li> <li>transform_unions</li> <li>xy_flip_model</li> <li>xy_random_rotate90_model</li> </ul> </li> <li>validators<ul> <li>model_validators</li> <li>validator_utils</li> </ul> </li> </ul> </li> <li>conftest</li> <li>dataset<ul> <li>dataset_utils<ul> <li>dataset_utils</li> <li>file_utils</li> <li>iterate_over_files</li> <li>running_stats</li> </ul> </li> <li>in_memory_dataset</li> <li>in_memory_pred_dataset</li> <li>in_memory_tiled_pred_dataset</li> <li>iterable_dataset</li> <li>iterable_pred_dataset</li> <li>iterable_tiled_pred_dataset</li> <li>patching<ul> <li>patching</li> <li>random_patching</li> <li>sequential_patching</li> <li>validate_patch_dimension</li> </ul> </li> <li>tiling<ul> <li>collate_tiles</li> <li>lvae_tiled_patching</li> <li>tiled_patching</li> </ul> </li> <li>zarr_dataset</li> </ul> </li> <li>dataset_ng<ul> <li>patch_extractor<ul> <li>image_stack<ul> <li>image_stack_protocol</li> <li>in_memory_image_stack</li> <li>zarr_image_stack</li> </ul> </li> <li>patch_extractor</li> <li>patch_extractor_factory</li> </ul> </li> <li>patching_strategies<ul> <li>patch_specs_generator</li> </ul> </li> </ul> </li> <li>file_io<ul> <li>read<ul> <li>get_func</li> <li>tiff</li> <li>zarr</li> </ul> </li> <li>write<ul> <li>get_func</li> <li>tiff</li> </ul> </li> </ul> </li> <li>lightning<ul> <li>callbacks<ul> <li>hyperparameters_callback</li> <li>prediction_writer_callback<ul> <li>file_path_utils</li> <li>prediction_writer_callback</li> <li>write_strategy</li> <li>write_strategy_factory</li> </ul> </li> <li>progress_bar_callback</li> </ul> </li> <li>lightning_module</li> <li>predict_data_module</li> <li>train_data_module</li> </ul> </li> <li>losses<ul> <li>fcn<ul> <li>losses</li> </ul> </li> <li>loss_factory</li> <li>lvae<ul> <li>loss_utils</li> <li>losses</li> </ul> </li> </ul> </li> <li>lvae_training<ul> <li>calibration</li> <li>dataset<ul> <li>config</li> <li>lc_dataset</li> <li>multich_dataset</li> <li>multifile_dataset</li> <li>types</li> <li>utils<ul> <li>data_utils</li> <li>empty_patch_fetcher</li> <li>index_manager</li> <li>index_switcher</li> </ul> </li> </ul> </li> <li>eval_utils</li> <li>get_config</li> <li>lightning_module</li> <li>metrics</li> <li>train_lvae</li> <li>train_utils</li> </ul> </li> <li>model_io<ul> <li>bioimage<ul> <li>_readme_factory</li> <li>bioimage_utils</li> <li>cover_factory</li> <li>model_description</li> </ul> </li> <li>bmz_io</li> <li>model_io_utils</li> </ul> </li> <li>models<ul> <li>activation</li> <li>layers</li> <li>lvae<ul> <li>layers</li> <li>likelihoods</li> <li>lvae</li> <li>noise_models</li> <li>stochastic</li> <li>utils</li> </ul> </li> <li>model_factory</li> <li>unet</li> </ul> </li> <li>prediction_utils<ul> <li>lvae_prediction</li> <li>lvae_tiling_manager</li> <li>prediction_outputs</li> <li>stitch_prediction</li> </ul> </li> <li>transforms<ul> <li>compose</li> <li>n2v_manipulate</li> <li>n2v_manipulate_torch</li> <li>normalize</li> <li>pixel_manipulation</li> <li>pixel_manipulation_torch</li> <li>struct_mask_parameters</li> <li>transform</li> <li>tta</li> <li>xy_flip</li> <li>xy_random_rotate90</li> </ul> </li> <li>utils<ul> <li>autocorrelation</li> <li>base_enum</li> <li>context</li> <li>lightning_utils</li> <li>logging</li> <li>metrics</li> <li>path_utils</li> <li>plotting</li> <li>ram</li> <li>receptive_field</li> <li>serializers</li> <li>torch_utils</li> <li>version</li> </ul> </li> </ul> </li> <li>careamics_napari<ul> <li>index.md</li> <li>_version</li> <li>careamics_utils<ul> <li>algorithms</li> <li>callback</li> <li>configuration</li> <li>free_memory</li> </ul> </li> <li>prediction_plugin</li> <li>resources<ul> <li>resources</li> </ul> </li> <li>sample_data</li> <li>signals<ul> <li>prediction_signal</li> <li>prediction_status</li> <li>saving_signal</li> <li>saving_status</li> <li>training_signal</li> <li>training_status</li> </ul> </li> <li>training_plugin</li> <li>utils<ul> <li>axes_utils</li> <li>gpu_utils</li> </ul> </li> <li>widgets<ul> <li>algorithm_choice</li> <li>axes_widget</li> <li>banner_widget</li> <li>configuration_window</li> <li>folder_widget</li> <li>gpu_widget</li> <li>magicgui_widgets</li> <li>predict_data_widget</li> <li>prediction_widget</li> <li>qt_widgets</li> <li>saving_widget</li> <li>scroll_wrapper</li> <li>tbplot_widget</li> <li>train_data_widget</li> <li>train_progress_widget</li> <li>training_configuration_widget</li> <li>training_widget</li> </ul> </li> <li>workers<ul> <li>prediction_worker</li> <li>saving_worker</li> <li>training_worker</li> </ul> </li> </ul> </li> <li>careamics_portfolio<ul> <li>index.md</li> <li>denoiseg_datasets</li> <li>denoising_datasets</li> <li>portfolio</li> <li>portfolio_entry</li> <li>utils<ul> <li>download_utils</li> <li>pale_blue_dot</li> <li>pale_blue_dot_zip</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/careamics/","title":"CAREamics","text":"<p>Use the navigation index on the left to explore the documentation.</p>"},{"location":"reference/careamics/careamist/","title":"careamist","text":"<p>A class to train, predict and export models in CAREamics.</p>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist","title":"<code>CAREamist</code>","text":"<p>Main CAREamics class, allowing training and prediction using various algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>pathlib.Path or str or CAREamics Configuration</code> <p>Path to a configuration file or a trained model.</p> required <code>work_dir</code> <code>str</code> <p>Path to working directory in which to save checkpoints and logs, by default None.</p> <code>None</code> <code>callbacks</code> <code>list of Callback</code> <p>List of callbacks to use during training and prediction, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>CAREamicsModule</code> <p>CAREamics model.</p> <code>cfg</code> <code>Configuration</code> <p>CAREamics configuration.</p> <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> <code>experiment_logger</code> <code>TensorBoardLogger or WandbLogger</code> <p>Experiment logger, \"wandb\" or \"tensorboard\".</p> <code>work_dir</code> <code>Path</code> <p>Working directory.</p> <code>train_datamodule</code> <code>TrainDataModule</code> <p>Training datamodule.</p> <code>pred_datamodule</code> <code>PredictDataModule</code> <p>Prediction datamodule.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>class CAREamist:\n    \"\"\"Main CAREamics class, allowing training and prediction using various algorithms.\n\n    Parameters\n    ----------\n    source : pathlib.Path or str or CAREamics Configuration\n        Path to a configuration file or a trained model.\n    work_dir : str, optional\n        Path to working directory in which to save checkpoints and logs,\n        by default None.\n    callbacks : list of Callback, optional\n        List of callbacks to use during training and prediction, by default None.\n\n    Attributes\n    ----------\n    model : CAREamicsModule\n        CAREamics model.\n    cfg : Configuration\n        CAREamics configuration.\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    experiment_logger : TensorBoardLogger or WandbLogger\n        Experiment logger, \"wandb\" or \"tensorboard\".\n    work_dir : pathlib.Path\n        Working directory.\n    train_datamodule : TrainDataModule\n        Training datamodule.\n    pred_datamodule : PredictDataModule\n        Prediction datamodule.\n    \"\"\"\n\n    @overload\n    def __init__(  # numpydoc ignore=GL08\n        self,\n        source: Union[Path, str],\n        work_dir: Optional[Union[Path, str]] = None,\n        callbacks: Optional[list[Callback]] = None,\n    ) -&gt; None: ...\n\n    @overload\n    def __init__(  # numpydoc ignore=GL08\n        self,\n        source: Configuration,\n        work_dir: Optional[Union[Path, str]] = None,\n        callbacks: Optional[list[Callback]] = None,\n    ) -&gt; None: ...\n\n    def __init__(\n        self,\n        source: Union[Path, str, Configuration],\n        work_dir: Optional[Union[Path, str]] = None,\n        callbacks: Optional[list[Callback]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize CAREamist with a configuration object or a path.\n\n        A configuration object can be created using directly by calling `Configuration`,\n        using the configuration factory or loading a configuration from a yaml file.\n\n        Path can contain either a yaml file with parameters, or a saved checkpoint.\n\n        If no working directory is provided, the current working directory is used.\n\n        Parameters\n        ----------\n        source : pathlib.Path or str or CAREamics Configuration\n            Path to a configuration file or a trained model.\n        work_dir : str or pathlib.Path, optional\n            Path to working directory in which to save checkpoints and logs,\n            by default None.\n        callbacks : list of Callback, optional\n            List of callbacks to use during training and prediction, by default None.\n\n        Raises\n        ------\n        NotImplementedError\n            If the model is loaded from BioImage Model Zoo.\n        ValueError\n            If no hyper parameters are found in the checkpoint.\n        ValueError\n            If no data module hyper parameters are found in the checkpoint.\n        \"\"\"\n        # select current working directory if work_dir is None\n        if work_dir is None:\n            self.work_dir = Path.cwd()\n            logger.warning(\n                f\"No working directory provided. Using current working directory: \"\n                f\"{self.work_dir}.\"\n            )\n        else:\n            self.work_dir = Path(work_dir)\n\n        # configuration object\n        if isinstance(source, Configuration):\n            self.cfg = source\n\n            # instantiate model\n            if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                self.model = FCNModule(\n                    algorithm_config=self.cfg.algorithm_config,\n                )\n            else:\n                raise NotImplementedError(\"Architecture not supported.\")\n\n        # path to configuration file or model\n        else:\n            # TODO: update this check so models can be downloaded directly from BMZ\n            source = check_path_exists(source)\n\n            # configuration file\n            if source.is_file() and (\n                source.suffix == \".yaml\" or source.suffix == \".yml\"\n            ):\n                # load configuration\n                self.cfg = load_configuration(source)\n\n                # instantiate model\n                if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                    self.model = FCNModule(\n                        algorithm_config=self.cfg.algorithm_config,\n                    )  # type: ignore\n                else:\n                    raise NotImplementedError(\"Architecture not supported.\")\n\n            # attempt loading a pre-trained model\n            else:\n                self.model, self.cfg = load_pretrained(source)\n\n        # define the checkpoint saving callback\n        self._define_callbacks(callbacks)\n\n        # instantiate logger\n        csv_logger = CSVLogger(\n            name=self.cfg.experiment_name,\n            save_dir=self.work_dir / \"csv_logs\",\n        )\n\n        if self.cfg.training_config.has_logger():\n            if self.cfg.training_config.logger == SupportedLogger.WANDB:\n                experiment_logger: LOGGER_TYPES = [\n                    WandbLogger(\n                        name=self.cfg.experiment_name,\n                        save_dir=self.work_dir / Path(\"wandb_logs\"),\n                    ),\n                    csv_logger,\n                ]\n            elif self.cfg.training_config.logger == SupportedLogger.TENSORBOARD:\n                experiment_logger = [\n                    TensorBoardLogger(\n                        save_dir=self.work_dir / Path(\"tb_logs\"),\n                    ),\n                    csv_logger,\n                ]\n        else:\n            experiment_logger = [csv_logger]\n\n        # instantiate trainer\n        self.trainer = Trainer(\n            max_epochs=self.cfg.training_config.num_epochs,\n            precision=self.cfg.training_config.precision,\n            max_steps=self.cfg.training_config.max_steps,\n            check_val_every_n_epoch=self.cfg.training_config.check_val_every_n_epoch,\n            enable_progress_bar=self.cfg.training_config.enable_progress_bar,\n            accumulate_grad_batches=self.cfg.training_config.accumulate_grad_batches,\n            gradient_clip_val=self.cfg.training_config.gradient_clip_val,\n            gradient_clip_algorithm=self.cfg.training_config.gradient_clip_algorithm,\n            callbacks=self.callbacks,\n            default_root_dir=self.work_dir,\n            logger=experiment_logger,\n        )\n\n        # place holder for the datamodules\n        self.train_datamodule: Optional[TrainDataModule] = None\n        self.pred_datamodule: Optional[PredictDataModule] = None\n\n    def _define_callbacks(self, callbacks: Optional[list[Callback]] = None) -&gt; None:\n        \"\"\"Define the callbacks for the training loop.\n\n        Parameters\n        ----------\n        callbacks : list of Callback, optional\n            List of callbacks to use during training and prediction, by default None.\n        \"\"\"\n        self.callbacks = [] if callbacks is None else callbacks\n\n        # check that user callbacks are not any of the CAREamics callbacks\n        for c in self.callbacks:\n            if isinstance(c, ModelCheckpoint) or isinstance(c, EarlyStopping):\n                raise ValueError(\n                    \"ModelCheckpoint and EarlyStopping callbacks are already defined \"\n                    \"in CAREamics and should only be modified through the \"\n                    \"training configuration (see TrainingConfig).\"\n                )\n\n            if isinstance(c, HyperParametersCallback) or isinstance(\n                c, ProgressBarCallback\n            ):\n                raise ValueError(\n                    \"HyperParameter and ProgressBar callbacks are defined internally \"\n                    \"and should not be passed as callbacks.\"\n                )\n\n        # checkpoint callback saves checkpoints during training\n        self.callbacks.extend(\n            [\n                HyperParametersCallback(self.cfg),\n                ModelCheckpoint(\n                    dirpath=self.work_dir / Path(\"checkpoints\"),\n                    filename=self.cfg.experiment_name,\n                    **self.cfg.training_config.checkpoint_callback.model_dump(),\n                ),\n                ProgressBarCallback(),\n            ]\n        )\n\n        # early stopping callback\n        if self.cfg.training_config.early_stopping_callback is not None:\n            self.callbacks.append(\n                EarlyStopping(self.cfg.training_config.early_stopping_callback)\n            )\n\n    def stop_training(self) -&gt; None:\n        \"\"\"Stop the training loop.\"\"\"\n        # raise stop training flag\n        self.trainer.should_stop = True\n        self.trainer.limit_val_batches = 0  # skip  validation\n\n    # TODO: is there are more elegant way than calling train again after _train_on_paths\n    def train(\n        self,\n        *,\n        datamodule: Optional[TrainDataModule] = None,\n        train_source: Optional[Union[Path, str, NDArray]] = None,\n        val_source: Optional[Union[Path, str, NDArray]] = None,\n        train_target: Optional[Union[Path, str, NDArray]] = None,\n        val_target: Optional[Union[Path, str, NDArray]] = None,\n        use_in_memory: bool = True,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data.\n\n        If a datamodule is provided, then training will be performed using it.\n        Alternatively, the training data can be provided as arrays or paths.\n\n        If `use_in_memory` is set to True, the source provided as Path or str will be\n        loaded in memory if it fits. Otherwise, training will be performed by loading\n        patches from the files one by one. Training on arrays is always performed\n        in memory.\n\n        If no validation source is provided, then the validation is extracted from\n        the training data using `val_percentage` and `val_minimum_split`. In the case\n        of data provided as Path or str, the percentage and minimum number are applied\n        to the number of files. For arrays, it is the number of patches.\n\n        Parameters\n        ----------\n        datamodule : TrainDataModule, optional\n            Datamodule to train on, by default None.\n        train_source : pathlib.Path or str or NDArray, optional\n            Train source, if no datamodule is provided, by default None.\n        val_source : pathlib.Path or str or NDArray, optional\n            Validation source, if no datamodule is provided, by default None.\n        train_target : pathlib.Path or str or NDArray, optional\n            Train target source, if no datamodule is provided, by default None.\n        val_target : pathlib.Path or str or NDArray, optional\n            Validation target source, if no datamodule is provided, by default None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n        val_percentage : float, optional\n            Percentage of validation extracted from training data, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of validation (patch or file) extracted from training data,\n            by default 1.\n\n        Raises\n        ------\n        ValueError\n            If both `datamodule` and `train_source` are provided.\n        ValueError\n            If sources are not of the same type (e.g. train is an array and val is\n            a Path).\n        ValueError\n            If the training target is provided to N2V.\n        ValueError\n            If neither a datamodule nor a source is provided.\n        \"\"\"\n        if datamodule is not None and train_source is not None:\n            raise ValueError(\n                \"Only one of `datamodule` and `train_source` can be provided.\"\n            )\n\n        # check that inputs are the same type\n        source_types = {\n            type(s)\n            for s in (train_source, val_source, train_target, val_target)\n            if s is not None\n        }\n        if len(source_types) &gt; 1:\n            raise ValueError(\"All sources should be of the same type.\")\n\n        # train\n        if datamodule is not None:\n            self._train_on_datamodule(datamodule=datamodule)\n\n        else:\n            # raise error if target is provided to N2V\n            if self.cfg.algorithm_config.algorithm == SupportedAlgorithm.N2V.value:\n                if train_target is not None:\n                    raise ValueError(\n                        \"Training target not compatible with N2V training.\"\n                    )\n\n            # dispatch the training\n            if isinstance(train_source, np.ndarray):\n                # mypy checks\n                assert isinstance(val_source, np.ndarray) or val_source is None\n                assert isinstance(train_target, np.ndarray) or train_target is None\n                assert isinstance(val_target, np.ndarray) or val_target is None\n\n                self._train_on_array(\n                    train_source,\n                    val_source,\n                    train_target,\n                    val_target,\n                    val_percentage,\n                    val_minimum_split,\n                )\n\n            elif isinstance(train_source, Path) or isinstance(train_source, str):\n                # mypy checks\n                assert (\n                    isinstance(val_source, Path)\n                    or isinstance(val_source, str)\n                    or val_source is None\n                )\n                assert (\n                    isinstance(train_target, Path)\n                    or isinstance(train_target, str)\n                    or train_target is None\n                )\n                assert (\n                    isinstance(val_target, Path)\n                    or isinstance(val_target, str)\n                    or val_target is None\n                )\n\n                self._train_on_path(\n                    train_source,\n                    val_source,\n                    train_target,\n                    val_target,\n                    use_in_memory,\n                    val_percentage,\n                    val_minimum_split,\n                )\n\n            else:\n                raise ValueError(\n                    f\"Invalid input, expected a str, Path, array or TrainDataModule \"\n                    f\"instance (got {type(train_source)}).\"\n                )\n\n    def _train_on_datamodule(self, datamodule: TrainDataModule) -&gt; None:\n        \"\"\"\n        Train the model on the provided datamodule.\n\n        Parameters\n        ----------\n        datamodule : TrainDataModule\n            Datamodule to train on.\n        \"\"\"\n        # register datamodule\n        self.train_datamodule = datamodule\n\n        # set defaults (in case `stop_training` was called before)\n        self.trainer.should_stop = False\n        self.trainer.limit_val_batches = 1.0  # 100%\n\n        # train\n        self.trainer.fit(self.model, datamodule=datamodule)\n\n    def _train_on_array(\n        self,\n        train_data: NDArray,\n        val_data: Optional[NDArray] = None,\n        train_target: Optional[NDArray] = None,\n        val_target: Optional[NDArray] = None,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 5,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data arrays.\n\n        Parameters\n        ----------\n        train_data : NDArray\n            Training data.\n        val_data : NDArray, optional\n            Validation data, by default None.\n        train_target : NDArray, optional\n            Train target data, by default None.\n        val_target : NDArray, optional\n            Validation target data, by default None.\n        val_percentage : float, optional\n            Percentage of patches to use for validation, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of patches to use for validation, by default 5.\n        \"\"\"\n        # create datamodule\n        datamodule = TrainDataModule(\n            data_config=self.cfg.data_config,\n            train_data=train_data,\n            val_data=val_data,\n            train_data_target=train_target,\n            val_data_target=val_target,\n            val_percentage=val_percentage,\n            val_minimum_split=val_minimum_split,\n        )\n\n        # train\n        self.train(datamodule=datamodule)\n\n    def _train_on_path(\n        self,\n        path_to_train_data: Union[Path, str],\n        path_to_val_data: Optional[Union[Path, str]] = None,\n        path_to_train_target: Optional[Union[Path, str]] = None,\n        path_to_val_target: Optional[Union[Path, str]] = None,\n        use_in_memory: bool = True,\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Train the model on the provided data paths.\n\n        Parameters\n        ----------\n        path_to_train_data : pathlib.Path or str\n            Path to the training data.\n        path_to_val_data : pathlib.Path or str, optional\n            Path to validation data, by default None.\n        path_to_train_target : pathlib.Path or str, optional\n            Path to train target data, by default None.\n        path_to_val_target : pathlib.Path or str, optional\n            Path to validation target data, by default None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n        val_percentage : float, optional\n            Percentage of files to use for validation, by default 0.1.\n        val_minimum_split : int, optional\n            Minimum number of files to use for validation, by default 1.\n        \"\"\"\n        # sanity check on data (path exists)\n        path_to_train_data = check_path_exists(path_to_train_data)\n\n        if path_to_val_data is not None:\n            path_to_val_data = check_path_exists(path_to_val_data)\n\n        if path_to_train_target is not None:\n            path_to_train_target = check_path_exists(path_to_train_target)\n\n        if path_to_val_target is not None:\n            path_to_val_target = check_path_exists(path_to_val_target)\n\n        # create datamodule\n        datamodule = TrainDataModule(\n            data_config=self.cfg.data_config,\n            train_data=path_to_train_data,\n            val_data=path_to_val_data,\n            train_data_target=path_to_train_target,\n            val_data_target=path_to_val_target,\n            use_in_memory=use_in_memory,\n            val_percentage=val_percentage,\n            val_minimum_split=val_minimum_split,\n        )\n\n        # train\n        self.train(datamodule=datamodule)\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self, source: PredictDataModule\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self,\n        source: Union[Path, str],\n        *,\n        batch_size: int = 1,\n        tile_size: Optional[tuple[int, ...]] = None,\n        tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n        axes: Optional[str] = None,\n        data_type: Optional[Literal[\"tiff\", \"custom\"]] = None,\n        tta_transforms: bool = False,\n        dataloader_params: Optional[dict] = None,\n        read_source_func: Optional[Callable] = None,\n        extension_filter: str = \"\",\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    @overload\n    def predict(  # numpydoc ignore=GL08\n        self,\n        source: NDArray,\n        *,\n        batch_size: int = 1,\n        tile_size: Optional[tuple[int, ...]] = None,\n        tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n        axes: Optional[str] = None,\n        data_type: Optional[Literal[\"array\"]] = None,\n        tta_transforms: bool = False,\n        dataloader_params: Optional[dict] = None,\n    ) -&gt; Union[list[NDArray], NDArray]: ...\n\n    def predict(\n        self,\n        source: Union[PredictDataModule, Path, str, NDArray],\n        *,\n        batch_size: int = 1,\n        tile_size: Optional[tuple[int, ...]] = None,\n        tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n        axes: Optional[str] = None,\n        data_type: Optional[Literal[\"array\", \"tiff\", \"custom\"]] = None,\n        tta_transforms: bool = False,\n        dataloader_params: Optional[dict] = None,\n        read_source_func: Optional[Callable] = None,\n        extension_filter: str = \"\",\n        **kwargs: Any,\n    ) -&gt; Union[list[NDArray], NDArray]:\n        \"\"\"\n        Make predictions on the provided data.\n\n        Input can be a CAREamicsPredData instance, a path to a data file, or a numpy\n        array.\n\n        If `data_type`, `axes` and `tile_size` are not provided, the training\n        configuration parameters will be used, with the `patch_size` instead of\n        `tile_size`.\n\n        Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n        parameter. The TTA augmentation applies all possible flip and 90 degrees\n        rotations to the prediction input and averages the predictions. TTA augmentation\n        should not be used if you did not train with these augmentations.\n\n        Note that if you are using a UNet model and tiling, the tile size must be\n        divisible in every dimension by 2**d, where d is the depth of the model. This\n        avoids artefacts arising from the broken shift invariance induced by the\n        pooling layers of the UNet. If your image has less dimensions, as it may\n        happen in the Z dimension, consider padding your image.\n\n        Parameters\n        ----------\n        source : PredictDataModule, pathlib.Path, str or numpy.ndarray\n            Data to predict on.\n        batch_size : int, default=1\n            Batch size for prediction.\n        tile_size : tuple of int, optional\n            Size of the tiles to use for prediction.\n        tile_overlap : tuple of int, default=(48, 48)\n            Overlap between tiles, can be None.\n        axes : str, optional\n            Axes of the input data, by default None.\n        data_type : {\"array\", \"tiff\", \"custom\"}, optional\n            Type of the input data.\n        tta_transforms : bool, default=True\n            Whether to apply test-time augmentation.\n        dataloader_params : dict, optional\n            Parameters to pass to the dataloader.\n        read_source_func : Callable, optional\n            Function to read the source data.\n        extension_filter : str, default=\"\"\n            Filter for the file extension.\n        **kwargs : Any\n            Unused.\n\n        Returns\n        -------\n        list of NDArray or NDArray\n            Predictions made by the model.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the configuration.\n        ValueError\n            If tile size is not divisible by 2**depth for UNet models.\n        ValueError\n            If tile overlap is not specified.\n        \"\"\"\n        if (\n            self.cfg.data_config.image_means is None\n            or self.cfg.data_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided in the configuration.\")\n\n        # tile size for UNets\n        if tile_size is not None:\n            model = self.cfg.algorithm_config.model\n\n            if model.architecture == SupportedArchitecture.UNET.value:\n                # tile size must be equal to k*2^n, where n is the number of pooling\n                # layers (equal to the depth) and k is an integer\n                depth = model.depth\n                tile_increment = 2**depth\n\n                for i, t in enumerate(tile_size):\n                    if t % tile_increment != 0:\n                        raise ValueError(\n                            f\"Tile size must be divisible by {tile_increment} along \"\n                            f\"all axes (got {t} for axis {i}). If your image size is \"\n                            f\"smaller along one axis (e.g. Z), consider padding the \"\n                            f\"image.\"\n                        )\n\n            # tile overlaps must be specified\n            if tile_overlap is None:\n                raise ValueError(\"Tile overlap must be specified.\")\n\n        # create the prediction\n        self.pred_datamodule = create_predict_datamodule(\n            pred_data=source,\n            data_type=data_type or self.cfg.data_config.data_type,\n            axes=axes or self.cfg.data_config.axes,\n            image_means=self.cfg.data_config.image_means,\n            image_stds=self.cfg.data_config.image_stds,\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            batch_size=batch_size or self.cfg.data_config.batch_size,\n            tta_transforms=tta_transforms,\n            read_source_func=read_source_func,\n            extension_filter=extension_filter,\n            dataloader_params=dataloader_params,\n        )\n\n        # predict\n        predictions = self.trainer.predict(\n            model=self.model, datamodule=self.pred_datamodule\n        )\n        return convert_outputs(predictions, self.pred_datamodule.tiled)\n\n    def predict_to_disk(\n        self,\n        source: Union[PredictDataModule, Path, str],\n        *,\n        batch_size: int = 1,\n        tile_size: Optional[tuple[int, ...]] = None,\n        tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n        axes: Optional[str] = None,\n        data_type: Optional[Literal[\"tiff\", \"custom\"]] = None,\n        tta_transforms: bool = False,\n        dataloader_params: Optional[dict] = None,\n        read_source_func: Optional[Callable] = None,\n        extension_filter: str = \"\",\n        write_type: Literal[\"tiff\", \"custom\"] = \"tiff\",\n        write_extension: Optional[str] = None,\n        write_func: Optional[WriteFunc] = None,\n        write_func_kwargs: Optional[dict[str, Any]] = None,\n        prediction_dir: Union[Path, str] = \"predictions\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Make predictions on the provided data and save outputs to files.\n\n        The predictions will be saved in a new directory 'predictions' within the set\n        working directory. The directory stucture within the 'predictions' directory\n        will match that of the source directory.\n\n        The `source` must be from files and not arrays. The file names of the\n        predictions will match those of the source. If there is more than one sample\n        within a file, the samples will be saved to seperate files. The file names of\n        samples will have the name of the corresponding source file but with the sample\n        index appended. E.g. If the the source file name is 'images.tiff' then the first\n        sample's prediction will be saved with the file name \"image_0.tiff\".\n        Input can be a PredictDataModule instance, a path to a data file, or a numpy\n        array.\n\n        If `data_type`, `axes` and `tile_size` are not provided, the training\n        configuration parameters will be used, with the `patch_size` instead of\n        `tile_size`.\n\n        Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n        parameter. The TTA augmentation applies all possible flip and 90 degrees\n        rotations to the prediction input and averages the predictions. TTA augmentation\n        should not be used if you did not train with these augmentations.\n\n        Note that if you are using a UNet model and tiling, the tile size must be\n        divisible in every dimension by 2**d, where d is the depth of the model. This\n        avoids artefacts arising from the broken shift invariance induced by the\n        pooling layers of the UNet. If your image has less dimensions, as it may\n        happen in the Z dimension, consider padding your image.\n\n        Parameters\n        ----------\n        source : PredictDataModule or pathlib.Path, str\n            Data to predict on.\n        batch_size : int, default=1\n            Batch size for prediction.\n        tile_size : tuple of int, optional\n            Size of the tiles to use for prediction.\n        tile_overlap : tuple of int, default=(48, 48)\n            Overlap between tiles.\n        axes : str, optional\n            Axes of the input data, by default None.\n        data_type : {\"array\", \"tiff\", \"custom\"}, optional\n            Type of the input data.\n        tta_transforms : bool, default=True\n            Whether to apply test-time augmentation.\n        dataloader_params : dict, optional\n            Parameters to pass to the dataloader.\n        read_source_func : Callable, optional\n            Function to read the source data.\n        extension_filter : str, default=\"\"\n            Filter for the file extension.\n        write_type : {\"tiff\", \"custom\"}, default=\"tiff\"\n            The data type to save as, includes custom.\n        write_extension : str, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` an extension to save the data with must be passed.\n        write_func : WriteFunc, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` a function to save the data must be passed. See notes below.\n        write_func_kwargs : dict of {str: any}, optional\n            Additional keyword arguments to be passed to the save function.\n        prediction_dir : Path | str, default=\"predictions\"\n            The path to save the prediction results to. If `prediction_dir` is not\n            absolute, the directory will be assumed to be relative to the pre-set\n            `work_dir`. If the directory does not exist it will be created.\n        **kwargs : Any\n            Unused.\n\n        Raises\n        ------\n        ValueError\n            If `write_type` is custom and `write_extension` is None.\n        ValueError\n            If `write_type` is custom and `write_fun is None.\n        ValueError\n            If `source` is not `str`, `Path` or `PredictDataModule`\n        \"\"\"\n        if write_func_kwargs is None:\n            write_func_kwargs = {}\n\n        if Path(prediction_dir).is_absolute():\n            write_dir = Path(prediction_dir)\n        else:\n            write_dir = self.work_dir / prediction_dir\n        write_dir.mkdir(exist_ok=True, parents=True)\n\n        # guards for custom types\n        if write_type == SupportedData.CUSTOM:\n            if write_extension is None:\n                raise ValueError(\n                    \"A `write_extension` must be provided for custom write types.\"\n                )\n            if write_func is None:\n                raise ValueError(\n                    \"A `write_func` must be provided for custom write types.\"\n                )\n        else:\n            write_func = get_write_func(write_type)\n            write_extension = SupportedData.get_extension(write_type)\n\n        # extract file names\n        source_path: Union[Path, str, NDArray]\n        source_data_type: Literal[\"array\", \"tiff\", \"custom\"]\n        if isinstance(source, PredictDataModule):\n            source_path = source.pred_data\n            source_data_type = source.data_type\n            extension_filter = source.extension_filter\n        elif isinstance(source, (str, Path)):\n            source_path = source\n            source_data_type = data_type or self.cfg.data_config.data_type\n            extension_filter = SupportedData.get_extension_pattern(\n                SupportedData(source_data_type)\n            )\n        else:\n            raise ValueError(f\"Unsupported source type: '{type(source)}'.\")\n\n        if source_data_type == \"array\":\n            raise ValueError(\n                \"Predicting to disk is not supported for input type 'array'.\"\n            )\n        assert isinstance(source_path, (Path, str))  # because data_type != \"array\"\n        source_path = Path(source_path)\n\n        file_paths = list_files(source_path, source_data_type, extension_filter)\n\n        # predict and write each file in turn\n        for file_path in file_paths:\n            # source_path is relative to original source path...\n            # should mirror original directory structure\n            prediction = self.predict(\n                source=file_path,\n                batch_size=batch_size,\n                tile_size=tile_size,\n                tile_overlap=tile_overlap,\n                axes=axes,\n                data_type=data_type,\n                tta_transforms=tta_transforms,\n                dataloader_params=dataloader_params,\n                read_source_func=read_source_func,\n                extension_filter=extension_filter,\n                **kwargs,\n            )\n            # TODO: cast to float16?\n            write_data = np.concatenate(prediction)\n\n            # create directory structure and write path\n            if not source_path.is_file():\n                file_write_dir = write_dir / file_path.parent.relative_to(source_path)\n            else:\n                file_write_dir = write_dir\n            file_write_dir.mkdir(parents=True, exist_ok=True)\n            write_path = (file_write_dir / file_path.name).with_suffix(write_extension)\n\n            # write data\n            write_func(file_path=write_path, img=write_data)\n\n    def export_to_bmz(\n        self,\n        path_to_archive: Union[Path, str],\n        friendly_model_name: str,\n        input_array: NDArray,\n        authors: list[dict],\n        general_description: str,\n        data_description: str,\n        covers: Optional[list[Union[Path, str]]] = None,\n        channel_names: Optional[list[str]] = None,\n        model_version: str = \"0.1.0\",\n    ) -&gt; None:\n        \"\"\"Export the model to the BioImage Model Zoo format.\n\n        This method packages the current weights into a zip file that can be uploaded\n        to the BioImage Model Zoo. The archive consists of the model weights, the model\n        specifications and various files (inputs, outputs, README, env.yaml etc.).\n\n        `path_to_archive` should point to a file with a \".zip\" extension.\n\n        `friendly_model_name` is the name used for the model in the BMZ specs\n        and website, it should consist of letters, numbers, dashes, underscores and\n        parentheses only.\n\n        Input array must be of the same dimensions as the axes recorded in the\n        configuration of the `CAREamist`.\n\n        Parameters\n        ----------\n        path_to_archive : pathlib.Path or str\n            Path in which to save the model, including file name, which should end with\n            \".zip\".\n        friendly_model_name : str\n            Name of the model as used in the BMZ specs, it should consist of letters,\n            numbers, dashes, underscores and parentheses only.\n        input_array : NDArray\n            Input array used to validate the model and as example.\n        authors : list of dict\n            List of authors of the model.\n        general_description : str\n            General description of the model used in the BMZ metadata.\n        data_description : str\n            Description of the data the model was trained on.\n        covers : list of pathlib.Path or str, default=None\n            Paths to the cover images.\n        channel_names : list of str, default=None\n            Channel names.\n        model_version : str, default=\"0.1.0\"\n            Version of the model.\n        \"\"\"\n        # TODO: add in docs that it is expected that input_array dimensions match\n        # those in data_config\n\n        output_patch = self.predict(\n            input_array,\n            data_type=SupportedData.ARRAY.value,\n            tta_transforms=False,\n        )\n        output = np.concatenate(output_patch, axis=0)\n        input_array = reshape_array(input_array, self.cfg.data_config.axes)\n\n        export_to_bmz(\n            model=self.model,\n            config=self.cfg,\n            path_to_archive=path_to_archive,\n            model_name=friendly_model_name,\n            general_description=general_description,\n            data_description=data_description,\n            authors=authors,\n            input_array=input_array,\n            output_array=output,\n            covers=covers,\n            channel_names=channel_names,\n            model_version=model_version,\n        )\n\n    def get_losses(self) -&gt; dict[str, list]:\n        \"\"\"Return data that can be used to plot train and validation loss curves.\n\n        Returns\n        -------\n        dict of str: list\n            Dictionary containing the losses for each epoch.\n        \"\"\"\n        return read_csv_logger(self.cfg.experiment_name, self.work_dir / \"csv_logs\")\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.__init__","title":"<code>__init__(source, work_dir=None, callbacks=None)</code>","text":"<pre><code>__init__(source: Union[Path, str], work_dir: Optional[Union[Path, str]] = None, callbacks: Optional[list[Callback]] = None) -&gt; None\n</code></pre><pre><code>__init__(source: Configuration, work_dir: Optional[Union[Path, str]] = None, callbacks: Optional[list[Callback]] = None) -&gt; None\n</code></pre> <p>Initialize CAREamist with a configuration object or a path.</p> <p>A configuration object can be created using directly by calling <code>Configuration</code>, using the configuration factory or loading a configuration from a yaml file.</p> <p>Path can contain either a yaml file with parameters, or a saved checkpoint.</p> <p>If no working directory is provided, the current working directory is used.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>pathlib.Path or str or CAREamics Configuration</code> <p>Path to a configuration file or a trained model.</p> required <code>work_dir</code> <code>str or Path</code> <p>Path to working directory in which to save checkpoints and logs, by default None.</p> <code>None</code> <code>callbacks</code> <code>list of Callback</code> <p>List of callbacks to use during training and prediction, by default None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the model is loaded from BioImage Model Zoo.</p> <code>ValueError</code> <p>If no hyper parameters are found in the checkpoint.</p> <code>ValueError</code> <p>If no data module hyper parameters are found in the checkpoint.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def __init__(\n    self,\n    source: Union[Path, str, Configuration],\n    work_dir: Optional[Union[Path, str]] = None,\n    callbacks: Optional[list[Callback]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize CAREamist with a configuration object or a path.\n\n    A configuration object can be created using directly by calling `Configuration`,\n    using the configuration factory or loading a configuration from a yaml file.\n\n    Path can contain either a yaml file with parameters, or a saved checkpoint.\n\n    If no working directory is provided, the current working directory is used.\n\n    Parameters\n    ----------\n    source : pathlib.Path or str or CAREamics Configuration\n        Path to a configuration file or a trained model.\n    work_dir : str or pathlib.Path, optional\n        Path to working directory in which to save checkpoints and logs,\n        by default None.\n    callbacks : list of Callback, optional\n        List of callbacks to use during training and prediction, by default None.\n\n    Raises\n    ------\n    NotImplementedError\n        If the model is loaded from BioImage Model Zoo.\n    ValueError\n        If no hyper parameters are found in the checkpoint.\n    ValueError\n        If no data module hyper parameters are found in the checkpoint.\n    \"\"\"\n    # select current working directory if work_dir is None\n    if work_dir is None:\n        self.work_dir = Path.cwd()\n        logger.warning(\n            f\"No working directory provided. Using current working directory: \"\n            f\"{self.work_dir}.\"\n        )\n    else:\n        self.work_dir = Path(work_dir)\n\n    # configuration object\n    if isinstance(source, Configuration):\n        self.cfg = source\n\n        # instantiate model\n        if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n            self.model = FCNModule(\n                algorithm_config=self.cfg.algorithm_config,\n            )\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n\n    # path to configuration file or model\n    else:\n        # TODO: update this check so models can be downloaded directly from BMZ\n        source = check_path_exists(source)\n\n        # configuration file\n        if source.is_file() and (\n            source.suffix == \".yaml\" or source.suffix == \".yml\"\n        ):\n            # load configuration\n            self.cfg = load_configuration(source)\n\n            # instantiate model\n            if isinstance(self.cfg.algorithm_config, UNetBasedAlgorithm):\n                self.model = FCNModule(\n                    algorithm_config=self.cfg.algorithm_config,\n                )  # type: ignore\n            else:\n                raise NotImplementedError(\"Architecture not supported.\")\n\n        # attempt loading a pre-trained model\n        else:\n            self.model, self.cfg = load_pretrained(source)\n\n    # define the checkpoint saving callback\n    self._define_callbacks(callbacks)\n\n    # instantiate logger\n    csv_logger = CSVLogger(\n        name=self.cfg.experiment_name,\n        save_dir=self.work_dir / \"csv_logs\",\n    )\n\n    if self.cfg.training_config.has_logger():\n        if self.cfg.training_config.logger == SupportedLogger.WANDB:\n            experiment_logger: LOGGER_TYPES = [\n                WandbLogger(\n                    name=self.cfg.experiment_name,\n                    save_dir=self.work_dir / Path(\"wandb_logs\"),\n                ),\n                csv_logger,\n            ]\n        elif self.cfg.training_config.logger == SupportedLogger.TENSORBOARD:\n            experiment_logger = [\n                TensorBoardLogger(\n                    save_dir=self.work_dir / Path(\"tb_logs\"),\n                ),\n                csv_logger,\n            ]\n    else:\n        experiment_logger = [csv_logger]\n\n    # instantiate trainer\n    self.trainer = Trainer(\n        max_epochs=self.cfg.training_config.num_epochs,\n        precision=self.cfg.training_config.precision,\n        max_steps=self.cfg.training_config.max_steps,\n        check_val_every_n_epoch=self.cfg.training_config.check_val_every_n_epoch,\n        enable_progress_bar=self.cfg.training_config.enable_progress_bar,\n        accumulate_grad_batches=self.cfg.training_config.accumulate_grad_batches,\n        gradient_clip_val=self.cfg.training_config.gradient_clip_val,\n        gradient_clip_algorithm=self.cfg.training_config.gradient_clip_algorithm,\n        callbacks=self.callbacks,\n        default_root_dir=self.work_dir,\n        logger=experiment_logger,\n    )\n\n    # place holder for the datamodules\n    self.train_datamodule: Optional[TrainDataModule] = None\n    self.pred_datamodule: Optional[PredictDataModule] = None\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist._define_callbacks","title":"<code>_define_callbacks(callbacks=None)</code>","text":"<p>Define the callbacks for the training loop.</p> <p>Parameters:</p> Name Type Description Default <code>callbacks</code> <code>list of Callback</code> <p>List of callbacks to use during training and prediction, by default None.</p> <code>None</code> Source code in <code>src/careamics/careamist.py</code> <pre><code>def _define_callbacks(self, callbacks: Optional[list[Callback]] = None) -&gt; None:\n    \"\"\"Define the callbacks for the training loop.\n\n    Parameters\n    ----------\n    callbacks : list of Callback, optional\n        List of callbacks to use during training and prediction, by default None.\n    \"\"\"\n    self.callbacks = [] if callbacks is None else callbacks\n\n    # check that user callbacks are not any of the CAREamics callbacks\n    for c in self.callbacks:\n        if isinstance(c, ModelCheckpoint) or isinstance(c, EarlyStopping):\n            raise ValueError(\n                \"ModelCheckpoint and EarlyStopping callbacks are already defined \"\n                \"in CAREamics and should only be modified through the \"\n                \"training configuration (see TrainingConfig).\"\n            )\n\n        if isinstance(c, HyperParametersCallback) or isinstance(\n            c, ProgressBarCallback\n        ):\n            raise ValueError(\n                \"HyperParameter and ProgressBar callbacks are defined internally \"\n                \"and should not be passed as callbacks.\"\n            )\n\n    # checkpoint callback saves checkpoints during training\n    self.callbacks.extend(\n        [\n            HyperParametersCallback(self.cfg),\n            ModelCheckpoint(\n                dirpath=self.work_dir / Path(\"checkpoints\"),\n                filename=self.cfg.experiment_name,\n                **self.cfg.training_config.checkpoint_callback.model_dump(),\n            ),\n            ProgressBarCallback(),\n        ]\n    )\n\n    # early stopping callback\n    if self.cfg.training_config.early_stopping_callback is not None:\n        self.callbacks.append(\n            EarlyStopping(self.cfg.training_config.early_stopping_callback)\n        )\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist._train_on_array","title":"<code>_train_on_array(train_data, val_data=None, train_target=None, val_target=None, val_percentage=0.1, val_minimum_split=5)</code>","text":"<p>Train the model on the provided data arrays.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>NDArray</code> <p>Training data.</p> required <code>val_data</code> <code>NDArray</code> <p>Validation data, by default None.</p> <code>None</code> <code>train_target</code> <code>NDArray</code> <p>Train target data, by default None.</p> <code>None</code> <code>val_target</code> <code>NDArray</code> <p>Validation target data, by default None.</p> <code>None</code> <code>val_percentage</code> <code>float</code> <p>Percentage of patches to use for validation, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches to use for validation, by default 5.</p> <code>5</code> Source code in <code>src/careamics/careamist.py</code> <pre><code>def _train_on_array(\n    self,\n    train_data: NDArray,\n    val_data: Optional[NDArray] = None,\n    train_target: Optional[NDArray] = None,\n    val_target: Optional[NDArray] = None,\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 5,\n) -&gt; None:\n    \"\"\"\n    Train the model on the provided data arrays.\n\n    Parameters\n    ----------\n    train_data : NDArray\n        Training data.\n    val_data : NDArray, optional\n        Validation data, by default None.\n    train_target : NDArray, optional\n        Train target data, by default None.\n    val_target : NDArray, optional\n        Validation target data, by default None.\n    val_percentage : float, optional\n        Percentage of patches to use for validation, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of patches to use for validation, by default 5.\n    \"\"\"\n    # create datamodule\n    datamodule = TrainDataModule(\n        data_config=self.cfg.data_config,\n        train_data=train_data,\n        val_data=val_data,\n        train_data_target=train_target,\n        val_data_target=val_target,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_split,\n    )\n\n    # train\n    self.train(datamodule=datamodule)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist._train_on_datamodule","title":"<code>_train_on_datamodule(datamodule)</code>","text":"<p>Train the model on the provided datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <code>TrainDataModule</code> <p>Datamodule to train on.</p> required Source code in <code>src/careamics/careamist.py</code> <pre><code>def _train_on_datamodule(self, datamodule: TrainDataModule) -&gt; None:\n    \"\"\"\n    Train the model on the provided datamodule.\n\n    Parameters\n    ----------\n    datamodule : TrainDataModule\n        Datamodule to train on.\n    \"\"\"\n    # register datamodule\n    self.train_datamodule = datamodule\n\n    # set defaults (in case `stop_training` was called before)\n    self.trainer.should_stop = False\n    self.trainer.limit_val_batches = 1.0  # 100%\n\n    # train\n    self.trainer.fit(self.model, datamodule=datamodule)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist._train_on_path","title":"<code>_train_on_path(path_to_train_data, path_to_val_data=None, path_to_train_target=None, path_to_val_target=None, use_in_memory=True, val_percentage=0.1, val_minimum_split=1)</code>","text":"<p>Train the model on the provided data paths.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_train_data</code> <code>Path or str</code> <p>Path to the training data.</p> required <code>path_to_val_data</code> <code>Path or str</code> <p>Path to validation data, by default None.</p> <code>None</code> <code>path_to_train_target</code> <code>Path or str</code> <p>Path to train target data, by default None.</p> <code>None</code> <code>path_to_val_target</code> <code>Path or str</code> <p>Path to validation target data, by default None.</p> <code>None</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <code>val_percentage</code> <code>float</code> <p>Percentage of files to use for validation, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of files to use for validation, by default 1.</p> <code>1</code> Source code in <code>src/careamics/careamist.py</code> <pre><code>def _train_on_path(\n    self,\n    path_to_train_data: Union[Path, str],\n    path_to_val_data: Optional[Union[Path, str]] = None,\n    path_to_train_target: Optional[Union[Path, str]] = None,\n    path_to_val_target: Optional[Union[Path, str]] = None,\n    use_in_memory: bool = True,\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 1,\n) -&gt; None:\n    \"\"\"\n    Train the model on the provided data paths.\n\n    Parameters\n    ----------\n    path_to_train_data : pathlib.Path or str\n        Path to the training data.\n    path_to_val_data : pathlib.Path or str, optional\n        Path to validation data, by default None.\n    path_to_train_target : pathlib.Path or str, optional\n        Path to train target data, by default None.\n    path_to_val_target : pathlib.Path or str, optional\n        Path to validation target data, by default None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n    val_percentage : float, optional\n        Percentage of files to use for validation, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of files to use for validation, by default 1.\n    \"\"\"\n    # sanity check on data (path exists)\n    path_to_train_data = check_path_exists(path_to_train_data)\n\n    if path_to_val_data is not None:\n        path_to_val_data = check_path_exists(path_to_val_data)\n\n    if path_to_train_target is not None:\n        path_to_train_target = check_path_exists(path_to_train_target)\n\n    if path_to_val_target is not None:\n        path_to_val_target = check_path_exists(path_to_val_target)\n\n    # create datamodule\n    datamodule = TrainDataModule(\n        data_config=self.cfg.data_config,\n        train_data=path_to_train_data,\n        val_data=path_to_val_data,\n        train_data_target=path_to_train_target,\n        val_data_target=path_to_val_target,\n        use_in_memory=use_in_memory,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_split,\n    )\n\n    # train\n    self.train(datamodule=datamodule)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.export_to_bmz","title":"<code>export_to_bmz(path_to_archive, friendly_model_name, input_array, authors, general_description, data_description, covers=None, channel_names=None, model_version='0.1.0')</code>","text":"<p>Export the model to the BioImage Model Zoo format.</p> <p>This method packages the current weights into a zip file that can be uploaded to the BioImage Model Zoo. The archive consists of the model weights, the model specifications and various files (inputs, outputs, README, env.yaml etc.).</p> <p><code>path_to_archive</code> should point to a file with a \".zip\" extension.</p> <p><code>friendly_model_name</code> is the name used for the model in the BMZ specs and website, it should consist of letters, numbers, dashes, underscores and parentheses only.</p> <p>Input array must be of the same dimensions as the axes recorded in the configuration of the <code>CAREamist</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_archive</code> <code>Path or str</code> <p>Path in which to save the model, including file name, which should end with \".zip\".</p> required <code>friendly_model_name</code> <code>str</code> <p>Name of the model as used in the BMZ specs, it should consist of letters, numbers, dashes, underscores and parentheses only.</p> required <code>input_array</code> <code>NDArray</code> <p>Input array used to validate the model and as example.</p> required <code>authors</code> <code>list of dict</code> <p>List of authors of the model.</p> required <code>general_description</code> <code>str</code> <p>General description of the model used in the BMZ metadata.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to the cover images.</p> <code>None</code> <code>channel_names</code> <code>list of str</code> <p>Channel names.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Version of the model.</p> <code>\"0.1.0\"</code> Source code in <code>src/careamics/careamist.py</code> <pre><code>def export_to_bmz(\n    self,\n    path_to_archive: Union[Path, str],\n    friendly_model_name: str,\n    input_array: NDArray,\n    authors: list[dict],\n    general_description: str,\n    data_description: str,\n    covers: Optional[list[Union[Path, str]]] = None,\n    channel_names: Optional[list[str]] = None,\n    model_version: str = \"0.1.0\",\n) -&gt; None:\n    \"\"\"Export the model to the BioImage Model Zoo format.\n\n    This method packages the current weights into a zip file that can be uploaded\n    to the BioImage Model Zoo. The archive consists of the model weights, the model\n    specifications and various files (inputs, outputs, README, env.yaml etc.).\n\n    `path_to_archive` should point to a file with a \".zip\" extension.\n\n    `friendly_model_name` is the name used for the model in the BMZ specs\n    and website, it should consist of letters, numbers, dashes, underscores and\n    parentheses only.\n\n    Input array must be of the same dimensions as the axes recorded in the\n    configuration of the `CAREamist`.\n\n    Parameters\n    ----------\n    path_to_archive : pathlib.Path or str\n        Path in which to save the model, including file name, which should end with\n        \".zip\".\n    friendly_model_name : str\n        Name of the model as used in the BMZ specs, it should consist of letters,\n        numbers, dashes, underscores and parentheses only.\n    input_array : NDArray\n        Input array used to validate the model and as example.\n    authors : list of dict\n        List of authors of the model.\n    general_description : str\n        General description of the model used in the BMZ metadata.\n    data_description : str\n        Description of the data the model was trained on.\n    covers : list of pathlib.Path or str, default=None\n        Paths to the cover images.\n    channel_names : list of str, default=None\n        Channel names.\n    model_version : str, default=\"0.1.0\"\n        Version of the model.\n    \"\"\"\n    # TODO: add in docs that it is expected that input_array dimensions match\n    # those in data_config\n\n    output_patch = self.predict(\n        input_array,\n        data_type=SupportedData.ARRAY.value,\n        tta_transforms=False,\n    )\n    output = np.concatenate(output_patch, axis=0)\n    input_array = reshape_array(input_array, self.cfg.data_config.axes)\n\n    export_to_bmz(\n        model=self.model,\n        config=self.cfg,\n        path_to_archive=path_to_archive,\n        model_name=friendly_model_name,\n        general_description=general_description,\n        data_description=data_description,\n        authors=authors,\n        input_array=input_array,\n        output_array=output,\n        covers=covers,\n        channel_names=channel_names,\n        model_version=model_version,\n    )\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.get_losses","title":"<code>get_losses()</code>","text":"<p>Return data that can be used to plot train and validation loss curves.</p> <p>Returns:</p> Type Description <code>dict of str: list</code> <p>Dictionary containing the losses for each epoch.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def get_losses(self) -&gt; dict[str, list]:\n    \"\"\"Return data that can be used to plot train and validation loss curves.\n\n    Returns\n    -------\n    dict of str: list\n        Dictionary containing the losses for each epoch.\n    \"\"\"\n    return read_csv_logger(self.cfg.experiment_name, self.work_dir / \"csv_logs\")\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.predict","title":"<code>predict(source, *, batch_size=1, tile_size=None, tile_overlap=(48, 48), axes=None, data_type=None, tta_transforms=False, dataloader_params=None, read_source_func=None, extension_filter='', **kwargs)</code>","text":"<pre><code>predict(source: PredictDataModule) -&gt; Union[list[NDArray], NDArray]\n</code></pre><pre><code>predict(source: Union[Path, str], *, batch_size: int = 1, tile_size: Optional[tuple[int, ...]] = None, tile_overlap: Optional[tuple[int, ...]] = (48, 48), axes: Optional[str] = None, data_type: Optional[Literal['tiff', 'custom']] = None, tta_transforms: bool = False, dataloader_params: Optional[dict] = None, read_source_func: Optional[Callable] = None, extension_filter: str = '') -&gt; Union[list[NDArray], NDArray]\n</code></pre><pre><code>predict(source: NDArray, *, batch_size: int = 1, tile_size: Optional[tuple[int, ...]] = None, tile_overlap: Optional[tuple[int, ...]] = (48, 48), axes: Optional[str] = None, data_type: Optional[Literal['array']] = None, tta_transforms: bool = False, dataloader_params: Optional[dict] = None) -&gt; Union[list[NDArray], NDArray]\n</code></pre> <p>Make predictions on the provided data.</p> <p>Input can be a CAREamicsPredData instance, a path to a data file, or a numpy array.</p> <p>If <code>data_type</code>, <code>axes</code> and <code>tile_size</code> are not provided, the training configuration parameters will be used, with the <code>patch_size</code> instead of <code>tile_size</code>.</p> <p>Test-time augmentation (TTA) can be switched on using the <code>tta_transforms</code> parameter. The TTA augmentation applies all possible flip and 90 degrees rotations to the prediction input and averages the predictions. TTA augmentation should not be used if you did not train with these augmentations.</p> <p>Note that if you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>(PredictDataModule, Path, str or ndarray)</code> <p>Data to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>1</code> <code>tile_size</code> <code>tuple of int</code> <p>Size of the tiles to use for prediction.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Overlap between tiles, can be None.</p> <code>(48, 48)</code> <code>axes</code> <code>str</code> <p>Axes of the input data, by default None.</p> <code>None</code> <code>data_type</code> <code>(array, tiff, custom)</code> <p>Type of the input data.</p> <code>\"array\"</code> <code>tta_transforms</code> <code>bool</code> <p>Whether to apply test-time augmentation.</p> <code>True</code> <code>dataloader_params</code> <code>dict</code> <p>Parameters to pass to the dataloader.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for the file extension.</p> <code>\"\"</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list of NDArray or NDArray</code> <p>Predictions made by the model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the configuration.</p> <code>ValueError</code> <p>If tile size is not divisible by 2**depth for UNet models.</p> <code>ValueError</code> <p>If tile overlap is not specified.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def predict(\n    self,\n    source: Union[PredictDataModule, Path, str, NDArray],\n    *,\n    batch_size: int = 1,\n    tile_size: Optional[tuple[int, ...]] = None,\n    tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n    axes: Optional[str] = None,\n    data_type: Optional[Literal[\"array\", \"tiff\", \"custom\"]] = None,\n    tta_transforms: bool = False,\n    dataloader_params: Optional[dict] = None,\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    **kwargs: Any,\n) -&gt; Union[list[NDArray], NDArray]:\n    \"\"\"\n    Make predictions on the provided data.\n\n    Input can be a CAREamicsPredData instance, a path to a data file, or a numpy\n    array.\n\n    If `data_type`, `axes` and `tile_size` are not provided, the training\n    configuration parameters will be used, with the `patch_size` instead of\n    `tile_size`.\n\n    Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n    parameter. The TTA augmentation applies all possible flip and 90 degrees\n    rotations to the prediction input and averages the predictions. TTA augmentation\n    should not be used if you did not train with these augmentations.\n\n    Note that if you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n\n    Parameters\n    ----------\n    source : PredictDataModule, pathlib.Path, str or numpy.ndarray\n        Data to predict on.\n    batch_size : int, default=1\n        Batch size for prediction.\n    tile_size : tuple of int, optional\n        Size of the tiles to use for prediction.\n    tile_overlap : tuple of int, default=(48, 48)\n        Overlap between tiles, can be None.\n    axes : str, optional\n        Axes of the input data, by default None.\n    data_type : {\"array\", \"tiff\", \"custom\"}, optional\n        Type of the input data.\n    tta_transforms : bool, default=True\n        Whether to apply test-time augmentation.\n    dataloader_params : dict, optional\n        Parameters to pass to the dataloader.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, default=\"\"\n        Filter for the file extension.\n    **kwargs : Any\n        Unused.\n\n    Returns\n    -------\n    list of NDArray or NDArray\n        Predictions made by the model.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the configuration.\n    ValueError\n        If tile size is not divisible by 2**depth for UNet models.\n    ValueError\n        If tile overlap is not specified.\n    \"\"\"\n    if (\n        self.cfg.data_config.image_means is None\n        or self.cfg.data_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided in the configuration.\")\n\n    # tile size for UNets\n    if tile_size is not None:\n        model = self.cfg.algorithm_config.model\n\n        if model.architecture == SupportedArchitecture.UNET.value:\n            # tile size must be equal to k*2^n, where n is the number of pooling\n            # layers (equal to the depth) and k is an integer\n            depth = model.depth\n            tile_increment = 2**depth\n\n            for i, t in enumerate(tile_size):\n                if t % tile_increment != 0:\n                    raise ValueError(\n                        f\"Tile size must be divisible by {tile_increment} along \"\n                        f\"all axes (got {t} for axis {i}). If your image size is \"\n                        f\"smaller along one axis (e.g. Z), consider padding the \"\n                        f\"image.\"\n                    )\n\n        # tile overlaps must be specified\n        if tile_overlap is None:\n            raise ValueError(\"Tile overlap must be specified.\")\n\n    # create the prediction\n    self.pred_datamodule = create_predict_datamodule(\n        pred_data=source,\n        data_type=data_type or self.cfg.data_config.data_type,\n        axes=axes or self.cfg.data_config.axes,\n        image_means=self.cfg.data_config.image_means,\n        image_stds=self.cfg.data_config.image_stds,\n        tile_size=tile_size,\n        tile_overlap=tile_overlap,\n        batch_size=batch_size or self.cfg.data_config.batch_size,\n        tta_transforms=tta_transforms,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        dataloader_params=dataloader_params,\n    )\n\n    # predict\n    predictions = self.trainer.predict(\n        model=self.model, datamodule=self.pred_datamodule\n    )\n    return convert_outputs(predictions, self.pred_datamodule.tiled)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.predict_to_disk","title":"<code>predict_to_disk(source, *, batch_size=1, tile_size=None, tile_overlap=(48, 48), axes=None, data_type=None, tta_transforms=False, dataloader_params=None, read_source_func=None, extension_filter='', write_type='tiff', write_extension=None, write_func=None, write_func_kwargs=None, prediction_dir='predictions', **kwargs)</code>","text":"<p>Make predictions on the provided data and save outputs to files.</p> <p>The predictions will be saved in a new directory 'predictions' within the set working directory. The directory stucture within the 'predictions' directory will match that of the source directory.</p> <p>The <code>source</code> must be from files and not arrays. The file names of the predictions will match those of the source. If there is more than one sample within a file, the samples will be saved to seperate files. The file names of samples will have the name of the corresponding source file but with the sample index appended. E.g. If the the source file name is 'images.tiff' then the first sample's prediction will be saved with the file name \"image_0.tiff\". Input can be a PredictDataModule instance, a path to a data file, or a numpy array.</p> <p>If <code>data_type</code>, <code>axes</code> and <code>tile_size</code> are not provided, the training configuration parameters will be used, with the <code>patch_size</code> instead of <code>tile_size</code>.</p> <p>Test-time augmentation (TTA) can be switched on using the <code>tta_transforms</code> parameter. The TTA augmentation applies all possible flip and 90 degrees rotations to the prediction input and averages the predictions. TTA augmentation should not be used if you did not train with these augmentations.</p> <p>Note that if you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>(PredictDataModule or Path, str)</code> <p>Data to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for prediction.</p> <code>1</code> <code>tile_size</code> <code>tuple of int</code> <p>Size of the tiles to use for prediction.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Overlap between tiles.</p> <code>(48, 48)</code> <code>axes</code> <code>str</code> <p>Axes of the input data, by default None.</p> <code>None</code> <code>data_type</code> <code>(array, tiff, custom)</code> <p>Type of the input data.</p> <code>\"array\"</code> <code>tta_transforms</code> <code>bool</code> <p>Whether to apply test-time augmentation.</p> <code>True</code> <code>dataloader_params</code> <code>dict</code> <p>Parameters to pass to the dataloader.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for the file extension.</p> <code>\"\"</code> <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {str: any}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <code>prediction_dir</code> <code>Path | str</code> <p>The path to save the prediction results to. If <code>prediction_dir</code> is not absolute, the directory will be assumed to be relative to the pre-set <code>work_dir</code>. If the directory does not exist it will be created.</p> <code>\"predictions\"</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>write_type</code> is custom and <code>write_extension</code> is None.</p> <code>ValueError</code> <p>If <code>write_type</code> is custom and `write_fun is None.</p> <code>ValueError</code> <p>If <code>source</code> is not <code>str</code>, <code>Path</code> or <code>PredictDataModule</code></p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def predict_to_disk(\n    self,\n    source: Union[PredictDataModule, Path, str],\n    *,\n    batch_size: int = 1,\n    tile_size: Optional[tuple[int, ...]] = None,\n    tile_overlap: Optional[tuple[int, ...]] = (48, 48),\n    axes: Optional[str] = None,\n    data_type: Optional[Literal[\"tiff\", \"custom\"]] = None,\n    tta_transforms: bool = False,\n    dataloader_params: Optional[dict] = None,\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    write_type: Literal[\"tiff\", \"custom\"] = \"tiff\",\n    write_extension: Optional[str] = None,\n    write_func: Optional[WriteFunc] = None,\n    write_func_kwargs: Optional[dict[str, Any]] = None,\n    prediction_dir: Union[Path, str] = \"predictions\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Make predictions on the provided data and save outputs to files.\n\n    The predictions will be saved in a new directory 'predictions' within the set\n    working directory. The directory stucture within the 'predictions' directory\n    will match that of the source directory.\n\n    The `source` must be from files and not arrays. The file names of the\n    predictions will match those of the source. If there is more than one sample\n    within a file, the samples will be saved to seperate files. The file names of\n    samples will have the name of the corresponding source file but with the sample\n    index appended. E.g. If the the source file name is 'images.tiff' then the first\n    sample's prediction will be saved with the file name \"image_0.tiff\".\n    Input can be a PredictDataModule instance, a path to a data file, or a numpy\n    array.\n\n    If `data_type`, `axes` and `tile_size` are not provided, the training\n    configuration parameters will be used, with the `patch_size` instead of\n    `tile_size`.\n\n    Test-time augmentation (TTA) can be switched on using the `tta_transforms`\n    parameter. The TTA augmentation applies all possible flip and 90 degrees\n    rotations to the prediction input and averages the predictions. TTA augmentation\n    should not be used if you did not train with these augmentations.\n\n    Note that if you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n\n    Parameters\n    ----------\n    source : PredictDataModule or pathlib.Path, str\n        Data to predict on.\n    batch_size : int, default=1\n        Batch size for prediction.\n    tile_size : tuple of int, optional\n        Size of the tiles to use for prediction.\n    tile_overlap : tuple of int, default=(48, 48)\n        Overlap between tiles.\n    axes : str, optional\n        Axes of the input data, by default None.\n    data_type : {\"array\", \"tiff\", \"custom\"}, optional\n        Type of the input data.\n    tta_transforms : bool, default=True\n        Whether to apply test-time augmentation.\n    dataloader_params : dict, optional\n        Parameters to pass to the dataloader.\n    read_source_func : Callable, optional\n        Function to read the source data.\n    extension_filter : str, default=\"\"\n        Filter for the file extension.\n    write_type : {\"tiff\", \"custom\"}, default=\"tiff\"\n        The data type to save as, includes custom.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_func_kwargs : dict of {str: any}, optional\n        Additional keyword arguments to be passed to the save function.\n    prediction_dir : Path | str, default=\"predictions\"\n        The path to save the prediction results to. If `prediction_dir` is not\n        absolute, the directory will be assumed to be relative to the pre-set\n        `work_dir`. If the directory does not exist it will be created.\n    **kwargs : Any\n        Unused.\n\n    Raises\n    ------\n    ValueError\n        If `write_type` is custom and `write_extension` is None.\n    ValueError\n        If `write_type` is custom and `write_fun is None.\n    ValueError\n        If `source` is not `str`, `Path` or `PredictDataModule`\n    \"\"\"\n    if write_func_kwargs is None:\n        write_func_kwargs = {}\n\n    if Path(prediction_dir).is_absolute():\n        write_dir = Path(prediction_dir)\n    else:\n        write_dir = self.work_dir / prediction_dir\n    write_dir.mkdir(exist_ok=True, parents=True)\n\n    # guards for custom types\n    if write_type == SupportedData.CUSTOM:\n        if write_extension is None:\n            raise ValueError(\n                \"A `write_extension` must be provided for custom write types.\"\n            )\n        if write_func is None:\n            raise ValueError(\n                \"A `write_func` must be provided for custom write types.\"\n            )\n    else:\n        write_func = get_write_func(write_type)\n        write_extension = SupportedData.get_extension(write_type)\n\n    # extract file names\n    source_path: Union[Path, str, NDArray]\n    source_data_type: Literal[\"array\", \"tiff\", \"custom\"]\n    if isinstance(source, PredictDataModule):\n        source_path = source.pred_data\n        source_data_type = source.data_type\n        extension_filter = source.extension_filter\n    elif isinstance(source, (str, Path)):\n        source_path = source\n        source_data_type = data_type or self.cfg.data_config.data_type\n        extension_filter = SupportedData.get_extension_pattern(\n            SupportedData(source_data_type)\n        )\n    else:\n        raise ValueError(f\"Unsupported source type: '{type(source)}'.\")\n\n    if source_data_type == \"array\":\n        raise ValueError(\n            \"Predicting to disk is not supported for input type 'array'.\"\n        )\n    assert isinstance(source_path, (Path, str))  # because data_type != \"array\"\n    source_path = Path(source_path)\n\n    file_paths = list_files(source_path, source_data_type, extension_filter)\n\n    # predict and write each file in turn\n    for file_path in file_paths:\n        # source_path is relative to original source path...\n        # should mirror original directory structure\n        prediction = self.predict(\n            source=file_path,\n            batch_size=batch_size,\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            axes=axes,\n            data_type=data_type,\n            tta_transforms=tta_transforms,\n            dataloader_params=dataloader_params,\n            read_source_func=read_source_func,\n            extension_filter=extension_filter,\n            **kwargs,\n        )\n        # TODO: cast to float16?\n        write_data = np.concatenate(prediction)\n\n        # create directory structure and write path\n        if not source_path.is_file():\n            file_write_dir = write_dir / file_path.parent.relative_to(source_path)\n        else:\n            file_write_dir = write_dir\n        file_write_dir.mkdir(parents=True, exist_ok=True)\n        write_path = (file_write_dir / file_path.name).with_suffix(write_extension)\n\n        # write data\n        write_func(file_path=write_path, img=write_data)\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.stop_training","title":"<code>stop_training()</code>","text":"<p>Stop the training loop.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def stop_training(self) -&gt; None:\n    \"\"\"Stop the training loop.\"\"\"\n    # raise stop training flag\n    self.trainer.should_stop = True\n    self.trainer.limit_val_batches = 0  # skip  validation\n</code></pre>"},{"location":"reference/careamics/careamist/#careamics.careamist.CAREamist.train","title":"<code>train(*, datamodule=None, train_source=None, val_source=None, train_target=None, val_target=None, use_in_memory=True, val_percentage=0.1, val_minimum_split=1)</code>","text":"<p>Train the model on the provided data.</p> <p>If a datamodule is provided, then training will be performed using it. Alternatively, the training data can be provided as arrays or paths.</p> <p>If <code>use_in_memory</code> is set to True, the source provided as Path or str will be loaded in memory if it fits. Otherwise, training will be performed by loading patches from the files one by one. Training on arrays is always performed in memory.</p> <p>If no validation source is provided, then the validation is extracted from the training data using <code>val_percentage</code> and <code>val_minimum_split</code>. In the case of data provided as Path or str, the percentage and minimum number are applied to the number of files. For arrays, it is the number of patches.</p> <p>Parameters:</p> Name Type Description Default <code>datamodule</code> <code>TrainDataModule</code> <p>Datamodule to train on, by default None.</p> <code>None</code> <code>train_source</code> <code>Path or str or NDArray</code> <p>Train source, if no datamodule is provided, by default None.</p> <code>None</code> <code>val_source</code> <code>Path or str or NDArray</code> <p>Validation source, if no datamodule is provided, by default None.</p> <code>None</code> <code>train_target</code> <code>Path or str or NDArray</code> <p>Train target source, if no datamodule is provided, by default None.</p> <code>None</code> <code>val_target</code> <code>Path or str or NDArray</code> <p>Validation target source, if no datamodule is provided, by default None.</p> <code>None</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <code>val_percentage</code> <code>float</code> <p>Percentage of validation extracted from training data, by default 0.1.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of validation (patch or file) extracted from training data, by default 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>datamodule</code> and <code>train_source</code> are provided.</p> <code>ValueError</code> <p>If sources are not of the same type (e.g. train is an array and val is a Path).</p> <code>ValueError</code> <p>If the training target is provided to N2V.</p> <code>ValueError</code> <p>If neither a datamodule nor a source is provided.</p> Source code in <code>src/careamics/careamist.py</code> <pre><code>def train(\n    self,\n    *,\n    datamodule: Optional[TrainDataModule] = None,\n    train_source: Optional[Union[Path, str, NDArray]] = None,\n    val_source: Optional[Union[Path, str, NDArray]] = None,\n    train_target: Optional[Union[Path, str, NDArray]] = None,\n    val_target: Optional[Union[Path, str, NDArray]] = None,\n    use_in_memory: bool = True,\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 1,\n) -&gt; None:\n    \"\"\"\n    Train the model on the provided data.\n\n    If a datamodule is provided, then training will be performed using it.\n    Alternatively, the training data can be provided as arrays or paths.\n\n    If `use_in_memory` is set to True, the source provided as Path or str will be\n    loaded in memory if it fits. Otherwise, training will be performed by loading\n    patches from the files one by one. Training on arrays is always performed\n    in memory.\n\n    If no validation source is provided, then the validation is extracted from\n    the training data using `val_percentage` and `val_minimum_split`. In the case\n    of data provided as Path or str, the percentage and minimum number are applied\n    to the number of files. For arrays, it is the number of patches.\n\n    Parameters\n    ----------\n    datamodule : TrainDataModule, optional\n        Datamodule to train on, by default None.\n    train_source : pathlib.Path or str or NDArray, optional\n        Train source, if no datamodule is provided, by default None.\n    val_source : pathlib.Path or str or NDArray, optional\n        Validation source, if no datamodule is provided, by default None.\n    train_target : pathlib.Path or str or NDArray, optional\n        Train target source, if no datamodule is provided, by default None.\n    val_target : pathlib.Path or str or NDArray, optional\n        Validation target source, if no datamodule is provided, by default None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n    val_percentage : float, optional\n        Percentage of validation extracted from training data, by default 0.1.\n    val_minimum_split : int, optional\n        Minimum number of validation (patch or file) extracted from training data,\n        by default 1.\n\n    Raises\n    ------\n    ValueError\n        If both `datamodule` and `train_source` are provided.\n    ValueError\n        If sources are not of the same type (e.g. train is an array and val is\n        a Path).\n    ValueError\n        If the training target is provided to N2V.\n    ValueError\n        If neither a datamodule nor a source is provided.\n    \"\"\"\n    if datamodule is not None and train_source is not None:\n        raise ValueError(\n            \"Only one of `datamodule` and `train_source` can be provided.\"\n        )\n\n    # check that inputs are the same type\n    source_types = {\n        type(s)\n        for s in (train_source, val_source, train_target, val_target)\n        if s is not None\n    }\n    if len(source_types) &gt; 1:\n        raise ValueError(\"All sources should be of the same type.\")\n\n    # train\n    if datamodule is not None:\n        self._train_on_datamodule(datamodule=datamodule)\n\n    else:\n        # raise error if target is provided to N2V\n        if self.cfg.algorithm_config.algorithm == SupportedAlgorithm.N2V.value:\n            if train_target is not None:\n                raise ValueError(\n                    \"Training target not compatible with N2V training.\"\n                )\n\n        # dispatch the training\n        if isinstance(train_source, np.ndarray):\n            # mypy checks\n            assert isinstance(val_source, np.ndarray) or val_source is None\n            assert isinstance(train_target, np.ndarray) or train_target is None\n            assert isinstance(val_target, np.ndarray) or val_target is None\n\n            self._train_on_array(\n                train_source,\n                val_source,\n                train_target,\n                val_target,\n                val_percentage,\n                val_minimum_split,\n            )\n\n        elif isinstance(train_source, Path) or isinstance(train_source, str):\n            # mypy checks\n            assert (\n                isinstance(val_source, Path)\n                or isinstance(val_source, str)\n                or val_source is None\n            )\n            assert (\n                isinstance(train_target, Path)\n                or isinstance(train_target, str)\n                or train_target is None\n            )\n            assert (\n                isinstance(val_target, Path)\n                or isinstance(val_target, str)\n                or val_target is None\n            )\n\n            self._train_on_path(\n                train_source,\n                val_source,\n                train_target,\n                val_target,\n                use_in_memory,\n                val_percentage,\n                val_minimum_split,\n            )\n\n        else:\n            raise ValueError(\n                f\"Invalid input, expected a str, Path, array or TrainDataModule \"\n                f\"instance (got {type(train_source)}).\"\n            )\n</code></pre>"},{"location":"reference/careamics/conftest/","title":"conftest","text":"<p>File used to discover python modules and run doctest.</p> <p>See https://sybil.readthedocs.io/en/latest/use.html#pytest</p>"},{"location":"reference/careamics/conftest/#careamics.conftest.my_path","title":"<code>my_path(tmpdir_factory)</code>","text":"<p>Fixture used in doctest to create a temporary directory.</p> <p>Parameters:</p> Name Type Description Default <code>tmpdir_factory</code> <code>TempPathFactory</code> <p>Temporary path factory from pytest.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Temporary directory path.</p> Source code in <code>src/careamics/conftest.py</code> <pre><code>@pytest.fixture(scope=\"module\")\ndef my_path(tmpdir_factory: TempPathFactory) -&gt; Path:\n    \"\"\"Fixture used in doctest to create a temporary directory.\n\n    Parameters\n    ----------\n    tmpdir_factory : TempPathFactory\n        Temporary path factory from pytest.\n\n    Returns\n    -------\n    Path\n        Temporary directory path.\n    \"\"\"\n    return tmpdir_factory.mktemp(\"my_path\")\n</code></pre>"},{"location":"reference/careamics/cli/conf/","title":"conf","text":"<p>Configuration building convenience functions for the CAREamics CLI.</p>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.ConfOptions","title":"<code>ConfOptions</code>  <code>dataclass</code>","text":"<p>Data class for containing CLI <code>conf</code> command option values.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@dataclass\nclass ConfOptions:\n    \"\"\"Data class for containing CLI `conf` command option values.\"\"\"\n\n    dir: Path\n    name: str\n    force: bool\n    print: bool\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf._config_builder_exit","title":"<code>_config_builder_exit(ctx, config)</code>","text":"<p>Function to be called at the end of a CLI configuration builder.</p> <p>Saves the <code>config</code> object and performs other functionality depending on the command context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Context</code> <p>Typer Context.</p> required <code>config</code> <code>Configuration</code> <p>CAREamics configuration.</p> required Source code in <code>src/careamics/cli/conf.py</code> <pre><code>def _config_builder_exit(ctx: typer.Context, config: Configuration) -&gt; None:\n    \"\"\"\n    Function to be called at the end of a CLI configuration builder.\n\n    Saves the `config` object and performs other functionality depending on the command\n    context.\n\n    Parameters\n    ----------\n    ctx : typer.Context\n        Typer Context.\n    config : Configuration\n        CAREamics configuration.\n    \"\"\"\n    conf_path = (ctx.obj.dir / ctx.obj.name).with_suffix(\".yaml\")\n    save_configuration(config, conf_path)\n    if ctx.obj.print:\n        print(yaml.dump(config.model_dump(), indent=2))\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.care","title":"<code>care(ctx, experiment_name, axes, patch_size, batch_size, num_epochs, data_type='tiff', use_augmentations=True, independent_channels=False, loss='mae', n_channels_in=None, n_channels_out=None, logger='none')</code>","text":"<p>Create a configuration for training CARE.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>use_augmentations</code> to False, the only transformation applied will be normalization.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef care(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")],\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = False,\n    loss: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"mae\", \"mse\"]),\n            help=\"Loss function to use.\",\n        ),\n    ] = \"mae\",\n    n_channels_in: Annotated[\n        Optional[int], typer.Option(help=\"Number of channels in\")\n    ] = None,\n    n_channels_out: Annotated[\n        Optional[int], typer.Option(help=\"Number of channels out\")\n    ] = None,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training CARE.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `use_augmentations` to False, the only transformation applied will be\n    normalization.\n    \"\"\"\n    config = create_care_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        loss=loss,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        logger=logger,\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.conf_options","title":"<code>conf_options(ctx, dir=WORK_DIR, name='config', force=False, print=False)</code>","text":"<p>Build and save CAREamics configuration files.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.callback()\ndef conf_options(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    dir: Annotated[\n        Path,\n        typer.Option(\n            \"--dir\", \"-d\", exists=True, help=\"Directory to save the config file to.\"\n        ),\n    ] = WORK_DIR,\n    name: Annotated[\n        str, typer.Option(\"--name\", \"-n\", help=\"The config file name.\")\n    ] = \"config\",\n    force: Annotated[\n        bool,\n        typer.Option(\n            \"--force\", \"-f\", help=\"Whether to overwrite existing config files.\"\n        ),\n    ] = False,\n    print: Annotated[\n        bool,\n        typer.Option(\n            \"--print\",\n            \"-p\",\n            help=\"Whether to print the config file to the console.\",\n        ),\n    ] = False,\n):\n    \"\"\"Build and save CAREamics configuration files.\"\"\"\n    # Callback is called still on --help command\n    # If a config exists it will complain that you need to use the -f flag\n    if \"--help\" in sys.argv:\n        return\n    conf_path = (dir / name).with_suffix(\".yaml\")\n    if conf_path.exists() and not force:\n        raise FileExistsError(f\"To overwrite '{conf_path}' use flag --force/-f.\")\n\n    ctx.obj = ConfOptions(dir, name, force, print)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.n2n","title":"<code>n2n(ctx, experiment_name, axes, patch_size, batch_size, num_epochs, data_type='tiff', use_augmentations=True, independent_channels=False, loss='mae', n_channels_in=None, n_channels_out=None, logger='none')</code>","text":"<p>Create a configuration for training Noise2Noise.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>use_augmentations</code> to False, the only transformation applied will be normalization.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef n2n(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")],\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = False,\n    loss: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"mae\", \"mse\"]),\n            help=\"Loss function to use.\",\n        ),\n    ] = \"mae\",\n    n_channels_in: Annotated[\n        Optional[int], typer.Option(help=\"Number of channels in\")\n    ] = None,\n    n_channels_out: Annotated[\n        Optional[int], typer.Option(help=\"Number of channels out\")\n    ] = None,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training Noise2Noise.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `use_augmentations` to False, the only transformation applied will be\n    normalization.\n    \"\"\"\n    config = create_n2n_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        loss=loss,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        logger=logger,\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/conf/#careamics.cli.conf.n2v","title":"<code>n2v(ctx, experiment_name, axes, patch_size, batch_size, num_epochs, data_type='tiff', use_augmentations=True, independent_channels=True, use_n2v2=False, n_channels=None, roi_size=11, masked_pixel_percentage=0.2, struct_n2v_axis='none', struct_n2v_span=5, logger='none')</code>","text":"<p>Create a configuration for training Noise2Void.</p> <p>N2V uses a UNet model to denoise images in a self-supervised manner. To use its variants structN2V and N2V2, set the <code>struct_n2v_axis</code> and <code>struct_n2v_span</code> (structN2V) parameters, or set <code>use_n2v2</code> to True (N2V2).</p> <p>N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip connections, thus removing checkboard artefacts. StructN2V is used when vertical or horizontal correlations are present in the noise; it applies an additional mask to the manipulated pixel neighbors.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels.</p> <p>By default, all channels are trained independently. To train all channels together, set <code>independent_channels</code> to False.</p> <p>By setting <code>use_augmentations</code> to False, the only transformations applied will be normalization and N2V manipulation.</p> <p>The <code>roi_size</code> parameter specifies the size of the area around each pixel that will be manipulated by N2V. The <code>masked_pixel_percentage</code> parameter specifies how many pixels per patch will be manipulated.</p> <p>The parameters of the UNet can be specified in the <code>model_kwargs</code> (passed as a parameter-value dictionary). Note that <code>use_n2v2</code> and 'n_channels' override the corresponding parameters passed in <code>model_kwargs</code>.</p> <p>If you pass \"horizontal\" or \"vertical\" to <code>struct_n2v_axis</code>, then structN2V mask will be applied to each manipulated pixel.</p> Source code in <code>src/careamics/cli/conf.py</code> <pre><code>@app.command()\ndef n2v(  # numpydoc ignore=PR01\n    ctx: typer.Context,\n    experiment_name: Annotated[str, typer.Option(help=\"Name of the experiment.\")],\n    axes: Annotated[str, typer.Option(help=\"Axes of the data (e.g. SYX).\")],\n    patch_size: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Size of the patches along the spatial dimensions (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --patch-size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")],\n    num_epochs: Annotated[int, typer.Option(help=\"Number of epochs.\")],\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the data.\"),\n    ] = \"tiff\",\n    use_augmentations: Annotated[\n        bool, typer.Option(help=\"Whether to use augmentations.\")\n    ] = True,\n    independent_channels: Annotated[\n        bool, typer.Option(help=\"Whether to train all channels independently.\")\n    ] = True,\n    use_n2v2: Annotated[bool, typer.Option(help=\"Whether to use N2V2\")] = False,\n    n_channels: Annotated[\n        Optional[int], typer.Option(help=\"Number of channels (in and out)\")\n    ] = None,\n    roi_size: Annotated[int, typer.Option(help=\"N2V pixel manipulation area.\")] = 11,\n    masked_pixel_percentage: Annotated[\n        float, typer.Option(help=\"Percentage of pixels masked in each patch.\")\n    ] = 0.2,\n    struct_n2v_axis: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"horizontal\", \"vertical\", \"none\"])),\n    ] = \"none\",\n    struct_n2v_span: Annotated[\n        int, typer.Option(help=\"Span of the structN2V mask.\")\n    ] = 5,\n    logger: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"wandb\", \"tensorboard\", \"none\"]),\n            help=\"Logger to use.\",\n        ),\n    ] = \"none\",\n    # TODO: How to address model kwargs\n) -&gt; None:\n    \"\"\"\n    Create a configuration for training Noise2Void.\n\n    N2V uses a UNet model to denoise images in a self-supervised manner. To use its\n    variants structN2V and N2V2, set the `struct_n2v_axis` and `struct_n2v_span`\n    (structN2V) parameters, or set `use_n2v2` to True (N2V2).\n\n    N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip\n    connections, thus removing checkboard artefacts. StructN2V is used when vertical\n    or horizontal correlations are present in the noise; it applies an additional mask\n    to the manipulated pixel neighbors.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels.\n\n    By default, all channels are trained independently. To train all channels together,\n    set `independent_channels` to False.\n\n    By setting `use_augmentations` to False, the only transformations applied will be\n    normalization and N2V manipulation.\n\n    The `roi_size` parameter specifies the size of the area around each pixel that will\n    be manipulated by N2V. The `masked_pixel_percentage` parameter specifies how many\n    pixels per patch will be manipulated.\n\n    The parameters of the UNet can be specified in the `model_kwargs` (passed as a\n    parameter-value dictionary). Note that `use_n2v2` and 'n_channels' override the\n    corresponding parameters passed in `model_kwargs`.\n\n    If you pass \"horizontal\" or \"vertical\" to `struct_n2v_axis`, then structN2V mask\n    will be applied to each manipulated pixel.\n    \"\"\"\n    config = create_n2v_configuration(\n        experiment_name=experiment_name,\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        # TODO: fix choosing augmentations\n        augmentations=None if use_augmentations else [],\n        independent_channels=independent_channels,\n        use_n2v2=use_n2v2,\n        n_channels=n_channels,\n        roi_size=roi_size,\n        masked_pixel_percentage=masked_pixel_percentage,\n        struct_n2v_axis=struct_n2v_axis,\n        struct_n2v_span=struct_n2v_span,\n        logger=logger,\n        # TODO: Model kwargs\n    )\n    _config_builder_exit(ctx, config)\n</code></pre>"},{"location":"reference/careamics/cli/main/","title":"main","text":"<p>Module for CLI functionality and entrypoint.</p> <p>Contains the CLI entrypoint, the <code>run</code> function; and first level subcommands <code>train</code> and <code>predict</code>. The <code>conf</code> subcommand is added through the <code>app.add_typer</code> function, and its implementation is contained in the conf.py file.</p>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.predict","title":"<code>predict(model, source, batch_size=1, tile_size=None, tile_overlap=(48, 48, -1), axes=None, data_type='tiff', tta_transforms=False, write_type='tiff', work_dir=None, prediction_dir=Path('predictions'))</code>","text":"<p>Create and save predictions from CAREamics models.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>@app.command()\ndef predict(  # numpydoc ignore=PR01\n    model: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to a configuration file or a trained model.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n        ),\n    ],\n    source: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to the training data. Can be a directory or single file.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ],\n    batch_size: Annotated[int, typer.Option(help=\"Batch size.\")] = 1,\n    tile_size: Annotated[\n        Optional[click.Tuple],\n        typer.Option(\n            help=(\n                \"Size of the tiles to use for prediction, (if the data \"\n                \"is not 3D pass the last value as -1 e.g. --tile_size 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ] = None,\n    tile_overlap: Annotated[\n        click.Tuple,\n        typer.Option(\n            help=(\n                \"Overlap between tiles, (if the data is not 3D pass the last value as \"\n                \"-1 e.g. --tile_overlap 64 64 -1).\"\n            ),\n            click_type=click.Tuple([int, int, int]),\n            callback=handle_2D_3D_callback,\n        ),\n    ] = (48, 48, -1),\n    axes: Annotated[\n        Optional[str],\n        typer.Option(\n            help=\"Axes of the input data. If unused the data is assumed to have the \"\n            \"same axes as the original training data.\"\n        ),\n    ] = None,\n    data_type: Annotated[\n        click.Choice,\n        typer.Option(click_type=click.Choice([\"tiff\"]), help=\"Type of the input data.\"),\n    ] = \"tiff\",\n    tta_transforms: Annotated[\n        bool,\n        typer.Option(\n            \"--tta-transforms/--no-tta-transforms\",\n            \"-t/-T\",\n            help=\"Whether to apply test-time augmentation.\",\n        ),\n    ] = False,\n    write_type: Annotated[\n        click.Choice,\n        typer.Option(\n            click_type=click.Choice([\"tiff\"]), help=\"Type of the output data.\"\n        ),\n    ] = \"tiff\",\n    # TODO: could make dataloader_params as json, necessary?\n    work_dir: Annotated[\n        Optional[Path],\n        typer.Option(\n            \"--work-dir\",\n            \"-wd\",\n            help=(\"Path to working directory.\"),\n            exists=True,\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = None,\n    prediction_dir: Annotated[\n        Path,\n        typer.Option(\n            \"--prediction-dir\",\n            \"-pd\",\n            help=(\n                \"Directory to save predictions to. If not an abosulte path it will be \"\n                \"relative to the set working directory.\"\n            ),\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = Path(\"predictions\"),\n):\n    \"\"\"Create and save predictions from CAREamics models.\"\"\"\n    engine = CAREamist(source=model, work_dir=work_dir)\n    engine.predict_to_disk(\n        source=source,\n        batch_size=batch_size,\n        tile_size=tile_size,\n        tile_overlap=tile_overlap,\n        axes=axes,\n        data_type=data_type,\n        tta_transforms=tta_transforms,\n        write_type=write_type,\n        prediction_dir=prediction_dir,\n    )\n</code></pre>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.run","title":"<code>run()</code>","text":"<p>CLI Entry point.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>def run():\n    \"\"\"CLI Entry point.\"\"\"\n    app()\n</code></pre>"},{"location":"reference/careamics/cli/main/#careamics.cli.main.train","title":"<code>train(source, train_source, train_target=None, val_source=None, val_target=None, use_in_memory=True, val_percentage=0.1, val_minimum_split=1, work_dir=None)</code>","text":"<p>Train CAREamics models.</p> Source code in <code>src/careamics/cli/main.py</code> <pre><code>@app.command()\ndef train(  # numpydoc ignore=PR01\n    source: Annotated[\n        Path,\n        typer.Argument(\n            help=\"Path to a configuration file or a trained model.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n        ),\n    ],\n    train_source: Annotated[\n        Path,\n        typer.Option(\n            \"--train-source\",\n            \"-ts\",\n            help=\"Path to the training data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ],\n    train_target: Annotated[\n        Optional[Path],\n        typer.Option(\n            \"--train-target\",\n            \"-tt\",\n            help=\"Path to train target data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    val_source: Annotated[\n        Optional[Path],\n        typer.Option(\n            \"--val-source\",\n            \"-vs\",\n            help=\"Path to validation data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    val_target: Annotated[\n        Optional[Path],\n        typer.Option(\n            \"--val-target\",\n            \"-vt\",\n            help=\"Path to validation target data.\",\n            exists=True,\n            file_okay=True,\n            dir_okay=True,\n        ),\n    ] = None,\n    use_in_memory: Annotated[\n        bool,\n        typer.Option(\n            \"--use-in-memory/--not-in-memory\",\n            \"-m/-M\",\n            help=\"Use in memory dataset if possible.\",\n        ),\n    ] = True,\n    val_percentage: Annotated[\n        float,\n        typer.Option(help=\"Percentage of files to use for validation.\"),\n    ] = 0.1,\n    val_minimum_split: Annotated[\n        int,\n        typer.Option(help=\"Minimum number of files to use for validation,\"),\n    ] = 1,\n    work_dir: Annotated[\n        Optional[Path],\n        typer.Option(\n            \"--work-dir\",\n            \"-wd\",\n            help=(\"Path to working directory in which to save checkpoints and logs\"),\n            exists=True,\n            file_okay=False,\n            dir_okay=True,\n        ),\n    ] = None,\n):\n    \"\"\"Train CAREamics models.\"\"\"\n    engine = CAREamist(source=source, work_dir=work_dir)\n    engine.train(\n        train_source=train_source,\n        val_source=val_source,\n        train_target=train_target,\n        val_target=val_target,\n        use_in_memory=use_in_memory,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_split,\n    )\n</code></pre>"},{"location":"reference/careamics/cli/utils/","title":"utils","text":"<p>Utility functions for the CAREamics CLI.</p>"},{"location":"reference/careamics/cli/utils/#careamics.cli.utils.handle_2D_3D_callback","title":"<code>handle_2D_3D_callback(value)</code>","text":"<p>Callback for options that require 2D or 3D inputs.</p> <p>In the case of 2D, the 3rd element should be set to -1.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>(int, int, int)</code> <p>Tile size value.</p> required <p>Returns:</p> Type Description <code>(int, int, int) | (int, int)</code> <p>If the last element in <code>value</code> is -1 the tuple is reduced to the first two values.</p> Source code in <code>src/careamics/cli/utils.py</code> <pre><code>def handle_2D_3D_callback(\n    value: Optional[tuple[int, int, int]]\n) -&gt; Optional[tuple[int, ...]]:\n    \"\"\"\n    Callback for options that require 2D or 3D inputs.\n\n    In the case of 2D, the 3rd element should be set to -1.\n\n    Parameters\n    ----------\n    value : (int, int, int)\n        Tile size value.\n\n    Returns\n    -------\n    (int, int, int) | (int, int)\n        If the last element in `value` is -1 the tuple is reduced to the first two\n        values.\n    \"\"\"\n    if value is None:\n        return value\n    if value[2] == -1:\n        return value[:2]\n    return value\n</code></pre>"},{"location":"reference/careamics/config/callback_model/","title":"callback_model","text":"<p>Callback Pydantic models.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel","title":"<code>CheckpointModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Checkpoint saving callback Pydantic model.</p> <p>The parameters corresponds to those of <code>pytorch_lightning.callbacks.ModelCheckpoint</code>.</p> <p>See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html#modelcheckpoint</p> Source code in <code>src/careamics/config/callback_model.py</code> <pre><code>class CheckpointModel(BaseModel):\n    \"\"\"Checkpoint saving callback Pydantic model.\n\n    The parameters corresponds to those of\n    `pytorch_lightning.callbacks.ModelCheckpoint`.\n\n    See:\n    https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html#modelcheckpoint\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    monitor: Literal[\"val_loss\"] = Field(default=\"val_loss\", validate_default=True)\n    \"\"\"Quantity to monitor.\"\"\"\n\n    verbose: bool = Field(default=False, validate_default=True)\n    \"\"\"Verbosity mode.\"\"\"\n\n    save_weights_only: bool = Field(default=False, validate_default=True)\n    \"\"\"When `True`, only the model's weights will be saved (model.save_weights).\"\"\"\n\n    save_last: Optional[Literal[True, False, \"link\"]] = Field(\n        default=True, validate_default=True\n    )\n    \"\"\"When `True`, saves a last.ckpt copy whenever a checkpoint file gets saved.\"\"\"\n\n    save_top_k: int = Field(default=3, ge=1, le=10, validate_default=True)\n    \"\"\"If `save_top_k == kz, the best k models according to the quantity monitored\n    will be saved. If `save_top_k == 0`, no models are saved. if `save_top_k == -1`,\n    all models are saved.\"\"\"\n\n    mode: Literal[\"min\", \"max\"] = Field(default=\"min\", validate_default=True)\n    \"\"\"One of {min, max}. If `save_top_k != 0`, the decision to overwrite the current\n    save file is made based on either the maximization or the minimization of the\n    monitored quantity. For 'val_acc', this should be 'max', for 'val_loss' this should\n    be 'min', etc.\n    \"\"\"\n\n    auto_insert_metric_name: bool = Field(default=False, validate_default=True)\n    \"\"\"When `True`, the checkpoints filenames will contain the metric name.\"\"\"\n\n    every_n_train_steps: Optional[int] = Field(\n        default=None, ge=1, le=10, validate_default=True\n    )\n    \"\"\"Number of training steps between checkpoints.\"\"\"\n\n    train_time_interval: Optional[timedelta] = Field(\n        default=None, validate_default=True\n    )\n    \"\"\"Checkpoints are monitored at the specified time interval.\"\"\"\n\n    every_n_epochs: Optional[int] = Field(\n        default=None, ge=1, le=10, validate_default=True\n    )\n    \"\"\"Number of epochs between checkpoints.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.auto_insert_metric_name","title":"<code>auto_insert_metric_name = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, the checkpoints filenames will contain the metric name.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.every_n_epochs","title":"<code>every_n_epochs = Field(default=None, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs between checkpoints.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.every_n_train_steps","title":"<code>every_n_train_steps = Field(default=None, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training steps between checkpoints.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.mode","title":"<code>mode = Field(default='min', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>One of {min, max}. If <code>save_top_k != 0</code>, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For 'val_acc', this should be 'max', for 'val_loss' this should be 'min', etc.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.monitor","title":"<code>monitor = Field(default='val_loss', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantity to monitor.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_last","title":"<code>save_last = Field(default=True, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, saves a last.ckpt copy whenever a checkpoint file gets saved.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_top_k","title":"<code>save_top_k = Field(default=3, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>save_top_k == kz, the best k models according to the quantity monitored will be saved. If</code>save_top_k == 0<code>, no models are saved. if</code>save_top_k == -1`, all models are saved.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.save_weights_only","title":"<code>save_weights_only = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, only the model's weights will be saved (model.save_weights).</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.train_time_interval","title":"<code>train_time_interval = Field(default=None, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoints are monitored at the specified time interval.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.CheckpointModel.verbose","title":"<code>verbose = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Verbosity mode.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel","title":"<code>EarlyStoppingModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Early stopping callback Pydantic model.</p> <p>The parameters corresponds to those of <code>pytorch_lightning.callbacks.ModelCheckpoint</code>.</p> <p>See: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping</p> Source code in <code>src/careamics/config/callback_model.py</code> <pre><code>class EarlyStoppingModel(BaseModel):\n    \"\"\"Early stopping callback Pydantic model.\n\n    The parameters corresponds to those of\n    `pytorch_lightning.callbacks.ModelCheckpoint`.\n\n    See:\n    https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.EarlyStopping.html#lightning.pytorch.callbacks.EarlyStopping\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    monitor: Literal[\"val_loss\"] = Field(default=\"val_loss\", validate_default=True)\n    \"\"\"Quantity to monitor.\"\"\"\n\n    min_delta: float = Field(default=0.0, ge=0.0, le=1.0, validate_default=True)\n    \"\"\"Minimum change in the monitored quantity to qualify as an improvement, i.e. an\n    absolute change of less than or equal to min_delta, will count as no improvement.\"\"\"\n\n    patience: int = Field(default=3, ge=1, le=10, validate_default=True)\n    \"\"\"Number of checks with no improvement after which training will be stopped.\"\"\"\n\n    verbose: bool = Field(default=False, validate_default=True)\n    \"\"\"Verbosity mode.\"\"\"\n\n    mode: Literal[\"min\", \"max\", \"auto\"] = Field(default=\"min\", validate_default=True)\n    \"\"\"One of {min, max, auto}.\"\"\"\n\n    check_finite: bool = Field(default=True, validate_default=True)\n    \"\"\"When `True`, stops training when the monitored quantity becomes `NaN` or\n    `inf`.\"\"\"\n\n    stopping_threshold: Optional[float] = Field(default=None, validate_default=True)\n    \"\"\"Stop training immediately once the monitored quantity reaches this threshold.\"\"\"\n\n    divergence_threshold: Optional[float] = Field(default=None, validate_default=True)\n    \"\"\"Stop training as soon as the monitored quantity becomes worse than this\n    threshold.\"\"\"\n\n    check_on_train_epoch_end: Optional[bool] = Field(\n        default=False, validate_default=True\n    )\n    \"\"\"Whether to run early stopping at the end of the training epoch. If this is\n    `False`, then the check runs at the end of the validation.\"\"\"\n\n    log_rank_zero_only: bool = Field(default=False, validate_default=True)\n    \"\"\"When set `True`, logs the status of the early stopping callback only for rank 0\n    process.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.check_finite","title":"<code>check_finite = Field(default=True, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When <code>True</code>, stops training when the monitored quantity becomes <code>NaN</code> or <code>inf</code>.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.check_on_train_epoch_end","title":"<code>check_on_train_epoch_end = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to run early stopping at the end of the training epoch. If this is <code>False</code>, then the check runs at the end of the validation.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.divergence_threshold","title":"<code>divergence_threshold = Field(default=None, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop training as soon as the monitored quantity becomes worse than this threshold.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.log_rank_zero_only","title":"<code>log_rank_zero_only = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When set <code>True</code>, logs the status of the early stopping callback only for rank 0 process.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.min_delta","title":"<code>min_delta = Field(default=0.0, ge=0.0, le=1.0, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.mode","title":"<code>mode = Field(default='min', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>One of {min, max, auto}.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.monitor","title":"<code>monitor = Field(default='val_loss', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantity to monitor.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.patience","title":"<code>patience = Field(default=3, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of checks with no improvement after which training will be stopped.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.stopping_threshold","title":"<code>stopping_threshold = Field(default=None, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop training immediately once the monitored quantity reaches this threshold.</p>"},{"location":"reference/careamics/config/callback_model/#careamics.config.callback_model.EarlyStoppingModel.verbose","title":"<code>verbose = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Verbosity mode.</p>"},{"location":"reference/careamics/config/configuration/","title":"configuration","text":"<p>Pydantic CAREamics configuration.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration","title":"<code>Configuration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>CAREamics configuration.</p> <p>The configuration defines all parameters used to build and train a CAREamics model. These parameters are validated to ensure that they are compatible with each other.</p> <p>It contains three sub-configurations:</p> <ul> <li>AlgorithmModel: configuration for the algorithm training, which includes the     architecture, loss function, optimizer, and other hyperparameters.</li> <li>DataModel: configuration for the dataloader, which includes the type of data,     transformations, mean/std and other parameters.</li> <li>TrainingModel: configuration for the training, which includes the number of     epochs or the callbacks.</li> </ul> <p>Attributes:</p> Name Type Description <code>experiment_name</code> <code>str</code> <p>Name of the experiment, used when saving logs and checkpoints.</p> <code>algorithm</code> <code>AlgorithmModel</code> <p>Algorithm configuration.</p> <code>data</code> <code>DataModel</code> <p>Data configuration.</p> <code>training</code> <code>TrainingModel</code> <p>Training configuration.</p> <p>Methods:</p> Name Description <code>set_3D</code> <p>Switch configuration between 2D and 3D.</p> <code>model_dump</code> <p>exclude_defaults: bool = False, exclude_none: bool = True, **kwargs: Dict ) -&gt; Dict Export configuration to a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Configuration parameter type validation errors.</p> <code>ValueError</code> <p>If the experiment name contains invalid characters or is empty.</p> <code>ValueError</code> <p>If the algorithm is 3D but there is not \"Z\" in the data axes, or 2D algorithm with \"Z\" in data axes.</p> <code>ValueError</code> <p>Algorithm, data or training validation errors.</p> Notes <p>We provide convenience methods to create standards configurations, for instance:</p> <p>from careamics.config import create_n2v_configuration config = create_n2v_configuration( ...     experiment_name=\"n2v_experiment\", ...     data_type=\"array\", ...     axes=\"YX\", ...     patch_size=[64, 64], ...     batch_size=32, ...     num_epochs=100 ... )</p> <p>The configuration can be exported to a dictionary using the model_dump method:</p> <p>config_dict = config.model_dump()</p> <p>Configurations can also be exported or imported from yaml files:</p> <p>from careamics.config import save_configuration, load_configuration path_to_config = save_configuration(config, my_path / \"config.yml\") other_config = load_configuration(path_to_config)</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; from careamics import Configuration\n&gt;&gt;&gt; config_dict = {\n...         \"experiment_name\": \"N2V_experiment\",\n...         \"algorithm_config\": {\n...             \"algorithm\": \"n2v\",\n...             \"loss\": \"n2v\",\n...             \"model\": {\n...                 \"architecture\": \"UNet\",\n...             },\n...         },\n...         \"training_config\": {\n...             \"num_epochs\": 200,\n...         },\n...         \"data_config\": {\n...             \"data_type\": \"tiff\",\n...             \"patch_size\": [64, 64],\n...             \"axes\": \"SYX\",\n...         },\n...     }\n&gt;&gt;&gt; config = Configuration(**config_dict)\n</code></pre> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>class Configuration(BaseModel):\n    \"\"\"\n    CAREamics configuration.\n\n    The configuration defines all parameters used to build and train a CAREamics model.\n    These parameters are validated to ensure that they are compatible with each other.\n\n    It contains three sub-configurations:\n\n    - AlgorithmModel: configuration for the algorithm training, which includes the\n        architecture, loss function, optimizer, and other hyperparameters.\n    - DataModel: configuration for the dataloader, which includes the type of data,\n        transformations, mean/std and other parameters.\n    - TrainingModel: configuration for the training, which includes the number of\n        epochs or the callbacks.\n\n    Attributes\n    ----------\n    experiment_name : str\n        Name of the experiment, used when saving logs and checkpoints.\n    algorithm : AlgorithmModel\n        Algorithm configuration.\n    data : DataModel\n        Data configuration.\n    training : TrainingModel\n        Training configuration.\n\n    Methods\n    -------\n    set_3D(is_3D: bool, axes: str, patch_size: List[int]) -&gt; None\n        Switch configuration between 2D and 3D.\n    model_dump(\n        exclude_defaults: bool = False, exclude_none: bool = True, **kwargs: Dict\n        ) -&gt; Dict\n        Export configuration to a dictionary.\n\n    Raises\n    ------\n    ValueError\n        Configuration parameter type validation errors.\n    ValueError\n        If the experiment name contains invalid characters or is empty.\n    ValueError\n        If the algorithm is 3D but there is not \"Z\" in the data axes, or 2D algorithm\n        with \"Z\" in data axes.\n    ValueError\n        Algorithm, data or training validation errors.\n\n    Notes\n    -----\n    We provide convenience methods to create standards configurations, for instance:\n    &gt;&gt;&gt; from careamics.config import create_n2v_configuration\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    The configuration can be exported to a dictionary using the model_dump method:\n    &gt;&gt;&gt; config_dict = config.model_dump()\n\n    Configurations can also be exported or imported from yaml files:\n    &gt;&gt;&gt; from careamics.config import save_configuration, load_configuration\n    &gt;&gt;&gt; path_to_config = save_configuration(config, my_path / \"config.yml\")\n    &gt;&gt;&gt; other_config = load_configuration(path_to_config)\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; from careamics import Configuration\n    &gt;&gt;&gt; config_dict = {\n    ...         \"experiment_name\": \"N2V_experiment\",\n    ...         \"algorithm_config\": {\n    ...             \"algorithm\": \"n2v\",\n    ...             \"loss\": \"n2v\",\n    ...             \"model\": {\n    ...                 \"architecture\": \"UNet\",\n    ...             },\n    ...         },\n    ...         \"training_config\": {\n    ...             \"num_epochs\": 200,\n    ...         },\n    ...         \"data_config\": {\n    ...             \"data_type\": \"tiff\",\n    ...             \"patch_size\": [64, 64],\n    ...             \"axes\": \"SYX\",\n    ...         },\n    ...     }\n    &gt;&gt;&gt; config = Configuration(**config_dict)\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n        arbitrary_types_allowed=True,\n    )\n\n    # version\n    version: Literal[\"0.1.0\"] = \"0.1.0\"\n    \"\"\"CAREamics configuration version.\"\"\"\n\n    # required parameters\n    experiment_name: str\n    \"\"\"Name of the experiment, used to name logs and checkpoints.\"\"\"\n\n    # Sub-configurations\n    algorithm_config: ALGORITHMS = Field(discriminator=\"algorithm\")\n    \"\"\"Algorithm configuration, holding all parameters required to configure the\n    model.\"\"\"\n\n    data_config: DataConfig\n    \"\"\"Data configuration, holding all parameters required to configure the training\n    data loader.\"\"\"\n\n    training_config: TrainingConfig\n    \"\"\"Training configuration, holding all parameters required to configure the\n    training process.\"\"\"\n\n    @field_validator(\"experiment_name\")\n    @classmethod\n    def no_symbol(cls, name: str) -&gt; str:\n        \"\"\"\n        Validate experiment name.\n\n        A valid experiment name is a non-empty string with only contains letters,\n        numbers, underscores, dashes and spaces.\n\n        Parameters\n        ----------\n        name : str\n            Name to validate.\n\n        Returns\n        -------\n        str\n            Validated name.\n\n        Raises\n        ------\n        ValueError\n            If the name is empty or contains invalid characters.\n        \"\"\"\n        if len(name) == 0 or name.isspace():\n            raise ValueError(\"Experiment name is empty.\")\n\n        # Validate using a regex that it contains only letters, numbers, underscores,\n        # dashes and spaces\n        if not re.match(r\"^[a-zA-Z0-9_\\- ]*$\", name):\n            raise ValueError(\n                f\"Experiment name contains invalid characters (got {name}). \"\n                f\"Only letters, numbers, underscores, dashes and spaces are allowed.\"\n            )\n\n        return name\n\n    @model_validator(mode=\"after\")\n    def validate_3D(self: Self) -&gt; Self:\n        \"\"\"\n        Change algorithm dimensions to match data.axes.\n\n        Returns\n        -------\n        Self\n            Validated configuration.\n        \"\"\"\n        if \"Z\" in self.data_config.axes and not self.algorithm_config.model.is_3D():\n            # change algorithm to 3D\n            self.algorithm_config.model.set_3D(True)\n        elif \"Z\" not in self.data_config.axes and self.algorithm_config.model.is_3D():\n            # change algorithm to 2D\n            self.algorithm_config.model.set_3D(False)\n\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def set_3D(self, is_3D: bool, axes: str, patch_size: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D flag and axes.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        axes : str\n            Axes of the data.\n        patch_size : list[int]\n            Patch size.\n        \"\"\"\n        # set the flag and axes (this will not trigger validation at the config level)\n        self.algorithm_config.model.set_3D(is_3D)\n        self.data_config.set_3D(axes, patch_size)\n\n        # cheap hack: trigger validation\n        self.algorithm_config = self.algorithm_config\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm name.\n\n        Returns\n        -------\n        str\n            Algorithm name.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_friendly_name()\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Return a description of the algorithm.\n\n        This method is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Description of the algorithm.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_description()\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_citations()\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_references()\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return self.algorithm_config.get_algorithm_keywords()\n\n    def model_dump(\n        self,\n        *,\n        mode: Literal[\"json\", \"python\"] | str = \"python\",\n        include: Any | None = None,\n        exclude: Any | None = None,\n        context: Any | None = None,\n        by_alias: bool = False,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = True,\n        round_trip: bool = False,\n        warnings: bool | Literal[\"none\", \"warn\", \"error\"] = True,\n        serialize_as_any: bool = False,\n    ) -&gt; dict:\n        \"\"\"\n        Override model_dump method in order to set default values.\n\n        As opposed to the parent model_dump method, this method sets exclude none by\n        default.\n\n        Parameters\n        ----------\n        mode : Literal['json', 'python'] | str, default='python'\n            The serialization format.\n        include : Any | None, default=None\n            Attributes to include.\n        exclude : Any | None, default=None\n            Attributes to exclude.\n        context : Any | None, default=None\n            Additional context to pass to the serialization functions.\n        by_alias : bool, default=False\n            Whether to use attribute aliases.\n        exclude_unset : bool, default=False\n            Whether to exclude fields that are not set.\n        exclude_defaults : bool, default=False\n            Whether to exclude fields that have default values.\n        exclude_none : bool, default=true\n            Whether to exclude fields that have None values.\n        round_trip : bool, default=False\n            Whether to dump and load the data to ensure that the output is a valid\n            representation.\n        warnings : bool | Literal['none', 'warn', 'error'], default=True\n            Whether to emit warnings.\n        serialize_as_any : bool, default=False\n            Whether to serialize all types as Any.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the model parameters.\n        \"\"\"\n        dictionary = super().model_dump(\n            mode=mode,\n            include=include,\n            exclude=exclude,\n            context=context,\n            by_alias=by_alias,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n            round_trip=round_trip,\n            warnings=warnings,\n            serialize_as_any=serialize_as_any,\n        )\n\n        return dictionary\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.algorithm_config","title":"<code>algorithm_config = Field(discriminator='algorithm')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Algorithm configuration, holding all parameters required to configure the model.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.data_config","title":"<code>data_config</code>  <code>instance-attribute</code>","text":"<p>Data configuration, holding all parameters required to configure the training data loader.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.experiment_name","title":"<code>experiment_name</code>  <code>instance-attribute</code>","text":"<p>Name of the experiment, used to name logs and checkpoints.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.training_config","title":"<code>training_config</code>  <code>instance-attribute</code>","text":"<p>Training configuration, holding all parameters required to configure the training process.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.version","title":"<code>version = '0.1.0'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CAREamics configuration version.</p>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_citations()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Return a description of the algorithm.</p> <p>This method is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the algorithm.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Return a description of the algorithm.\n\n    This method is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Description of the algorithm.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_description()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm name.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm name.\n\n    Returns\n    -------\n    str\n        Algorithm name.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_friendly_name()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_keywords()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return self.algorithm_config.get_algorithm_references()\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.model_dump","title":"<code>model_dump(*, mode='python', include=None, exclude=None, context=None, by_alias=False, exclude_unset=False, exclude_defaults=False, exclude_none=True, round_trip=False, warnings=True, serialize_as_any=False)</code>","text":"<p>Override model_dump method in order to set default values.</p> <p>As opposed to the parent model_dump method, this method sets exclude none by default.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['json', 'python'] | str</code> <p>The serialization format.</p> <code>'python'</code> <code>include</code> <code>Any | None</code> <p>Attributes to include.</p> <code>None</code> <code>exclude</code> <code>Any | None</code> <p>Attributes to exclude.</p> <code>None</code> <code>context</code> <code>Any | None</code> <p>Additional context to pass to the serialization functions.</p> <code>None</code> <code>by_alias</code> <code>bool</code> <p>Whether to use attribute aliases.</p> <code>False</code> <code>exclude_unset</code> <code>bool</code> <p>Whether to exclude fields that are not set.</p> <code>False</code> <code>exclude_defaults</code> <code>bool</code> <p>Whether to exclude fields that have default values.</p> <code>False</code> <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields that have None values.</p> <code>true</code> <code>round_trip</code> <code>bool</code> <p>Whether to dump and load the data to ensure that the output is a valid representation.</p> <code>False</code> <code>warnings</code> <code>bool | Literal['none', 'warn', 'error']</code> <p>Whether to emit warnings.</p> <code>True</code> <code>serialize_as_any</code> <code>bool</code> <p>Whether to serialize all types as Any.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the model parameters.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def model_dump(\n    self,\n    *,\n    mode: Literal[\"json\", \"python\"] | str = \"python\",\n    include: Any | None = None,\n    exclude: Any | None = None,\n    context: Any | None = None,\n    by_alias: bool = False,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = True,\n    round_trip: bool = False,\n    warnings: bool | Literal[\"none\", \"warn\", \"error\"] = True,\n    serialize_as_any: bool = False,\n) -&gt; dict:\n    \"\"\"\n    Override model_dump method in order to set default values.\n\n    As opposed to the parent model_dump method, this method sets exclude none by\n    default.\n\n    Parameters\n    ----------\n    mode : Literal['json', 'python'] | str, default='python'\n        The serialization format.\n    include : Any | None, default=None\n        Attributes to include.\n    exclude : Any | None, default=None\n        Attributes to exclude.\n    context : Any | None, default=None\n        Additional context to pass to the serialization functions.\n    by_alias : bool, default=False\n        Whether to use attribute aliases.\n    exclude_unset : bool, default=False\n        Whether to exclude fields that are not set.\n    exclude_defaults : bool, default=False\n        Whether to exclude fields that have default values.\n    exclude_none : bool, default=true\n        Whether to exclude fields that have None values.\n    round_trip : bool, default=False\n        Whether to dump and load the data to ensure that the output is a valid\n        representation.\n    warnings : bool | Literal['none', 'warn', 'error'], default=True\n        Whether to emit warnings.\n    serialize_as_any : bool, default=False\n        Whether to serialize all types as Any.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the model parameters.\n    \"\"\"\n    dictionary = super().model_dump(\n        mode=mode,\n        include=include,\n        exclude=exclude,\n        context=context,\n        by_alias=by_alias,\n        exclude_unset=exclude_unset,\n        exclude_defaults=exclude_defaults,\n        exclude_none=exclude_none,\n        round_trip=round_trip,\n        warnings=warnings,\n        serialize_as_any=serialize_as_any,\n    )\n\n    return dictionary\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.no_symbol","title":"<code>no_symbol(name)</code>  <code>classmethod</code>","text":"<p>Validate experiment name.</p> <p>A valid experiment name is a non-empty string with only contains letters, numbers, underscores, dashes and spaces.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name is empty or contains invalid characters.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>@field_validator(\"experiment_name\")\n@classmethod\ndef no_symbol(cls, name: str) -&gt; str:\n    \"\"\"\n    Validate experiment name.\n\n    A valid experiment name is a non-empty string with only contains letters,\n    numbers, underscores, dashes and spaces.\n\n    Parameters\n    ----------\n    name : str\n        Name to validate.\n\n    Returns\n    -------\n    str\n        Validated name.\n\n    Raises\n    ------\n    ValueError\n        If the name is empty or contains invalid characters.\n    \"\"\"\n    if len(name) == 0 or name.isspace():\n        raise ValueError(\"Experiment name is empty.\")\n\n    # Validate using a regex that it contains only letters, numbers, underscores,\n    # dashes and spaces\n    if not re.match(r\"^[a-zA-Z0-9_\\- ]*$\", name):\n        raise ValueError(\n            f\"Experiment name contains invalid characters (got {name}). \"\n            f\"Only letters, numbers, underscores, dashes and spaces are allowed.\"\n        )\n\n    return name\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.set_3D","title":"<code>set_3D(is_3D, axes, patch_size)</code>","text":"<p>Set 3D flag and axes.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list[int]</code> <p>Patch size.</p> required Source code in <code>src/careamics/config/configuration.py</code> <pre><code>def set_3D(self, is_3D: bool, axes: str, patch_size: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D flag and axes.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    axes : str\n        Axes of the data.\n    patch_size : list[int]\n        Patch size.\n    \"\"\"\n    # set the flag and axes (this will not trigger validation at the config level)\n    self.algorithm_config.model.set_3D(is_3D)\n    self.data_config.set_3D(axes, patch_size)\n\n    # cheap hack: trigger validation\n    self.algorithm_config = self.algorithm_config\n</code></pre>"},{"location":"reference/careamics/config/configuration/#careamics.config.configuration.Configuration.validate_3D","title":"<code>validate_3D()</code>","text":"<p>Change algorithm dimensions to match data.axes.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated configuration.</p> Source code in <code>src/careamics/config/configuration.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_3D(self: Self) -&gt; Self:\n    \"\"\"\n    Change algorithm dimensions to match data.axes.\n\n    Returns\n    -------\n    Self\n        Validated configuration.\n    \"\"\"\n    if \"Z\" in self.data_config.axes and not self.algorithm_config.model.is_3D():\n        # change algorithm to 3D\n        self.algorithm_config.model.set_3D(True)\n    elif \"Z\" not in self.data_config.axes and self.algorithm_config.model.is_3D():\n        # change algorithm to 2D\n        self.algorithm_config.model.set_3D(False)\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/","title":"configuration_factories","text":"<p>Convenience functions to create configurations for training and inference.</p>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._create_algorithm_configuration","title":"<code>_create_algorithm_configuration(axes, algorithm, loss, independent_channels, n_channels_in, n_channels_out, use_n2v2=False, model_params=None)</code>","text":"<p>Create a dictionary with the parameters of the algorithm model.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>algorithm</code> <code>(n2v, care, n2n)</code> <p>Algorithm to use.</p> <code>\"n2v\"</code> <code>loss</code> <code>(n2v, mae, mse)</code> <p>Loss function to use.</p> <code>\"n2v\"</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently.</p> required <code>n_channels_in</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels_out</code> <code>int</code> <p>Number of output channels.</p> required <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2, by default False.</p> <code>False</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Algorithm model as dictionnary with the specified parameters.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _create_algorithm_configuration(\n    axes: str,\n    algorithm: Literal[\"n2v\", \"care\", \"n2n\"],\n    loss: Literal[\"n2v\", \"mae\", \"mse\"],\n    independent_channels: bool,\n    n_channels_in: int,\n    n_channels_out: int,\n    use_n2v2: bool = False,\n    model_params: Optional[dict] = None,\n) -&gt; dict:\n    \"\"\"\n    Create a dictionary with the parameters of the algorithm model.\n\n    Parameters\n    ----------\n    axes : str\n        Axes of the data.\n    algorithm : {\"n2v\", \"care\", \"n2n\"}\n        Algorithm to use.\n    loss : {\"n2v\", \"mae\", \"mse\"}\n        Loss function to use.\n    independent_channels : bool\n        Whether to train all channels independently.\n    n_channels_in : int\n        Number of input channels.\n    n_channels_out : int\n        Number of output channels.\n    use_n2v2 : bool, optional\n        Whether to use N2V2, by default False.\n    model_params : dict\n        UNetModel parameters.\n\n    Returns\n    -------\n    dict\n        Algorithm model as dictionnary with the specified parameters.\n    \"\"\"\n    # model\n    unet_model = _create_unet_configuration(\n        axes=axes,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        independent_channels=independent_channels,\n        use_n2v2=use_n2v2,\n        model_params=model_params,\n    )\n\n    return {\n        \"algorithm\": algorithm,\n        \"loss\": loss,\n        \"model\": unet_model,\n    }\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._create_data_configuration","title":"<code>_create_data_configuration(data_type, axes, patch_size, batch_size, augmentations, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a dictionary with the parameters of the data model.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>(array, tiff, custom)</code> <p>Type of the data.</p> <code>\"array\"</code> <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list of int</code> <p>Size of the patches along the spatial dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply.</p> required <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see PyTorch notes, by default None.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch notes, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataConfig</code> <p>Data model with the specified parameters.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _create_data_configuration(\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: list[int],\n    batch_size: int,\n    augmentations: Union[list[SPATIAL_TRANSFORMS_UNION]],\n    train_dataloader_params: Optional[dict[str, Any]] = None,\n    val_dataloader_params: Optional[dict[str, Any]] = None,\n) -&gt; DataConfig:\n    \"\"\"\n    Create a dictionary with the parameters of the data model.\n\n    Parameters\n    ----------\n    data_type : {\"array\", \"tiff\", \"custom\"}\n        Type of the data.\n    axes : str\n        Axes of the data.\n    patch_size : list of int\n        Size of the patches along the spatial dimensions.\n    batch_size : int\n        Batch size.\n    augmentations : list of transforms\n        List of transforms to apply.\n    train_dataloader_params : dict\n        Parameters for the training dataloader, see PyTorch notes, by default None.\n    val_dataloader_params : dict\n        Parameters for the validation dataloader, see PyTorch notes, by default None.\n\n    Returns\n    -------\n    DataConfig\n        Data model with the specified parameters.\n    \"\"\"\n    # data model\n    data = {\n        \"data_type\": data_type,\n        \"axes\": axes,\n        \"patch_size\": patch_size,\n        \"batch_size\": batch_size,\n        \"transforms\": augmentations,\n    }\n    # Don't override defaults set in DataConfig class\n    if train_dataloader_params is not None:\n        # DataConfig enforces the presence of `shuffle` key in the dataloader parameters\n        if \"shuffle\" not in train_dataloader_params:\n            train_dataloader_params[\"shuffle\"] = True\n\n        data[\"train_dataloader_params\"] = train_dataloader_params\n\n    if val_dataloader_params is not None:\n        data[\"val_dataloader_params\"] = val_dataloader_params\n\n    return DataConfig(**data)\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._create_supervised_config_dict","title":"<code>_create_supervised_config_dict(algorithm, experiment_name, data_type, axes, patch_size, batch_size, num_epochs, augmentations=None, independent_channels=True, loss='mae', n_channels_in=None, n_channels_out=None, logger='none', model_params=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training CARE or Noise2Noise.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>Literal['care', 'n2n']</code> <p>Algorithm to use.</p> required <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently, by default False.</p> <code>True</code> <code>loss</code> <code>Literal['mae', 'mse']</code> <p>Loss function to use, by default \"mae\".</p> <code>'mae'</code> <code>n_channels_in</code> <code>int or None</code> <p>Number of channels in.</p> <code>None</code> <code>n_channels_out</code> <code>int or None</code> <p>Number of channels out.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use, by default \"none\".</p> <code>'none'</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters, by default {}.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see PyTorch notes, by default None.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch notes, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training CARE or Noise2Noise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is not specified when using channels.</p> <code>ValueError</code> <p>If the number of channels is specified but \"C\" is not in the axes.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _create_supervised_config_dict(\n    algorithm: Literal[\"care\", \"n2n\"],\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: list[int],\n    batch_size: int,\n    num_epochs: int,\n    augmentations: Optional[list[SPATIAL_TRANSFORMS_UNION]] = None,\n    independent_channels: bool = True,\n    loss: Literal[\"mae\", \"mse\"] = \"mae\",\n    n_channels_in: Optional[int] = None,\n    n_channels_out: Optional[int] = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    model_params: Optional[dict] = None,\n    train_dataloader_params: Optional[dict[str, Any]] = None,\n    val_dataloader_params: Optional[dict[str, Any]] = None,\n) -&gt; dict:\n    \"\"\"\n    Create a configuration for training CARE or Noise2Noise.\n\n    Parameters\n    ----------\n    algorithm : Literal[\"care\", \"n2n\"]\n        Algorithm to use.\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int\n        Number of epochs.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels independently, by default False.\n    loss : Literal[\"mae\", \"mse\"], optional\n        Loss function to use, by default \"mae\".\n    n_channels_in : int or None, default=None\n        Number of channels in.\n    n_channels_out : int or None, default=None\n        Number of channels out.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use, by default \"none\".\n    model_params : dict, optional\n        UNetModel parameters, by default {}.\n    train_dataloader_params : dict\n        Parameters for the training dataloader, see PyTorch notes, by default None.\n    val_dataloader_params : dict\n        Parameters for the validation dataloader, see PyTorch notes, by default None.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training CARE or Noise2Noise.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is not specified when using channels.\n    ValueError\n        If the number of channels is specified but \"C\" is not in the axes.\n    \"\"\"\n    # if there are channels, we need to specify their number\n    if \"C\" in axes and n_channels_in is None:\n        raise ValueError(\"Number of channels in must be specified when using channels \")\n    elif \"C\" not in axes and (n_channels_in is not None and n_channels_in &gt; 1):\n        raise ValueError(\n            f\"C is not present in the axes, but number of channels is specified \"\n            f\"(got {n_channels_in} channels).\"\n        )\n\n    if n_channels_in is None:\n        n_channels_in = 1\n\n    if n_channels_out is None:\n        n_channels_out = n_channels_in\n\n    # augmentations\n    spatial_transform_list = _list_spatial_augmentations(augmentations)\n\n    # algorithm\n    algorithm_params = _create_algorithm_configuration(\n        axes=axes,\n        algorithm=algorithm,\n        loss=loss,\n        independent_channels=independent_channels,\n        n_channels_in=n_channels_in,\n        n_channels_out=n_channels_out,\n        model_params=model_params,\n    )\n\n    # data\n    data_params = _create_data_configuration(\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        augmentations=spatial_transform_list,\n        train_dataloader_params=train_dataloader_params,\n        val_dataloader_params=val_dataloader_params,\n    )\n\n    # training\n    training_params = _create_training_configuration(\n        num_epochs=num_epochs,\n        logger=logger,\n    )\n\n    return {\n        \"experiment_name\": experiment_name,\n        \"algorithm_config\": algorithm_params,\n        \"data_config\": data_params,\n        \"training_config\": training_params,\n    }\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._create_training_configuration","title":"<code>_create_training_configuration(num_epochs, logger)</code>","text":"<p>Create a dictionary with the parameters of the training model.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>logger</code> <code>(wandb, tensorboard, none)</code> <p>Logger to use.</p> <code>\"wandb\"</code> <p>Returns:</p> Type Description <code>TrainingConfig</code> <p>Training model with the specified parameters.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _create_training_configuration(\n    num_epochs: int, logger: Literal[\"wandb\", \"tensorboard\", \"none\"]\n) -&gt; TrainingConfig:\n    \"\"\"\n    Create a dictionary with the parameters of the training model.\n\n    Parameters\n    ----------\n    num_epochs : int\n        Number of epochs.\n    logger : {\"wandb\", \"tensorboard\", \"none\"}\n        Logger to use.\n\n    Returns\n    -------\n    TrainingConfig\n        Training model with the specified parameters.\n    \"\"\"\n    return TrainingConfig(\n        num_epochs=num_epochs,\n        logger=None if logger == \"none\" else logger,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._create_unet_configuration","title":"<code>_create_unet_configuration(axes, n_channels_in, n_channels_out, independent_channels, use_n2v2, model_params=None)</code>","text":"<p>Create a dictionary with the parameters of the UNet model.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>n_channels_in</code> <code>int</code> <p>Number of input channels.</p> required <code>n_channels_out</code> <code>int</code> <p>Number of output channels.</p> required <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently.</p> required <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2.</p> required <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>UNetModel</code> <p>UNet model with the specified parameters.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _create_unet_configuration(\n    axes: str,\n    n_channels_in: int,\n    n_channels_out: int,\n    independent_channels: bool,\n    use_n2v2: bool,\n    model_params: Optional[dict[str, Any]] = None,\n) -&gt; UNetModel:\n    \"\"\"\n    Create a dictionary with the parameters of the UNet model.\n\n    Parameters\n    ----------\n    axes : str\n        Axes of the data.\n    n_channels_in : int\n        Number of input channels.\n    n_channels_out : int\n        Number of output channels.\n    independent_channels : bool\n        Whether to train all channels independently.\n    use_n2v2 : bool\n        Whether to use N2V2.\n    model_params : dict\n        UNetModel parameters.\n\n    Returns\n    -------\n    UNetModel\n        UNet model with the specified parameters.\n    \"\"\"\n    if model_params is None:\n        model_params = {}\n\n    model_params[\"n2v2\"] = use_n2v2\n    model_params[\"conv_dims\"] = 3 if \"Z\" in axes else 2\n    model_params[\"in_channels\"] = n_channels_in\n    model_params[\"num_classes\"] = n_channels_out\n    model_params[\"independent_channels\"] = independent_channels\n\n    return UNetModel(\n        architecture=SupportedArchitecture.UNET.value,\n        **model_params,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories._list_spatial_augmentations","title":"<code>_list_spatial_augmentations(augmentations)</code>","text":"<p>List the augmentations to apply.</p> <p>Parameters:</p> Name Type Description Default <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model.</p> required <p>Returns:</p> Type Description <code>list of transforms</code> <p>List of transforms to apply.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transforms are not XYFlipModel or XYRandomRotate90Model.</p> <code>ValueError</code> <p>If there are duplicate transforms.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def _list_spatial_augmentations(\n    augmentations: Optional[list[SPATIAL_TRANSFORMS_UNION]],\n) -&gt; list[SPATIAL_TRANSFORMS_UNION]:\n    \"\"\"\n    List the augmentations to apply.\n\n    Parameters\n    ----------\n    augmentations : list of transforms, optional\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model.\n\n    Returns\n    -------\n    list of transforms\n        List of transforms to apply.\n\n    Raises\n    ------\n    ValueError\n        If the transforms are not XYFlipModel or XYRandomRotate90Model.\n    ValueError\n        If there are duplicate transforms.\n    \"\"\"\n    if augmentations is None:\n        transform_list: list[SPATIAL_TRANSFORMS_UNION] = [\n            XYFlipModel(),\n            XYRandomRotate90Model(),\n        ]\n    else:\n        # throw error if not all transforms are pydantic models\n        if not all(\n            isinstance(t, XYFlipModel) or isinstance(t, XYRandomRotate90Model)\n            for t in augmentations\n        ):\n            raise ValueError(\n                \"Accepted transforms are either XYFlipModel or \"\n                \"XYRandomRotate90Model.\"\n            )\n\n        # check that there is no duplication\n        aug_types = [t.__class__ for t in augmentations]\n        if len(set(aug_types)) != len(aug_types):\n            raise ValueError(\"Duplicate transforms are not allowed.\")\n\n        transform_list = augmentations\n\n    return transform_list\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.algorithm_factory","title":"<code>algorithm_factory(algorithm)</code>","text":"<p>Create an algorithm model for training CAREamics.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>dict</code> <p>Algorithm dictionary.</p> required <p>Returns:</p> Type Description <code>N2VAlgorithm or N2NAlgorithm or CAREAlgorithm</code> <p>Algorithm model for training CAREamics.</p> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def algorithm_factory(\n    algorithm: dict[str, Any]\n) -&gt; Union[N2VAlgorithm, N2NAlgorithm, CAREAlgorithm]:\n    \"\"\"\n    Create an algorithm model for training CAREamics.\n\n    Parameters\n    ----------\n    algorithm : dict\n        Algorithm dictionary.\n\n    Returns\n    -------\n    N2VAlgorithm or N2NAlgorithm or CAREAlgorithm\n        Algorithm model for training CAREamics.\n    \"\"\"\n    adapter: TypeAdapter = TypeAdapter(\n        Annotated[\n            Union[N2VAlgorithm, N2NAlgorithm, CAREAlgorithm],\n            Field(discriminator=\"algorithm\"),\n        ]\n    )\n    return adapter.validate_python(algorithm)\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_care_configuration","title":"<code>create_care_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs, augmentations=None, independent_channels=True, loss='mae', n_channels_in=None, n_channels_out=None, logger='none', model_params=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training CARE.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently, by default False.</p> <code>True</code> <code>loss</code> <code>Literal['mae', 'mse']</code> <p>Loss function to use.</p> <code>\"mae\"</code> <code>n_channels_in</code> <code>int or None</code> <p>Number of channels in.</p> <code>None</code> <code>n_channels_out</code> <code>int or None</code> <p>Number of channels out.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use.</p> <code>\"none\"</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training CARE.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter to replace the default augmentations:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels input (and optionally, the number of channels output):</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=3, # number of input channels\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_care_configuration(\n...     experiment_name=\"care_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels_in=3,\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_care_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: list[int],\n    batch_size: int,\n    num_epochs: int,\n    augmentations: Optional[list[Union[XYFlipModel, XYRandomRotate90Model]]] = None,\n    independent_channels: bool = True,\n    loss: Literal[\"mae\", \"mse\"] = \"mae\",\n    n_channels_in: Optional[int] = None,\n    n_channels_out: Optional[int] = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    model_params: Optional[dict] = None,\n    train_dataloader_params: Optional[dict[str, Any]] = None,\n    val_dataloader_params: Optional[dict[str, Any]] = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training CARE.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int\n        Number of epochs.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels independently, by default False.\n    loss : Literal[\"mae\", \"mse\"], default=\"mae\"\n        Loss function to use.\n    n_channels_in : int or None, default=None\n        Number of channels in.\n    n_channels_out : int or None, default=None\n        Number of channels out.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], default=\"none\"\n        Logger to use.\n    model_params : dict, default=None\n        UNetModel parameters.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training CARE.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter to replace the\n    default augmentations:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels input (and optionally, the number\n    of channels output):\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=3, # number of input channels\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_care_configuration(\n    ...     experiment_name=\"care_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels_in=3,\n    ...     n_channels_out=1 # if applicable\n    ... )\n    \"\"\"\n    return Configuration(\n        **_create_supervised_config_dict(\n            algorithm=\"care\",\n            experiment_name=experiment_name,\n            data_type=data_type,\n            axes=axes,\n            patch_size=patch_size,\n            batch_size=batch_size,\n            num_epochs=num_epochs,\n            augmentations=augmentations,\n            independent_channels=independent_channels,\n            loss=loss,\n            n_channels_in=n_channels_in,\n            n_channels_out=n_channels_out,\n            logger=logger,\n            model_params=model_params,\n            train_dataloader_params=train_dataloader_params,\n            val_dataloader_params=val_dataloader_params,\n        )\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_n2n_configuration","title":"<code>create_n2n_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs, augmentations=None, independent_channels=True, loss='mae', n_channels_in=None, n_channels_out=None, logger='none', model_params=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training Noise2Noise.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels_in</code> to the number of channels. Likewise, if you set the number of channels, then \"C\" must be present in <code>axes</code>.</p> <p>To set the number of output channels, use the <code>n_channels_out</code> parameter. If it is not specified, it will be assumed to be equal to <code>n_channels_in</code>.</p> <p>By default, all channels are trained together. To train all channels independently, set <code>independent_channels</code> to True.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels independently, by default False.</p> <code>True</code> <code>loss</code> <code>Literal['mae', 'mse']</code> <p>Loss function to use, by default \"mae\".</p> <code>'mae'</code> <code>n_channels_in</code> <code>int or None</code> <p>Number of channels in.</p> <code>None</code> <code>n_channels_out</code> <code>int or None</code> <p>Number of channels out.</p> <code>None</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use, by default \"none\".</p> <code>'none'</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters, by default {}.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training Noise2Noise.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter to replace the default augmentations:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels input (and optionally, the number of channels output):</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels_in=3, # number of input channels\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2n_configuration(\n...     experiment_name=\"n2n_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\", # channels must be in the axes\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels_in=3,\n...     n_channels_out=1 # if applicable\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_n2n_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: list[int],\n    batch_size: int,\n    num_epochs: int,\n    augmentations: Optional[list[Union[XYFlipModel, XYRandomRotate90Model]]] = None,\n    independent_channels: bool = True,\n    loss: Literal[\"mae\", \"mse\"] = \"mae\",\n    n_channels_in: Optional[int] = None,\n    n_channels_out: Optional[int] = None,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    model_params: Optional[dict] = None,\n    train_dataloader_params: Optional[dict[str, Any]] = None,\n    val_dataloader_params: Optional[dict[str, Any]] = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training Noise2Noise.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels_in` to the number of\n    channels. Likewise, if you set the number of channels, then \"C\" must be present in\n    `axes`.\n\n    To set the number of output channels, use the `n_channels_out` parameter. If it is\n    not specified, it will be assumed to be equal to `n_channels_in`.\n\n    By default, all channels are trained together. To train all channels independently,\n    set `independent_channels` to True.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int\n        Number of epochs.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels independently, by default False.\n    loss : Literal[\"mae\", \"mse\"], optional\n        Loss function to use, by default \"mae\".\n    n_channels_in : int or None, default=None\n        Number of channels in.\n    n_channels_out : int or None, default=None\n        Number of channels out.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use, by default \"none\".\n    model_params : dict, optional\n        UNetModel parameters, by default {}.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training Noise2Noise.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter to replace the\n    default augmentations:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels input (and optionally, the number\n    of channels output):\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels_in=3, # number of input channels\n    ...     n_channels_out=1 # if applicable\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_n2n_configuration(\n    ...     experiment_name=\"n2n_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\", # channels must be in the axes\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels_in=3,\n    ...     n_channels_out=1 # if applicable\n    ... )\n    \"\"\"\n    return Configuration(\n        **_create_supervised_config_dict(\n            algorithm=\"n2n\",\n            experiment_name=experiment_name,\n            data_type=data_type,\n            axes=axes,\n            patch_size=patch_size,\n            batch_size=batch_size,\n            num_epochs=num_epochs,\n            augmentations=augmentations,\n            independent_channels=independent_channels,\n            loss=loss,\n            n_channels_in=n_channels_in,\n            n_channels_out=n_channels_out,\n            logger=logger,\n            model_params=model_params,\n            train_dataloader_params=train_dataloader_params,\n            val_dataloader_params=val_dataloader_params,\n        )\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_factories/#careamics.config.configuration_factories.create_n2v_configuration","title":"<code>create_n2v_configuration(experiment_name, data_type, axes, patch_size, batch_size, num_epochs, augmentations=None, independent_channels=True, use_n2v2=False, n_channels=None, roi_size=11, masked_pixel_percentage=0.2, struct_n2v_axis='none', struct_n2v_span=5, logger='none', model_params=None, train_dataloader_params=None, val_dataloader_params=None)</code>","text":"<p>Create a configuration for training Noise2Void.</p> <p>N2V uses a UNet model to denoise images in a self-supervised manner. To use its variants structN2V and N2V2, set the <code>struct_n2v_axis</code> and <code>struct_n2v_span</code> (structN2V) parameters, or set <code>use_n2v2</code> to True (N2V2).</p> <p>N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip connections, thus removing checkboard artefacts. StructN2V is used when vertical or horizontal correlations are present in the noise; it applies an additional mask to the manipulated pixel neighbors.</p> <p>If \"Z\" is present in <code>axes</code>, then <code>path_size</code> must be a list of length 3, otherwise 2.</p> <p>If \"C\" is present in <code>axes</code>, then you need to set <code>n_channels</code> to the number of channels.</p> <p>By default, all channels are trained independently. To train all channels together, set <code>independent_channels</code> to False.</p> <p>By default, the transformations applied are a random flip along X or Y, and a random 90 degrees rotation in the XY plane. Normalization is always applied, as well as the N2V manipulation.</p> <p>By setting <code>augmentations</code> to <code>None</code>, the default transformations (flip in X and Y, rotations by 90 degrees in the XY plane) are applied. Rather than the default transforms, a list of transforms can be passed to the <code>augmentations</code> parameter. To disable the transforms, simply pass an empty list.</p> <p>The <code>roi_size</code> parameter specifies the size of the area around each pixel that will be manipulated by N2V. The <code>masked_pixel_percentage</code> parameter specifies how many pixels per patch will be manipulated.</p> <p>The parameters of the UNet can be specified in the <code>model_params</code> (passed as a parameter-value dictionary). Note that <code>use_n2v2</code> and 'n_channels' override the corresponding parameters passed in <code>model_params</code>.</p> <p>If you pass \"horizontal\" or \"vertical\" to <code>struct_n2v_axis</code>, then structN2V mask will be applied to each manipulated pixel.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>data_type</code> <code>Literal['array', 'tiff', 'custom']</code> <p>Type of the data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data (e.g. SYX).</p> required <code>patch_size</code> <code>List[int]</code> <p>Size of the patches along the spatial dimensions (e.g. [64, 64]).</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs.</p> required <code>augmentations</code> <code>list of transforms</code> <p>List of transforms to apply, either both or one of XYFlipModel and XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y) and XYRandomRotate90 (in XY) to the images.</p> <code>None</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train all channels together, by default True.</p> <code>True</code> <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2, by default False.</p> <code>False</code> <code>n_channels</code> <code>int or None</code> <p>Number of channels (in and out).</p> <code>None</code> <code>roi_size</code> <code>int</code> <p>N2V pixel manipulation area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels masked in each patch, by default 0.2.</p> <code>0.2</code> <code>struct_n2v_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>Axis along which to apply structN2V mask, by default \"none\".</p> <code>'none'</code> <code>struct_n2v_span</code> <code>int</code> <p>Span of the structN2V mask, by default 5.</p> <code>5</code> <code>logger</code> <code>Literal['wandb', 'tensorboard', 'none']</code> <p>Logger to use, by default \"none\".</p> <code>'none'</code> <code>model_params</code> <code>dict</code> <p>UNetModel parameters, by default None.</p> <code>None</code> <code>train_dataloader_params</code> <code>dict</code> <p>Parameters for the training dataloader, see the PyTorch docs for <code>DataLoader</code>. If left as <code>None</code>, the dict <code>{\"shuffle\": True}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <code>val_dataloader_params</code> <code>dict</code> <p>Parameters for the validation dataloader, see PyTorch the docs for <code>DataLoader</code>. If left as <code>None</code>, the empty dict <code>{}</code> will be used, this is set in the <code>GeneralDataConfig</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration for training N2V.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100\n... )\n</code></pre> <p>To disable transforms, simply set <code>augmentations</code> to an empty list:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[]\n... )\n</code></pre> <p>A list of transforms can be passed to the <code>augmentations</code> parameter:</p> <pre><code>&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     augmentations=[\n...         # No rotation and only Y flipping\n...         XYFlipModel(flip_x = False, flip_y = True)\n...     ]\n... )\n</code></pre> <p>To use N2V2, simply pass the <code>use_n2v2</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v2_experiment\",\n...     data_type=\"tiff\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     use_n2v2=True\n... )\n</code></pre> <p>For structN2V, there are two parameters to set, <code>struct_n2v_axis</code> and <code>struct_n2v_span</code>:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"structn2v_experiment\",\n...     data_type=\"tiff\",\n...     axes=\"YX\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     struct_n2v_axis=\"horizontal\",\n...     struct_n2v_span=7\n... )\n</code></pre> <p>If you are training multiple channels they will be trained independently by default, you simply need to specify the number of channels:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     n_channels=3\n... )\n</code></pre> <p>If instead you want to train multiple channels together, you need to turn off the <code>independent_channels</code> parameter:</p> <pre><code>&gt;&gt;&gt; config = create_n2v_configuration(\n...     experiment_name=\"n2v_experiment\",\n...     data_type=\"array\",\n...     axes=\"YXC\",\n...     patch_size=[64, 64],\n...     batch_size=32,\n...     num_epochs=100,\n...     independent_channels=False,\n...     n_channels=3\n... )\n</code></pre> Source code in <code>src/careamics/config/configuration_factories.py</code> <pre><code>def create_n2v_configuration(\n    experiment_name: str,\n    data_type: Literal[\"array\", \"tiff\", \"custom\"],\n    axes: str,\n    patch_size: list[int],\n    batch_size: int,\n    num_epochs: int,\n    augmentations: Optional[list[Union[XYFlipModel, XYRandomRotate90Model]]] = None,\n    independent_channels: bool = True,\n    use_n2v2: bool = False,\n    n_channels: Optional[int] = None,\n    roi_size: int = 11,\n    masked_pixel_percentage: float = 0.2,\n    struct_n2v_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_n2v_span: int = 5,\n    logger: Literal[\"wandb\", \"tensorboard\", \"none\"] = \"none\",\n    model_params: Optional[dict] = None,\n    train_dataloader_params: Optional[dict[str, Any]] = None,\n    val_dataloader_params: Optional[dict[str, Any]] = None,\n) -&gt; Configuration:\n    \"\"\"\n    Create a configuration for training Noise2Void.\n\n    N2V uses a UNet model to denoise images in a self-supervised manner. To use its\n    variants structN2V and N2V2, set the `struct_n2v_axis` and `struct_n2v_span`\n    (structN2V) parameters, or set `use_n2v2` to True (N2V2).\n\n    N2V2 modifies the UNet architecture by adding blur pool layers and removes the skip\n    connections, thus removing checkboard artefacts. StructN2V is used when vertical\n    or horizontal correlations are present in the noise; it applies an additional mask\n    to the manipulated pixel neighbors.\n\n    If \"Z\" is present in `axes`, then `path_size` must be a list of length 3, otherwise\n    2.\n\n    If \"C\" is present in `axes`, then you need to set `n_channels` to the number of\n    channels.\n\n    By default, all channels are trained independently. To train all channels together,\n    set `independent_channels` to False.\n\n    By default, the transformations applied are a random flip along X or Y, and a random\n    90 degrees rotation in the XY plane. Normalization is always applied, as well as the\n    N2V manipulation.\n\n    By setting `augmentations` to `None`, the default transformations (flip in X and Y,\n    rotations by 90 degrees in the XY plane) are applied. Rather than the default\n    transforms, a list of transforms can be passed to the `augmentations` parameter. To\n    disable the transforms, simply pass an empty list.\n\n    The `roi_size` parameter specifies the size of the area around each pixel that will\n    be manipulated by N2V. The `masked_pixel_percentage` parameter specifies how many\n    pixels per patch will be manipulated.\n\n    The parameters of the UNet can be specified in the `model_params` (passed as a\n    parameter-value dictionary). Note that `use_n2v2` and 'n_channels' override the\n    corresponding parameters passed in `model_params`.\n\n    If you pass \"horizontal\" or \"vertical\" to `struct_n2v_axis`, then structN2V mask\n    will be applied to each manipulated pixel.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    data_type : Literal[\"array\", \"tiff\", \"custom\"]\n        Type of the data.\n    axes : str\n        Axes of the data (e.g. SYX).\n    patch_size : List[int]\n        Size of the patches along the spatial dimensions (e.g. [64, 64]).\n    batch_size : int\n        Batch size.\n    num_epochs : int\n        Number of epochs.\n    augmentations : list of transforms, default=None\n        List of transforms to apply, either both or one of XYFlipModel and\n        XYRandomRotate90Model. By default, it applies both XYFlip (on X and Y)\n        and XYRandomRotate90 (in XY) to the images.\n    independent_channels : bool, optional\n        Whether to train all channels together, by default True.\n    use_n2v2 : bool, optional\n        Whether to use N2V2, by default False.\n    n_channels : int or None, default=None\n        Number of channels (in and out).\n    roi_size : int, optional\n        N2V pixel manipulation area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels masked in each patch, by default 0.2.\n    struct_n2v_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        Axis along which to apply structN2V mask, by default \"none\".\n    struct_n2v_span : int, optional\n        Span of the structN2V mask, by default 5.\n    logger : Literal[\"wandb\", \"tensorboard\", \"none\"], optional\n        Logger to use, by default \"none\".\n    model_params : dict, optional\n        UNetModel parameters, by default None.\n    train_dataloader_params : dict, optional\n        Parameters for the training dataloader, see the PyTorch docs for `DataLoader`.\n        If left as `None`, the dict `{\"shuffle\": True}` will be used, this is set in\n        the `GeneralDataConfig`.\n    val_dataloader_params : dict, optional\n        Parameters for the validation dataloader, see PyTorch the docs for `DataLoader`.\n        If left as `None`, the empty dict `{}` will be used, this is set in the\n        `GeneralDataConfig`.\n\n    Returns\n    -------\n    Configuration\n        Configuration for training N2V.\n\n    Examples\n    --------\n    Minimum example:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100\n    ... )\n\n    To disable transforms, simply set `augmentations` to an empty list:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[]\n    ... )\n\n    A list of transforms can be passed to the `augmentations` parameter:\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     augmentations=[\n    ...         # No rotation and only Y flipping\n    ...         XYFlipModel(flip_x = False, flip_y = True)\n    ...     ]\n    ... )\n\n    To use N2V2, simply pass the `use_n2v2` parameter:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v2_experiment\",\n    ...     data_type=\"tiff\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     use_n2v2=True\n    ... )\n\n    For structN2V, there are two parameters to set, `struct_n2v_axis` and\n    `struct_n2v_span`:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"structn2v_experiment\",\n    ...     data_type=\"tiff\",\n    ...     axes=\"YX\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     struct_n2v_axis=\"horizontal\",\n    ...     struct_n2v_span=7\n    ... )\n\n    If you are training multiple channels they will be trained independently by default,\n    you simply need to specify the number of channels:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     n_channels=3\n    ... )\n\n    If instead you want to train multiple channels together, you need to turn off the\n    `independent_channels` parameter:\n    &gt;&gt;&gt; config = create_n2v_configuration(\n    ...     experiment_name=\"n2v_experiment\",\n    ...     data_type=\"array\",\n    ...     axes=\"YXC\",\n    ...     patch_size=[64, 64],\n    ...     batch_size=32,\n    ...     num_epochs=100,\n    ...     independent_channels=False,\n    ...     n_channels=3\n    ... )\n    \"\"\"\n    # if there are channels, we need to specify their number\n    if \"C\" in axes and n_channels is None:\n        raise ValueError(\"Number of channels must be specified when using channels.\")\n    elif \"C\" not in axes and (n_channels is not None and n_channels &gt; 1):\n        raise ValueError(\n            f\"C is not present in the axes, but number of channels is specified \"\n            f\"(got {n_channels} channel).\"\n        )\n\n    if n_channels is None:\n        n_channels = 1\n\n    # augmentations\n    spatial_transforms = _list_spatial_augmentations(augmentations)\n\n    # create the N2VManipulate transform using the supplied parameters\n    n2v_transform = N2VManipulateModel(\n        name=SupportedTransform.N2V_MANIPULATE.value,\n        strategy=(\n            SupportedPixelManipulation.MEDIAN.value\n            if use_n2v2\n            else SupportedPixelManipulation.UNIFORM.value\n        ),\n        roi_size=roi_size,\n        masked_pixel_percentage=masked_pixel_percentage,\n        struct_mask_axis=struct_n2v_axis,\n        struct_mask_span=struct_n2v_span,\n    )\n\n    # algorithm\n    algorithm_params = _create_algorithm_configuration(\n        axes=axes,\n        algorithm=\"n2v\",\n        loss=\"n2v\",\n        independent_channels=independent_channels,\n        n_channels_in=n_channels,\n        n_channels_out=n_channels,\n        use_n2v2=use_n2v2,\n        model_params=model_params,\n    )\n    algorithm_params[\"n2v_config\"] = n2v_transform\n\n    # data\n    data_params = _create_data_configuration(\n        data_type=data_type,\n        axes=axes,\n        patch_size=patch_size,\n        batch_size=batch_size,\n        augmentations=spatial_transforms,\n        train_dataloader_params=train_dataloader_params,\n        val_dataloader_params=val_dataloader_params,\n    )\n\n    # training\n    training_params = _create_training_configuration(\n        num_epochs=num_epochs,\n        logger=logger,\n    )\n\n    return Configuration(\n        experiment_name=experiment_name,\n        algorithm_config=algorithm_params,\n        data_config=data_params,\n        training_config=training_params,\n    )\n</code></pre>"},{"location":"reference/careamics/config/configuration_io/","title":"configuration_io","text":"<p>I/O functions for Configuration objects.</p>"},{"location":"reference/careamics/config/configuration_io/#careamics.config.configuration_io.load_configuration","title":"<code>load_configuration(path)</code>","text":"<p>Load configuration from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to the configuration.</p> required <p>Returns:</p> Type Description <code>Configuration</code> <p>Configuration.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file does not exist.</p> Source code in <code>src/careamics/config/configuration_io.py</code> <pre><code>def load_configuration(path: Union[str, Path]) -&gt; Configuration:\n    \"\"\"\n    Load configuration from a yaml file.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to the configuration.\n\n    Returns\n    -------\n    Configuration\n        Configuration.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the configuration file does not exist.\n    \"\"\"\n    # load dictionary from yaml\n    if not Path(path).exists():\n        raise FileNotFoundError(\n            f\"Configuration file {path} does not exist in \" f\" {Path.cwd()!s}\"\n        )\n\n    dictionary = yaml.load(Path(path).open(\"r\"), Loader=yaml.SafeLoader)\n\n    return Configuration(**dictionary)\n</code></pre>"},{"location":"reference/careamics/config/configuration_io/#careamics.config.configuration_io.save_configuration","title":"<code>save_configuration(config, path)</code>","text":"<p>Save configuration to path.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>Configuration to save.</p> required <code>path</code> <code>str or Path</code> <p>Path to a existing folder in which to save the configuration, or to a valid configuration file path (uses a .yml or .yaml extension).</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path object representing the configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path does not point to an existing directory or .yml file.</p> Source code in <code>src/careamics/config/configuration_io.py</code> <pre><code>def save_configuration(config: Configuration, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save configuration to path.\n\n    Parameters\n    ----------\n    config : Configuration\n        Configuration to save.\n    path : str or Path\n        Path to a existing folder in which to save the configuration, or to a valid\n        configuration file path (uses a .yml or .yaml extension).\n\n    Returns\n    -------\n    Path\n        Path object representing the configuration.\n\n    Raises\n    ------\n    ValueError\n        If the path does not point to an existing directory or .yml file.\n    \"\"\"\n    # make sure path is a Path object\n    config_path = Path(path)\n\n    # check if path is pointing to an existing directory or .yml file\n    if config_path.exists():\n        if config_path.is_dir():\n            config_path = Path(config_path, \"config.yml\")\n        elif config_path.suffix != \".yml\" and config_path.suffix != \".yaml\":\n            raise ValueError(\n                f\"Path must be a directory or .yml or .yaml file (got {config_path}).\"\n            )\n    else:\n        if config_path.suffix != \".yml\" and config_path.suffix != \".yaml\":\n            raise ValueError(\n                f\"Path must be a directory or .yml or .yaml file (got {config_path}).\"\n            )\n\n    # save configuration as dictionary to yaml\n    with open(config_path, \"w\") as f:\n        # dump configuration\n        yaml.dump(config.model_dump(), f, default_flow_style=False, sort_keys=False)\n\n    return config_path\n</code></pre>"},{"location":"reference/careamics/config/inference_model/","title":"inference_model","text":"<p>Pydantic model representing CAREamics prediction configuration.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig","title":"<code>InferenceConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration class for the prediction model.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>class InferenceConfig(BaseModel):\n    \"\"\"Configuration class for the prediction model.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, arbitrary_types_allowed=True)\n\n    data_type: Literal[\"array\", \"tiff\", \"custom\"]  # As defined in SupportedData\n    \"\"\"Type of input data: numpy.ndarray (array) or path (tiff or custom).\"\"\"\n\n    tile_size: Optional[Union[list[int]]] = Field(\n        default=None, min_length=2, max_length=3\n    )\n    \"\"\"Tile size of prediction, only effective if `tile_overlap` is specified.\"\"\"\n\n    tile_overlap: Optional[Union[list[int]]] = Field(\n        default=None, min_length=2, max_length=3\n    )\n    \"\"\"Overlap between tiles, only effective if `tile_size` is specified.\"\"\"\n\n    axes: str\n    \"\"\"Data axes (TSCZYX) in the order of the input data.\"\"\"\n\n    image_means: list = Field(..., min_length=0, max_length=32)\n    \"\"\"Mean values for each input channel.\"\"\"\n\n    image_stds: list = Field(..., min_length=0, max_length=32)\n    \"\"\"Standard deviation values for each input channel.\"\"\"\n\n    # TODO only default TTAs are supported for now\n    tta_transforms: bool = Field(default=True)\n    \"\"\"Whether to apply test-time augmentation (all 90 degrees rotations and flips).\"\"\"\n\n    # Dataloader parameters\n    batch_size: int = Field(default=1, ge=1)\n    \"\"\"Batch size for prediction.\"\"\"\n\n    @field_validator(\"tile_overlap\")\n    @classmethod\n    def all_elements_non_zero_even(\n        cls, tile_overlap: Optional[list[int]]\n    ) -&gt; Optional[list[int]]:\n        \"\"\"\n        Validate tile overlap.\n\n        Overlaps must be non-zero, positive and even.\n\n        Parameters\n        ----------\n        tile_overlap : list[int] or None\n            Patch size.\n\n        Returns\n        -------\n        list[int] or None\n            Validated tile overlap.\n\n        Raises\n        ------\n        ValueError\n            If the patch size is 0.\n        ValueError\n            If the patch size is not even.\n        \"\"\"\n        if tile_overlap is not None:\n            for dim in tile_overlap:\n                if dim &lt; 1:\n                    raise ValueError(\n                        f\"Patch size must be non-zero positive (got {dim}).\"\n                    )\n\n                if dim % 2 != 0:\n                    raise ValueError(f\"Patch size must be even (got {dim}).\")\n\n        return tile_overlap\n\n    @field_validator(\"tile_size\")\n    @classmethod\n    def tile_min_8_power_of_2(\n        cls, tile_list: Optional[list[int]]\n    ) -&gt; Optional[list[int]]:\n        \"\"\"\n        Validate that each entry is greater or equal than 8 and a power of 2.\n\n        Parameters\n        ----------\n        tile_list : list of int\n            Patch size.\n\n        Returns\n        -------\n        list of int\n            Validated patch size.\n\n        Raises\n        ------\n        ValueError\n            If the patch size if smaller than 8.\n        ValueError\n            If the patch size is not a power of 2.\n        \"\"\"\n        patch_size_ge_than_8_power_of_2(tile_list)\n\n        return tile_list\n\n    @field_validator(\"axes\")\n    @classmethod\n    def axes_valid(cls, axes: str) -&gt; str:\n        \"\"\"\n        Validate axes.\n\n        Axes must:\n        - be a combination of 'STCZYX'\n        - not contain duplicates\n        - contain at least 2 contiguous axes: X and Y\n        - contain at most 4 axes\n        - not contain both S and T axes\n\n        Parameters\n        ----------\n        axes : str\n            Axes to validate.\n\n        Returns\n        -------\n        str\n            Validated axes.\n\n        Raises\n        ------\n        ValueError\n            If axes are not valid.\n        \"\"\"\n        # Validate axes\n        check_axes_validity(axes)\n\n        return axes\n\n    @model_validator(mode=\"after\")\n    def validate_dimensions(self: Self) -&gt; Self:\n        \"\"\"\n        Validate 2D/3D dimensions between axes and tile size.\n\n        Returns\n        -------\n        Self\n            Validated prediction model.\n        \"\"\"\n        expected_len = 3 if \"Z\" in self.axes else 2\n\n        if self.tile_size is not None and self.tile_overlap is not None:\n            if len(self.tile_size) != expected_len:\n                raise ValueError(\n                    f\"Tile size must have {expected_len} dimensions given axes \"\n                    f\"{self.axes} (got {self.tile_size}).\"\n                )\n\n            if len(self.tile_overlap) != expected_len:\n                raise ValueError(\n                    f\"Tile overlap must have {expected_len} dimensions given axes \"\n                    f\"{self.axes} (got {self.tile_overlap}).\"\n                )\n\n            if any((i &gt;= j) for i, j in zip(self.tile_overlap, self.tile_size)):\n                raise ValueError(\"Tile overlap must be smaller than tile size.\")\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def std_only_with_mean(self: Self) -&gt; Self:\n        \"\"\"\n        Check that mean and std are either both None, or both specified.\n\n        Returns\n        -------\n        Self\n            Validated prediction model.\n\n        Raises\n        ------\n        ValueError\n            If std is not None and mean is None.\n        \"\"\"\n        # check that mean and std are either both None, or both specified\n        if not self.image_means and not self.image_stds:\n            raise ValueError(\"Mean and std must be specified during inference.\")\n\n        if (self.image_means and not self.image_stds) or (\n            self.image_stds and not self.image_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified.\"\n            )\n\n        elif (self.image_means is not None and self.image_stds is not None) and (\n            len(self.image_means) != len(self.image_stds)\n        ):\n            raise ValueError(\n                \"Mean and std must be specified for each \" \"input channel.\"\n            )\n\n        return self\n\n    def _update(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Update multiple arguments at once.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Key-value pairs of arguments to update.\n        \"\"\"\n        self.__dict__.update(kwargs)\n        self.__class__.model_validate(self.__dict__)\n\n    def set_3D(self, axes: str, tile_size: list[int], tile_overlap: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D parameters.\n\n        Parameters\n        ----------\n        axes : str\n            Axes.\n        tile_size : list of int\n            Tile size.\n        tile_overlap : list of int\n            Tile overlap.\n        \"\"\"\n        self._update(axes=axes, tile_size=tile_size, tile_overlap=tile_overlap)\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.axes","title":"<code>axes</code>  <code>instance-attribute</code>","text":"<p>Data axes (TSCZYX) in the order of the input data.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.batch_size","title":"<code>batch_size = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for prediction.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of input data: numpy.ndarray (array) or path (tiff or custom).</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.image_means","title":"<code>image_means = Field(..., min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Mean values for each input channel.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.image_stds","title":"<code>image_stds = Field(..., min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviation values for each input channel.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_overlap","title":"<code>tile_overlap = Field(default=None, min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between tiles, only effective if <code>tile_size</code> is specified.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_size","title":"<code>tile_size = Field(default=None, min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tile size of prediction, only effective if <code>tile_overlap</code> is specified.</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tta_transforms","title":"<code>tta_transforms = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply test-time augmentation (all 90 degrees rotations and flips).</p>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig._update","title":"<code>_update(**kwargs)</code>","text":"<p>Update multiple arguments at once.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Key-value pairs of arguments to update.</p> <code>{}</code> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>def _update(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Update multiple arguments at once.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Key-value pairs of arguments to update.\n    \"\"\"\n    self.__dict__.update(kwargs)\n    self.__class__.model_validate(self.__dict__)\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.all_elements_non_zero_even","title":"<code>all_elements_non_zero_even(tile_overlap)</code>  <code>classmethod</code>","text":"<p>Validate tile overlap.</p> <p>Overlaps must be non-zero, positive and even.</p> <p>Parameters:</p> Name Type Description Default <code>tile_overlap</code> <code>list[int] or None</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list[int] or None</code> <p>Validated tile overlap.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is 0.</p> <code>ValueError</code> <p>If the patch size is not even.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"tile_overlap\")\n@classmethod\ndef all_elements_non_zero_even(\n    cls, tile_overlap: Optional[list[int]]\n) -&gt; Optional[list[int]]:\n    \"\"\"\n    Validate tile overlap.\n\n    Overlaps must be non-zero, positive and even.\n\n    Parameters\n    ----------\n    tile_overlap : list[int] or None\n        Patch size.\n\n    Returns\n    -------\n    list[int] or None\n        Validated tile overlap.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is 0.\n    ValueError\n        If the patch size is not even.\n    \"\"\"\n    if tile_overlap is not None:\n        for dim in tile_overlap:\n            if dim &lt; 1:\n                raise ValueError(\n                    f\"Patch size must be non-zero positive (got {dim}).\"\n                )\n\n            if dim % 2 != 0:\n                raise ValueError(f\"Patch size must be even (got {dim}).\")\n\n    return tile_overlap\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.axes_valid","title":"<code>axes_valid(axes)</code>  <code>classmethod</code>","text":"<p>Validate axes.</p> <p>Axes must: - be a combination of 'STCZYX' - not contain duplicates - contain at least 2 contiguous axes: X and Y - contain at most 4 axes - not contain both S and T axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axes are not valid.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"axes\")\n@classmethod\ndef axes_valid(cls, axes: str) -&gt; str:\n    \"\"\"\n    Validate axes.\n\n    Axes must:\n    - be a combination of 'STCZYX'\n    - not contain duplicates\n    - contain at least 2 contiguous axes: X and Y\n    - contain at most 4 axes\n    - not contain both S and T axes\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n\n    Returns\n    -------\n    str\n        Validated axes.\n\n    Raises\n    ------\n    ValueError\n        If axes are not valid.\n    \"\"\"\n    # Validate axes\n    check_axes_validity(axes)\n\n    return axes\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.set_3D","title":"<code>set_3D(axes, tile_size, tile_overlap)</code>","text":"<p>Set 3D parameters.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes.</p> required <code>tile_size</code> <code>list of int</code> <p>Tile size.</p> required <code>tile_overlap</code> <code>list of int</code> <p>Tile overlap.</p> required Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>def set_3D(self, axes: str, tile_size: list[int], tile_overlap: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D parameters.\n\n    Parameters\n    ----------\n    axes : str\n        Axes.\n    tile_size : list of int\n        Tile size.\n    tile_overlap : list of int\n        Tile overlap.\n    \"\"\"\n    self._update(axes=axes, tile_size=tile_size, tile_overlap=tile_overlap)\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.std_only_with_mean","title":"<code>std_only_with_mean()</code>","text":"<p>Check that mean and std are either both None, or both specified.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated prediction model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If std is not None and mean is None.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef std_only_with_mean(self: Self) -&gt; Self:\n    \"\"\"\n    Check that mean and std are either both None, or both specified.\n\n    Returns\n    -------\n    Self\n        Validated prediction model.\n\n    Raises\n    ------\n    ValueError\n        If std is not None and mean is None.\n    \"\"\"\n    # check that mean and std are either both None, or both specified\n    if not self.image_means and not self.image_stds:\n        raise ValueError(\"Mean and std must be specified during inference.\")\n\n    if (self.image_means and not self.image_stds) or (\n        self.image_stds and not self.image_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified.\"\n        )\n\n    elif (self.image_means is not None and self.image_stds is not None) and (\n        len(self.image_means) != len(self.image_stds)\n    ):\n        raise ValueError(\n            \"Mean and std must be specified for each \" \"input channel.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.tile_min_8_power_of_2","title":"<code>tile_min_8_power_of_2(tile_list)</code>  <code>classmethod</code>","text":"<p>Validate that each entry is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>tile_list</code> <code>list of int</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list of int</code> <p>Validated patch size.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size if smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@field_validator(\"tile_size\")\n@classmethod\ndef tile_min_8_power_of_2(\n    cls, tile_list: Optional[list[int]]\n) -&gt; Optional[list[int]]:\n    \"\"\"\n    Validate that each entry is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    tile_list : list of int\n        Patch size.\n\n    Returns\n    -------\n    list of int\n        Validated patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size if smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    patch_size_ge_than_8_power_of_2(tile_list)\n\n    return tile_list\n</code></pre>"},{"location":"reference/careamics/config/inference_model/#careamics.config.inference_model.InferenceConfig.validate_dimensions","title":"<code>validate_dimensions()</code>","text":"<p>Validate 2D/3D dimensions between axes and tile size.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated prediction model.</p> Source code in <code>src/careamics/config/inference_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dimensions(self: Self) -&gt; Self:\n    \"\"\"\n    Validate 2D/3D dimensions between axes and tile size.\n\n    Returns\n    -------\n    Self\n        Validated prediction model.\n    \"\"\"\n    expected_len = 3 if \"Z\" in self.axes else 2\n\n    if self.tile_size is not None and self.tile_overlap is not None:\n        if len(self.tile_size) != expected_len:\n            raise ValueError(\n                f\"Tile size must have {expected_len} dimensions given axes \"\n                f\"{self.axes} (got {self.tile_size}).\"\n            )\n\n        if len(self.tile_overlap) != expected_len:\n            raise ValueError(\n                f\"Tile overlap must have {expected_len} dimensions given axes \"\n                f\"{self.axes} (got {self.tile_overlap}).\"\n            )\n\n        if any((i &gt;= j) for i, j in zip(self.tile_overlap, self.tile_size)):\n            raise ValueError(\"Tile overlap must be smaller than tile size.\")\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/","title":"likelihood_model","text":"<p>Likelihood model.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.Tensor","title":"<code>Tensor = Annotated[Union[np.ndarray, torch.Tensor], PlainSerializer(_array_to_json, return_type=str), PlainValidator(_to_torch)]</code>  <code>module-attribute</code>","text":"<p>Annotated tensor type, used to serialize arrays or tensors to JSON strings and deserialize them back to tensors.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig","title":"<code>GaussianLikelihoodConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaussian likelihood configuration.</p> Source code in <code>src/careamics/config/likelihood_model.py</code> <pre><code>class GaussianLikelihoodConfig(BaseModel):\n    \"\"\"Gaussian likelihood configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    predict_logvar: Optional[Literal[\"pixelwise\"]] = None\n    \"\"\"If `pixelwise`, log-variance is computed for each pixel, else log-variance\n    is not computed.\"\"\"\n\n    logvar_lowerbound: Union[float, None] = None\n    \"\"\"The lowerbound value for log-variance.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig.logvar_lowerbound","title":"<code>logvar_lowerbound = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The lowerbound value for log-variance.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.GaussianLikelihoodConfig.predict_logvar","title":"<code>predict_logvar = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>pixelwise</code>, log-variance is computed for each pixel, else log-variance is not computed.</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig","title":"<code>NMLikelihoodConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Noise model likelihood configuration.</p> <p>NOTE: we need to define the data mean and std here because the noise model is trained on not-normalized data. Hence, we need to unnormalize the model output to compute the noise model likelihood.</p> Source code in <code>src/careamics/config/likelihood_model.py</code> <pre><code>class NMLikelihoodConfig(BaseModel):\n    \"\"\"Noise model likelihood configuration.\n\n    NOTE: we need to define the data mean and std here because the noise model\n    is trained on not-normalized data. Hence, we need to unnormalize the model\n    output to compute the noise model likelihood.\n    \"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, arbitrary_types_allowed=True)\n\n    # TODO remove and use as parameters to the likelihood functions?\n    data_mean: Tensor = torch.zeros(1)\n    \"\"\"The mean of the data, used to unnormalize data for noise model evaluation.\n    Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).\"\"\"\n\n    # TODO remove and use as parameters to the likelihood functions?\n    data_std: Tensor = torch.ones(1)\n    \"\"\"The standard deviation of the data, used to unnormalize data for noise\n    model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig.data_mean","title":"<code>data_mean = torch.zeros(1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The mean of the data, used to unnormalize data for noise model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).</p>"},{"location":"reference/careamics/config/likelihood_model/#careamics.config.likelihood_model.NMLikelihoodConfig.data_std","title":"<code>data_std = torch.ones(1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The standard deviation of the data, used to unnormalize data for noise model evaluation. Shape is (target_ch,) (or (1, target_ch, [1], 1, 1)).</p>"},{"location":"reference/careamics/config/loss_model/","title":"loss_model","text":"<p>Configuration classes for LVAE losses.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig","title":"<code>KLLossConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>KL loss configuration.</p> Source code in <code>src/careamics/config/loss_model.py</code> <pre><code>class KLLossConfig(BaseModel):\n    \"\"\"KL loss configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, validate_default=True)\n\n    loss_type: Literal[\"kl\", \"kl_restricted\"] = \"kl\"\n    \"\"\"Type of KL divergence used as KL loss.\"\"\"\n    rescaling: Literal[\"latent_dim\", \"image_dim\"] = \"latent_dim\"\n    \"\"\"Rescaling of the KL loss.\"\"\"\n    aggregation: Literal[\"sum\", \"mean\"] = \"mean\"\n    \"\"\"Aggregation of the KL loss across different layers.\"\"\"\n    free_bits_coeff: float = 0.0\n    \"\"\"Free bits coefficient for the KL loss.\"\"\"\n    annealing: bool = False\n    \"\"\"Whether to apply KL loss annealing.\"\"\"\n    start: int = -1\n    \"\"\"Epoch at which KL loss annealing starts.\"\"\"\n    annealtime: int = 10\n    \"\"\"Number of epochs for which KL loss annealing is applied.\"\"\"\n    current_epoch: int = 0\n    \"\"\"Current epoch in the training loop.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.aggregation","title":"<code>aggregation = 'mean'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Aggregation of the KL loss across different layers.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.annealing","title":"<code>annealing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply KL loss annealing.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.annealtime","title":"<code>annealtime = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs for which KL loss annealing is applied.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.current_epoch","title":"<code>current_epoch = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current epoch in the training loop.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.free_bits_coeff","title":"<code>free_bits_coeff = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Free bits coefficient for the KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.loss_type","title":"<code>loss_type = 'kl'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of KL divergence used as KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.rescaling","title":"<code>rescaling = 'latent_dim'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rescaling of the KL loss.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.KLLossConfig.start","title":"<code>start = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Epoch at which KL loss annealing starts.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig","title":"<code>LVAELossConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LVAE loss configuration.</p> Source code in <code>src/careamics/config/loss_model.py</code> <pre><code>class LVAELossConfig(BaseModel):\n    \"\"\"LVAE loss configuration.\"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True, validate_default=True, arbitrary_types_allowed=True\n    )\n\n    loss_type: Literal[\"musplit\", \"denoisplit\", \"denoisplit_musplit\"]\n    \"\"\"Type of loss to use for LVAE.\"\"\"\n\n    reconstruction_weight: float = 1.0\n    \"\"\"Weight for the reconstruction loss in the total net loss\n    (i.e., `net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss`).\"\"\"\n    kl_weight: float = 1.0\n    \"\"\"Weight for the KL loss in the total net loss.\n    (i.e., `net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss`).\"\"\"\n    musplit_weight: float = 0.1\n    \"\"\"Weight for the muSplit loss (used in the muSplit-denoiSplit loss).\"\"\"\n    denoisplit_weight: float = 0.9\n    \"\"\"Weight for the denoiSplit loss (used in the muSplit-deonoiSplit loss).\"\"\"\n    kl_params: KLLossConfig = KLLossConfig()\n    \"\"\"KL loss configuration.\"\"\"\n\n    # TODO: remove?\n    non_stochastic: bool = False\n    \"\"\"Whether to sample latents and compute KL.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.denoisplit_weight","title":"<code>denoisplit_weight = 0.9</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the denoiSplit loss (used in the muSplit-deonoiSplit loss).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.kl_params","title":"<code>kl_params = KLLossConfig()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>KL loss configuration.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.kl_weight","title":"<code>kl_weight = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the KL loss in the total net loss. (i.e., <code>net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss</code>).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.loss_type","title":"<code>loss_type</code>  <code>instance-attribute</code>","text":"<p>Type of loss to use for LVAE.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.musplit_weight","title":"<code>musplit_weight = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the muSplit loss (used in the muSplit-denoiSplit loss).</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.non_stochastic","title":"<code>non_stochastic = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to sample latents and compute KL.</p>"},{"location":"reference/careamics/config/loss_model/#careamics.config.loss_model.LVAELossConfig.reconstruction_weight","title":"<code>reconstruction_weight = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weight for the reconstruction loss in the total net loss (i.e., <code>net_loss = reconstruction_weight * rec_loss + kl_weight * kl_loss</code>).</p>"},{"location":"reference/careamics/config/nm_model/","title":"nm_model","text":"<p>Noise models config.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.Array","title":"<code>Array = Annotated[Union[np.ndarray, torch.Tensor], PlainSerializer(_array_to_json, return_type=str), PlainValidator(_to_numpy)]</code>  <code>module-attribute</code>","text":"<p>Annotated array type, used to serialize arrays or tensors to JSON strings and deserialize them back to arrays.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig","title":"<code>GaussianMixtureNMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaussian mixture noise model.</p> Source code in <code>src/careamics/config/nm_model.py</code> <pre><code>class GaussianMixtureNMConfig(BaseModel):\n    \"\"\"Gaussian mixture noise model.\"\"\"\n\n    model_config = ConfigDict(\n        protected_namespaces=(),\n        validate_assignment=True,\n        arbitrary_types_allowed=True,\n        extra=\"allow\",\n    )\n    # model type\n    model_type: Literal[\"GaussianMixtureNoiseModel\"]\n\n    path: Optional[Union[Path, str]] = None\n    \"\"\"Path to the directory where the trained noise model (*.npz) is saved in the\n    `train` method.\"\"\"\n\n    # TODO remove and use as parameters to the NM functions?\n    signal: Optional[Union[str, Path, np.ndarray]] = Field(default=None, exclude=True)\n    \"\"\"Path to the file containing signal or respective numpy array.\"\"\"\n\n    # TODO remove and use as parameters to the NM functions?\n    observation: Optional[Union[str, Path, np.ndarray]] = Field(\n        default=None, exclude=True\n    )\n    \"\"\"Path to the file containing observation or respective numpy array.\"\"\"\n\n    weight: Optional[Array] = None\n    \"\"\"A [3*n_gaussian, n_coeff] sized array containing the values of the weights\n    describing the GMM noise model, with each row corresponding to one\n    parameter of each gaussian, namely [mean, standard deviation and weight].\n    Specifically, rows are organized as follows:\n    - first n_gaussian rows correspond to the means\n    - next n_gaussian rows correspond to the weights\n    - last n_gaussian rows correspond to the standard deviations\n    If `weight=None`, the weight array is initialized using the `min_signal`\n    and `max_signal` parameters.\"\"\"\n\n    n_gaussian: int = Field(default=1, ge=1)\n    \"\"\"Number of gaussians used for the GMM.\"\"\"\n\n    n_coeff: int = Field(default=2, ge=2)\n    \"\"\"Number of coefficients to describe the functional relationship between gaussian\n    parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic\n    relationship and so on.\"\"\"\n\n    min_signal: float = Field(default=0.0, ge=0.0)\n    \"\"\"Minimum signal intensity expected in the image.\"\"\"\n\n    max_signal: float = Field(default=1.0, ge=0.0)\n    \"\"\"Maximum signal intensity expected in the image.\"\"\"\n\n    min_sigma: float = Field(default=200.0, ge=0.0)  # TODO took from nb in pn2v\n    \"\"\"Minimum value of `standard deviation` allowed in the GMM.\n    All values of `standard deviation` below this are clamped to this value.\"\"\"\n\n    tol: float = Field(default=1e-10)\n    \"\"\"Tolerance used in the computation of the noise model likelihood.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.max_signal","title":"<code>max_signal = Field(default=1.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum signal intensity expected in the image.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.min_sigma","title":"<code>min_sigma = Field(default=200.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum value of <code>standard deviation</code> allowed in the GMM. All values of <code>standard deviation</code> below this are clamped to this value.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.min_signal","title":"<code>min_signal = Field(default=0.0, ge=0.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum signal intensity expected in the image.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.n_coeff","title":"<code>n_coeff = Field(default=2, ge=2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of coefficients to describe the functional relationship between gaussian parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic relationship and so on.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.n_gaussian","title":"<code>n_gaussian = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of gaussians used for the GMM.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.observation","title":"<code>observation = Field(default=None, exclude=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the file containing observation or respective numpy array.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.path","title":"<code>path = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the directory where the trained noise model (*.npz) is saved in the <code>train</code> method.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.signal","title":"<code>signal = Field(default=None, exclude=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the file containing signal or respective numpy array.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.tol","title":"<code>tol = Field(default=1e-10)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tolerance used in the computation of the noise model likelihood.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.GaussianMixtureNMConfig.weight","title":"<code>weight = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A [3*n_gaussian, n_coeff] sized array containing the values of the weights describing the GMM noise model, with each row corresponding to one parameter of each gaussian, namely [mean, standard deviation and weight]. Specifically, rows are organized as follows: - first n_gaussian rows correspond to the means - next n_gaussian rows correspond to the weights - last n_gaussian rows correspond to the standard deviations If <code>weight=None</code>, the weight array is initialized using the <code>min_signal</code> and <code>max_signal</code> parameters.</p>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.MultiChannelNMConfig","title":"<code>MultiChannelNMConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Noise Model config aggregating noise models for single output channels.</p> Source code in <code>src/careamics/config/nm_model.py</code> <pre><code>class MultiChannelNMConfig(BaseModel):\n    \"\"\"Noise Model config aggregating noise models for single output channels.\"\"\"\n\n    # TODO: check that this model config is OK\n    model_config = ConfigDict(\n        validate_assignment=True, arbitrary_types_allowed=True, extra=\"allow\"\n    )\n    noise_models: list[GaussianMixtureNMConfig]\n    \"\"\"List of noise models, one for each target channel.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/nm_model/#careamics.config.nm_model.MultiChannelNMConfig.noise_models","title":"<code>noise_models</code>  <code>instance-attribute</code>","text":"<p>List of noise models, one for each target channel.</p>"},{"location":"reference/careamics/config/optimizer_models/","title":"optimizer_models","text":"<p>Optimizers and schedulers Pydantic models.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel","title":"<code>LrSchedulerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Torch learning rate scheduler Pydantic model.</p> <p>Only parameters supported by the corresponding torch lr scheduler will be taken into account. For more details, check: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate</p> <p>Note that mandatory parameters (see the specific LrScheduler signature in the link above) must be provided. For example, StepLR requires <code>step_size</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>{'ReduceLROnPlateau', 'StepLR'}</code> <p>Name of the learning rate scheduler.</p> <code>parameters</code> <code>dict</code> <p>Parameters of the learning rate scheduler (see torch documentation).</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>class LrSchedulerModel(BaseModel):\n    \"\"\"Torch learning rate scheduler Pydantic model.\n\n    Only parameters supported by the corresponding torch lr scheduler will be taken\n    into account. For more details, check:\n    https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n\n    Note that mandatory parameters (see the specific LrScheduler signature in the\n    link above) must be provided. For example, StepLR requires `step_size`.\n\n    Attributes\n    ----------\n    name : {\"ReduceLROnPlateau\", \"StepLR\"}\n        Name of the learning rate scheduler.\n    parameters : dict\n        Parameters of the learning rate scheduler (see torch documentation).\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Mandatory field\n    name: Literal[\"ReduceLROnPlateau\", \"StepLR\"] = Field(default=\"ReduceLROnPlateau\")\n    \"\"\"Name of the learning rate scheduler, supported schedulers are defined in\n    SupportedScheduler.\"\"\"\n\n    # Optional parameters\n    parameters: dict = Field(default={}, validate_default=True)\n    \"\"\"Parameters of the learning rate scheduler, see PyTorch documentation for more\n    details.\"\"\"\n\n    @field_validator(\"parameters\")\n    @classmethod\n    def filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n        \"\"\"Filter parameters based on the learning rate scheduler's signature.\n\n        Parameters\n        ----------\n        user_params : dict\n            User parameters.\n        values : ValidationInfo\n            Pydantic field validation info, used to get the scheduler name.\n\n        Returns\n        -------\n        dict\n            Filtered scheduler parameters.\n\n        Raises\n        ------\n        ValueError\n            If the scheduler is StepLR and the step_size parameter is not specified.\n        \"\"\"\n        # retrieve the corresponding scheduler class\n        scheduler_class = getattr(optim.lr_scheduler, values.data[\"name\"])\n\n        # filter the user parameters according to the scheduler's signature\n        parameters = filter_parameters(scheduler_class, user_params)\n\n        if values.data[\"name\"] == \"StepLR\" and \"step_size\" not in parameters:\n            raise ValueError(\n                \"StepLR scheduler requires `step_size` parameter, check that it has \"\n                \"correctly been specified in `parameters`.\"\n            )\n\n        return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.name","title":"<code>name = Field(default='ReduceLROnPlateau')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the learning rate scheduler, supported schedulers are defined in SupportedScheduler.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.parameters","title":"<code>parameters = Field(default={}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters of the learning rate scheduler, see PyTorch documentation for more details.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.LrSchedulerModel.filter_parameters","title":"<code>filter_parameters(user_params, values)</code>  <code>classmethod</code>","text":"<p>Filter parameters based on the learning rate scheduler's signature.</p> <p>Parameters:</p> Name Type Description Default <code>user_params</code> <code>dict</code> <p>User parameters.</p> required <code>values</code> <code>ValidationInfo</code> <p>Pydantic field validation info, used to get the scheduler name.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Filtered scheduler parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scheduler is StepLR and the step_size parameter is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@field_validator(\"parameters\")\n@classmethod\ndef filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n    \"\"\"Filter parameters based on the learning rate scheduler's signature.\n\n    Parameters\n    ----------\n    user_params : dict\n        User parameters.\n    values : ValidationInfo\n        Pydantic field validation info, used to get the scheduler name.\n\n    Returns\n    -------\n    dict\n        Filtered scheduler parameters.\n\n    Raises\n    ------\n    ValueError\n        If the scheduler is StepLR and the step_size parameter is not specified.\n    \"\"\"\n    # retrieve the corresponding scheduler class\n    scheduler_class = getattr(optim.lr_scheduler, values.data[\"name\"])\n\n    # filter the user parameters according to the scheduler's signature\n    parameters = filter_parameters(scheduler_class, user_params)\n\n    if values.data[\"name\"] == \"StepLR\" and \"step_size\" not in parameters:\n        raise ValueError(\n            \"StepLR scheduler requires `step_size` parameter, check that it has \"\n            \"correctly been specified in `parameters`.\"\n        )\n\n    return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel","title":"<code>OptimizerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Torch optimizer Pydantic model.</p> <p>Only parameters supported by the corresponding torch optimizer will be taken into account. For more details, check: https://pytorch.org/docs/stable/optim.html#algorithms</p> <p>Note that mandatory parameters (see the specific Optimizer signature in the link above) must be provided. For example, SGD requires <code>lr</code>.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>{'Adam', 'SGD'}</code> <p>Name of the optimizer.</p> <code>parameters</code> <code>dict</code> <p>Parameters of the optimizer (see torch documentation).</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>class OptimizerModel(BaseModel):\n    \"\"\"Torch optimizer Pydantic model.\n\n    Only parameters supported by the corresponding torch optimizer will be taken\n    into account. For more details, check:\n    https://pytorch.org/docs/stable/optim.html#algorithms\n\n    Note that mandatory parameters (see the specific Optimizer signature in the\n    link above) must be provided. For example, SGD requires `lr`.\n\n    Attributes\n    ----------\n    name : {\"Adam\", \"SGD\"}\n        Name of the optimizer.\n    parameters : dict\n        Parameters of the optimizer (see torch documentation).\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Mandatory field\n    name: Literal[\"Adam\", \"SGD\", \"Adamax\"] = Field(\n        default=\"Adam\", validate_default=True\n    )\n    \"\"\"Name of the optimizer, supported optimizers are defined in SupportedOptimizer.\"\"\"\n\n    # Optional parameters, empty dict default value to allow filtering dictionary\n    parameters: dict = Field(\n        default={\n            \"lr\": 1e-4,\n        },\n        validate_default=True,\n    )\n    \"\"\"Parameters of the optimizer, see PyTorch documentation for more details.\"\"\"\n\n    @field_validator(\"parameters\")\n    @classmethod\n    def filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n        \"\"\"\n        Validate optimizer parameters.\n\n        This method filters out unknown parameters, given the optimizer name.\n\n        Parameters\n        ----------\n        user_params : dict\n            Parameters passed on to the torch optimizer.\n        values : ValidationInfo\n            Pydantic field validation info, used to get the optimizer name.\n\n        Returns\n        -------\n        dict\n            Filtered optimizer parameters.\n\n        Raises\n        ------\n        ValueError\n            If the optimizer name is not specified.\n        \"\"\"\n        optimizer_name = values.data[\"name\"]\n\n        # retrieve the corresponding optimizer class\n        optimizer_class = getattr(optim, optimizer_name)\n\n        # filter the user parameters according to the optimizer's signature\n        parameters = filter_parameters(optimizer_class, user_params)\n\n        return parameters\n\n    @model_validator(mode=\"after\")\n    def sgd_lr_parameter(self) -&gt; Self:\n        \"\"\"\n        Check that SGD optimizer has the mandatory `lr` parameter specified.\n\n        This is specific for PyTorch &lt; 2.2.\n\n        Returns\n        -------\n        Self\n            Validated optimizer.\n\n        Raises\n        ------\n        ValueError\n            If the optimizer is SGD and the lr parameter is not specified.\n        \"\"\"\n        if self.name == SupportedOptimizer.SGD and \"lr\" not in self.parameters:\n            raise ValueError(\n                \"SGD optimizer requires `lr` parameter, check that it has correctly \"\n                \"been specified in `parameters`.\"\n            )\n\n        return self\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.name","title":"<code>name = Field(default='Adam', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the optimizer, supported optimizers are defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.parameters","title":"<code>parameters = Field(default={'lr': 0.0001}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters of the optimizer, see PyTorch documentation for more details.</p>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.filter_parameters","title":"<code>filter_parameters(user_params, values)</code>  <code>classmethod</code>","text":"<p>Validate optimizer parameters.</p> <p>This method filters out unknown parameters, given the optimizer name.</p> <p>Parameters:</p> Name Type Description Default <code>user_params</code> <code>dict</code> <p>Parameters passed on to the torch optimizer.</p> required <code>values</code> <code>ValidationInfo</code> <p>Pydantic field validation info, used to get the optimizer name.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Filtered optimizer parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the optimizer name is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@field_validator(\"parameters\")\n@classmethod\ndef filter_parameters(cls, user_params: dict, values: ValidationInfo) -&gt; dict:\n    \"\"\"\n    Validate optimizer parameters.\n\n    This method filters out unknown parameters, given the optimizer name.\n\n    Parameters\n    ----------\n    user_params : dict\n        Parameters passed on to the torch optimizer.\n    values : ValidationInfo\n        Pydantic field validation info, used to get the optimizer name.\n\n    Returns\n    -------\n    dict\n        Filtered optimizer parameters.\n\n    Raises\n    ------\n    ValueError\n        If the optimizer name is not specified.\n    \"\"\"\n    optimizer_name = values.data[\"name\"]\n\n    # retrieve the corresponding optimizer class\n    optimizer_class = getattr(optim, optimizer_name)\n\n    # filter the user parameters according to the optimizer's signature\n    parameters = filter_parameters(optimizer_class, user_params)\n\n    return parameters\n</code></pre>"},{"location":"reference/careamics/config/optimizer_models/#careamics.config.optimizer_models.OptimizerModel.sgd_lr_parameter","title":"<code>sgd_lr_parameter()</code>","text":"<p>Check that SGD optimizer has the mandatory <code>lr</code> parameter specified.</p> <p>This is specific for PyTorch &lt; 2.2.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated optimizer.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the optimizer is SGD and the lr parameter is not specified.</p> Source code in <code>src/careamics/config/optimizer_models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef sgd_lr_parameter(self) -&gt; Self:\n    \"\"\"\n    Check that SGD optimizer has the mandatory `lr` parameter specified.\n\n    This is specific for PyTorch &lt; 2.2.\n\n    Returns\n    -------\n    Self\n        Validated optimizer.\n\n    Raises\n    ------\n    ValueError\n        If the optimizer is SGD and the lr parameter is not specified.\n    \"\"\"\n    if self.name == SupportedOptimizer.SGD and \"lr\" not in self.parameters:\n        raise ValueError(\n            \"SGD optimizer requires `lr` parameter, check that it has correctly \"\n            \"been specified in `parameters`.\"\n        )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/tile_information/","title":"tile_information","text":"<p>Pydantic model representing the metadata of a prediction tile.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation","title":"<code>TileInformation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model containing tile information.</p> <p>This model is used to represent the information required to stitch back a tile into a larger image. It is used throughout the prediction pipeline of CAREamics.</p> <p>Array shape should be C(Z)YX, where Z is an optional dimensions.</p> Source code in <code>src/careamics/config/tile_information.py</code> <pre><code>class TileInformation(BaseModel):\n    \"\"\"\n    Pydantic model containing tile information.\n\n    This model is used to represent the information required to stitch back a tile into\n    a larger image. It is used throughout the prediction pipeline of CAREamics.\n\n    Array shape should be C(Z)YX, where Z is an optional dimensions.\n    \"\"\"\n\n    model_config = ConfigDict(validate_default=True)\n\n    array_shape: DimTuple  # TODO: find a way to add custom error message?\n    \"\"\"Shape of the original (untiled) array.\"\"\"\n\n    last_tile: bool = False\n    \"\"\"Whether this tile is the last one of the array.\"\"\"\n\n    overlap_crop_coords: tuple[tuple[int, ...], ...]\n    \"\"\"Inner coordinates of the tile where to crop the prediction in order to stitch\n    it back into the original image.\"\"\"\n\n    stitch_coords: tuple[tuple[int, ...], ...]\n    \"\"\"Coordinates in the original image where to stitch the cropped tile back.\"\"\"\n\n    sample_id: int\n    \"\"\"Sample ID of the tile.\"\"\"\n\n    # TODO: Test that ZYX axes are not singleton ?\n\n    def __eq__(self, other_tile: object):\n        \"\"\"Check if two tile information objects are equal.\n\n        Parameters\n        ----------\n        other_tile : object\n            Tile information object to compare with.\n\n        Returns\n        -------\n        bool\n            Whether the two tile information objects are equal.\n        \"\"\"\n        if not isinstance(other_tile, TileInformation):\n            return NotImplemented\n\n        return (\n            self.array_shape == other_tile.array_shape\n            and self.last_tile == other_tile.last_tile\n            and self.overlap_crop_coords == other_tile.overlap_crop_coords\n            and self.stitch_coords == other_tile.stitch_coords\n            and self.sample_id == other_tile.sample_id\n        )\n</code></pre>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.array_shape","title":"<code>array_shape</code>  <code>instance-attribute</code>","text":"<p>Shape of the original (untiled) array.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.last_tile","title":"<code>last_tile = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether this tile is the last one of the array.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.overlap_crop_coords","title":"<code>overlap_crop_coords</code>  <code>instance-attribute</code>","text":"<p>Inner coordinates of the tile where to crop the prediction in order to stitch it back into the original image.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.sample_id","title":"<code>sample_id</code>  <code>instance-attribute</code>","text":"<p>Sample ID of the tile.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.stitch_coords","title":"<code>stitch_coords</code>  <code>instance-attribute</code>","text":"<p>Coordinates in the original image where to stitch the cropped tile back.</p>"},{"location":"reference/careamics/config/tile_information/#careamics.config.tile_information.TileInformation.__eq__","title":"<code>__eq__(other_tile)</code>","text":"<p>Check if two tile information objects are equal.</p> <p>Parameters:</p> Name Type Description Default <code>other_tile</code> <code>object</code> <p>Tile information object to compare with.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the two tile information objects are equal.</p> Source code in <code>src/careamics/config/tile_information.py</code> <pre><code>def __eq__(self, other_tile: object):\n    \"\"\"Check if two tile information objects are equal.\n\n    Parameters\n    ----------\n    other_tile : object\n        Tile information object to compare with.\n\n    Returns\n    -------\n    bool\n        Whether the two tile information objects are equal.\n    \"\"\"\n    if not isinstance(other_tile, TileInformation):\n        return NotImplemented\n\n    return (\n        self.array_shape == other_tile.array_shape\n        and self.last_tile == other_tile.last_tile\n        and self.overlap_crop_coords == other_tile.overlap_crop_coords\n        and self.stitch_coords == other_tile.stitch_coords\n        and self.sample_id == other_tile.sample_id\n    )\n</code></pre>"},{"location":"reference/careamics/config/training_model/","title":"training_model","text":"<p>Training configuration.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig","title":"<code>TrainingConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters related to the training.</p> <p>Mandatory parameters are:     - num_epochs: number of epochs, greater than 0.     - batch_size: batch size, greater than 0.     - augmentation: whether to use data augmentation or not (True or False).</p> <p>Attributes:</p> Name Type Description <code>num_epochs</code> <code>int</code> <p>Number of epochs, greater than 0.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"\n    Parameters related to the training.\n\n    Mandatory parameters are:\n        - num_epochs: number of epochs, greater than 0.\n        - batch_size: batch size, greater than 0.\n        - augmentation: whether to use data augmentation or not (True or False).\n\n    Attributes\n    ----------\n    num_epochs : int\n        Number of epochs, greater than 0.\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    num_epochs: int = Field(default=20, ge=1)\n    \"\"\"Number of epochs, greater than 0.\"\"\"\n\n    precision: Literal[\"64\", \"32\", \"16-mixed\", \"bf16-mixed\"] = Field(default=\"32\")\n    \"\"\"Numerical precision\"\"\"\n    max_steps: int = Field(default=-1, ge=-1)\n    \"\"\"Maximum number of steps to train for. -1 means no limit.\"\"\"\n    check_val_every_n_epoch: int = Field(default=1, ge=1)\n    \"\"\"Validation step frequency.\"\"\"\n    enable_progress_bar: bool = Field(default=True)\n    \"\"\"Whether to enable the progress bar.\"\"\"\n    accumulate_grad_batches: int = Field(default=1, ge=1)\n    \"\"\"Number of batches to accumulate gradients over before stepping the optimizer.\"\"\"\n    gradient_clip_val: Optional[Union[int, float]] = None\n    \"\"\"The value to which to clip the gradient\"\"\"\n    gradient_clip_algorithm: Literal[\"value\", \"norm\"] = \"norm\"\n    \"\"\"The algorithm to use for gradient clipping (see lightning `Trainer`).\"\"\"\n    logger: Optional[Literal[\"wandb\", \"tensorboard\"]] = None\n    \"\"\"Logger to use during training. If None, no logger will be used. Available\n    loggers are defined in SupportedLogger.\"\"\"\n\n    checkpoint_callback: CheckpointModel = CheckpointModel()\n    \"\"\"Checkpoint callback configuration, following PyTorch Lightning Checkpoint\n    callback.\"\"\"\n\n    early_stopping_callback: Optional[EarlyStoppingModel] = Field(\n        default=None, validate_default=True\n    )\n    \"\"\"Early stopping callback configuration, following PyTorch Lightning Checkpoint\n    callback.\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def has_logger(self) -&gt; bool:\n        \"\"\"Check if the logger is defined.\n\n        Returns\n        -------\n        bool\n            Whether the logger is defined or not.\n        \"\"\"\n        return self.logger is not None\n\n    @field_validator(\"max_steps\")\n    @classmethod\n    def validate_max_steps(cls, max_steps: int) -&gt; int:\n        \"\"\"Validate the max_steps parameter.\n\n        Parameters\n        ----------\n        max_steps : int\n            Maximum number of steps to train for. -1 means no limit.\n\n        Returns\n        -------\n        int\n            Validated max_steps.\n        \"\"\"\n        if max_steps == 0:\n            raise ValueError(\"max_steps must be greater than 0. Use -1 for no limit.\")\n        return max_steps\n</code></pre>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.accumulate_grad_batches","title":"<code>accumulate_grad_batches = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches to accumulate gradients over before stepping the optimizer.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.check_val_every_n_epoch","title":"<code>check_val_every_n_epoch = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Validation step frequency.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.checkpoint_callback","title":"<code>checkpoint_callback = CheckpointModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Checkpoint callback configuration, following PyTorch Lightning Checkpoint callback.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.early_stopping_callback","title":"<code>early_stopping_callback = Field(default=None, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Early stopping callback configuration, following PyTorch Lightning Checkpoint callback.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.enable_progress_bar","title":"<code>enable_progress_bar = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to enable the progress bar.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.gradient_clip_algorithm","title":"<code>gradient_clip_algorithm = 'norm'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The algorithm to use for gradient clipping (see lightning <code>Trainer</code>).</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.gradient_clip_val","title":"<code>gradient_clip_val = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The value to which to clip the gradient</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.logger","title":"<code>logger = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Logger to use during training. If None, no logger will be used. Available loggers are defined in SupportedLogger.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.max_steps","title":"<code>max_steps = Field(default=-1, ge=-1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of steps to train for. -1 means no limit.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.num_epochs","title":"<code>num_epochs = Field(default=20, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs, greater than 0.</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.precision","title":"<code>precision = Field(default='32')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Numerical precision</p>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.has_logger","title":"<code>has_logger()</code>","text":"<p>Check if the logger is defined.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the logger is defined or not.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>def has_logger(self) -&gt; bool:\n    \"\"\"Check if the logger is defined.\n\n    Returns\n    -------\n    bool\n        Whether the logger is defined or not.\n    \"\"\"\n    return self.logger is not None\n</code></pre>"},{"location":"reference/careamics/config/training_model/#careamics.config.training_model.TrainingConfig.validate_max_steps","title":"<code>validate_max_steps(max_steps)</code>  <code>classmethod</code>","text":"<p>Validate the max_steps parameter.</p> <p>Parameters:</p> Name Type Description Default <code>max_steps</code> <code>int</code> <p>Maximum number of steps to train for. -1 means no limit.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated max_steps.</p> Source code in <code>src/careamics/config/training_model.py</code> <pre><code>@field_validator(\"max_steps\")\n@classmethod\ndef validate_max_steps(cls, max_steps: int) -&gt; int:\n    \"\"\"Validate the max_steps parameter.\n\n    Parameters\n    ----------\n    max_steps : int\n        Maximum number of steps to train for. -1 means no limit.\n\n    Returns\n    -------\n    int\n        Validated max_steps.\n    \"\"\"\n    if max_steps == 0:\n        raise ValueError(\"max_steps must be greater than 0. Use -1 for no limit.\")\n    return max_steps\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/","title":"care_algorithm_model","text":"<p>CARE algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm","title":"<code>CAREAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>CARE algorithm configuration.</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>care</code> <p>CARE Algorithm name.</p> <code>loss</code> <code>{mae, mse}</code> <p>CARE-compatible loss function.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>class CAREAlgorithm(UNetBasedAlgorithm):\n    \"\"\"CARE algorithm configuration.\n\n    Attributes\n    ----------\n    algorithm : \"care\"\n        CARE Algorithm name.\n    loss : {\"mae\", \"mse\"}\n        CARE-compatible loss function.\n    \"\"\"\n\n    algorithm: Literal[\"care\"] = \"care\"\n    \"\"\"CARE Algorithm name.\"\"\"\n\n    loss: Literal[\"mae\", \"mse\"] = \"mae\"\n    \"\"\"CARE-compatible loss function.\"\"\"\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_without_n2v2),\n        AfterValidator(model_without_final_activation),\n    ]\n    \"\"\"UNet without a final activation function and without the `n2v2` modifications.\"\"\"\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return CARE\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            CARE,\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return CARE_REF.text + \" doi: \" + CARE_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [CARE_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return CARE_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.algorithm","title":"<code>algorithm = 'care'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CARE Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.loss","title":"<code>loss = 'mae'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CARE-compatible loss function.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet without a final activation function and without the <code>n2v2</code> modifications.</p>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [CARE_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return CARE_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return CARE\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        CARE,\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/care_algorithm_model/#careamics.config.algorithms.care_algorithm_model.CAREAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/care_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return CARE_REF.text + \" doi: \" + CARE_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/","title":"n2n_algorithm_model","text":"<p>N2N Algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm","title":"<code>N2NAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>Noise2Noise Algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>class N2NAlgorithm(UNetBasedAlgorithm):\n    \"\"\"Noise2Noise Algorithm configuration.\"\"\"\n\n    algorithm: Literal[\"n2n\"] = \"n2n\"\n    \"\"\"N2N Algorithm name.\"\"\"\n\n    loss: Literal[\"mae\", \"mse\"] = \"mae\"\n    \"\"\"N2N-compatible loss function.\"\"\"\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_without_n2v2),\n        AfterValidator(model_without_final_activation),\n    ]\n    \"\"\"UNet without a final activation function and without the `n2v2` modifications.\"\"\"\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the algorithm friendly name.\n\n        Returns\n        -------\n        str\n            Friendly name of the algorithm.\n        \"\"\"\n        return N2N\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        return [\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            N2N,\n        ]\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        return N2N_REF.text + \" doi: \" + N2N_REF.doi\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        return [N2N_REF]\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Get the algorithm description.\n\n        Returns\n        -------\n        str\n            Algorithm description.\n        \"\"\"\n        return N2N_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.algorithm","title":"<code>algorithm = 'n2n'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2N Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.loss","title":"<code>loss = 'mae'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2N-compatible loss function.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet without a final activation function and without the <code>n2v2</code> modifications.</p>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    return [N2N_REF]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Get the algorithm description.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm description.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Get the algorithm description.\n\n    Returns\n    -------\n    str\n        Algorithm description.\n    \"\"\"\n    return N2N_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the algorithm friendly name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the algorithm friendly name.\n\n    Returns\n    -------\n    str\n        Friendly name of the algorithm.\n    \"\"\"\n    return N2N\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    return [\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        N2N,\n    ]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2n_algorithm_model/#careamics.config.algorithms.n2n_algorithm_model.N2NAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/n2n_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    return N2N_REF.text + \" doi: \" + N2N_REF.doi\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/","title":"n2v_algorithm_model","text":"<p>\"N2V Algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm","title":"<code>N2VAlgorithm</code>","text":"<p>               Bases: <code>UNetBasedAlgorithm</code></p> <p>N2V Algorithm configuration.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>class N2VAlgorithm(UNetBasedAlgorithm):\n    \"\"\"N2V Algorithm configuration.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True)\n\n    algorithm: Literal[\"n2v\"] = \"n2v\"\n    \"\"\"N2V Algorithm name.\"\"\"\n\n    loss: Literal[\"n2v\"] = \"n2v\"\n    \"\"\"N2V loss function.\"\"\"\n\n    n2v_config: N2VManipulateModel = N2VManipulateModel()\n\n    model: Annotated[\n        UNetModel,\n        AfterValidator(model_matching_in_out_channels),\n        AfterValidator(model_without_final_activation),\n    ]\n\n    @model_validator(mode=\"after\")\n    def validate_n2v2(self) -&gt; Self:\n        \"\"\"Validate that the N2V2 strategy and models are set correctly.\n\n        Returns\n        -------\n        Self\n            The validateed configuration.\n\n        Raises\n        ------\n        ValueError\n            If N2V2 is used with the wrong pixel manipulation strategy.\n        \"\"\"\n        if self.model.n2v2:\n            if self.n2v_config.strategy != SupportedPixelManipulation.MEDIAN.value:\n                raise ValueError(\n                    f\"N2V2 can only be used with the \"\n                    f\"{SupportedPixelManipulation.MEDIAN} pixel manipulation strategy. \"\n                    f\"Change the `strategy` parameters in `n2v_config` to \"\n                    f\"{SupportedPixelManipulation.MEDIAN}.\"\n                )\n        else:\n            if self.n2v_config.strategy != SupportedPixelManipulation.UNIFORM.value:\n                raise ValueError(\n                    f\"N2V can only be used with the \"\n                    f\"{SupportedPixelManipulation.UNIFORM} pixel manipulation strategy.\"\n                    f\" Change the `strategy` parameters in `n2v_config` to \"\n                    f\"{SupportedPixelManipulation.UNIFORM}.\"\n                )\n        return self\n\n    def set_n2v2(self, use_n2v2: bool) -&gt; None:\n        \"\"\"\n        Set the configuration to use N2V2 or the vanilla Noise2Void.\n\n        This method ensures that N2V2 is set correctly and remain coherent, as opposed\n        to setting the different parameters individually.\n\n        Parameters\n        ----------\n        use_n2v2 : bool\n            Whether to use N2V2.\n        \"\"\"\n        if use_n2v2:\n            self.n2v_config.strategy = SupportedPixelManipulation.MEDIAN.value\n            self.model.n2v2 = True\n        else:\n            self.n2v_config.strategy = SupportedPixelManipulation.UNIFORM.value\n            self.model.n2v2 = False\n\n    def is_struct_n2v(self) -&gt; bool:\n        \"\"\"Check if the configuration is using structN2V.\n\n        Returns\n        -------\n        bool\n            Whether the configuration is using structN2V.\n        \"\"\"\n        return self.n2v_config.struct_mask_axis != SupportedStructAxis.NONE.value\n\n    def get_algorithm_friendly_name(self) -&gt; str:\n        \"\"\"\n        Get the friendly name of the algorithm.\n\n        Returns\n        -------\n        str\n            Friendly name.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        if use_n2v2 and use_structN2V:\n            return STRUCT_N2V2\n        elif use_n2v2:\n            return N2V2\n        elif use_structN2V:\n            return STRUCT_N2V\n        else:\n            return N2V\n\n    def get_algorithm_keywords(self) -&gt; list[str]:\n        \"\"\"\n        Get algorithm keywords.\n\n        Returns\n        -------\n        list[str]\n            List of keywords.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        keywords = [\n            \"denoising\",\n            \"restoration\",\n            \"UNet\",\n            \"3D\" if self.model.is_3D() else \"2D\",\n            \"CAREamics\",\n            \"pytorch\",\n            N2V,\n        ]\n\n        if use_n2v2:\n            keywords.append(N2V2)\n        if use_structN2V:\n            keywords.append(STRUCT_N2V)\n\n        return keywords\n\n    def get_algorithm_references(self) -&gt; str:\n        \"\"\"\n        Get the algorithm references.\n\n        This is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Algorithm references.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        references = [\n            N2V_REF.text + \" doi: \" + N2V_REF.doi,\n            N2V2_REF.text + \" doi: \" + N2V2_REF.doi,\n            STRUCTN2V_REF.text + \" doi: \" + STRUCTN2V_REF.doi,\n        ]\n\n        # return the (struct)N2V(2) references\n        if use_n2v2 and use_structN2V:\n            return \"\\n\".join(references)\n        elif use_n2v2:\n            references.pop(-1)\n            return \"\\n\".join(references)\n        elif use_structN2V:\n            references.pop(-2)\n            return \"\\n\".join(references)\n        else:\n            return references[0]\n\n    def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n        \"\"\"\n        Return a list of citation entries of the current algorithm.\n\n        This is used to generate the model description for the BioImage Model Zoo.\n\n        Returns\n        -------\n        List[CiteEntry]\n            List of citation entries.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        references = [N2V_REF]\n\n        if use_n2v2:\n            references.append(N2V2_REF)\n\n        if use_structN2V:\n            references.append(STRUCTN2V_REF)\n\n        return references\n\n    def get_algorithm_description(self) -&gt; str:\n        \"\"\"\n        Return a description of the algorithm.\n\n        This method is used to generate the README of the BioImage Model Zoo export.\n\n        Returns\n        -------\n        str\n            Description of the algorithm.\n        \"\"\"\n        use_n2v2 = self.model.n2v2\n        use_structN2V = self.is_struct_n2v()\n\n        if use_n2v2 and use_structN2V:\n            return STR_N2V2_DESCRIPTION\n        elif use_n2v2:\n            return N2V2_DESCRIPTION\n        elif use_structN2V:\n            return STR_N2V_DESCRIPTION\n        else:\n            return N2V_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.algorithm","title":"<code>algorithm = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2V Algorithm name.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.loss","title":"<code>loss = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>N2V loss function.</p>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_citations","title":"<code>get_algorithm_citations()</code>","text":"<p>Return a list of citation entries of the current algorithm.</p> <p>This is used to generate the model description for the BioImage Model Zoo.</p> <p>Returns:</p> Type Description <code>List[CiteEntry]</code> <p>List of citation entries.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_citations(self) -&gt; list[CiteEntry]:\n    \"\"\"\n    Return a list of citation entries of the current algorithm.\n\n    This is used to generate the model description for the BioImage Model Zoo.\n\n    Returns\n    -------\n    List[CiteEntry]\n        List of citation entries.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    references = [N2V_REF]\n\n    if use_n2v2:\n        references.append(N2V2_REF)\n\n    if use_structN2V:\n        references.append(STRUCTN2V_REF)\n\n    return references\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_description","title":"<code>get_algorithm_description()</code>","text":"<p>Return a description of the algorithm.</p> <p>This method is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the algorithm.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_description(self) -&gt; str:\n    \"\"\"\n    Return a description of the algorithm.\n\n    This method is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Description of the algorithm.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    if use_n2v2 and use_structN2V:\n        return STR_N2V2_DESCRIPTION\n    elif use_n2v2:\n        return N2V2_DESCRIPTION\n    elif use_structN2V:\n        return STR_N2V_DESCRIPTION\n    else:\n        return N2V_DESCRIPTION\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_friendly_name","title":"<code>get_algorithm_friendly_name()</code>","text":"<p>Get the friendly name of the algorithm.</p> <p>Returns:</p> Type Description <code>str</code> <p>Friendly name.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_friendly_name(self) -&gt; str:\n    \"\"\"\n    Get the friendly name of the algorithm.\n\n    Returns\n    -------\n    str\n        Friendly name.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    if use_n2v2 and use_structN2V:\n        return STRUCT_N2V2\n    elif use_n2v2:\n        return N2V2\n    elif use_structN2V:\n        return STRUCT_N2V\n    else:\n        return N2V\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_keywords","title":"<code>get_algorithm_keywords()</code>","text":"<p>Get algorithm keywords.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of keywords.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_keywords(self) -&gt; list[str]:\n    \"\"\"\n    Get algorithm keywords.\n\n    Returns\n    -------\n    list[str]\n        List of keywords.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    keywords = [\n        \"denoising\",\n        \"restoration\",\n        \"UNet\",\n        \"3D\" if self.model.is_3D() else \"2D\",\n        \"CAREamics\",\n        \"pytorch\",\n        N2V,\n    ]\n\n    if use_n2v2:\n        keywords.append(N2V2)\n    if use_structN2V:\n        keywords.append(STRUCT_N2V)\n\n    return keywords\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.get_algorithm_references","title":"<code>get_algorithm_references()</code>","text":"<p>Get the algorithm references.</p> <p>This is used to generate the README of the BioImage Model Zoo export.</p> <p>Returns:</p> Type Description <code>str</code> <p>Algorithm references.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def get_algorithm_references(self) -&gt; str:\n    \"\"\"\n    Get the algorithm references.\n\n    This is used to generate the README of the BioImage Model Zoo export.\n\n    Returns\n    -------\n    str\n        Algorithm references.\n    \"\"\"\n    use_n2v2 = self.model.n2v2\n    use_structN2V = self.is_struct_n2v()\n\n    references = [\n        N2V_REF.text + \" doi: \" + N2V_REF.doi,\n        N2V2_REF.text + \" doi: \" + N2V2_REF.doi,\n        STRUCTN2V_REF.text + \" doi: \" + STRUCTN2V_REF.doi,\n    ]\n\n    # return the (struct)N2V(2) references\n    if use_n2v2 and use_structN2V:\n        return \"\\n\".join(references)\n    elif use_n2v2:\n        references.pop(-1)\n        return \"\\n\".join(references)\n    elif use_structN2V:\n        references.pop(-2)\n        return \"\\n\".join(references)\n    else:\n        return references[0]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.is_struct_n2v","title":"<code>is_struct_n2v()</code>","text":"<p>Check if the configuration is using structN2V.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the configuration is using structN2V.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def is_struct_n2v(self) -&gt; bool:\n    \"\"\"Check if the configuration is using structN2V.\n\n    Returns\n    -------\n    bool\n        Whether the configuration is using structN2V.\n    \"\"\"\n    return self.n2v_config.struct_mask_axis != SupportedStructAxis.NONE.value\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.set_n2v2","title":"<code>set_n2v2(use_n2v2)</code>","text":"<p>Set the configuration to use N2V2 or the vanilla Noise2Void.</p> <p>This method ensures that N2V2 is set correctly and remain coherent, as opposed to setting the different parameters individually.</p> <p>Parameters:</p> Name Type Description Default <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2.</p> required Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>def set_n2v2(self, use_n2v2: bool) -&gt; None:\n    \"\"\"\n    Set the configuration to use N2V2 or the vanilla Noise2Void.\n\n    This method ensures that N2V2 is set correctly and remain coherent, as opposed\n    to setting the different parameters individually.\n\n    Parameters\n    ----------\n    use_n2v2 : bool\n        Whether to use N2V2.\n    \"\"\"\n    if use_n2v2:\n        self.n2v_config.strategy = SupportedPixelManipulation.MEDIAN.value\n        self.model.n2v2 = True\n    else:\n        self.n2v_config.strategy = SupportedPixelManipulation.UNIFORM.value\n        self.model.n2v2 = False\n</code></pre>"},{"location":"reference/careamics/config/algorithms/n2v_algorithm_model/#careamics.config.algorithms.n2v_algorithm_model.N2VAlgorithm.validate_n2v2","title":"<code>validate_n2v2()</code>","text":"<p>Validate that the N2V2 strategy and models are set correctly.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validateed configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If N2V2 is used with the wrong pixel manipulation strategy.</p> Source code in <code>src/careamics/config/algorithms/n2v_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_n2v2(self) -&gt; Self:\n    \"\"\"Validate that the N2V2 strategy and models are set correctly.\n\n    Returns\n    -------\n    Self\n        The validateed configuration.\n\n    Raises\n    ------\n    ValueError\n        If N2V2 is used with the wrong pixel manipulation strategy.\n    \"\"\"\n    if self.model.n2v2:\n        if self.n2v_config.strategy != SupportedPixelManipulation.MEDIAN.value:\n            raise ValueError(\n                f\"N2V2 can only be used with the \"\n                f\"{SupportedPixelManipulation.MEDIAN} pixel manipulation strategy. \"\n                f\"Change the `strategy` parameters in `n2v_config` to \"\n                f\"{SupportedPixelManipulation.MEDIAN}.\"\n            )\n    else:\n        if self.n2v_config.strategy != SupportedPixelManipulation.UNIFORM.value:\n            raise ValueError(\n                f\"N2V can only be used with the \"\n                f\"{SupportedPixelManipulation.UNIFORM} pixel manipulation strategy.\"\n                f\" Change the `strategy` parameters in `n2v_config` to \"\n                f\"{SupportedPixelManipulation.UNIFORM}.\"\n            )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/","title":"unet_algorithm_model","text":"<p>UNet-based algorithm Pydantic model.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm","title":"<code>UNetBasedAlgorithm</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>General UNet-based algorithm configuration.</p> <p>This Pydantic model validates the parameters governing the components of the training algorithm: which algorithm, loss function, model architecture, optimizer, and learning rate scheduler to use.</p> <p>Currently, we only support N2V, CARE, and N2N algorithms. In order to train these algorithms, use the corresponding configuration child classes (e.g. <code>N2VAlgorithm</code>) to ensure coherent parameters (e.g. specific losses).</p> <p>Attributes:</p> Name Type Description <code>algorithm</code> <code>{n2v, care, n2n}</code> <p>Algorithm to use.</p> <code>loss</code> <code>{n2v, mae, mse}</code> <p>Loss function to use.</p> <code>model</code> <code>UNetModel</code> <p>Model architecture to use.</p> <code>optimizer</code> <code>(OptimizerModel, optional)</code> <p>Optimizer to use.</p> <code>lr_scheduler</code> <code>(LrSchedulerModel, optional)</code> <p>Learning rate scheduler to use.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Algorithm parameter type validation errors.</p> <code>ValueError</code> <p>If the algorithm, loss and model are not compatible.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>class UNetBasedAlgorithm(BaseModel):\n    \"\"\"General UNet-based algorithm configuration.\n\n    This Pydantic model validates the parameters governing the components of the\n    training algorithm: which algorithm, loss function, model architecture, optimizer,\n    and learning rate scheduler to use.\n\n    Currently, we only support N2V, CARE, and N2N algorithms. In order to train these\n    algorithms, use the corresponding configuration child classes (e.g.\n    `N2VAlgorithm`) to ensure coherent parameters (e.g. specific losses).\n\n\n    Attributes\n    ----------\n    algorithm : {\"n2v\", \"care\", \"n2n\"}\n        Algorithm to use.\n    loss : {\"n2v\", \"mae\", \"mse\"}\n        Loss function to use.\n    model : UNetModel\n        Model architecture to use.\n    optimizer : OptimizerModel, optional\n        Optimizer to use.\n    lr_scheduler : LrSchedulerModel, optional\n        Learning rate scheduler to use.\n\n    Raises\n    ------\n    ValueError\n        Algorithm parameter type validation errors.\n    ValueError\n        If the algorithm, loss and model are not compatible.\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        protected_namespaces=(),  # allows to use model_* as a field name\n        validate_assignment=True,\n        extra=\"allow\",\n    )\n\n    # Mandatory fields\n    algorithm: Literal[\"n2v\", \"care\", \"n2n\"]\n    \"\"\"Algorithm name, as defined in SupportedAlgorithm.\"\"\"\n\n    loss: Literal[\"n2v\", \"mae\", \"mse\"]\n    \"\"\"Loss function to use, as defined in SupportedLoss.\"\"\"\n\n    model: UNetModel\n    \"\"\"UNet model configuration.\"\"\"\n\n    # Optional fields\n    optimizer: OptimizerModel = OptimizerModel()\n    \"\"\"Optimizer to use, defined in SupportedOptimizer.\"\"\"\n\n    lr_scheduler: LrSchedulerModel = LrSchedulerModel()\n    \"\"\"Learning rate scheduler to use, defined in SupportedLrScheduler.\"\"\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string representing the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    @classmethod\n    def get_compatible_algorithms(cls) -&gt; list[str]:\n        \"\"\"Get the list of compatible algorithms.\n\n        Returns\n        -------\n        list of str\n            List of compatible algorithms.\n        \"\"\"\n        return [\"n2v\", \"care\", \"n2n\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.algorithm","title":"<code>algorithm</code>  <code>instance-attribute</code>","text":"<p>Algorithm name, as defined in SupportedAlgorithm.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.loss","title":"<code>loss</code>  <code>instance-attribute</code>","text":"<p>Loss function to use, as defined in SupportedLoss.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.lr_scheduler","title":"<code>lr_scheduler = LrSchedulerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Learning rate scheduler to use, defined in SupportedLrScheduler.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>UNet model configuration.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.optimizer","title":"<code>optimizer = OptimizerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optimizer to use, defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string representing the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string representing the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/algorithms/unet_algorithm_model/#careamics.config.algorithms.unet_algorithm_model.UNetBasedAlgorithm.get_compatible_algorithms","title":"<code>get_compatible_algorithms()</code>  <code>classmethod</code>","text":"<p>Get the list of compatible algorithms.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of compatible algorithms.</p> Source code in <code>src/careamics/config/algorithms/unet_algorithm_model.py</code> <pre><code>@classmethod\ndef get_compatible_algorithms(cls) -&gt; list[str]:\n    \"\"\"Get the list of compatible algorithms.\n\n    Returns\n    -------\n    list of str\n        List of compatible algorithms.\n    \"\"\"\n    return [\"n2v\", \"care\", \"n2n\"]\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/","title":"vae_algorithm_model","text":"<p>VAE-based algorithm Pydantic model.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm","title":"<code>VAEBasedAlgorithm</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>VAE-based algorithm configuration.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm--todo","title":"TODO","text":"<p>Examples:</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm--todo-add-once-finalized","title":"TODO add once finalized","text":"Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>class VAEBasedAlgorithm(BaseModel):\n    \"\"\"VAE-based algorithm configuration.\n\n    # TODO\n\n    Examples\n    --------\n    # TODO add once finalized\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        protected_namespaces=(),  # allows to use model_* as a field name\n        validate_assignment=True,\n        extra=\"allow\",\n    )\n\n    # Mandatory fields\n    # defined in SupportedAlgorithm\n    # TODO: Use supported Enum classes for typing?\n    #   - values can still be passed as strings and they will be cast to Enum\n    algorithm: Literal[\"musplit\", \"denoisplit\"]\n\n    # NOTE: these are all configs (pydantic models)\n    loss: LVAELossConfig\n    model: LVAEModel\n    noise_model: Optional[MultiChannelNMConfig] = None\n    noise_model_likelihood: Optional[NMLikelihoodConfig] = None\n    gaussian_likelihood: Optional[GaussianLikelihoodConfig] = None\n\n    # Optional fields\n    optimizer: OptimizerModel = OptimizerModel()\n    \"\"\"Optimizer to use, defined in SupportedOptimizer.\"\"\"\n\n    lr_scheduler: LrSchedulerModel = LrSchedulerModel()\n\n    @model_validator(mode=\"after\")\n    def algorithm_cross_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the algorithm model based on `algorithm`.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        # musplit\n        if self.algorithm == SupportedAlgorithm.MUSPLIT:\n            if self.loss.loss_type != SupportedLoss.MUSPLIT:\n                raise ValueError(\n                    f\"Algorithm {self.algorithm} only supports loss `musplit`.\"\n                )\n\n        if self.algorithm == SupportedAlgorithm.DENOISPLIT:\n            if self.loss.loss_type not in [\n                SupportedLoss.DENOISPLIT,\n                SupportedLoss.DENOISPLIT_MUSPLIT,\n            ]:\n                raise ValueError(\n                    f\"Algorithm {self.algorithm} only supports loss `denoisplit` \"\n                    \"or `denoisplit_musplit.\"\n                )\n            if (\n                self.loss.loss_type == SupportedLoss.DENOISPLIT\n                and self.model.predict_logvar is not None\n            ):\n                raise ValueError(\n                    \"Algorithm `denoisplit` with loss `denoisplit` only supports \"\n                    \"`predict_logvar` as `None`.\"\n                )\n\n            if self.noise_model is None:\n                raise ValueError(\"Algorithm `denoisplit` requires a noise model.\")\n        # TODO: what if algorithm is not musplit or denoisplit\n        return self\n\n    @model_validator(mode=\"after\")\n    def output_channels_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the consistency between number of out channels and noise models.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.noise_model is not None:\n            assert self.model.output_channels == len(self.noise_model.noise_models), (\n                f\"Number of output channels ({self.model.output_channels}) must match \"\n                f\"the number of noise models ({len(self.noise_model.noise_models)}).\"\n            )\n        return self\n\n    @model_validator(mode=\"after\")\n    def predict_logvar_validation(self: Self) -&gt; Self:\n        \"\"\"Validate the consistency of `predict_logvar` throughout the model.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.gaussian_likelihood is not None:\n            assert (\n                self.model.predict_logvar == self.gaussian_likelihood.predict_logvar\n            ), (\n                f\"Model `predict_logvar` ({self.model.predict_logvar}) must match \"\n                \"Gaussian likelihood model `predict_logvar` \"\n                f\"({self.gaussian_likelihood.predict_logvar}).\",\n            )\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty string representing the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.optimizer","title":"<code>optimizer = OptimizerModel()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optimizer to use, defined in SupportedOptimizer.</p>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string representing the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Pretty string representing the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.algorithm_cross_validation","title":"<code>algorithm_cross_validation()</code>","text":"<p>Validate the algorithm model based on <code>algorithm</code>.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef algorithm_cross_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the algorithm model based on `algorithm`.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    # musplit\n    if self.algorithm == SupportedAlgorithm.MUSPLIT:\n        if self.loss.loss_type != SupportedLoss.MUSPLIT:\n            raise ValueError(\n                f\"Algorithm {self.algorithm} only supports loss `musplit`.\"\n            )\n\n    if self.algorithm == SupportedAlgorithm.DENOISPLIT:\n        if self.loss.loss_type not in [\n            SupportedLoss.DENOISPLIT,\n            SupportedLoss.DENOISPLIT_MUSPLIT,\n        ]:\n            raise ValueError(\n                f\"Algorithm {self.algorithm} only supports loss `denoisplit` \"\n                \"or `denoisplit_musplit.\"\n            )\n        if (\n            self.loss.loss_type == SupportedLoss.DENOISPLIT\n            and self.model.predict_logvar is not None\n        ):\n            raise ValueError(\n                \"Algorithm `denoisplit` with loss `denoisplit` only supports \"\n                \"`predict_logvar` as `None`.\"\n            )\n\n        if self.noise_model is None:\n            raise ValueError(\"Algorithm `denoisplit` requires a noise model.\")\n    # TODO: what if algorithm is not musplit or denoisplit\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.output_channels_validation","title":"<code>output_channels_validation()</code>","text":"<p>Validate the consistency between number of out channels and noise models.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef output_channels_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the consistency between number of out channels and noise models.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.noise_model is not None:\n        assert self.model.output_channels == len(self.noise_model.noise_models), (\n            f\"Number of output channels ({self.model.output_channels}) must match \"\n            f\"the number of noise models ({len(self.noise_model.noise_models)}).\"\n        )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/algorithms/vae_algorithm_model/#careamics.config.algorithms.vae_algorithm_model.VAEBasedAlgorithm.predict_logvar_validation","title":"<code>predict_logvar_validation()</code>","text":"<p>Validate the consistency of <code>predict_logvar</code> throughout the model.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/algorithms/vae_algorithm_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef predict_logvar_validation(self: Self) -&gt; Self:\n    \"\"\"Validate the consistency of `predict_logvar` throughout the model.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.gaussian_likelihood is not None:\n        assert (\n            self.model.predict_logvar == self.gaussian_likelihood.predict_logvar\n        ), (\n            f\"Model `predict_logvar` ({self.model.predict_logvar}) must match \"\n            \"Gaussian likelihood model `predict_logvar` \"\n            f\"({self.gaussian_likelihood.predict_logvar}).\",\n        )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/architecture_model/","title":"architecture_model","text":"<p>Base model for the various CAREamics architectures.</p>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel","title":"<code>ArchitectureModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base Pydantic model for all model architectures.</p> <p>The <code>model_dump</code> method allows removing the <code>architecture</code> key from the model.</p> Source code in <code>src/careamics/config/architectures/architecture_model.py</code> <pre><code>class ArchitectureModel(BaseModel):\n    \"\"\"\n    Base Pydantic model for all model architectures.\n\n    The `model_dump` method allows removing the `architecture` key from the model.\n    \"\"\"\n\n    architecture: str\n    \"\"\"Name of the architecture.\"\"\"\n\n    def model_dump(self, **kwargs: Any) -&gt; dict[str, Any]:\n        \"\"\"\n        Dump the model as a dictionary, ignoring the architecture keyword.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Additional keyword arguments from Pydantic BaseModel model_dump method.\n\n        Returns\n        -------\n        {str: Any}\n            Model as a dictionary.\n        \"\"\"\n        model_dict = super().model_dump(**kwargs)\n\n        # remove the architecture key\n        model_dict.pop(\"architecture\")\n\n        return model_dict\n</code></pre>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":"<p>Name of the architecture.</p>"},{"location":"reference/careamics/config/architectures/architecture_model/#careamics.config.architectures.architecture_model.ArchitectureModel.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Dump the model as a dictionary, ignoring the architecture keyword.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments from Pydantic BaseModel model_dump method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>{str: Any}</code> <p>Model as a dictionary.</p> Source code in <code>src/careamics/config/architectures/architecture_model.py</code> <pre><code>def model_dump(self, **kwargs: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Dump the model as a dictionary, ignoring the architecture keyword.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Additional keyword arguments from Pydantic BaseModel model_dump method.\n\n    Returns\n    -------\n    {str: Any}\n        Model as a dictionary.\n    \"\"\"\n    model_dict = super().model_dump(**kwargs)\n\n    # remove the architecture key\n    model_dict.pop(\"architecture\")\n\n    return model_dict\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/","title":"lvae_model","text":"<p>LVAE Pydantic model.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel","title":"<code>LVAEModel</code>","text":"<p>               Bases: <code>ArchitectureModel</code></p> <p>LVAE model.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>class LVAEModel(ArchitectureModel):\n    \"\"\"LVAE model.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, validate_default=True)\n\n    architecture: Literal[\"LVAE\"]\n    \"\"\"Name of the architecture.\"\"\"\n\n    input_shape: list[int] = Field(default=[64, 64], validate_default=True)\n    \"\"\"Shape of the input patch (C, Z, Y, X) or (C, Y, X) if the data is 2D.\"\"\"\n\n    encoder_conv_strides: list = Field(default=[2, 2], validate_default=True)\n\n    # TODO make this per hierarchy step ?\n    decoder_conv_strides: list = Field(default=[2, 2], validate_default=True)\n    \"\"\"Dimensions (2D or 3D) of the convolutional layers.\"\"\"\n\n    multiscale_count: int = Field(default=1)\n    # TODO there should be a check for multiscale_count in dataset !!\n\n    # 1 - off, len(z_dims) + 1 # TODO Consider starting from 0\n    z_dims: list = Field(default=[128, 128, 128, 128])\n    output_channels: int = Field(default=1, ge=1)\n    encoder_n_filters: int = Field(default=64, ge=8, le=1024)\n    decoder_n_filters: int = Field(default=64, ge=8, le=1024)\n    encoder_dropout: float = Field(default=0.1, ge=0.0, le=0.9)\n    decoder_dropout: float = Field(default=0.1, ge=0.0, le=0.9)\n    nonlinearity: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\", \"ELU\"\n    ] = Field(\n        default=\"ELU\",\n    )\n\n    predict_logvar: Literal[None, \"pixelwise\"] = None\n    analytical_kl: bool = Field(default=False)\n\n    @model_validator(mode=\"after\")\n    def validate_conv_strides(self: Self) -&gt; Self:\n        \"\"\"\n        Validate the convolutional strides.\n\n        Returns\n        -------\n        list\n            Validated strides.\n\n        Raises\n        ------\n        ValueError\n            If the number of strides is not 2.\n        \"\"\"\n        if len(self.encoder_conv_strides) &lt; 2 or len(self.encoder_conv_strides) &gt; 3:\n            raise ValueError(\n                f\"Strides must be 2 or 3 (got {len(self.encoder_conv_strides)}).\"\n            )\n\n        if len(self.decoder_conv_strides) &lt; 2 or len(self.decoder_conv_strides) &gt; 3:\n            raise ValueError(\n                f\"Strides must be 2 or 3 (got {len(self.decoder_conv_strides)}).\"\n            )\n\n        # adding 1 to encoder strides for the number of input channels\n        if len(self.input_shape) != len(self.encoder_conv_strides):\n            raise ValueError(\n                f\"Input dimensions must be equal to the number of encoder conv strides\"\n                f\" (got {len(self.input_shape)} and {len(self.encoder_conv_strides)}).\"\n            )\n\n        if len(self.encoder_conv_strides) &lt; len(self.decoder_conv_strides):\n            raise ValueError(\n                f\"Decoder can't be 3D when encoder is 2D (got\"\n                f\" {len(self.encoder_conv_strides)} and\"\n                f\"{len(self.decoder_conv_strides)}).\"\n            )\n\n        if any(s &lt; 1 for s in self.encoder_conv_strides) or any(\n            s &lt; 1 for s in self.decoder_conv_strides\n        ):\n            raise ValueError(\n                f\"All strides must be greater or equal to 1\"\n                f\"(got {self.encoder_conv_strides} and {self.decoder_conv_strides}).\"\n            )\n        # TODO: validate max stride size ?\n        return self\n\n    @field_validator(\"input_shape\")\n    @classmethod\n    def validate_input_shape(cls, input_shape: list) -&gt; list:\n        \"\"\"\n        Validate the input shape.\n\n        Parameters\n        ----------\n        input_shape : list\n            Shape of the input patch.\n\n        Returns\n        -------\n        list\n            Validated input shape.\n\n        Raises\n        ------\n        ValueError\n            If the number of dimensions is not 3 or 4.\n        \"\"\"\n        if len(input_shape) &lt; 2 or len(input_shape) &gt; 3:\n            raise ValueError(\n                f\"Number of input dimensions must be 2 for 2D data 3 for 3D\"\n                f\"(got {len(input_shape)}).\"\n            )\n\n        if any(s &lt; 1 for s in input_shape):\n            raise ValueError(\n                f\"Input shape must be greater than 1 in all dimensions\"\n                f\"(got {input_shape}).\"\n            )\n        return input_shape\n\n    @field_validator(\"encoder_n_filters\")\n    @classmethod\n    def validate_encoder_even(cls, encoder_n_filters: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        encoder_n_filters : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if encoder_n_filters % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {encoder_n_filters}).\"\n            )\n\n        return encoder_n_filters\n\n    @field_validator(\"decoder_n_filters\")\n    @classmethod\n    def validate_decoder_even(cls, decoder_n_filters: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        decoder_n_filters : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if decoder_n_filters % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {decoder_n_filters}).\"\n            )\n\n        return decoder_n_filters\n\n    @field_validator(\"z_dims\")\n    def validate_z_dims(cls, z_dims: tuple) -&gt; tuple:\n        \"\"\"\n        Validate the z_dims.\n\n        Parameters\n        ----------\n        z_dims : tuple\n            Tuple of z dimensions.\n\n        Returns\n        -------\n        tuple\n            Validated z dimensions.\n\n        Raises\n        ------\n        ValueError\n            If the number of z dimensions is not 4.\n        \"\"\"\n        if len(z_dims) &lt; 2:\n            raise ValueError(\n                f\"Number of z dimensions must be at least 2 (got {len(z_dims)}).\"\n            )\n\n        return z_dims\n\n    @model_validator(mode=\"after\")\n    def validate_multiscale_count(self: Self) -&gt; Self:\n        \"\"\"\n        Validate the multiscale count.\n\n        Returns\n        -------\n        Self\n            The validated model.\n        \"\"\"\n        if self.multiscale_count &lt; 1 or self.multiscale_count &gt; len(self.z_dims) + 1:\n            raise ValueError(\n                f\"Multiscale count must be 1 for LC off or less or equal to the number\"\n                f\" of Z dims + 1 (got {self.multiscale_count} and {len(self.z_dims)}).\"\n            )\n        return self\n\n    def set_3D(self, is_3D: bool) -&gt; None:\n        \"\"\"\n        Set 3D model by setting the `conv_dims` parameters.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        \"\"\"\n        if is_3D:\n            self.conv_dims = 3\n        else:\n            self.conv_dims = 2\n\n    def is_3D(self) -&gt; bool:\n        \"\"\"\n        Return whether the model is 3D or not.\n\n        Returns\n        -------\n        bool\n            Whether the model is 3D or not.\n        \"\"\"\n        return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":"<p>Name of the architecture.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.decoder_conv_strides","title":"<code>decoder_conv_strides = Field(default=[2, 2], validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dimensions (2D or 3D) of the convolutional layers.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.input_shape","title":"<code>input_shape = Field(default=[64, 64], validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Shape of the input patch (C, Z, Y, X) or (C, Y, X) if the data is 2D.</p>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.is_3D","title":"<code>is_3D()</code>","text":"<p>Return whether the model is 3D or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the model is 3D or not.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>def is_3D(self) -&gt; bool:\n    \"\"\"\n    Return whether the model is 3D or not.\n\n    Returns\n    -------\n    bool\n        Whether the model is 3D or not.\n    \"\"\"\n    return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.set_3D","title":"<code>set_3D(is_3D)</code>","text":"<p>Set 3D model by setting the <code>conv_dims</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>def set_3D(self, is_3D: bool) -&gt; None:\n    \"\"\"\n    Set 3D model by setting the `conv_dims` parameters.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    \"\"\"\n    if is_3D:\n        self.conv_dims = 3\n    else:\n        self.conv_dims = 2\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_conv_strides","title":"<code>validate_conv_strides()</code>","text":"<p>Validate the convolutional strides.</p> <p>Returns:</p> Type Description <code>list</code> <p>Validated strides.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of strides is not 2.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_conv_strides(self: Self) -&gt; Self:\n    \"\"\"\n    Validate the convolutional strides.\n\n    Returns\n    -------\n    list\n        Validated strides.\n\n    Raises\n    ------\n    ValueError\n        If the number of strides is not 2.\n    \"\"\"\n    if len(self.encoder_conv_strides) &lt; 2 or len(self.encoder_conv_strides) &gt; 3:\n        raise ValueError(\n            f\"Strides must be 2 or 3 (got {len(self.encoder_conv_strides)}).\"\n        )\n\n    if len(self.decoder_conv_strides) &lt; 2 or len(self.decoder_conv_strides) &gt; 3:\n        raise ValueError(\n            f\"Strides must be 2 or 3 (got {len(self.decoder_conv_strides)}).\"\n        )\n\n    # adding 1 to encoder strides for the number of input channels\n    if len(self.input_shape) != len(self.encoder_conv_strides):\n        raise ValueError(\n            f\"Input dimensions must be equal to the number of encoder conv strides\"\n            f\" (got {len(self.input_shape)} and {len(self.encoder_conv_strides)}).\"\n        )\n\n    if len(self.encoder_conv_strides) &lt; len(self.decoder_conv_strides):\n        raise ValueError(\n            f\"Decoder can't be 3D when encoder is 2D (got\"\n            f\" {len(self.encoder_conv_strides)} and\"\n            f\"{len(self.decoder_conv_strides)}).\"\n        )\n\n    if any(s &lt; 1 for s in self.encoder_conv_strides) or any(\n        s &lt; 1 for s in self.decoder_conv_strides\n    ):\n        raise ValueError(\n            f\"All strides must be greater or equal to 1\"\n            f\"(got {self.encoder_conv_strides} and {self.decoder_conv_strides}).\"\n        )\n    # TODO: validate max stride size ?\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_decoder_even","title":"<code>validate_decoder_even(decoder_n_filters)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>decoder_n_filters</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"decoder_n_filters\")\n@classmethod\ndef validate_decoder_even(cls, decoder_n_filters: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    decoder_n_filters : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if decoder_n_filters % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {decoder_n_filters}).\"\n        )\n\n    return decoder_n_filters\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_encoder_even","title":"<code>validate_encoder_even(encoder_n_filters)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_n_filters</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"encoder_n_filters\")\n@classmethod\ndef validate_encoder_even(cls, encoder_n_filters: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    encoder_n_filters : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if encoder_n_filters % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {encoder_n_filters}).\"\n        )\n\n    return encoder_n_filters\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_input_shape","title":"<code>validate_input_shape(input_shape)</code>  <code>classmethod</code>","text":"<p>Validate the input shape.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>list</code> <p>Shape of the input patch.</p> required <p>Returns:</p> Type Description <code>list</code> <p>Validated input shape.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of dimensions is not 3 or 4.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"input_shape\")\n@classmethod\ndef validate_input_shape(cls, input_shape: list) -&gt; list:\n    \"\"\"\n    Validate the input shape.\n\n    Parameters\n    ----------\n    input_shape : list\n        Shape of the input patch.\n\n    Returns\n    -------\n    list\n        Validated input shape.\n\n    Raises\n    ------\n    ValueError\n        If the number of dimensions is not 3 or 4.\n    \"\"\"\n    if len(input_shape) &lt; 2 or len(input_shape) &gt; 3:\n        raise ValueError(\n            f\"Number of input dimensions must be 2 for 2D data 3 for 3D\"\n            f\"(got {len(input_shape)}).\"\n        )\n\n    if any(s &lt; 1 for s in input_shape):\n        raise ValueError(\n            f\"Input shape must be greater than 1 in all dimensions\"\n            f\"(got {input_shape}).\"\n        )\n    return input_shape\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_multiscale_count","title":"<code>validate_multiscale_count()</code>","text":"<p>Validate the multiscale count.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The validated model.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_multiscale_count(self: Self) -&gt; Self:\n    \"\"\"\n    Validate the multiscale count.\n\n    Returns\n    -------\n    Self\n        The validated model.\n    \"\"\"\n    if self.multiscale_count &lt; 1 or self.multiscale_count &gt; len(self.z_dims) + 1:\n        raise ValueError(\n            f\"Multiscale count must be 1 for LC off or less or equal to the number\"\n            f\" of Z dims + 1 (got {self.multiscale_count} and {len(self.z_dims)}).\"\n        )\n    return self\n</code></pre>"},{"location":"reference/careamics/config/architectures/lvae_model/#careamics.config.architectures.lvae_model.LVAEModel.validate_z_dims","title":"<code>validate_z_dims(z_dims)</code>","text":"<p>Validate the z_dims.</p> <p>Parameters:</p> Name Type Description Default <code>z_dims</code> <code>tuple</code> <p>Tuple of z dimensions.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Validated z dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of z dimensions is not 4.</p> Source code in <code>src/careamics/config/architectures/lvae_model.py</code> <pre><code>@field_validator(\"z_dims\")\ndef validate_z_dims(cls, z_dims: tuple) -&gt; tuple:\n    \"\"\"\n    Validate the z_dims.\n\n    Parameters\n    ----------\n    z_dims : tuple\n        Tuple of z dimensions.\n\n    Returns\n    -------\n    tuple\n        Validated z dimensions.\n\n    Raises\n    ------\n    ValueError\n        If the number of z dimensions is not 4.\n    \"\"\"\n    if len(z_dims) &lt; 2:\n        raise ValueError(\n            f\"Number of z dimensions must be at least 2 (got {len(z_dims)}).\"\n        )\n\n    return z_dims\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/","title":"unet_model","text":"<p>UNet Pydantic model.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel","title":"<code>UNetModel</code>","text":"<p>               Bases: <code>ArchitectureModel</code></p> <p>Pydantic model for a N2V(2)-compatible UNet.</p> <p>Attributes:</p> Name Type Description <code>depth</code> <code>int</code> <p>Depth of the model, between 1 and 10 (default 2).</p> <code>num_channels_init</code> <code>int</code> <p>Number of filters of the first level of the network, should be even and minimum 8 (default 96).</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>class UNetModel(ArchitectureModel):\n    \"\"\"\n    Pydantic model for a N2V(2)-compatible UNet.\n\n    Attributes\n    ----------\n    depth : int\n        Depth of the model, between 1 and 10 (default 2).\n    num_channels_init : int\n        Number of filters of the first level of the network, should be even\n        and minimum 8 (default 96).\n    \"\"\"\n\n    # pydantic model config\n    model_config = ConfigDict(validate_assignment=True)\n\n    # discriminator used for choosing the pydantic model in Model\n    architecture: Literal[\"UNet\"]\n    \"\"\"Name of the architecture.\"\"\"\n\n    # parameters\n    # validate_defaults allow ignoring default values in the dump if they were not set\n    conv_dims: Literal[2, 3] = Field(default=2, validate_default=True)\n    \"\"\"Dimensions (2D or 3D) of the convolutional layers.\"\"\"\n\n    num_classes: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Number of classes or channels in the model output.\"\"\"\n\n    in_channels: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Number of channels in the input to the model.\"\"\"\n\n    depth: int = Field(default=2, ge=1, le=10, validate_default=True)\n    \"\"\"Number of levels in the UNet.\"\"\"\n\n    num_channels_init: int = Field(default=32, ge=8, le=1024, validate_default=True)\n    \"\"\"Number of convolutional filters in the first layer of the UNet.\"\"\"\n\n    # TODO we are not using this, so why make it a choice?\n    final_activation: Literal[\n        \"None\", \"Sigmoid\", \"Softmax\", \"Tanh\", \"ReLU\", \"LeakyReLU\"\n    ] = Field(default=\"None\", validate_default=True)\n    \"\"\"Final activation function.\"\"\"\n\n    n2v2: bool = Field(default=False, validate_default=True)\n    \"\"\"Whether to use N2V2 architecture modifications, with blur pool layers and fewer\n    skip connections.\n    \"\"\"\n\n    independent_channels: bool = Field(default=True, validate_default=True)\n    \"\"\"Whether information is processed independently in each channel, used to train\n    channels independently.\"\"\"\n\n    @field_validator(\"num_channels_init\")\n    @classmethod\n    def validate_num_channels_init(cls, num_channels_init: int) -&gt; int:\n        \"\"\"\n        Validate that num_channels_init is even.\n\n        Parameters\n        ----------\n        num_channels_init : int\n            Number of channels.\n\n        Returns\n        -------\n        int\n            Validated number of channels.\n\n        Raises\n        ------\n        ValueError\n            If the number of channels is odd.\n        \"\"\"\n        # if odd\n        if num_channels_init % 2 != 0:\n            raise ValueError(\n                f\"Number of channels for the bottom layer must be even\"\n                f\" (got {num_channels_init}).\"\n            )\n\n        return num_channels_init\n\n    def set_3D(self, is_3D: bool) -&gt; None:\n        \"\"\"\n        Set 3D model by setting the `conv_dims` parameters.\n\n        Parameters\n        ----------\n        is_3D : bool\n            Whether the algorithm is 3D or not.\n        \"\"\"\n        if is_3D:\n            self.conv_dims = 3\n        else:\n            self.conv_dims = 2\n\n    def is_3D(self) -&gt; bool:\n        \"\"\"\n        Return whether the model is 3D or not.\n\n        Returns\n        -------\n        bool\n            Whether the model is 3D or not.\n        \"\"\"\n        return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":"<p>Name of the architecture.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.conv_dims","title":"<code>conv_dims = Field(default=2, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dimensions (2D or 3D) of the convolutional layers.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.depth","title":"<code>depth = Field(default=2, ge=1, le=10, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of levels in the UNet.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.final_activation","title":"<code>final_activation = Field(default='None', validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Final activation function.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.in_channels","title":"<code>in_channels = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels in the input to the model.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.independent_channels","title":"<code>independent_channels = Field(default=True, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether information is processed independently in each channel, used to train channels independently.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.n2v2","title":"<code>n2v2 = Field(default=False, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use N2V2 architecture modifications, with blur pool layers and fewer skip connections.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.num_channels_init","title":"<code>num_channels_init = Field(default=32, ge=8, le=1024, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of convolutional filters in the first layer of the UNet.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.num_classes","title":"<code>num_classes = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of classes or channels in the model output.</p>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.is_3D","title":"<code>is_3D()</code>","text":"<p>Return whether the model is 3D or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the model is 3D or not.</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>def is_3D(self) -&gt; bool:\n    \"\"\"\n    Return whether the model is 3D or not.\n\n    Returns\n    -------\n    bool\n        Whether the model is 3D or not.\n    \"\"\"\n    return self.conv_dims == 3\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.set_3D","title":"<code>set_3D(is_3D)</code>","text":"<p>Set 3D model by setting the <code>conv_dims</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>is_3D</code> <code>bool</code> <p>Whether the algorithm is 3D or not.</p> required Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>def set_3D(self, is_3D: bool) -&gt; None:\n    \"\"\"\n    Set 3D model by setting the `conv_dims` parameters.\n\n    Parameters\n    ----------\n    is_3D : bool\n        Whether the algorithm is 3D or not.\n    \"\"\"\n    if is_3D:\n        self.conv_dims = 3\n    else:\n        self.conv_dims = 2\n</code></pre>"},{"location":"reference/careamics/config/architectures/unet_model/#careamics.config.architectures.unet_model.UNetModel.validate_num_channels_init","title":"<code>validate_num_channels_init(num_channels_init)</code>  <code>classmethod</code>","text":"<p>Validate that num_channels_init is even.</p> <p>Parameters:</p> Name Type Description Default <code>num_channels_init</code> <code>int</code> <p>Number of channels.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Validated number of channels.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of channels is odd.</p> Source code in <code>src/careamics/config/architectures/unet_model.py</code> <pre><code>@field_validator(\"num_channels_init\")\n@classmethod\ndef validate_num_channels_init(cls, num_channels_init: int) -&gt; int:\n    \"\"\"\n    Validate that num_channels_init is even.\n\n    Parameters\n    ----------\n    num_channels_init : int\n        Number of channels.\n\n    Returns\n    -------\n    int\n        Validated number of channels.\n\n    Raises\n    ------\n    ValueError\n        If the number of channels is odd.\n    \"\"\"\n    # if odd\n    if num_channels_init % 2 != 0:\n        raise ValueError(\n            f\"Number of channels for the bottom layer must be even\"\n            f\" (got {num_channels_init}).\"\n        )\n\n    return num_channels_init\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/","title":"data_model","text":"<p>Data configuration.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.Float","title":"<code>Float = Annotated[float, PlainSerializer(np_float_to_scientific_str, return_type=str)]</code>  <code>module-attribute</code>","text":"<p>Annotated float type, used to serialize floats to strings.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig","title":"<code>DataConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data configuration.</p> <p>If std is specified, mean must be specified as well. Note that setting the std first and then the mean (if they were both <code>None</code> before) will raise a validation error. Prefer instead <code>set_mean_and_std</code> to set both at once. Means and stds are expected to be lists of floats, one for each channel. For supervised tasks, the mean and std of the target could be different from the input data.</p> <p>All supported transforms are defined in the SupportedTransform enum.</p> <p>Examples:</p> <p>Minimum example:</p> <pre><code>&gt;&gt;&gt; data = DataConfig(\n...     data_type=\"array\", # defined in SupportedData\n...     patch_size=[128, 128],\n...     batch_size=4,\n...     axes=\"YX\"\n... )\n</code></pre> <p>To change the image_means and image_stds of the data:</p> <pre><code>&gt;&gt;&gt; data.set_means_and_stds(image_means=[214.3], image_stds=[84.5])\n</code></pre> <p>One can pass also a list of transformations, by keyword, using the SupportedTransform value:</p> <pre><code>&gt;&gt;&gt; from careamics.config.support import SupportedTransform\n&gt;&gt;&gt; data = DataConfig(\n...     data_type=\"tiff\",\n...     patch_size=[128, 128],\n...     batch_size=4,\n...     axes=\"YX\",\n...     transforms=[\n...         {\n...             \"name\": \"XYFlip\",\n...         }\n...     ]\n... )\n</code></pre> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>class DataConfig(BaseModel):\n    \"\"\"Data configuration.\n\n    If std is specified, mean must be specified as well. Note that setting the std first\n    and then the mean (if they were both `None` before) will raise a validation error.\n    Prefer instead `set_mean_and_std` to set both at once. Means and stds are expected\n    to be lists of floats, one for each channel. For supervised tasks, the mean and std\n    of the target could be different from the input data.\n\n    All supported transforms are defined in the SupportedTransform enum.\n\n    Examples\n    --------\n    Minimum example:\n\n    &gt;&gt;&gt; data = DataConfig(\n    ...     data_type=\"array\", # defined in SupportedData\n    ...     patch_size=[128, 128],\n    ...     batch_size=4,\n    ...     axes=\"YX\"\n    ... )\n\n    To change the image_means and image_stds of the data:\n    &gt;&gt;&gt; data.set_means_and_stds(image_means=[214.3], image_stds=[84.5])\n\n    One can pass also a list of transformations, by keyword, using the\n    SupportedTransform value:\n    &gt;&gt;&gt; from careamics.config.support import SupportedTransform\n    &gt;&gt;&gt; data = DataConfig(\n    ...     data_type=\"tiff\",\n    ...     patch_size=[128, 128],\n    ...     batch_size=4,\n    ...     axes=\"YX\",\n    ...     transforms=[\n    ...         {\n    ...             \"name\": \"XYFlip\",\n    ...         }\n    ...     ]\n    ... )\n    \"\"\"\n\n    # Pydantic class configuration\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    # Dataset configuration\n    data_type: Literal[\"array\", \"tiff\", \"custom\"]\n    \"\"\"Type of input data, numpy.ndarray (array) or paths (tiff and custom), as defined\n    in SupportedData.\"\"\"\n\n    axes: str\n    \"\"\"Axes of the data, as defined in SupportedAxes.\"\"\"\n\n    patch_size: Union[list[int]] = Field(..., min_length=2, max_length=3)\n    \"\"\"Patch size, as used during training.\"\"\"\n\n    batch_size: int = Field(default=1, ge=1, validate_default=True)\n    \"\"\"Batch size for training.\"\"\"\n\n    # Optional fields\n    image_means: Optional[list[Float]] = Field(\n        default=None, min_length=0, max_length=32\n    )\n    \"\"\"Means of the data across channels, used for normalization.\"\"\"\n\n    image_stds: Optional[list[Float]] = Field(default=None, min_length=0, max_length=32)\n    \"\"\"Standard deviations of the data across channels, used for normalization.\"\"\"\n\n    target_means: Optional[list[Float]] = Field(\n        default=None, min_length=0, max_length=32\n    )\n    \"\"\"Means of the target data across channels, used for normalization.\"\"\"\n\n    target_stds: Optional[list[Float]] = Field(\n        default=None, min_length=0, max_length=32\n    )\n    \"\"\"Standard deviations of the target data across channels, used for\n    normalization.\"\"\"\n\n    transforms: Sequence[Union[XYFlipModel, XYRandomRotate90Model]] = Field(\n        default=[\n            XYFlipModel(),\n            XYRandomRotate90Model(),\n        ],\n        validate_default=True,\n    )\n    \"\"\"List of transformations to apply to the data, available transforms are defined\n    in SupportedTransform.\"\"\"\n\n    train_dataloader_params: dict[str, Any] = Field(\n        default={\"shuffle\": True}, validate_default=True\n    )\n    \"\"\"Dictionary of PyTorch training dataloader parameters. The dataloader parameters,\n    should include the `shuffle` key, which is set to `True` by default. We strongly\n    recommend to keep it as `True` to ensure the best training results.\"\"\"\n\n    val_dataloader_params: dict[str, Any] = Field(default={})\n    \"\"\"Dictionary of PyTorch validation dataloader parameters.\"\"\"\n\n    @field_validator(\"patch_size\")\n    @classmethod\n    def all_elements_power_of_2_minimum_8(\n        cls, patch_list: Union[list[int]]\n    ) -&gt; Union[list[int]]:\n        \"\"\"\n        Validate patch size.\n\n        Patch size must be powers of 2 and minimum 8.\n\n        Parameters\n        ----------\n        patch_list : list of int\n            Patch size.\n\n        Returns\n        -------\n        list of int\n            Validated patch size.\n\n        Raises\n        ------\n        ValueError\n            If the patch size is smaller than 8.\n        ValueError\n            If the patch size is not a power of 2.\n        \"\"\"\n        patch_size_ge_than_8_power_of_2(patch_list)\n\n        return patch_list\n\n    @field_validator(\"axes\")\n    @classmethod\n    def axes_valid(cls, axes: str) -&gt; str:\n        \"\"\"\n        Validate axes.\n\n        Axes must:\n        - be a combination of 'STCZYX'\n        - not contain duplicates\n        - contain at least 2 contiguous axes: X and Y\n        - contain at most 4 axes\n        - not contain both S and T axes\n\n        Parameters\n        ----------\n        axes : str\n            Axes to validate.\n\n        Returns\n        -------\n        str\n            Validated axes.\n\n        Raises\n        ------\n        ValueError\n            If axes are not valid.\n        \"\"\"\n        # Validate axes\n        check_axes_validity(axes)\n\n        return axes\n\n    @field_validator(\"train_dataloader_params\")\n    @classmethod\n    def shuffle_train_dataloader(\n        cls, train_dataloader_params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Validate that \"shuffle\" is included in the training dataloader params.\n\n        A warning will be raised if `shuffle=False`.\n\n        Parameters\n        ----------\n        train_dataloader_params : dict of {str: Any}\n            The training dataloader parameters.\n\n        Returns\n        -------\n        dict of {str: Any}\n            The validated training dataloader parameters.\n\n        Raises\n        ------\n        ValueError\n            If \"shuffle\" is not included in the training dataloader params.\n        \"\"\"\n        if \"shuffle\" not in train_dataloader_params:\n            raise ValueError(\n                \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n            )\n        elif (\"shuffle\" in train_dataloader_params) and (\n            not train_dataloader_params[\"shuffle\"]\n        ):\n            warn(\n                \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n                \"the training dataloader and may lead to lower quality results.\",\n                stacklevel=1,\n            )\n        return train_dataloader_params\n\n    @model_validator(mode=\"after\")\n    def std_only_with_mean(self: Self) -&gt; Self:\n        \"\"\"\n        Check that mean and std are either both None, or both specified.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If std is not None and mean is None.\n        \"\"\"\n        # check that mean and std are either both None, or both specified\n        if (self.image_means and not self.image_stds) or (\n            self.image_stds and not self.image_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified.\"\n            )\n\n        elif (self.image_means is not None and self.image_stds is not None) and (\n            len(self.image_means) != len(self.image_stds)\n        ):\n            raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n        if (self.target_means and not self.target_stds) or (\n            self.target_stds and not self.target_means\n        ):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified \"\n            )\n\n        elif self.target_means is not None and self.target_stds is not None:\n            if len(self.target_means) != len(self.target_stds):\n                raise ValueError(\n                    \"Mean and std must be either both None, or both specified for each \"\n                    \"target channel.\"\n                )\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_dimensions(self: Self) -&gt; Self:\n        \"\"\"\n        Validate 2D/3D dimensions between axes, patch size and transforms.\n\n        Returns\n        -------\n        Self\n            Validated data model.\n\n        Raises\n        ------\n        ValueError\n            If the transforms are not valid.\n        \"\"\"\n        if \"Z\" in self.axes:\n            if len(self.patch_size) != 3:\n                raise ValueError(\n                    f\"Patch size must have 3 dimensions if the data is 3D \"\n                    f\"({self.axes}).\"\n                )\n\n        else:\n            if len(self.patch_size) != 2:\n                raise ValueError(\n                    f\"Patch size must have 3 dimensions if the data is 3D \"\n                    f\"({self.axes}).\"\n                )\n\n        return self\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Pretty string reprensenting the configuration.\n\n        Returns\n        -------\n        str\n            Pretty string.\n        \"\"\"\n        return pformat(self.model_dump())\n\n    def _update(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Update multiple arguments at once.\n\n        Parameters\n        ----------\n        **kwargs : Any\n            Keyword arguments to update.\n        \"\"\"\n        self.__dict__.update(kwargs)\n        self.__class__.model_validate(self.__dict__)\n\n    def set_means_and_stds(\n        self,\n        image_means: Union[NDArray, tuple, list, None],\n        image_stds: Union[NDArray, tuple, list, None],\n        target_means: Optional[Union[NDArray, tuple, list, None]] = None,\n        target_stds: Optional[Union[NDArray, tuple, list, None]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Set mean and standard deviation of the data across channels.\n\n        This method should be used instead setting the fields directly, as it would\n        otherwise trigger a validation error.\n\n        Parameters\n        ----------\n        image_means : numpy.ndarray, tuple or list\n            Mean values for normalization.\n        image_stds : numpy.ndarray, tuple or list\n            Standard deviation values for normalization.\n        target_means : numpy.ndarray, tuple or list, optional\n            Target mean values for normalization, by default ().\n        target_stds : numpy.ndarray, tuple or list, optional\n            Target standard deviation values for normalization, by default ().\n        \"\"\"\n        # make sure we pass a list\n        if image_means is not None:\n            image_means = list(image_means)\n        if image_stds is not None:\n            image_stds = list(image_stds)\n        if target_means is not None:\n            target_means = list(target_means)\n        if target_stds is not None:\n            target_stds = list(target_stds)\n\n        self._update(\n            image_means=image_means,\n            image_stds=image_stds,\n            target_means=target_means,\n            target_stds=target_stds,\n        )\n\n    def set_3D(self, axes: str, patch_size: list[int]) -&gt; None:\n        \"\"\"\n        Set 3D parameters.\n\n        Parameters\n        ----------\n        axes : str\n            Axes.\n        patch_size : list of int\n            Patch size.\n        \"\"\"\n        self._update(axes=axes, patch_size=patch_size)\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.axes","title":"<code>axes</code>  <code>instance-attribute</code>","text":"<p>Axes of the data, as defined in SupportedAxes.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.batch_size","title":"<code>batch_size = Field(default=1, ge=1, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size for training.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of input data, numpy.ndarray (array) or paths (tiff and custom), as defined in SupportedData.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.image_means","title":"<code>image_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.image_stds","title":"<code>image_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.patch_size","title":"<code>patch_size = Field(..., min_length=2, max_length=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Patch size, as used during training.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.target_means","title":"<code>target_means = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Means of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.target_stds","title":"<code>target_stds = Field(default=None, min_length=0, max_length=32)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Standard deviations of the target data across channels, used for normalization.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.train_dataloader_params","title":"<code>train_dataloader_params = Field(default={'shuffle': True}, validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch training dataloader parameters. The dataloader parameters, should include the <code>shuffle</code> key, which is set to <code>True</code> by default. We strongly recommend to keep it as <code>True</code> to ensure the best training results.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.transforms","title":"<code>transforms = Field(default=[XYFlipModel(), XYRandomRotate90Model()], validate_default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations to apply to the data, available transforms are defined in SupportedTransform.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.val_dataloader_params","title":"<code>val_dataloader_params = Field(default={})</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dictionary of PyTorch validation dataloader parameters.</p>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.__str__","title":"<code>__str__()</code>","text":"<p>Pretty string reprensenting the configuration.</p> <p>Returns:</p> Type Description <code>str</code> <p>Pretty string.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Pretty string reprensenting the configuration.\n\n    Returns\n    -------\n    str\n        Pretty string.\n    \"\"\"\n    return pformat(self.model_dump())\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig._update","title":"<code>_update(**kwargs)</code>","text":"<p>Update multiple arguments at once.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to update.</p> <code>{}</code> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def _update(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Update multiple arguments at once.\n\n    Parameters\n    ----------\n    **kwargs : Any\n        Keyword arguments to update.\n    \"\"\"\n    self.__dict__.update(kwargs)\n    self.__class__.model_validate(self.__dict__)\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.all_elements_power_of_2_minimum_8","title":"<code>all_elements_power_of_2_minimum_8(patch_list)</code>  <code>classmethod</code>","text":"<p>Validate patch size.</p> <p>Patch size must be powers of 2 and minimum 8.</p> <p>Parameters:</p> Name Type Description Default <code>patch_list</code> <code>list of int</code> <p>Patch size.</p> required <p>Returns:</p> Type Description <code>list of int</code> <p>Validated patch size.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"patch_size\")\n@classmethod\ndef all_elements_power_of_2_minimum_8(\n    cls, patch_list: Union[list[int]]\n) -&gt; Union[list[int]]:\n    \"\"\"\n    Validate patch size.\n\n    Patch size must be powers of 2 and minimum 8.\n\n    Parameters\n    ----------\n    patch_list : list of int\n        Patch size.\n\n    Returns\n    -------\n    list of int\n        Validated patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    patch_size_ge_than_8_power_of_2(patch_list)\n\n    return patch_list\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.axes_valid","title":"<code>axes_valid(axes)</code>  <code>classmethod</code>","text":"<p>Validate axes.</p> <p>Axes must: - be a combination of 'STCZYX' - not contain duplicates - contain at least 2 contiguous axes: X and Y - contain at most 4 axes - not contain both S and T axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Validated axes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If axes are not valid.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"axes\")\n@classmethod\ndef axes_valid(cls, axes: str) -&gt; str:\n    \"\"\"\n    Validate axes.\n\n    Axes must:\n    - be a combination of 'STCZYX'\n    - not contain duplicates\n    - contain at least 2 contiguous axes: X and Y\n    - contain at most 4 axes\n    - not contain both S and T axes\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n\n    Returns\n    -------\n    str\n        Validated axes.\n\n    Raises\n    ------\n    ValueError\n        If axes are not valid.\n    \"\"\"\n    # Validate axes\n    check_axes_validity(axes)\n\n    return axes\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_3D","title":"<code>set_3D(axes, patch_size)</code>","text":"<p>Set 3D parameters.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes.</p> required <code>patch_size</code> <code>list of int</code> <p>Patch size.</p> required Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def set_3D(self, axes: str, patch_size: list[int]) -&gt; None:\n    \"\"\"\n    Set 3D parameters.\n\n    Parameters\n    ----------\n    axes : str\n        Axes.\n    patch_size : list of int\n        Patch size.\n    \"\"\"\n    self._update(axes=axes, patch_size=patch_size)\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.set_means_and_stds","title":"<code>set_means_and_stds(image_means, image_stds, target_means=None, target_stds=None)</code>","text":"<p>Set mean and standard deviation of the data across channels.</p> <p>This method should be used instead setting the fields directly, as it would otherwise trigger a validation error.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>(ndarray, tuple or list)</code> <p>Mean values for normalization.</p> required <code>image_stds</code> <code>(ndarray, tuple or list)</code> <p>Standard deviation values for normalization.</p> required <code>target_means</code> <code>(ndarray, tuple or list)</code> <p>Target mean values for normalization, by default ().</p> <code>None</code> <code>target_stds</code> <code>(ndarray, tuple or list)</code> <p>Target standard deviation values for normalization, by default ().</p> <code>None</code> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def set_means_and_stds(\n    self,\n    image_means: Union[NDArray, tuple, list, None],\n    image_stds: Union[NDArray, tuple, list, None],\n    target_means: Optional[Union[NDArray, tuple, list, None]] = None,\n    target_stds: Optional[Union[NDArray, tuple, list, None]] = None,\n) -&gt; None:\n    \"\"\"\n    Set mean and standard deviation of the data across channels.\n\n    This method should be used instead setting the fields directly, as it would\n    otherwise trigger a validation error.\n\n    Parameters\n    ----------\n    image_means : numpy.ndarray, tuple or list\n        Mean values for normalization.\n    image_stds : numpy.ndarray, tuple or list\n        Standard deviation values for normalization.\n    target_means : numpy.ndarray, tuple or list, optional\n        Target mean values for normalization, by default ().\n    target_stds : numpy.ndarray, tuple or list, optional\n        Target standard deviation values for normalization, by default ().\n    \"\"\"\n    # make sure we pass a list\n    if image_means is not None:\n        image_means = list(image_means)\n    if image_stds is not None:\n        image_stds = list(image_stds)\n    if target_means is not None:\n        target_means = list(target_means)\n    if target_stds is not None:\n        target_stds = list(target_stds)\n\n    self._update(\n        image_means=image_means,\n        image_stds=image_stds,\n        target_means=target_means,\n        target_stds=target_stds,\n    )\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.shuffle_train_dataloader","title":"<code>shuffle_train_dataloader(train_dataloader_params)</code>  <code>classmethod</code>","text":"<p>Validate that \"shuffle\" is included in the training dataloader params.</p> <p>A warning will be raised if <code>shuffle=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader_params</code> <code>dict of {str: Any}</code> <p>The training dataloader parameters.</p> required <p>Returns:</p> Type Description <code>dict of {str: Any}</code> <p>The validated training dataloader parameters.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If \"shuffle\" is not included in the training dataloader params.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@field_validator(\"train_dataloader_params\")\n@classmethod\ndef shuffle_train_dataloader(\n    cls, train_dataloader_params: dict[str, Any]\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Validate that \"shuffle\" is included in the training dataloader params.\n\n    A warning will be raised if `shuffle=False`.\n\n    Parameters\n    ----------\n    train_dataloader_params : dict of {str: Any}\n        The training dataloader parameters.\n\n    Returns\n    -------\n    dict of {str: Any}\n        The validated training dataloader parameters.\n\n    Raises\n    ------\n    ValueError\n        If \"shuffle\" is not included in the training dataloader params.\n    \"\"\"\n    if \"shuffle\" not in train_dataloader_params:\n        raise ValueError(\n            \"Value for 'shuffle' was not included in the `train_dataloader_params`.\"\n        )\n    elif (\"shuffle\" in train_dataloader_params) and (\n        not train_dataloader_params[\"shuffle\"]\n    ):\n        warn(\n            \"Dataloader parameters include `shuffle=False`, this will be passed to \"\n            \"the training dataloader and may lead to lower quality results.\",\n            stacklevel=1,\n        )\n    return train_dataloader_params\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.std_only_with_mean","title":"<code>std_only_with_mean()</code>","text":"<p>Check that mean and std are either both None, or both specified.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If std is not None and mean is None.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef std_only_with_mean(self: Self) -&gt; Self:\n    \"\"\"\n    Check that mean and std are either both None, or both specified.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If std is not None and mean is None.\n    \"\"\"\n    # check that mean and std are either both None, or both specified\n    if (self.image_means and not self.image_stds) or (\n        self.image_stds and not self.image_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified.\"\n        )\n\n    elif (self.image_means is not None and self.image_stds is not None) and (\n        len(self.image_means) != len(self.image_stds)\n    ):\n        raise ValueError(\"Mean and std must be specified for each input channel.\")\n\n    if (self.target_means and not self.target_stds) or (\n        self.target_stds and not self.target_means\n    ):\n        raise ValueError(\n            \"Mean and std must be either both None, or both specified \"\n        )\n\n    elif self.target_means is not None and self.target_stds is not None:\n        if len(self.target_means) != len(self.target_stds):\n            raise ValueError(\n                \"Mean and std must be either both None, or both specified for each \"\n                \"target channel.\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.DataConfig.validate_dimensions","title":"<code>validate_dimensions()</code>","text":"<p>Validate 2D/3D dimensions between axes, patch size and transforms.</p> <p>Returns:</p> Type Description <code>Self</code> <p>Validated data model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the transforms are not valid.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_dimensions(self: Self) -&gt; Self:\n    \"\"\"\n    Validate 2D/3D dimensions between axes, patch size and transforms.\n\n    Returns\n    -------\n    Self\n        Validated data model.\n\n    Raises\n    ------\n    ValueError\n        If the transforms are not valid.\n    \"\"\"\n    if \"Z\" in self.axes:\n        if len(self.patch_size) != 3:\n            raise ValueError(\n                f\"Patch size must have 3 dimensions if the data is 3D \"\n                f\"({self.axes}).\"\n            )\n\n    else:\n        if len(self.patch_size) != 2:\n            raise ValueError(\n                f\"Patch size must have 3 dimensions if the data is 3D \"\n                f\"({self.axes}).\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/data/data_model/#careamics.config.data.data_model.np_float_to_scientific_str","title":"<code>np_float_to_scientific_str(x)</code>","text":"<p>Return a string scientific representation of a float.</p> <p>In particular, this method is used to serialize floats to strings, allowing numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>Input value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Scientific string representation of the input value.</p> Source code in <code>src/careamics/config/data/data_model.py</code> <pre><code>def np_float_to_scientific_str(x: float) -&gt; str:\n    \"\"\"Return a string scientific representation of a float.\n\n    In particular, this method is used to serialize floats to strings, allowing\n    numpy.float32 to be passed in the Pydantic model and written to a yaml file as str.\n\n    Parameters\n    ----------\n    x : float\n        Input value.\n\n    Returns\n    -------\n    str\n        Scientific string representation of the input value.\n    \"\"\"\n    return np.format_float_scientific(x, precision=7)\n</code></pre>"},{"location":"reference/careamics/config/support/supported_activations/","title":"supported_activations","text":"<p>Activations supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_activations/#careamics.config.support.supported_activations.SupportedActivation","title":"<code>SupportedActivation</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported activation functions.</p> <ul> <li>None, no activation will be used.</li> <li>Sigmoid</li> <li>Softmax</li> <li>Tanh</li> <li>ReLU</li> <li>LeakyReLU</li> </ul> <p>All activations are defined in PyTorch.</p> <p>See: https://pytorch.org/docs/stable/nn.html#loss-functions</p> Source code in <code>src/careamics/config/support/supported_activations.py</code> <pre><code>class SupportedActivation(str, BaseEnum):\n    \"\"\"Supported activation functions.\n\n    - None, no activation will be used.\n    - Sigmoid\n    - Softmax\n    - Tanh\n    - ReLU\n    - LeakyReLU\n\n    All activations are defined in PyTorch.\n\n    See: https://pytorch.org/docs/stable/nn.html#loss-functions\n    \"\"\"\n\n    NONE = \"None\"\n    SIGMOID = \"Sigmoid\"\n    SOFTMAX = \"Softmax\"\n    TANH = \"Tanh\"\n    RELU = \"ReLU\"\n    LEAKYRELU = \"LeakyReLU\"\n    ELU = \"ELU\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_algorithms/","title":"supported_algorithms","text":"<p>Algorithms supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm","title":"<code>SupportedAlgorithm</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Algorithms available in CAREamics.</p> <p>These definitions are the same as the keyword <code>name</code> of the algorithm configurations.</p> Source code in <code>src/careamics/config/support/supported_algorithms.py</code> <pre><code>class SupportedAlgorithm(str, BaseEnum):\n    \"\"\"Algorithms available in CAREamics.\n\n    These definitions are the same as the keyword `name` of the algorithm\n    configurations.\n    \"\"\"\n\n    N2V = \"n2v\"\n    \"\"\"Noise2Void algorithm, a self-supervised approach based on blind denoising.\"\"\"\n\n    CARE = \"care\"\n    \"\"\"Content-aware image restoration, a supervised algorithm used for a variety\n    of tasks.\"\"\"\n\n    N2N = \"n2n\"\n    \"\"\"Noise2Noise algorithm, a self-supervised denoising scheme based on comparing\n    noisy images of the same sample.\"\"\"\n\n    MUSPLIT = \"musplit\"\n    \"\"\"An image splitting approach based on ladder VAE architectures.\"\"\"\n\n    DENOISPLIT = \"denoisplit\"\n    \"\"\"An image splitting and denoising approach based on ladder VAE architectures.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.CARE","title":"<code>CARE = 'care'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content-aware image restoration, a supervised algorithm used for a variety of tasks.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.DENOISPLIT","title":"<code>DENOISPLIT = 'denoisplit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An image splitting and denoising approach based on ladder VAE architectures.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.MUSPLIT","title":"<code>MUSPLIT = 'musplit'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An image splitting approach based on ladder VAE architectures.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.N2N","title":"<code>N2N = 'n2n'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise2Noise algorithm, a self-supervised denoising scheme based on comparing noisy images of the same sample.</p>"},{"location":"reference/careamics/config/support/supported_algorithms/#careamics.config.support.supported_algorithms.SupportedAlgorithm.N2V","title":"<code>N2V = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise2Void algorithm, a self-supervised approach based on blind denoising.</p>"},{"location":"reference/careamics/config/support/supported_architectures/","title":"supported_architectures","text":"<p>Architectures supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture","title":"<code>SupportedArchitecture</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported architectures.</p> Source code in <code>src/careamics/config/support/supported_architectures.py</code> <pre><code>class SupportedArchitecture(str, BaseEnum):\n    \"\"\"Supported architectures.\"\"\"\n\n    UNET = \"UNet\"\n    \"\"\"UNet architecture used with N2V, CARE and Noise2Noise.\"\"\"\n\n    LVAE = \"LVAE\"\n    \"\"\"Ladder Variational Autoencoder used for muSplit and denoiSplit.\"\"\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture.LVAE","title":"<code>LVAE = 'LVAE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ladder Variational Autoencoder used for muSplit and denoiSplit.</p>"},{"location":"reference/careamics/config/support/supported_architectures/#careamics.config.support.supported_architectures.SupportedArchitecture.UNET","title":"<code>UNET = 'UNet'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>UNet architecture used with N2V, CARE and Noise2Noise.</p>"},{"location":"reference/careamics/config/support/supported_data/","title":"supported_data","text":"<p>Data supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData","title":"<code>SupportedData</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported data types.</p> <p>Attributes:</p> Name Type Description <code>ARRAY</code> <code>str</code> <p>Array data.</p> <code>TIFF</code> <code>str</code> <p>TIFF image data.</p> <code>CUSTOM</code> <code>str</code> <p>Custom data.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>class SupportedData(str, BaseEnum):\n    \"\"\"Supported data types.\n\n    Attributes\n    ----------\n    ARRAY : str\n        Array data.\n    TIFF : str\n        TIFF image data.\n    CUSTOM : str\n        Custom data.\n    \"\"\"\n\n    ARRAY = \"array\"\n    TIFF = \"tiff\"\n    CUSTOM = \"custom\"\n    # ZARR = \"zarr\"\n\n    # TODO remove?\n    @classmethod\n    def _missing_(cls, value: object) -&gt; str:\n        \"\"\"\n        Override default behaviour for missing values.\n\n        This method is called when `value` is not found in the enum values. It converts\n        `value` to lowercase, removes \".\" if it is the first character and tries to\n        match it with enum values.\n\n        Parameters\n        ----------\n        value : object\n            Value to be matched with enum values.\n\n        Returns\n        -------\n        str\n            Matched enum value.\n        \"\"\"\n        if isinstance(value, str):\n            lower_value = value.lower()\n\n            if lower_value.startswith(\".\"):\n                lower_value = lower_value[1:]\n\n            # attempt to match lowercase value with enum values\n            for member in cls:\n                if member.value == lower_value:\n                    return member\n\n        # still missing\n        return super()._missing_(value)\n\n    @classmethod\n    def get_extension_pattern(cls, data_type: Union[str, SupportedData]) -&gt; str:\n        \"\"\"\n        Get Path.rglob and fnmatch compatible extension.\n\n        Parameters\n        ----------\n        data_type : SupportedData\n            Data type.\n\n        Returns\n        -------\n        str\n            Corresponding extension pattern.\n        \"\"\"\n        if data_type == cls.ARRAY:\n            raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n        elif data_type == cls.TIFF:\n            return \"*.tif*\"\n        elif data_type == cls.CUSTOM:\n            return \"*.*\"\n        else:\n            raise ValueError(f\"Data type {data_type} is not supported.\")\n\n    @classmethod\n    def get_extension(cls, data_type: Union[str, SupportedData]) -&gt; str:\n        \"\"\"\n        Get file extension of corresponding data type.\n\n        Parameters\n        ----------\n        data_type : str or SupportedData\n            Data type.\n\n        Returns\n        -------\n        str\n            Corresponding extension.\n        \"\"\"\n        if data_type == cls.ARRAY:\n            raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n        elif data_type == cls.TIFF:\n            return \".tiff\"\n        elif data_type == cls.CUSTOM:\n            # TODO: improve this message\n            raise NotImplementedError(\"Custom extensions have to be passed elsewhere.\")\n        else:\n            raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData._missing_","title":"<code>_missing_(value)</code>  <code>classmethod</code>","text":"<p>Override default behaviour for missing values.</p> <p>This method is called when <code>value</code> is not found in the enum values. It converts <code>value</code> to lowercase, removes \".\" if it is the first character and tries to match it with enum values.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>object</code> <p>Value to be matched with enum values.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Matched enum value.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>@classmethod\ndef _missing_(cls, value: object) -&gt; str:\n    \"\"\"\n    Override default behaviour for missing values.\n\n    This method is called when `value` is not found in the enum values. It converts\n    `value` to lowercase, removes \".\" if it is the first character and tries to\n    match it with enum values.\n\n    Parameters\n    ----------\n    value : object\n        Value to be matched with enum values.\n\n    Returns\n    -------\n    str\n        Matched enum value.\n    \"\"\"\n    if isinstance(value, str):\n        lower_value = value.lower()\n\n        if lower_value.startswith(\".\"):\n            lower_value = lower_value[1:]\n\n        # attempt to match lowercase value with enum values\n        for member in cls:\n            if member.value == lower_value:\n                return member\n\n    # still missing\n    return super()._missing_(value)\n</code></pre>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData.get_extension","title":"<code>get_extension(data_type)</code>  <code>classmethod</code>","text":"<p>Get file extension of corresponding data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str or SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Corresponding extension.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>@classmethod\ndef get_extension(cls, data_type: Union[str, SupportedData]) -&gt; str:\n    \"\"\"\n    Get file extension of corresponding data type.\n\n    Parameters\n    ----------\n    data_type : str or SupportedData\n        Data type.\n\n    Returns\n    -------\n    str\n        Corresponding extension.\n    \"\"\"\n    if data_type == cls.ARRAY:\n        raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n    elif data_type == cls.TIFF:\n        return \".tiff\"\n    elif data_type == cls.CUSTOM:\n        # TODO: improve this message\n        raise NotImplementedError(\"Custom extensions have to be passed elsewhere.\")\n    else:\n        raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_data/#careamics.config.support.supported_data.SupportedData.get_extension_pattern","title":"<code>get_extension_pattern(data_type)</code>  <code>classmethod</code>","text":"<p>Get Path.rglob and fnmatch compatible extension.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Corresponding extension pattern.</p> Source code in <code>src/careamics/config/support/supported_data.py</code> <pre><code>@classmethod\ndef get_extension_pattern(cls, data_type: Union[str, SupportedData]) -&gt; str:\n    \"\"\"\n    Get Path.rglob and fnmatch compatible extension.\n\n    Parameters\n    ----------\n    data_type : SupportedData\n        Data type.\n\n    Returns\n    -------\n    str\n        Corresponding extension pattern.\n    \"\"\"\n    if data_type == cls.ARRAY:\n        raise NotImplementedError(f\"Data '{data_type}' is not loaded from a file.\")\n    elif data_type == cls.TIFF:\n        return \"*.tif*\"\n    elif data_type == cls.CUSTOM:\n        return \"*.*\"\n    else:\n        raise ValueError(f\"Data type {data_type} is not supported.\")\n</code></pre>"},{"location":"reference/careamics/config/support/supported_loggers/","title":"supported_loggers","text":"<p>Logger supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_loggers/#careamics.config.support.supported_loggers.SupportedLogger","title":"<code>SupportedLogger</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Available loggers.</p> Source code in <code>src/careamics/config/support/supported_loggers.py</code> <pre><code>class SupportedLogger(str, BaseEnum):\n    \"\"\"Available loggers.\"\"\"\n\n    WANDB = \"wandb\"\n    TENSORBOARD = \"tensorboard\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_losses/","title":"supported_losses","text":"<p>Losses supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_losses/#careamics.config.support.supported_losses.SupportedLoss","title":"<code>SupportedLoss</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported losses.</p> <p>Attributes:</p> Name Type Description <code>MSE</code> <code>str</code> <p>Mean Squared Error loss.</p> <code>MAE</code> <code>str</code> <p>Mean Absolute Error loss.</p> <code>N2V</code> <code>str</code> <p>Noise2Void loss.</p> Source code in <code>src/careamics/config/support/supported_losses.py</code> <pre><code>class SupportedLoss(str, BaseEnum):\n    \"\"\"Supported losses.\n\n    Attributes\n    ----------\n    MSE : str\n        Mean Squared Error loss.\n    MAE : str\n        Mean Absolute Error loss.\n    N2V : str\n        Noise2Void loss.\n    \"\"\"\n\n    MSE = \"mse\"\n    MAE = \"mae\"\n    N2V = \"n2v\"\n    # PN2V = \"pn2v\"\n    # HDN = \"hdn\"\n    MUSPLIT = \"musplit\"\n    DENOISPLIT = \"denoisplit\"\n    DENOISPLIT_MUSPLIT = \"denoisplit_musplit\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_optimizers/","title":"supported_optimizers","text":"<p>Optimizers and schedulers supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_optimizers/#careamics.config.support.supported_optimizers.SupportedOptimizer","title":"<code>SupportedOptimizer</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported optimizers.</p> <p>Attributes:</p> Name Type Description <code>Adam</code> <code>str</code> <p>Adam optimizer.</p> <code>SGD</code> <code>str</code> <p>Stochastic Gradient Descent optimizer.</p> Source code in <code>src/careamics/config/support/supported_optimizers.py</code> <pre><code>class SupportedOptimizer(str, BaseEnum):\n    \"\"\"Supported optimizers.\n\n    Attributes\n    ----------\n    Adam : str\n        Adam optimizer.\n    SGD : str\n        Stochastic Gradient Descent optimizer.\n    \"\"\"\n\n    # ASGD = \"ASGD\"\n    # Adadelta = \"Adadelta\"\n    # Adagrad = \"Adagrad\"\n    ADAM = \"Adam\"\n    # AdamW = \"AdamW\"\n    ADAMAX = \"Adamax\"\n    # LBFGS = \"LBFGS\"\n    # NAdam = \"NAdam\"\n    # RAdam = \"RAdam\"\n    # RMSprop = \"RMSprop\"\n    # Rprop = \"Rprop\"\n    SGD = \"SGD\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_optimizers/#careamics.config.support.supported_optimizers.SupportedScheduler","title":"<code>SupportedScheduler</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported schedulers.</p> <p>Attributes:</p> Name Type Description <code>ReduceLROnPlateau</code> <code>str</code> <p>Reduce learning rate on plateau.</p> <code>StepLR</code> <code>str</code> <p>Step learning rate.</p> Source code in <code>src/careamics/config/support/supported_optimizers.py</code> <pre><code>class SupportedScheduler(str, BaseEnum):\n    \"\"\"Supported schedulers.\n\n    Attributes\n    ----------\n    ReduceLROnPlateau : str\n        Reduce learning rate on plateau.\n    StepLR : str\n        Step learning rate.\n    \"\"\"\n\n    # ChainedScheduler = \"ChainedScheduler\"\n    # ConstantLR = \"ConstantLR\"\n    # CosineAnnealingLR = \"CosineAnnealingLR\"\n    # CosineAnnealingWarmRestarts = \"CosineAnnealingWarmRestarts\"\n    # CyclicLR = \"CyclicLR\"\n    # ExponentialLR = \"ExponentialLR\"\n    # LambdaLR = \"LambdaLR\"\n    # LinearLR = \"LinearLR\"\n    # MultiStepLR = \"MultiStepLR\"\n    # MultiplicativeLR = \"MultiplicativeLR\"\n    # OneCycleLR = \"OneCycleLR\"\n    # PolynomialLR = \"PolynomialLR\"\n    REDUCE_LR_ON_PLATEAU = \"ReduceLROnPlateau\"\n    # SequentialLR = \"SequentialLR\"\n    STEP_LR = \"StepLR\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_pixel_manipulations/","title":"supported_pixel_manipulations","text":"<p>Pixel manipulation methods supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_pixel_manipulations/#careamics.config.support.supported_pixel_manipulations.SupportedPixelManipulation","title":"<code>SupportedPixelManipulation</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported Noise2Void pixel manipulations.</p> <ul> <li>Uniform: Replace masked pixel value by a (uniformly) randomly selected neighbor     pixel value.</li> <li>Median: Replace masked pixel value by the mean of the neighborhood.</li> </ul> Source code in <code>src/careamics/config/support/supported_pixel_manipulations.py</code> <pre><code>class SupportedPixelManipulation(str, BaseEnum):\n    \"\"\"Supported Noise2Void pixel manipulations.\n\n    - Uniform: Replace masked pixel value by a (uniformly) randomly selected neighbor\n        pixel value.\n    - Median: Replace masked pixel value by the mean of the neighborhood.\n    \"\"\"\n\n    UNIFORM = \"uniform\"\n    MEDIAN = \"median\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_struct_axis/","title":"supported_struct_axis","text":"<p>StructN2V axes supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_struct_axis/#careamics.config.support.supported_struct_axis.SupportedStructAxis","title":"<code>SupportedStructAxis</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Supported structN2V mask axes.</p> <p>Attributes:</p> Name Type Description <code>HORIZONTAL</code> <code>str</code> <p>Horizontal axis.</p> <code>VERTICAL</code> <code>str</code> <p>Vertical axis.</p> <code>NONE</code> <code>str</code> <p>No axis, the mask is not applied.</p> Source code in <code>src/careamics/config/support/supported_struct_axis.py</code> <pre><code>class SupportedStructAxis(str, BaseEnum):\n    \"\"\"Supported structN2V mask axes.\n\n    Attributes\n    ----------\n    HORIZONTAL : str\n        Horizontal axis.\n    VERTICAL : str\n        Vertical axis.\n    NONE : str\n        No axis, the mask is not applied.\n    \"\"\"\n\n    HORIZONTAL = \"horizontal\"\n    VERTICAL = \"vertical\"\n    NONE = \"none\"\n</code></pre>"},{"location":"reference/careamics/config/support/supported_transforms/","title":"supported_transforms","text":"<p>Transforms supported by CAREamics.</p>"},{"location":"reference/careamics/config/support/supported_transforms/#careamics.config.support.supported_transforms.SupportedTransform","title":"<code>SupportedTransform</code>","text":"<p>               Bases: <code>str</code>, <code>BaseEnum</code></p> <p>Transforms officially supported by CAREamics.</p> Source code in <code>src/careamics/config/support/supported_transforms.py</code> <pre><code>class SupportedTransform(str, BaseEnum):\n    \"\"\"Transforms officially supported by CAREamics.\"\"\"\n\n    XY_FLIP = \"XYFlip\"\n    XY_RANDOM_ROTATE90 = \"XYRandomRotate90\"\n    NORMALIZE = \"Normalize\"\n    N2V_MANIPULATE = \"N2VManipulate\"\n</code></pre>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/","title":"n2v_manipulate_model","text":"<p>Pydantic model for the N2VManipulate transform.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel","title":"<code>N2VManipulateModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent N2V manipulation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['N2VManipulate']</code> <p>Name of the transformation.</p> <code>roi_size</code> <code>int</code> <p>Size of the masking region, by default 11.</p> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of masked pixels, by default 0.2.</p> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Strategy pixel value replacement, by default \"uniform\".</p> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>Axis of the structN2V mask, by default \"none\".</p> <code>struct_mask_span</code> <code>int</code> <p>Span of the structN2V mask, by default 5.</p> Source code in <code>src/careamics/config/transformations/n2v_manipulate_model.py</code> <pre><code>class N2VManipulateModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent N2V manipulation.\n\n    Attributes\n    ----------\n    name : Literal[\"N2VManipulate\"]\n        Name of the transformation.\n    roi_size : int\n        Size of the masking region, by default 11.\n    masked_pixel_percentage : float\n        Percentage of masked pixels, by default 0.2.\n    strategy : Literal[\"uniform\", \"median\"]\n        Strategy pixel value replacement, by default \"uniform\".\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"]\n        Axis of the structN2V mask, by default \"none\".\n    struct_mask_span : int\n        Span of the structN2V mask, by default 5.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"N2VManipulate\"] = \"N2VManipulate\"\n\n    roi_size: int = Field(default=11, ge=3, le=21)\n    \"\"\"Size of the region where the pixel manipulation is applied.\"\"\"\n\n    masked_pixel_percentage: float = Field(default=0.2, ge=0.05, le=10.0)\n    \"\"\"Percentage of masked pixels per image.\"\"\"\n\n    remove_center: bool = Field(default=True)  # TODO remove it\n    \"\"\"Exclude center pixel from average calculation.\"\"\"  # TODO rephrase this\n\n    strategy: Literal[\"uniform\", \"median\"] = Field(default=\"uniform\")\n    \"\"\"Strategy for pixel value replacement.\"\"\"\n\n    struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = Field(default=\"none\")\n    \"\"\"Orientation of the structN2V mask. Set to `\\\"non\\\"` to not apply StructN2V.\"\"\"\n\n    struct_mask_span: int = Field(default=5, ge=3, le=15)\n    \"\"\"Size of the structN2V mask.\"\"\"\n\n    @field_validator(\"roi_size\", \"struct_mask_span\")\n    @classmethod\n    def odd_value(cls, v: int) -&gt; int:\n        \"\"\"\n        Validate that the value is odd.\n\n        Parameters\n        ----------\n        v : int\n            Value to validate.\n\n        Returns\n        -------\n        int\n            The validated value.\n\n        Raises\n        ------\n        ValueError\n            If the value is even.\n        \"\"\"\n        if v % 2 == 0:\n            raise ValueError(\"Size must be an odd number.\")\n        return v\n</code></pre>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.masked_pixel_percentage","title":"<code>masked_pixel_percentage = Field(default=0.2, ge=0.05, le=10.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Percentage of masked pixels per image.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.remove_center","title":"<code>remove_center = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exclude center pixel from average calculation.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.roi_size","title":"<code>roi_size = Field(default=11, ge=3, le=21)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the region where the pixel manipulation is applied.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.strategy","title":"<code>strategy = Field(default='uniform')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for pixel value replacement.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.struct_mask_axis","title":"<code>struct_mask_axis = Field(default='none')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Orientation of the structN2V mask. Set to <code>\"non\"</code> to not apply StructN2V.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.struct_mask_span","title":"<code>struct_mask_span = Field(default=5, ge=3, le=15)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the structN2V mask.</p>"},{"location":"reference/careamics/config/transformations/n2v_manipulate_model/#careamics.config.transformations.n2v_manipulate_model.N2VManipulateModel.odd_value","title":"<code>odd_value(v)</code>  <code>classmethod</code>","text":"<p>Validate that the value is odd.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>Value to validate.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The validated value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is even.</p> Source code in <code>src/careamics/config/transformations/n2v_manipulate_model.py</code> <pre><code>@field_validator(\"roi_size\", \"struct_mask_span\")\n@classmethod\ndef odd_value(cls, v: int) -&gt; int:\n    \"\"\"\n    Validate that the value is odd.\n\n    Parameters\n    ----------\n    v : int\n        Value to validate.\n\n    Returns\n    -------\n    int\n        The validated value.\n\n    Raises\n    ------\n    ValueError\n        If the value is even.\n    \"\"\"\n    if v % 2 == 0:\n        raise ValueError(\"Size must be an odd number.\")\n    return v\n</code></pre>"},{"location":"reference/careamics/config/transformations/normalize_model/","title":"normalize_model","text":"<p>Pydantic model for the Normalize transform.</p>"},{"location":"reference/careamics/config/transformations/normalize_model/#careamics.config.transformations.normalize_model.NormalizeModel","title":"<code>NormalizeModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent Normalize transformation.</p> <p>The Normalize transform is a zero mean and unit variance transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['Normalize']</code> <p>Name of the transformation.</p> <code>mean</code> <code>float</code> <p>Mean value for normalization.</p> <code>std</code> <code>float</code> <p>Standard deviation value for normalization.</p> Source code in <code>src/careamics/config/transformations/normalize_model.py</code> <pre><code>class NormalizeModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent Normalize transformation.\n\n    The Normalize transform is a zero mean and unit variance transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"Normalize\"]\n        Name of the transformation.\n    mean : float\n        Mean value for normalization.\n    std : float\n        Standard deviation value for normalization.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"Normalize\"] = \"Normalize\"\n    image_means: list = Field(..., min_length=0, max_length=32)\n    image_stds: list = Field(..., min_length=0, max_length=32)\n    target_means: Optional[list] = Field(default=None, min_length=0, max_length=32)\n    target_stds: Optional[list] = Field(default=None, min_length=0, max_length=32)\n\n    @model_validator(mode=\"after\")\n    def validate_means_stds(self: Self) -&gt; Self:\n        \"\"\"Validate that the means and stds have the same length.\n\n        Returns\n        -------\n        Self\n            The instance of the model.\n        \"\"\"\n        if len(self.image_means) != len(self.image_stds):\n            raise ValueError(\"The number of image means and stds must be the same.\")\n\n        if (self.target_means is None) != (self.target_stds is None):\n            raise ValueError(\n                \"Both target means and stds must be provided together, or bot None.\"\n            )\n\n        if self.target_means is not None and self.target_stds is not None:\n            if len(self.target_means) != len(self.target_stds):\n                raise ValueError(\n                    \"The number of target means and stds must be the same.\"\n                )\n\n        return self\n</code></pre>"},{"location":"reference/careamics/config/transformations/normalize_model/#careamics.config.transformations.normalize_model.NormalizeModel.validate_means_stds","title":"<code>validate_means_stds()</code>","text":"<p>Validate that the means and stds have the same length.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The instance of the model.</p> Source code in <code>src/careamics/config/transformations/normalize_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_means_stds(self: Self) -&gt; Self:\n    \"\"\"Validate that the means and stds have the same length.\n\n    Returns\n    -------\n    Self\n        The instance of the model.\n    \"\"\"\n    if len(self.image_means) != len(self.image_stds):\n        raise ValueError(\"The number of image means and stds must be the same.\")\n\n    if (self.target_means is None) != (self.target_stds is None):\n        raise ValueError(\n            \"Both target means and stds must be provided together, or bot None.\"\n        )\n\n    if self.target_means is not None and self.target_stds is not None:\n        if len(self.target_means) != len(self.target_stds):\n            raise ValueError(\n                \"The number of target means and stds must be the same.\"\n            )\n\n    return self\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_model/","title":"transform_model","text":"<p>Parent model for the transforms.</p>"},{"location":"reference/careamics/config/transformations/transform_model/#careamics.config.transformations.transform_model.TransformModel","title":"<code>TransformModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model used to represent a transformation.</p> <p>The <code>model_dump</code> method is overwritten to exclude the name field.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the transformation.</p> Source code in <code>src/careamics/config/transformations/transform_model.py</code> <pre><code>class TransformModel(BaseModel):\n    \"\"\"\n    Pydantic model used to represent a transformation.\n\n    The `model_dump` method is overwritten to exclude the name field.\n\n    Attributes\n    ----------\n    name : str\n        Name of the transformation.\n    \"\"\"\n\n    model_config = ConfigDict(\n        extra=\"forbid\",  # throw errors if the parameters are not properly passed\n    )\n\n    name: str\n\n    def model_dump(self, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the model as a dictionary.\n\n        Parameters\n        ----------\n        **kwargs\n            Pydantic BaseMode model_dump method keyword arguments.\n\n        Returns\n        -------\n        {str: Any}\n            Dictionary representation of the model.\n        \"\"\"\n        model_dict = super().model_dump(**kwargs)\n\n        # remove the name field\n        model_dict.pop(\"name\")\n\n        return model_dict\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_model/#careamics.config.transformations.transform_model.TransformModel.model_dump","title":"<code>model_dump(**kwargs)</code>","text":"<p>Return the model as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Pydantic BaseMode model_dump method keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>{str: Any}</code> <p>Dictionary representation of the model.</p> Source code in <code>src/careamics/config/transformations/transform_model.py</code> <pre><code>def model_dump(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the model as a dictionary.\n\n    Parameters\n    ----------\n    **kwargs\n        Pydantic BaseMode model_dump method keyword arguments.\n\n    Returns\n    -------\n    {str: Any}\n        Dictionary representation of the model.\n    \"\"\"\n    model_dict = super().model_dump(**kwargs)\n\n    # remove the name field\n    model_dict.pop(\"name\")\n\n    return model_dict\n</code></pre>"},{"location":"reference/careamics/config/transformations/transform_unions/","title":"transform_unions","text":"<p>Type used to represent all transformations users can create.</p>"},{"location":"reference/careamics/config/transformations/transform_unions/#careamics.config.transformations.transform_unions.NORM_AND_SPATIAL_UNION","title":"<code>NORM_AND_SPATIAL_UNION = Annotated[Union[NormalizeModel, XYFlipModel, XYRandomRotate90Model], Discriminator('name')]</code>  <code>module-attribute</code>","text":"<p>All transforms including normalization.</p>"},{"location":"reference/careamics/config/transformations/transform_unions/#careamics.config.transformations.transform_unions.SPATIAL_TRANSFORMS_UNION","title":"<code>SPATIAL_TRANSFORMS_UNION = Annotated[Union[XYFlipModel, XYRandomRotate90Model], Discriminator('name')]</code>  <code>module-attribute</code>","text":"<p>Available spatial transforms in CAREamics.</p>"},{"location":"reference/careamics/config/transformations/xy_flip_model/","title":"xy_flip_model","text":"<p>Pydantic model for the XYFlip transform.</p>"},{"location":"reference/careamics/config/transformations/xy_flip_model/#careamics.config.transformations.xy_flip_model.XYFlipModel","title":"<code>XYFlipModel</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent XYFlip transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['XYFlip']</code> <p>Name of the transformation.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed for the random number generator,  by default None.</p> Source code in <code>src/careamics/config/transformations/xy_flip_model.py</code> <pre><code>class XYFlipModel(TransformModel):\n    \"\"\"\n    Pydantic model used to represent XYFlip transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"XYFlip\"]\n        Name of the transformation.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Seed for the random number generator,  by default None.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"XYFlip\"] = \"XYFlip\"\n    flip_x: bool = Field(\n        True,\n        description=\"Whether to flip along the X axis.\",\n    )\n    flip_y: bool = Field(\n        True,\n        description=\"Whether to flip along the Y axis.\",\n    )\n    p: float = Field(\n        0.5,\n        description=\"Probability of applying the transform.\",\n        ge=0,\n        le=1,\n    )\n    seed: Optional[int] = None\n</code></pre>"},{"location":"reference/careamics/config/transformations/xy_random_rotate90_model/","title":"xy_random_rotate90_model","text":"<p>Pydantic model for the XYRandomRotate90 transform.</p>"},{"location":"reference/careamics/config/transformations/xy_random_rotate90_model/#careamics.config.transformations.xy_random_rotate90_model.XYRandomRotate90Model","title":"<code>XYRandomRotate90Model</code>","text":"<p>               Bases: <code>TransformModel</code></p> <p>Pydantic model used to represent the XY random 90 degree rotation transformation.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['XYRandomRotate90']</code> <p>Name of the transformation.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>seed</code> <code>Optional[int]</code> <p>Seed for the random number generator, by default None.</p> Source code in <code>src/careamics/config/transformations/xy_random_rotate90_model.py</code> <pre><code>class XYRandomRotate90Model(TransformModel):\n    \"\"\"\n    Pydantic model used to represent the XY random 90 degree rotation transformation.\n\n    Attributes\n    ----------\n    name : Literal[\"XYRandomRotate90\"]\n        Name of the transformation.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Seed for the random number generator, by default None.\n    \"\"\"\n\n    model_config = ConfigDict(\n        validate_assignment=True,\n    )\n\n    name: Literal[\"XYRandomRotate90\"] = \"XYRandomRotate90\"\n    p: float = Field(\n        0.5,\n        description=\"Probability of applying the transform.\",\n        ge=0,\n        le=1,\n    )\n    seed: Optional[int] = None\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/","title":"model_validators","text":"<p>Architecture model validators.</p>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_matching_in_out_channels","title":"<code>model_matching_in_out_channels(model)</code>","text":"<p>Validate that the UNet model has the same number of channel inputs and outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>Validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has different number of input and output channels.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_matching_in_out_channels(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the UNet model has the same number of channel inputs and outputs.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        Validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has different number of input and output channels.\n    \"\"\"\n    if model.num_classes != model.in_channels:\n        raise ValueError(\n            \"The algorithm requires the same number of input and output channels. \"\n            \"Make sure that `in_channels` and `num_classes` are equal.\"\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_without_final_activation","title":"<code>model_without_final_activation(model)</code>","text":"<p>Validate that the UNet model does not have the final_activation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>The validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has the final_activation attribute set.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_without_final_activation(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the UNet model does not have the final_activation.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        The validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has the final_activation attribute set.\n    \"\"\"\n    if model.final_activation != \"None\":\n        raise ValueError(\n            \"The algorithm does not support a `final_activation` in the model. \"\n            'Set it to `\"None\"`.'\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/model_validators/#careamics.config.validators.model_validators.model_without_n2v2","title":"<code>model_without_n2v2(model)</code>","text":"<p>Validate that the Unet model does not have the n2v2 attribute.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UNetModel</code> <p>Model to validate.</p> required <p>Returns:</p> Type Description <code>UNetModel</code> <p>The validated model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has the <code>n2v2</code> attribute set to <code>True</code>.</p> Source code in <code>src/careamics/config/validators/model_validators.py</code> <pre><code>def model_without_n2v2(model: UNetModel) -&gt; UNetModel:\n    \"\"\"Validate that the Unet model does not have the n2v2 attribute.\n\n    Parameters\n    ----------\n    model : UNetModel\n        Model to validate.\n\n    Returns\n    -------\n    UNetModel\n        The validated model.\n\n    Raises\n    ------\n    ValueError\n        If the model has the `n2v2` attribute set to `True`.\n    \"\"\"\n    if model.n2v2:\n        raise ValueError(\n            \"The algorithm does not support the `n2v2` attribute in the model. \"\n            \"Set it to `False`.\"\n        )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/","title":"validator_utils","text":"<p>Validator functions.</p> <p>These functions are used to validate dimensions and axes of inputs.</p>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.check_axes_validity","title":"<code>check_axes_validity(axes)</code>","text":"<p>Sanity check on axes.</p> <p>The constraints on the axes are the following: - must be a combination of 'STCZYX' - must not contain duplicates - must contain at least 2 contiguous axes: X and Y - must contain at most 4 axes - cannot contain both S and T axes</p> <p>Axes do not need to be in the order 'STCZYX', as this depends on the user data.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to validate.</p> required Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def check_axes_validity(axes: str) -&gt; None:\n    \"\"\"\n    Sanity check on axes.\n\n    The constraints on the axes are the following:\n    - must be a combination of 'STCZYX'\n    - must not contain duplicates\n    - must contain at least 2 contiguous axes: X and Y\n    - must contain at most 4 axes\n    - cannot contain both S and T axes\n\n    Axes do not need to be in the order 'STCZYX', as this depends on the user data.\n\n    Parameters\n    ----------\n    axes : str\n        Axes to validate.\n    \"\"\"\n    _axes = axes.upper()\n\n    # Minimum is 2 (XY) and maximum is 4 (TZYX)\n    if len(_axes) &lt; 2 or len(_axes) &gt; 6:\n        raise ValueError(\n            f\"Invalid axes {axes}. Must contain at least 2 and at most 6 axes.\"\n        )\n\n    if \"YX\" not in _axes and \"XY\" not in _axes:\n        raise ValueError(\n            f\"Invalid axes {axes}. Must contain at least X and Y axes consecutively.\"\n        )\n\n    # all characters must be in REF_AXES = 'STCZYX'\n    if not all(s in _AXES for s in _axes):\n        raise ValueError(f\"Invalid axes {axes}. Must be a combination of {_AXES}.\")\n\n    # check for repeating characters\n    for i, s in enumerate(_axes):\n        if i != _axes.rfind(s):\n            raise ValueError(\n                f\"Invalid axes {axes}. Cannot contain duplicate axes\"\n                f\" (got multiple {axes[i]}).\"\n            )\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.patch_size_ge_than_8_power_of_2","title":"<code>patch_size_ge_than_8_power_of_2(patch_list)</code>","text":"<p>Validate that each entry is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>patch_list</code> <code>list or typle of int, or None</code> <p>Patch size.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size if smaller than 8.</p> <code>ValueError</code> <p>If the patch size is not a power of 2.</p> Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def patch_size_ge_than_8_power_of_2(\n    patch_list: Optional[Union[list[int], Union[tuple[int, ...]]]],\n) -&gt; None:\n    \"\"\"\n    Validate that each entry is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    patch_list : list or typle of int, or None\n        Patch size.\n\n    Raises\n    ------\n    ValueError\n        If the patch size if smaller than 8.\n    ValueError\n        If the patch size is not a power of 2.\n    \"\"\"\n    if patch_list is not None:\n        for dim in patch_list:\n            value_ge_than_8_power_of_2(dim)\n</code></pre>"},{"location":"reference/careamics/config/validators/validator_utils/#careamics.config.validators.validator_utils.value_ge_than_8_power_of_2","title":"<code>value_ge_than_8_power_of_2(value)</code>","text":"<p>Validate that the value is greater or equal than 8 and a power of 2.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Value to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is smaller than 8.</p> <code>ValueError</code> <p>If the value is not a power of 2.</p> Source code in <code>src/careamics/config/validators/validator_utils.py</code> <pre><code>def value_ge_than_8_power_of_2(\n    value: int,\n) -&gt; None:\n    \"\"\"\n    Validate that the value is greater or equal than 8 and a power of 2.\n\n    Parameters\n    ----------\n    value : int\n        Value to validate.\n\n    Raises\n    ------\n    ValueError\n        If the value is smaller than 8.\n    ValueError\n        If the value is not a power of 2.\n    \"\"\"\n    if value &lt; 8:\n        raise ValueError(f\"Value must be greater than 8 (got {value}).\")\n\n    if (value &amp; (value - 1)) != 0:\n        raise ValueError(f\"Value must be a power of 2 (got {value}).\")\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/","title":"in_memory_dataset","text":"<p>In-memory dataset module.</p>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset","title":"<code>InMemoryDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset storing data in memory and allowing generating patches from it.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>CAREamics DataConfig</code> <p>(see careamics.config.data_model.DataConfig) Data configuration.</p> required <code>inputs</code> <code>ndarray or list[Path]</code> <p>Input data.</p> required <code>input_target</code> <code>ndarray or list[Path]</code> <p>Target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>class InMemoryDataset(Dataset):\n    \"\"\"Dataset storing data in memory and allowing generating patches from it.\n\n    Parameters\n    ----------\n    data_config : CAREamics DataConfig\n        (see careamics.config.data_model.DataConfig)\n        Data configuration.\n    inputs : numpy.ndarray or list[pathlib.Path]\n        Input data.\n    input_target : numpy.ndarray or list[pathlib.Path], optional\n        Target data, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        inputs: Union[np.ndarray, list[Path]],\n        input_target: Optional[Union[np.ndarray, list[Path]]] = None,\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        data_config : GeneralDataConfig\n            Data configuration.\n        inputs : numpy.ndarray or list[pathlib.Path]\n            Input data.\n        input_target : numpy.ndarray or list[pathlib.Path], optional\n            Target data, by default None.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n        \"\"\"\n        self.data_config = data_config\n        self.inputs = inputs\n        self.input_targets = input_target\n        self.axes = self.data_config.axes\n        self.patch_size = self.data_config.patch_size\n\n        # read function\n        self.read_source_func = read_source_func\n\n        # generate patches\n        supervised = self.input_targets is not None\n        patches_data = self._prepare_patches(supervised)\n\n        # unpack the dataclass\n        self.data = patches_data.patches\n        self.data_targets = patches_data.targets\n\n        # set image statistics\n        if self.data_config.image_means is None:\n            self.image_stats = patches_data.image_stats\n            logger.info(\n                f\"Computed dataset mean: {self.image_stats.means}, \"\n                f\"std: {self.image_stats.stds}\"\n            )\n        else:\n            self.image_stats = Stats(\n                self.data_config.image_means, self.data_config.image_stds\n            )\n\n        # set target statistics\n        if self.data_config.target_means is None:\n            self.target_stats = patches_data.target_stats\n        else:\n            self.target_stats = Stats(\n                self.data_config.target_means, self.data_config.target_stds\n            )\n\n        # update mean and std in configuration\n        # the object is mutable and should then be recorded in the CAREamist obj\n        self.data_config.set_means_and_stds(\n            image_means=self.image_stats.means,\n            image_stds=self.image_stats.stds,\n            target_means=self.target_stats.means,\n            target_stds=self.target_stats.stds,\n        )\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_stats.means,\n                    image_stds=self.image_stats.stds,\n                    target_means=self.target_stats.means,\n                    target_stds=self.target_stats.stds,\n                )\n            ]\n            + list(self.data_config.transforms),\n        )\n\n    def _prepare_patches(self, supervised: bool) -&gt; PatchedOutput:\n        \"\"\"\n        Iterate over data source and create an array of patches.\n\n        Parameters\n        ----------\n        supervised : bool\n            Whether the dataset is supervised or not.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of patches.\n        \"\"\"\n        if supervised:\n            if isinstance(self.inputs, np.ndarray) and isinstance(\n                self.input_targets, np.ndarray\n            ):\n                return prepare_patches_supervised_array(\n                    self.inputs,\n                    self.axes,\n                    self.input_targets,\n                    self.patch_size,\n                )\n            elif isinstance(self.inputs, list) and isinstance(self.input_targets, list):\n                return prepare_patches_supervised(\n                    self.inputs,\n                    self.input_targets,\n                    self.axes,\n                    self.patch_size,\n                    self.read_source_func,\n                )\n            else:\n                raise ValueError(\n                    f\"Data and target must be of the same type, either both numpy \"\n                    f\"arrays or both lists of paths, got {type(self.inputs)} (data) \"\n                    f\"and {type(self.input_targets)} (target).\"\n                )\n        else:\n            if isinstance(self.inputs, np.ndarray):\n                return prepare_patches_unsupervised_array(\n                    self.inputs,\n                    self.axes,\n                    self.patch_size,\n                )\n            else:\n                return prepare_patches_unsupervised(\n                    self.inputs,\n                    self.axes,\n                    self.patch_size,\n                    self.read_source_func,\n                )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return self.data.shape[0]\n\n    def __getitem__(self, index: int) -&gt; tuple[np.ndarray, ...]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple of numpy.ndarray\n            Patch.\n\n        Raises\n        ------\n        ValueError\n            If dataset mean and std are not set.\n        \"\"\"\n        patch = self.data[index]\n\n        # if there is a target\n        if self.data_targets is not None:\n            # get target\n            target = self.data_targets[index]\n            return self.patch_transform(patch=patch, target=target)\n\n        return self.patch_transform(patch=patch)\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        This does not return the target data statistics, only those of the input.\n\n        Returns\n        -------\n        tuple of list of floats\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.image_stats.get_statistics()\n\n    def split_dataset(\n        self,\n        percentage: float = 0.1,\n        minimum_patches: int = 1,\n    ) -&gt; InMemoryDataset:\n        \"\"\"Split a new dataset away from the current one.\n\n        This method is used to extract random validation patches from the dataset.\n\n        Parameters\n        ----------\n        percentage : float, optional\n            Percentage of patches to extract, by default 0.1.\n        minimum_patches : int, optional\n            Minimum number of patches to extract, by default 5.\n\n        Returns\n        -------\n        CAREamics InMemoryDataset\n            New dataset with the extracted patches.\n\n        Raises\n        ------\n        ValueError\n            If `percentage` is not between 0 and 1.\n        ValueError\n            If `minimum_number` is not between 1 and the number of patches.\n        \"\"\"\n        if percentage &lt; 0 or percentage &gt; 1:\n            raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n        if minimum_patches &lt; 1 or minimum_patches &gt; len(self):\n            raise ValueError(\n                f\"Minimum number of patches must be between 1 and \"\n                f\"{len(self)} (number of patches), got \"\n                f\"{minimum_patches}. Adjust the patch size or the minimum number of \"\n                f\"patches.\"\n            )\n\n        total_patches = len(self)\n\n        # number of patches to extract (either percentage rounded or minimum number)\n        n_patches = max(round(total_patches * percentage), minimum_patches)\n\n        # get random indices\n        indices = np.random.choice(total_patches, n_patches, replace=False)\n\n        # extract patches\n        val_patches = self.data[indices]\n\n        # remove patches from self.patch\n        self.data = np.delete(self.data, indices, axis=0)\n\n        # same for targets\n        if self.data_targets is not None:\n            val_targets = self.data_targets[indices]\n            self.data_targets = np.delete(self.data_targets, indices, axis=0)\n\n        # clone the dataset\n        dataset = copy.deepcopy(self)\n\n        # reassign patches\n        dataset.data = val_patches\n\n        # reassign targets\n        if self.data_targets is not None:\n            dataset.data_targets = val_targets\n\n        return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple of numpy.ndarray</code> <p>Patch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset mean and std are not set.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[np.ndarray, ...]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple of numpy.ndarray\n        Patch.\n\n    Raises\n    ------\n    ValueError\n        If dataset mean and std are not set.\n    \"\"\"\n    patch = self.data[index]\n\n    # if there is a target\n    if self.data_targets is not None:\n        # get target\n        target = self.data_targets[index]\n        return self.patch_transform(patch=patch, target=target)\n\n    return self.patch_transform(patch=patch)\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__init__","title":"<code>__init__(data_config, inputs, input_target=None, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>GeneralDataConfig</code> <p>Data configuration.</p> required <code>inputs</code> <code>ndarray or list[Path]</code> <p>Input data.</p> required <code>input_target</code> <code>ndarray or list[Path]</code> <p>Target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    inputs: Union[np.ndarray, list[Path]],\n    input_target: Optional[Union[np.ndarray, list[Path]]] = None,\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    data_config : GeneralDataConfig\n        Data configuration.\n    inputs : numpy.ndarray or list[pathlib.Path]\n        Input data.\n    input_target : numpy.ndarray or list[pathlib.Path], optional\n        Target data, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n    self.data_config = data_config\n    self.inputs = inputs\n    self.input_targets = input_target\n    self.axes = self.data_config.axes\n    self.patch_size = self.data_config.patch_size\n\n    # read function\n    self.read_source_func = read_source_func\n\n    # generate patches\n    supervised = self.input_targets is not None\n    patches_data = self._prepare_patches(supervised)\n\n    # unpack the dataclass\n    self.data = patches_data.patches\n    self.data_targets = patches_data.targets\n\n    # set image statistics\n    if self.data_config.image_means is None:\n        self.image_stats = patches_data.image_stats\n        logger.info(\n            f\"Computed dataset mean: {self.image_stats.means}, \"\n            f\"std: {self.image_stats.stds}\"\n        )\n    else:\n        self.image_stats = Stats(\n            self.data_config.image_means, self.data_config.image_stds\n        )\n\n    # set target statistics\n    if self.data_config.target_means is None:\n        self.target_stats = patches_data.target_stats\n    else:\n        self.target_stats = Stats(\n            self.data_config.target_means, self.data_config.target_stds\n        )\n\n    # update mean and std in configuration\n    # the object is mutable and should then be recorded in the CAREamist obj\n    self.data_config.set_means_and_stds(\n        image_means=self.image_stats.means,\n        image_stds=self.image_stats.stds,\n        target_means=self.target_stats.means,\n        target_stds=self.target_stats.stds,\n    )\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=self.target_stats.means,\n                target_stds=self.target_stats.stds,\n            )\n        ]\n        + list(self.data_config.transforms),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return self.data.shape[0]\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset._prepare_patches","title":"<code>_prepare_patches(supervised)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>Parameters:</p> Name Type Description Default <code>supervised</code> <code>bool</code> <p>Whether the dataset is supervised or not.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of patches.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def _prepare_patches(self, supervised: bool) -&gt; PatchedOutput:\n    \"\"\"\n    Iterate over data source and create an array of patches.\n\n    Parameters\n    ----------\n    supervised : bool\n        Whether the dataset is supervised or not.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of patches.\n    \"\"\"\n    if supervised:\n        if isinstance(self.inputs, np.ndarray) and isinstance(\n            self.input_targets, np.ndarray\n        ):\n            return prepare_patches_supervised_array(\n                self.inputs,\n                self.axes,\n                self.input_targets,\n                self.patch_size,\n            )\n        elif isinstance(self.inputs, list) and isinstance(self.input_targets, list):\n            return prepare_patches_supervised(\n                self.inputs,\n                self.input_targets,\n                self.axes,\n                self.patch_size,\n                self.read_source_func,\n            )\n        else:\n            raise ValueError(\n                f\"Data and target must be of the same type, either both numpy \"\n                f\"arrays or both lists of paths, got {type(self.inputs)} (data) \"\n                f\"and {type(self.input_targets)} (target).\"\n            )\n    else:\n        if isinstance(self.inputs, np.ndarray):\n            return prepare_patches_unsupervised_array(\n                self.inputs,\n                self.axes,\n                self.patch_size,\n            )\n        else:\n            return prepare_patches_unsupervised(\n                self.inputs,\n                self.axes,\n                self.patch_size,\n                self.read_source_func,\n            )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>This does not return the target data statistics, only those of the input.</p> <p>Returns:</p> Type Description <code>tuple of list of floats</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    This does not return the target data statistics, only those of the input.\n\n    Returns\n    -------\n    tuple of list of floats\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.image_stats.get_statistics()\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_dataset/#careamics.dataset.in_memory_dataset.InMemoryDataset.split_dataset","title":"<code>split_dataset(percentage=0.1, minimum_patches=1)</code>","text":"<p>Split a new dataset away from the current one.</p> <p>This method is used to extract random validation patches from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>percentage</code> <code>float</code> <p>Percentage of patches to extract, by default 0.1.</p> <code>0.1</code> <code>minimum_patches</code> <code>int</code> <p>Minimum number of patches to extract, by default 5.</p> <code>1</code> <p>Returns:</p> Type Description <code>CAREamics InMemoryDataset</code> <p>New dataset with the extracted patches.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>percentage</code> is not between 0 and 1.</p> <code>ValueError</code> <p>If <code>minimum_number</code> is not between 1 and the number of patches.</p> Source code in <code>src/careamics/dataset/in_memory_dataset.py</code> <pre><code>def split_dataset(\n    self,\n    percentage: float = 0.1,\n    minimum_patches: int = 1,\n) -&gt; InMemoryDataset:\n    \"\"\"Split a new dataset away from the current one.\n\n    This method is used to extract random validation patches from the dataset.\n\n    Parameters\n    ----------\n    percentage : float, optional\n        Percentage of patches to extract, by default 0.1.\n    minimum_patches : int, optional\n        Minimum number of patches to extract, by default 5.\n\n    Returns\n    -------\n    CAREamics InMemoryDataset\n        New dataset with the extracted patches.\n\n    Raises\n    ------\n    ValueError\n        If `percentage` is not between 0 and 1.\n    ValueError\n        If `minimum_number` is not between 1 and the number of patches.\n    \"\"\"\n    if percentage &lt; 0 or percentage &gt; 1:\n        raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n    if minimum_patches &lt; 1 or minimum_patches &gt; len(self):\n        raise ValueError(\n            f\"Minimum number of patches must be between 1 and \"\n            f\"{len(self)} (number of patches), got \"\n            f\"{minimum_patches}. Adjust the patch size or the minimum number of \"\n            f\"patches.\"\n        )\n\n    total_patches = len(self)\n\n    # number of patches to extract (either percentage rounded or minimum number)\n    n_patches = max(round(total_patches * percentage), minimum_patches)\n\n    # get random indices\n    indices = np.random.choice(total_patches, n_patches, replace=False)\n\n    # extract patches\n    val_patches = self.data[indices]\n\n    # remove patches from self.patch\n    self.data = np.delete(self.data, indices, axis=0)\n\n    # same for targets\n    if self.data_targets is not None:\n        val_targets = self.data_targets[indices]\n        self.data_targets = np.delete(self.data_targets, indices, axis=0)\n\n    # clone the dataset\n    dataset = copy.deepcopy(self)\n\n    # reassign patches\n    dataset.data = val_patches\n\n    # reassign targets\n    if self.data_targets is not None:\n        dataset.data_targets = val_targets\n\n    return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/","title":"in_memory_pred_dataset","text":"<p>In-memory prediction dataset.</p>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset","title":"<code>InMemoryPredDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Simple prediction dataset returning images along the sample axis.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>class InMemoryPredDataset(Dataset):\n    \"\"\"Simple prediction dataset returning images along the sample axis.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        inputs: NDArray,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Prediction configuration.\n        inputs : NDArray\n            Input data.\n\n        Raises\n        ------\n        ValueError\n            If data_path is not a directory.\n        \"\"\"\n        self.pred_config = prediction_config\n        self.input_array = inputs\n        self.axes = self.pred_config.axes\n        self.image_means = self.pred_config.image_means\n        self.image_stds = self.pred_config.image_stds\n\n        # Reshape data\n        self.data = reshape_array(self.input_array, self.axes)\n\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n            ],\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int) -&gt; tuple[NDArray, ...]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple(numpy.ndarray, ...)\n            Transformed patch.\n        \"\"\"\n        return self.patch_transform(patch=self.data[index])\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple(ndarray, ...)</code> <p>Transformed patch.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[NDArray, ...]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple(numpy.ndarray, ...)\n        Transformed patch.\n    \"\"\"\n    return self.patch_transform(patch=self.data[index])\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__init__","title":"<code>__init__(prediction_config, inputs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_path is not a directory.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    inputs: NDArray,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n\n    Raises\n    ------\n    ValueError\n        If data_path is not a directory.\n    \"\"\"\n    self.pred_config = prediction_config\n    self.input_array = inputs\n    self.axes = self.pred_config.axes\n    self.image_means = self.pred_config.image_means\n    self.image_stds = self.pred_config.image_stds\n\n    # Reshape data\n    self.data = reshape_array(self.input_array, self.axes)\n\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_pred_dataset/#careamics.dataset.in_memory_pred_dataset.InMemoryPredDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_pred_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/","title":"in_memory_tiled_pred_dataset","text":"<p>In-memory tiled prediction dataset.</p>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset","title":"<code>InMemoryTiledPredDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Prediction dataset storing data in memory and returning tiles of each image.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>class InMemoryTiledPredDataset(Dataset):\n    \"\"\"Prediction dataset storing data in memory and returning tiles of each image.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        inputs: NDArray,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Prediction configuration.\n        inputs : NDArray\n            Input data.\n\n        Raises\n        ------\n        ValueError\n            If data_path is not a directory.\n        \"\"\"\n        if (\n            prediction_config.tile_size is None\n            or prediction_config.tile_overlap is None\n        ):\n            raise ValueError(\n                \"Tile size and overlap must be provided to use the tiled prediction \"\n                \"dataset.\"\n            )\n\n        self.pred_config = prediction_config\n        self.input_array = inputs\n        self.axes = self.pred_config.axes\n        self.tile_size = prediction_config.tile_size\n        self.tile_overlap = prediction_config.tile_overlap\n        self.image_means = self.pred_config.image_means\n        self.image_stds = self.pred_config.image_stds\n\n        # Generate patches\n        self.data = self._prepare_tiles()\n\n        # get transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n            ],\n        )\n\n    def _prepare_tiles(self) -&gt; list[tuple[NDArray, TileInformation]]:\n        \"\"\"\n        Iterate over data source and create an array of patches.\n\n        Returns\n        -------\n        list of tuples of NDArray and TileInformation\n            List of tiles and tile information.\n        \"\"\"\n        # reshape array\n        reshaped_sample = reshape_array(self.input_array, self.axes)\n\n        # generate patches, which returns a generator\n        patch_generator = extract_tiles(\n            arr=reshaped_sample,\n            tile_size=self.tile_size,\n            overlaps=self.tile_overlap,\n        )\n        patches_list = list(patch_generator)\n\n        if len(patches_list) == 0:\n            raise ValueError(\"No tiles generated, \")\n\n        return patches_list\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the length of the dataset.\n\n        Returns\n        -------\n        int\n            Length of the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, index: int) -&gt; tuple[tuple[NDArray, ...], TileInformation]:\n        \"\"\"\n        Return the patch corresponding to the provided index.\n\n        Parameters\n        ----------\n        index : int\n            Index of the patch to return.\n\n        Returns\n        -------\n        tuple of NDArray and TileInformation\n            Transformed patch.\n        \"\"\"\n        tile_array, tile_info = self.data[index]\n\n        # Apply transforms\n        transformed_tile = self.patch_transform(patch=tile_array)\n\n        return transformed_tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Return the patch corresponding to the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the patch to return.</p> required <p>Returns:</p> Type Description <code>tuple of NDArray and TileInformation</code> <p>Transformed patch.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[tuple[NDArray, ...], TileInformation]:\n    \"\"\"\n    Return the patch corresponding to the provided index.\n\n    Parameters\n    ----------\n    index : int\n        Index of the patch to return.\n\n    Returns\n    -------\n    tuple of NDArray and TileInformation\n        Transformed patch.\n    \"\"\"\n    tile_array, tile_info = self.data[index]\n\n    # Apply transforms\n    transformed_tile = self.patch_transform(patch=tile_array)\n\n    return transformed_tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__init__","title":"<code>__init__(prediction_config, inputs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Prediction configuration.</p> required <code>inputs</code> <code>NDArray</code> <p>Input data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_path is not a directory.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    inputs: NDArray,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Prediction configuration.\n    inputs : NDArray\n        Input data.\n\n    Raises\n    ------\n    ValueError\n        If data_path is not a directory.\n    \"\"\"\n    if (\n        prediction_config.tile_size is None\n        or prediction_config.tile_overlap is None\n    ):\n        raise ValueError(\n            \"Tile size and overlap must be provided to use the tiled prediction \"\n            \"dataset.\"\n        )\n\n    self.pred_config = prediction_config\n    self.input_array = inputs\n    self.axes = self.pred_config.axes\n    self.tile_size = prediction_config.tile_size\n    self.tile_overlap = prediction_config.tile_overlap\n    self.image_means = self.pred_config.image_means\n    self.image_stds = self.pred_config.image_stds\n\n    # Generate patches\n    self.data = self._prepare_tiles()\n\n    # get transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(image_means=self.image_means, image_stds=self.image_stds)\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the dataset.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the length of the dataset.\n\n    Returns\n    -------\n    int\n        Length of the dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"reference/careamics/dataset/in_memory_tiled_pred_dataset/#careamics.dataset.in_memory_tiled_pred_dataset.InMemoryTiledPredDataset._prepare_tiles","title":"<code>_prepare_tiles()</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>Returns:</p> Type Description <code>list of tuples of NDArray and TileInformation</code> <p>List of tiles and tile information.</p> Source code in <code>src/careamics/dataset/in_memory_tiled_pred_dataset.py</code> <pre><code>def _prepare_tiles(self) -&gt; list[tuple[NDArray, TileInformation]]:\n    \"\"\"\n    Iterate over data source and create an array of patches.\n\n    Returns\n    -------\n    list of tuples of NDArray and TileInformation\n        List of tiles and tile information.\n    \"\"\"\n    # reshape array\n    reshaped_sample = reshape_array(self.input_array, self.axes)\n\n    # generate patches, which returns a generator\n    patch_generator = extract_tiles(\n        arr=reshaped_sample,\n        tile_size=self.tile_size,\n        overlaps=self.tile_overlap,\n    )\n    patches_list = list(patch_generator)\n\n    if len(patches_list) == 0:\n        raise ValueError(\"No tiles generated, \")\n\n    return patches_list\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/","title":"iterable_dataset","text":"<p>Iterable dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset","title":"<code>PathIterableDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Dataset allowing extracting patches w/o loading whole data into memory.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataConfig</code> <p>Data configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>Optional list of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>list of pathlib.Path</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>class PathIterableDataset(IterableDataset):\n    \"\"\"\n    Dataset allowing extracting patches w/o loading whole data into memory.\n\n    Parameters\n    ----------\n    data_config : DataConfig\n        Data configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    target_files : list of pathlib.Path, optional\n        Optional list of target files, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n\n    Attributes\n    ----------\n    data_path : list of pathlib.Path\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        src_files: list[Path],\n        target_files: Optional[list[Path]] = None,\n        read_source_func: Callable = read_tiff,\n    ) -&gt; None:\n        \"\"\"Constructors.\n\n        Parameters\n        ----------\n        data_config : GeneralDataConfig\n            Data configuration.\n        src_files : list[Path]\n            List of data files.\n        target_files : list[Path] or None, optional\n            Optional list of target files, by default None.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        \"\"\"\n        self.data_config = data_config\n        self.data_files = src_files\n        self.target_files = target_files\n        self.read_source_func = read_source_func\n\n        # compute mean and std over the dataset\n        # only checking the image_mean because the DataConfig class ensures that\n        # if image_mean is provided, image_std is also provided\n        if not self.data_config.image_means:\n            self.image_stats, self.target_stats = self._calculate_mean_and_std()\n            logger.info(\n                f\"Computed dataset mean: {self.image_stats.means},\"\n                f\"std: {self.image_stats.stds}\"\n            )\n\n            # update the mean in the config\n            self.data_config.set_means_and_stds(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=(\n                    list(self.target_stats.means)\n                    if self.target_stats.means is not None\n                    else None\n                ),\n                target_stds=(\n                    list(self.target_stats.stds)\n                    if self.target_stats.stds is not None\n                    else None\n                ),\n            )\n\n        else:\n            # if mean and std are provided in the config, use them\n            self.image_stats, self.target_stats = (\n                Stats(self.data_config.image_means, self.data_config.image_stds),\n                Stats(self.data_config.target_means, self.data_config.target_stds),\n            )\n\n        # create transform composed of normalization and other transforms\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_stats.means,\n                    image_stds=self.image_stats.stds,\n                    target_means=self.target_stats.means,\n                    target_stds=self.target_stats.stds,\n                )\n            ]\n            + list(data_config.transforms)\n        )\n\n    def _calculate_mean_and_std(self) -&gt; tuple[Stats, Stats]:\n        \"\"\"\n        Calculate mean and std of the dataset.\n\n        Returns\n        -------\n        tuple of Stats and optional Stats\n            Data classes containing the image and target statistics.\n        \"\"\"\n        num_samples = 0\n        image_stats = WelfordStatistics()\n        if self.target_files is not None:\n            target_stats = WelfordStatistics()\n\n        for sample, target in iterate_over_files(\n            self.data_config, self.data_files, self.target_files, self.read_source_func\n        ):\n            # update the image statistics\n            image_stats.update(sample, num_samples)\n\n            # update the target statistics if target is available\n            if target is not None:\n                target_stats.update(target, num_samples)\n\n            num_samples += 1\n\n        if num_samples == 0:\n            raise ValueError(\"No samples found in the dataset.\")\n\n        # Average the means and stds per sample\n        image_means, image_stds = image_stats.finalize()\n\n        if target is not None:\n            target_means, target_stds = target_stats.finalize()\n\n            return (\n                Stats(image_means, image_stds),\n                Stats(np.array(target_means), np.array(target_stds)),\n            )\n        else:\n            return Stats(image_means, image_stds), Stats(None, None)\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[np.ndarray, ...], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        np.ndarray\n            Single patch.\n        \"\"\"\n        assert (\n            self.image_stats.means is not None and self.image_stats.stds is not None\n        ), \"Mean and std must be provided\"\n\n        # iterate over files\n        for sample_input, sample_target in iterate_over_files(\n            self.data_config, self.data_files, self.target_files, self.read_source_func\n        ):\n            patches = extract_patches_random(\n                arr=sample_input,\n                patch_size=self.data_config.patch_size,\n                target=sample_target,\n            )\n\n            # iterate over patches\n            # patches are tuples of (patch, target) if target is available\n            # or (patch, None) only if no target is available\n            # patch is of dimensions (C)ZYX\n            for patch_data in patches:\n                yield self.patch_transform(\n                    patch=patch_data[0],\n                    target=patch_data[1],\n                )\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        Returns\n        -------\n        tuple of list of floats\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.image_stats.get_statistics()\n\n    def get_number_of_files(self) -&gt; int:\n        \"\"\"\n        Return the number of files in the dataset.\n\n        Returns\n        -------\n        int\n            Number of files in the dataset.\n        \"\"\"\n        return len(self.data_files)\n\n    def split_dataset(\n        self,\n        percentage: float = 0.1,\n        minimum_number: int = 5,\n    ) -&gt; PathIterableDataset:\n        \"\"\"Split up dataset in two.\n\n        Splits the datest sing a percentage of the data (files) to extract, or the\n        minimum number of the percentage is less than the minimum number.\n\n        Parameters\n        ----------\n        percentage : float, optional\n            Percentage of files to split up, by default 0.1.\n        minimum_number : int, optional\n            Minimum number of files to split up, by default 5.\n\n        Returns\n        -------\n        IterableDataset\n            Dataset containing the split data.\n\n        Raises\n        ------\n        ValueError\n            If the percentage is smaller than 0 or larger than 1.\n        ValueError\n            If the minimum number is smaller than 1 or larger than the number of files.\n        \"\"\"\n        if percentage &lt; 0 or percentage &gt; 1:\n            raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n        if minimum_number &lt; 1 or minimum_number &gt; self.get_number_of_files():\n            raise ValueError(\n                f\"Minimum number of files must be between 1 and \"\n                f\"{self.get_number_of_files()} (number of files), got \"\n                f\"{minimum_number}.\"\n            )\n\n        # compute number of files\n        total_files = self.get_number_of_files()\n        n_files = max(round(percentage * total_files), minimum_number)\n\n        # get random indices\n        indices = np.random.choice(total_files, n_files, replace=False)\n\n        # extract files\n        val_files = [self.data_files[i] for i in indices]\n\n        # remove patches from self.patch\n        data_files = []\n        for i, file in enumerate(self.data_files):\n            if i not in indices:\n                data_files.append(file)\n        self.data_files = data_files\n\n        # same for targets\n        if self.target_files is not None:\n            val_target_files = [self.target_files[i] for i in indices]\n\n            data_target_files = []\n            for i, file in enumerate(self.target_files):\n                if i not in indices:\n                    data_target_files.append(file)\n            self.target_files = data_target_files\n\n        # clone the dataset\n        dataset = copy.deepcopy(self)\n\n        # reassign patches\n        dataset.data_files = val_files\n\n        # reassign targets\n        if self.target_files is not None:\n            dataset.target_files = val_target_files\n\n        return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.__init__","title":"<code>__init__(data_config, src_files, target_files=None, read_source_func=read_tiff)</code>","text":"<p>Constructors.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>GeneralDataConfig</code> <p>Data configuration.</p> required <code>src_files</code> <code>list[Path]</code> <p>List of data files.</p> required <code>target_files</code> <code>list[Path] or None</code> <p>Optional list of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    src_files: list[Path],\n    target_files: Optional[list[Path]] = None,\n    read_source_func: Callable = read_tiff,\n) -&gt; None:\n    \"\"\"Constructors.\n\n    Parameters\n    ----------\n    data_config : GeneralDataConfig\n        Data configuration.\n    src_files : list[Path]\n        List of data files.\n    target_files : list[Path] or None, optional\n        Optional list of target files, by default None.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    \"\"\"\n    self.data_config = data_config\n    self.data_files = src_files\n    self.target_files = target_files\n    self.read_source_func = read_source_func\n\n    # compute mean and std over the dataset\n    # only checking the image_mean because the DataConfig class ensures that\n    # if image_mean is provided, image_std is also provided\n    if not self.data_config.image_means:\n        self.image_stats, self.target_stats = self._calculate_mean_and_std()\n        logger.info(\n            f\"Computed dataset mean: {self.image_stats.means},\"\n            f\"std: {self.image_stats.stds}\"\n        )\n\n        # update the mean in the config\n        self.data_config.set_means_and_stds(\n            image_means=self.image_stats.means,\n            image_stds=self.image_stats.stds,\n            target_means=(\n                list(self.target_stats.means)\n                if self.target_stats.means is not None\n                else None\n            ),\n            target_stds=(\n                list(self.target_stats.stds)\n                if self.target_stats.stds is not None\n                else None\n            ),\n        )\n\n    else:\n        # if mean and std are provided in the config, use them\n        self.image_stats, self.target_stats = (\n            Stats(self.data_config.image_means, self.data_config.image_stds),\n            Stats(self.data_config.target_means, self.data_config.target_stds),\n        )\n\n    # create transform composed of normalization and other transforms\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_stats.means,\n                image_stds=self.image_stats.stds,\n                target_means=self.target_stats.means,\n                target_stds=self.target_stats.stds,\n            )\n        ]\n        + list(data_config.transforms)\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>ndarray</code> <p>Single patch.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[np.ndarray, ...], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    np.ndarray\n        Single patch.\n    \"\"\"\n    assert (\n        self.image_stats.means is not None and self.image_stats.stds is not None\n    ), \"Mean and std must be provided\"\n\n    # iterate over files\n    for sample_input, sample_target in iterate_over_files(\n        self.data_config, self.data_files, self.target_files, self.read_source_func\n    ):\n        patches = extract_patches_random(\n            arr=sample_input,\n            patch_size=self.data_config.patch_size,\n            target=sample_target,\n        )\n\n        # iterate over patches\n        # patches are tuples of (patch, target) if target is available\n        # or (patch, None) only if no target is available\n        # patch is of dimensions (C)ZYX\n        for patch_data in patches:\n            yield self.patch_transform(\n                patch=patch_data[0],\n                target=patch_data[1],\n            )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset._calculate_mean_and_std","title":"<code>_calculate_mean_and_std()</code>","text":"<p>Calculate mean and std of the dataset.</p> <p>Returns:</p> Type Description <code>tuple of Stats and optional Stats</code> <p>Data classes containing the image and target statistics.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def _calculate_mean_and_std(self) -&gt; tuple[Stats, Stats]:\n    \"\"\"\n    Calculate mean and std of the dataset.\n\n    Returns\n    -------\n    tuple of Stats and optional Stats\n        Data classes containing the image and target statistics.\n    \"\"\"\n    num_samples = 0\n    image_stats = WelfordStatistics()\n    if self.target_files is not None:\n        target_stats = WelfordStatistics()\n\n    for sample, target in iterate_over_files(\n        self.data_config, self.data_files, self.target_files, self.read_source_func\n    ):\n        # update the image statistics\n        image_stats.update(sample, num_samples)\n\n        # update the target statistics if target is available\n        if target is not None:\n            target_stats.update(target, num_samples)\n\n        num_samples += 1\n\n    if num_samples == 0:\n        raise ValueError(\"No samples found in the dataset.\")\n\n    # Average the means and stds per sample\n    image_means, image_stds = image_stats.finalize()\n\n    if target is not None:\n        target_means, target_stds = target_stats.finalize()\n\n        return (\n            Stats(image_means, image_stds),\n            Stats(np.array(target_means), np.array(target_stds)),\n        )\n    else:\n        return Stats(image_means, image_stds), Stats(None, None)\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>Returns:</p> Type Description <code>tuple of list of floats</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    Returns\n    -------\n    tuple of list of floats\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.image_stats.get_statistics()\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.get_number_of_files","title":"<code>get_number_of_files()</code>","text":"<p>Return the number of files in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of files in the dataset.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def get_number_of_files(self) -&gt; int:\n    \"\"\"\n    Return the number of files in the dataset.\n\n    Returns\n    -------\n    int\n        Number of files in the dataset.\n    \"\"\"\n    return len(self.data_files)\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_dataset/#careamics.dataset.iterable_dataset.PathIterableDataset.split_dataset","title":"<code>split_dataset(percentage=0.1, minimum_number=5)</code>","text":"<p>Split up dataset in two.</p> <p>Splits the datest sing a percentage of the data (files) to extract, or the minimum number of the percentage is less than the minimum number.</p> <p>Parameters:</p> Name Type Description Default <code>percentage</code> <code>float</code> <p>Percentage of files to split up, by default 0.1.</p> <code>0.1</code> <code>minimum_number</code> <code>int</code> <p>Minimum number of files to split up, by default 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>IterableDataset</code> <p>Dataset containing the split data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the percentage is smaller than 0 or larger than 1.</p> <code>ValueError</code> <p>If the minimum number is smaller than 1 or larger than the number of files.</p> Source code in <code>src/careamics/dataset/iterable_dataset.py</code> <pre><code>def split_dataset(\n    self,\n    percentage: float = 0.1,\n    minimum_number: int = 5,\n) -&gt; PathIterableDataset:\n    \"\"\"Split up dataset in two.\n\n    Splits the datest sing a percentage of the data (files) to extract, or the\n    minimum number of the percentage is less than the minimum number.\n\n    Parameters\n    ----------\n    percentage : float, optional\n        Percentage of files to split up, by default 0.1.\n    minimum_number : int, optional\n        Minimum number of files to split up, by default 5.\n\n    Returns\n    -------\n    IterableDataset\n        Dataset containing the split data.\n\n    Raises\n    ------\n    ValueError\n        If the percentage is smaller than 0 or larger than 1.\n    ValueError\n        If the minimum number is smaller than 1 or larger than the number of files.\n    \"\"\"\n    if percentage &lt; 0 or percentage &gt; 1:\n        raise ValueError(f\"Percentage must be between 0 and 1, got {percentage}.\")\n\n    if minimum_number &lt; 1 or minimum_number &gt; self.get_number_of_files():\n        raise ValueError(\n            f\"Minimum number of files must be between 1 and \"\n            f\"{self.get_number_of_files()} (number of files), got \"\n            f\"{minimum_number}.\"\n        )\n\n    # compute number of files\n    total_files = self.get_number_of_files()\n    n_files = max(round(percentage * total_files), minimum_number)\n\n    # get random indices\n    indices = np.random.choice(total_files, n_files, replace=False)\n\n    # extract files\n    val_files = [self.data_files[i] for i in indices]\n\n    # remove patches from self.patch\n    data_files = []\n    for i, file in enumerate(self.data_files):\n        if i not in indices:\n            data_files.append(file)\n    self.data_files = data_files\n\n    # same for targets\n    if self.target_files is not None:\n        val_target_files = [self.target_files[i] for i in indices]\n\n        data_target_files = []\n        for i, file in enumerate(self.target_files):\n            if i not in indices:\n                data_target_files.append(file)\n        self.target_files = data_target_files\n\n    # clone the dataset\n    dataset = copy.deepcopy(self)\n\n    # reassign patches\n    dataset.data_files = val_files\n\n    # reassign targets\n    if self.target_files is not None:\n        dataset.target_files = val_target_files\n\n    return dataset\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/","title":"iterable_pred_dataset","text":"<p>Iterable prediction dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset","title":"<code>IterablePredDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Simple iterable prediction dataset.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>List[Path]</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>Union[str, Path]</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> <code>mean</code> <code>(Optional[float], optional)</code> <p>Expected mean of the dataset, by default None.</p> <code>std</code> <code>(Optional[float], optional)</code> <p>Expected standard deviation of the dataset, by default None.</p> <code>patch_transform</code> <code>(Optional[Callable], optional)</code> <p>Patch transform callable, by default None.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>class IterablePredDataset(IterableDataset):\n    \"\"\"Simple iterable prediction dataset.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : List[Path]\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Attributes\n    ----------\n    data_path : Union[str, Path]\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    mean : Optional[float], optional\n        Expected mean of the dataset, by default None.\n    std : Optional[float], optional\n        Expected standard deviation of the dataset, by default None.\n    patch_transform : Optional[Callable], optional\n        Patch transform callable, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        src_files: list[Path],\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Inference configuration.\n        src_files : list of pathlib.Path\n            List of data files.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the inference configuration.\n        \"\"\"\n        self.prediction_config = prediction_config\n        self.data_files = src_files\n        self.axes = prediction_config.axes\n        self.read_source_func = read_source_func\n\n        # check mean and std and create normalize transform\n        if (\n            self.prediction_config.image_means is None\n            or self.prediction_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided for prediction.\")\n        else:\n            self.image_means = self.prediction_config.image_means\n            self.image_stds = self.prediction_config.image_stds\n\n        # instantiate normalize transform\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_means,\n                    image_stds=self.image_stds,\n                )\n            ],\n        )\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[NDArray, ...], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        (numpy.ndarray, numpy.ndarray or None)\n            Single patch.\n        \"\"\"\n        assert (\n            self.image_means is not None and self.image_stds is not None\n        ), \"Mean and std must be provided\"\n\n        for sample, _ in iterate_over_files(\n            self.prediction_config,\n            self.data_files,\n            read_source_func=self.read_source_func,\n        ):\n            # sample has S dimension\n            for i in range(sample.shape[0]):\n\n                yield self.patch_transform(patch=sample[i])\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset.__init__","title":"<code>__init__(prediction_config, src_files, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the inference configuration.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    src_files: list[Path],\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the inference configuration.\n    \"\"\"\n    self.prediction_config = prediction_config\n    self.data_files = src_files\n    self.axes = prediction_config.axes\n    self.read_source_func = read_source_func\n\n    # check mean and std and create normalize transform\n    if (\n        self.prediction_config.image_means is None\n        or self.prediction_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided for prediction.\")\n    else:\n        self.image_means = self.prediction_config.image_means\n        self.image_stds = self.prediction_config.image_stds\n\n    # instantiate normalize transform\n    self.patch_transform = Compose(\n        transform_list=[\n            NormalizeModel(\n                image_means=self.image_means,\n                image_stds=self.image_stds,\n            )\n        ],\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_pred_dataset/#careamics.dataset.iterable_pred_dataset.IterablePredDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>(ndarray, ndarray or None)</code> <p>Single patch.</p> Source code in <code>src/careamics/dataset/iterable_pred_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[NDArray, ...], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    (numpy.ndarray, numpy.ndarray or None)\n        Single patch.\n    \"\"\"\n    assert (\n        self.image_means is not None and self.image_stds is not None\n    ), \"Mean and std must be provided\"\n\n    for sample, _ in iterate_over_files(\n        self.prediction_config,\n        self.data_files,\n        read_source_func=self.read_source_func,\n    ):\n        # sample has S dimension\n        for i in range(sample.shape[0]):\n\n            yield self.patch_transform(patch=sample[i])\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/","title":"iterable_tiled_pred_dataset","text":"<p>Iterable tiled prediction dataset used to load data file by file.</p>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset","title":"<code>IterableTiledPredDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Tiled prediction dataset.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str or Path</code> <p>Path to the data, must be a directory.</p> <code>axes</code> <code>str</code> <p>Description of axes in format STCZYX.</p> <code>mean</code> <code>(float, optional)</code> <p>Expected mean of the dataset, by default None.</p> <code>std</code> <code>(float, optional)</code> <p>Expected standard deviation of the dataset, by default None.</p> <code>patch_transform</code> <code>(Callable, optional)</code> <p>Patch transform callable, by default None.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>class IterableTiledPredDataset(IterableDataset):\n    \"\"\"Tiled prediction dataset.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : list of pathlib.Path\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Attributes\n    ----------\n    data_path : str or pathlib.Path\n        Path to the data, must be a directory.\n    axes : str\n        Description of axes in format STCZYX.\n    mean : float, optional\n        Expected mean of the dataset, by default None.\n    std : float, optional\n        Expected standard deviation of the dataset, by default None.\n    patch_transform : Callable, optional\n        Patch transform callable, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        prediction_config: InferenceConfig,\n        src_files: list[Path],\n        read_source_func: Callable = read_tiff,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        prediction_config : InferenceConfig\n            Inference configuration.\n        src_files : List[Path]\n            List of data files.\n        read_source_func : Callable, optional\n            Read source function for custom types, by default read_tiff.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Raises\n        ------\n        ValueError\n            If mean and std are not provided in the inference configuration.\n        \"\"\"\n        if (\n            prediction_config.tile_size is None\n            or prediction_config.tile_overlap is None\n        ):\n            raise ValueError(\n                \"Tile size and overlap must be provided for tiled prediction.\"\n            )\n\n        self.prediction_config = prediction_config\n        self.data_files = src_files\n        self.axes = prediction_config.axes\n        self.tile_size = prediction_config.tile_size\n        self.tile_overlap = prediction_config.tile_overlap\n        self.read_source_func = read_source_func\n\n        # check mean and std and create normalize transform\n        if (\n            self.prediction_config.image_means is None\n            or self.prediction_config.image_stds is None\n        ):\n            raise ValueError(\"Mean and std must be provided for prediction.\")\n        else:\n            self.image_means = self.prediction_config.image_means\n            self.image_stds = self.prediction_config.image_stds\n\n            # instantiate normalize transform\n            self.patch_transform = Compose(\n                transform_list=[\n                    NormalizeModel(\n                        image_means=self.image_means,\n                        image_stds=self.image_stds,\n                    )\n                ],\n            )\n\n    def __iter__(\n        self,\n    ) -&gt; Generator[tuple[tuple[NDArray, ...], TileInformation], None, None]:\n        \"\"\"\n        Iterate over data source and yield single patch.\n\n        Yields\n        ------\n        Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple\n            Generator of single tiles.\n        \"\"\"\n        assert (\n            self.image_means is not None and self.image_stds is not None\n        ), \"Mean and std must be provided\"\n\n        for sample, _ in iterate_over_files(\n            self.prediction_config,\n            self.data_files,\n            read_source_func=self.read_source_func,\n        ):\n            # generate patches, return a generator of single tiles\n            patch_gen = extract_tiles(\n                arr=sample,\n                tile_size=self.tile_size,\n                overlaps=self.tile_overlap,\n            )\n\n            # apply transform to patches\n            for patch_array, tile_info in patch_gen:\n                transformed_patch = self.patch_transform(patch=patch_array)\n\n                yield transformed_patch, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset.__init__","title":"<code>__init__(prediction_config, src_files, read_source_func=read_tiff, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_config</code> <code>InferenceConfig</code> <p>Inference configuration.</p> required <code>src_files</code> <code>List[Path]</code> <p>List of data files.</p> required <code>read_source_func</code> <code>Callable</code> <p>Read source function for custom types, by default read_tiff.</p> <code>read_tiff</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mean and std are not provided in the inference configuration.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>def __init__(\n    self,\n    prediction_config: InferenceConfig,\n    src_files: list[Path],\n    read_source_func: Callable = read_tiff,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    prediction_config : InferenceConfig\n        Inference configuration.\n    src_files : List[Path]\n        List of data files.\n    read_source_func : Callable, optional\n        Read source function for custom types, by default read_tiff.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Raises\n    ------\n    ValueError\n        If mean and std are not provided in the inference configuration.\n    \"\"\"\n    if (\n        prediction_config.tile_size is None\n        or prediction_config.tile_overlap is None\n    ):\n        raise ValueError(\n            \"Tile size and overlap must be provided for tiled prediction.\"\n        )\n\n    self.prediction_config = prediction_config\n    self.data_files = src_files\n    self.axes = prediction_config.axes\n    self.tile_size = prediction_config.tile_size\n    self.tile_overlap = prediction_config.tile_overlap\n    self.read_source_func = read_source_func\n\n    # check mean and std and create normalize transform\n    if (\n        self.prediction_config.image_means is None\n        or self.prediction_config.image_stds is None\n    ):\n        raise ValueError(\"Mean and std must be provided for prediction.\")\n    else:\n        self.image_means = self.prediction_config.image_means\n        self.image_stds = self.prediction_config.image_stds\n\n        # instantiate normalize transform\n        self.patch_transform = Compose(\n            transform_list=[\n                NormalizeModel(\n                    image_means=self.image_means,\n                    image_stds=self.image_stds,\n                )\n            ],\n        )\n</code></pre>"},{"location":"reference/careamics/dataset/iterable_tiled_pred_dataset/#careamics.dataset.iterable_tiled_pred_dataset.IterableTiledPredDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over data source and yield single patch.</p> <p>Yields:</p> Type Description <code>Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple</code> <p>Generator of single tiles.</p> Source code in <code>src/careamics/dataset/iterable_tiled_pred_dataset.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Generator[tuple[tuple[NDArray, ...], TileInformation], None, None]:\n    \"\"\"\n    Iterate over data source and yield single patch.\n\n    Yields\n    ------\n    Generator of (np.ndarray, np.ndarray or None) and TileInformation tuple\n        Generator of single tiles.\n    \"\"\"\n    assert (\n        self.image_means is not None and self.image_stds is not None\n    ), \"Mean and std must be provided\"\n\n    for sample, _ in iterate_over_files(\n        self.prediction_config,\n        self.data_files,\n        read_source_func=self.read_source_func,\n    ):\n        # generate patches, return a generator of single tiles\n        patch_gen = extract_tiles(\n            arr=sample,\n            tile_size=self.tile_size,\n            overlaps=self.tile_overlap,\n        )\n\n        # apply transform to patches\n        for patch_array, tile_info in patch_gen:\n            transformed_patch = self.patch_transform(patch=patch_array)\n\n            yield transformed_patch, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/zarr_dataset/","title":"zarr_dataset","text":"<p>Zarr dataset.</p>"},{"location":"reference/careamics/dataset/dataset_utils/dataset_utils/","title":"dataset_utils","text":"<p>Dataset utilities.</p>"},{"location":"reference/careamics/dataset/dataset_utils/dataset_utils/#careamics.dataset.dataset_utils.dataset_utils._get_shape_order","title":"<code>_get_shape_order(shape_in, axes_in, ref_axes='STCZYX')</code>","text":"<p>Compute a new shape for the array based on the reference axes.</p> <p>Parameters:</p> Name Type Description Default <code>shape_in</code> <code>tuple[int, ...]</code> <p>Input shape.</p> required <code>axes_in</code> <code>str</code> <p>Input axes.</p> required <code>ref_axes</code> <code>str</code> <p>Reference axes.</p> <code>'STCZYX'</code> <p>Returns:</p> Type Description <code>tuple[tuple[int, ...], str, list[int]]</code> <p>New shape, new axes, indices of axes in the new axes order.</p> Source code in <code>src/careamics/dataset/dataset_utils/dataset_utils.py</code> <pre><code>def _get_shape_order(\n    shape_in: tuple[int, ...], axes_in: str, ref_axes: str = \"STCZYX\"\n) -&gt; tuple[tuple[int, ...], str, list[int]]:\n    \"\"\"\n    Compute a new shape for the array based on the reference axes.\n\n    Parameters\n    ----------\n    shape_in : tuple[int, ...]\n        Input shape.\n    axes_in : str\n        Input axes.\n    ref_axes : str\n        Reference axes.\n\n    Returns\n    -------\n    tuple[tuple[int, ...], str, list[int]]\n        New shape, new axes, indices of axes in the new axes order.\n    \"\"\"\n    indices = [axes_in.find(k) for k in ref_axes]\n\n    # remove all non-existing axes (index == -1)\n    new_indices = list(filter(lambda k: k != -1, indices))\n\n    # find axes order and get new shape\n    new_axes = [axes_in[ind] for ind in new_indices]\n    new_shape = tuple([shape_in[ind] for ind in new_indices])\n\n    return new_shape, \"\".join(new_axes), new_indices\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/dataset_utils/#careamics.dataset.dataset_utils.dataset_utils.reshape_array","title":"<code>reshape_array(x, axes)</code>","text":"<p>Reshape the data to (S, C, (Z), Y, X) by moving axes.</p> <p>If the data has both S and T axes, the two axes will be merged. A singleton dimension is added if there are no C axis.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array.</p> required <code>axes</code> <code>str</code> <p>Description of axes in format <code>STCZYX</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Reshaped array with shape (S, C, (Z), Y, X).</p> Source code in <code>src/careamics/dataset/dataset_utils/dataset_utils.py</code> <pre><code>def reshape_array(x: np.ndarray, axes: str) -&gt; np.ndarray:\n    \"\"\"Reshape the data to (S, C, (Z), Y, X) by moving axes.\n\n    If the data has both S and T axes, the two axes will be merged. A singleton\n    dimension is added if there are no C axis.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input array.\n    axes : str\n        Description of axes in format `STCZYX`.\n\n    Returns\n    -------\n    np.ndarray\n        Reshaped array with shape (S, C, (Z), Y, X).\n    \"\"\"\n    _x = x\n    _axes = axes\n\n    # sanity checks\n    if len(_axes) != len(_x.shape):\n        raise ValueError(\n            f\"Incompatible data shape ({_x.shape}) and axes ({_axes}). Are the axes \"\n            f\"correct?\"\n        )\n\n    # get new x shape\n    new_x_shape, new_axes, indices = _get_shape_order(_x.shape, _axes)\n\n    # if S is not in the list of axes, then add a singleton S\n    if \"S\" not in new_axes:\n        new_axes = \"S\" + new_axes\n        _x = _x[np.newaxis, ...]\n        new_x_shape = (1,) + new_x_shape\n\n        # need to change the array of indices\n        indices = [0] + [1 + i for i in indices]\n\n    # reshape by moving axes\n    destination = list(range(len(indices)))\n    _x = np.moveaxis(_x, indices, destination)\n\n    # remove T if necessary\n    if \"T\" in new_axes:\n        new_x_shape = (-1,) + new_x_shape[2:]  # remove T and S\n        new_axes = new_axes.replace(\"T\", \"\")\n\n        # reshape S and T together\n        _x = _x.reshape(new_x_shape)\n\n    # add channel\n    if \"C\" not in new_axes:\n        # Add channel axis after S\n        _x = np.expand_dims(_x, new_axes.index(\"S\") + 1)\n\n    return _x\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/","title":"file_utils","text":"<p>File utilities.</p>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.get_files_size","title":"<code>get_files_size(files)</code>","text":"<p>Get files size in MB.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list of pathlib.Path</code> <p>List of files.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Total size of the files in MB.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def get_files_size(files: list[Path]) -&gt; float:\n    \"\"\"Get files size in MB.\n\n    Parameters\n    ----------\n    files : list of pathlib.Path\n        List of files.\n\n    Returns\n    -------\n    float\n        Total size of the files in MB.\n    \"\"\"\n    return np.sum([f.stat().st_size / 1024**2 for f in files])\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.list_files","title":"<code>list_files(data_path, data_type, extension_filter='')</code>","text":"<p>List recursively files in <code>data_path</code> and return a sorted list.</p> <p>If <code>data_path</code> is a file, its name is validated against the <code>data_type</code> using <code>fnmatch</code>, and the method returns <code>data_path</code> itself.</p> <p>By default, if <code>data_type</code> is equal to <code>custom</code>, all files will be listed. To further filter the files, use <code>extension_filter</code>.</p> <p><code>extension_filter</code> must be compatible with <code>fnmatch</code> and <code>Path.rglob</code>, e.g. \".npy\" or \".czi\".</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, Path]</code> <p>Path to the folder containing the data.</p> required <code>data_type</code> <code>Union[str, SupportedData]</code> <p>One of the supported data type (e.g. tif, custom).</p> required <code>extension_filter</code> <code>str</code> <p>Extension filter, by default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>list of pathlib.Path objects.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the data path does not exist.</p> <code>ValueError</code> <p>If the data path is empty or no files with the extension were found.</p> <code>ValueError</code> <p>If the file does not match the requested extension.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def list_files(\n    data_path: Union[str, Path],\n    data_type: Union[str, SupportedData],\n    extension_filter: str = \"\",\n) -&gt; list[Path]:\n    \"\"\"List recursively files in `data_path` and return a sorted list.\n\n    If `data_path` is a file, its name is validated against the `data_type` using\n    `fnmatch`, and the method returns `data_path` itself.\n\n    By default, if `data_type` is equal to `custom`, all files will be listed. To\n    further filter the files, use `extension_filter`.\n\n    `extension_filter` must be compatible with `fnmatch` and `Path.rglob`, e.g. \"*.npy\"\n    or \"*.czi\".\n\n    Parameters\n    ----------\n    data_path : Union[str, Path]\n        Path to the folder containing the data.\n    data_type : Union[str, SupportedData]\n        One of the supported data type (e.g. tif, custom).\n    extension_filter : str, optional\n        Extension filter, by default \"\".\n\n    Returns\n    -------\n    list[Path]\n        list of pathlib.Path objects.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the data path does not exist.\n    ValueError\n        If the data path is empty or no files with the extension were found.\n    ValueError\n        If the file does not match the requested extension.\n    \"\"\"\n    # convert to Path\n    data_path = Path(data_path)\n\n    # raise error if does not exists\n    if not data_path.exists():\n        raise FileNotFoundError(f\"Data path {data_path} does not exist.\")\n\n    # get extension compatible with fnmatch and rglob search\n    extension = SupportedData.get_extension_pattern(data_type)\n\n    if data_type == SupportedData.CUSTOM and extension_filter != \"\":\n        extension = extension_filter\n\n    # search recurively\n    if data_path.is_dir():\n        # search recursively the path for files with the extension\n        files = sorted(data_path.rglob(extension))\n    else:\n        # raise error if it has the wrong extension\n        if not fnmatch(str(data_path.absolute()), extension):\n            raise ValueError(\n                f\"File {data_path} does not match the requested extension \"\n                f'\"{extension}\".'\n            )\n\n        # save in list\n        files = [data_path]\n\n    # raise error if no files were found\n    if len(files) == 0:\n        raise ValueError(\n            f'Data path {data_path} is empty or files with extension \"{extension}\" '\n            f\"were not found.\"\n        )\n\n    return files\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/file_utils/#careamics.dataset.dataset_utils.file_utils.validate_source_target_files","title":"<code>validate_source_target_files(src_files, tar_files)</code>","text":"<p>Validate source and target path lists.</p> <p>The two lists should have the same number of files, and the filenames should match.</p> <p>Parameters:</p> Name Type Description Default <code>src_files</code> <code>list of pathlib.Path</code> <p>List of source files.</p> required <code>tar_files</code> <code>list of pathlib.Path</code> <p>List of target files.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of files in source and target folders is not the same.</p> <code>ValueError</code> <p>If some filenames in Train and target folders are not the same.</p> Source code in <code>src/careamics/dataset/dataset_utils/file_utils.py</code> <pre><code>def validate_source_target_files(src_files: list[Path], tar_files: list[Path]) -&gt; None:\n    \"\"\"\n    Validate source and target path lists.\n\n    The two lists should have the same number of files, and the filenames should match.\n\n    Parameters\n    ----------\n    src_files : list of pathlib.Path\n        List of source files.\n    tar_files : list of pathlib.Path\n        List of target files.\n\n    Raises\n    ------\n    ValueError\n        If the number of files in source and target folders is not the same.\n    ValueError\n        If some filenames in Train and target folders are not the same.\n    \"\"\"\n    # check equal length\n    if len(src_files) != len(tar_files):\n        raise ValueError(\n            f\"The number of source files ({len(src_files)}) is not equal to the number \"\n            f\"of target files ({len(tar_files)}).\"\n        )\n\n    # check identical names\n    src_names = {f.name for f in src_files}\n    tar_names = {f.name for f in tar_files}\n    difference = src_names.symmetric_difference(tar_names)\n\n    if len(difference) &gt; 0:\n        raise ValueError(f\"Source and target files have different names: {difference}.\")\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/iterate_over_files/","title":"iterate_over_files","text":"<p>Function to iterate over files.</p>"},{"location":"reference/careamics/dataset/dataset_utils/iterate_over_files/#careamics.dataset.dataset_utils.iterate_over_files.iterate_over_files","title":"<code>iterate_over_files(data_config, data_files, target_files=None, read_source_func=read_tiff)</code>","text":"<p>Iterate over data source and yield whole reshaped images.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>CAREamics DataConfig or InferenceConfig</code> <p>Configuration.</p> required <code>data_files</code> <code>list of pathlib.Path</code> <p>List of data files.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>List of target files, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source, by default read_tiff.</p> <code>read_tiff</code> <p>Yields:</p> Type Description <code>NDArray</code> <p>Image.</p> Source code in <code>src/careamics/dataset/dataset_utils/iterate_over_files.py</code> <pre><code>def iterate_over_files(\n    data_config: Union[DataConfig, InferenceConfig],\n    data_files: list[Path],\n    target_files: Optional[list[Path]] = None,\n    read_source_func: Callable = read_tiff,\n) -&gt; Generator[tuple[NDArray, Optional[NDArray]], None, None]:\n    \"\"\"Iterate over data source and yield whole reshaped images.\n\n    Parameters\n    ----------\n    data_config : CAREamics DataConfig or InferenceConfig\n        Configuration.\n    data_files : list of pathlib.Path\n        List of data files.\n    target_files : list of pathlib.Path, optional\n        List of target files, by default None.\n    read_source_func : Callable, optional\n        Function to read the source, by default read_tiff.\n\n    Yields\n    ------\n    NDArray\n        Image.\n    \"\"\"\n    # When num_workers &gt; 0, each worker process will have a different copy of the\n    # dataset object\n    # Configuring each copy independently to avoid having duplicate data returned\n    # from the workers\n    worker_info = get_worker_info()\n    worker_id = worker_info.id if worker_info is not None else 0\n    num_workers = worker_info.num_workers if worker_info is not None else 1\n\n    # iterate over the files\n    for i, filename in enumerate(data_files):\n        # retrieve file corresponding to the worker id\n        if i % num_workers == worker_id:\n            try:\n                # read data\n                sample = read_source_func(filename, data_config.axes)\n\n                # reshape array\n                reshaped_sample = reshape_array(sample, data_config.axes)\n\n                # read target, if available\n                if target_files is not None:\n                    if filename.name != target_files[i].name:\n                        raise ValueError(\n                            f\"File {filename} does not match target file \"\n                            f\"{target_files[i]}. Have you passed sorted \"\n                            f\"arrays?\"\n                        )\n\n                    # read target\n                    target = read_source_func(target_files[i], data_config.axes)\n\n                    # reshape target\n                    reshaped_target = reshape_array(target, data_config.axes)\n\n                    yield reshaped_sample, reshaped_target\n                else:\n                    yield reshaped_sample, None\n\n            except Exception as e:\n                logger.error(f\"Error reading file {filename}: {e}\")\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/","title":"running_stats","text":"<p>Computing data statistics.</p>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics","title":"<code>WelfordStatistics</code>","text":"<p>Compute Welford statistics iteratively.</p> <p>The Welford algorithm is used to compute the mean and variance of an array iteratively. Based on the implementation from: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>class WelfordStatistics:\n    \"\"\"Compute Welford statistics iteratively.\n\n    The Welford algorithm is used to compute the mean and variance of an array\n    iteratively. Based on the implementation from:\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm\n    \"\"\"\n\n    def update(self, array: NDArray, sample_idx: int) -&gt; None:\n        \"\"\"Update the Welford statistics.\n\n        Parameters\n        ----------\n        array : NDArray\n            Input array.\n        sample_idx : int\n            Current sample number.\n        \"\"\"\n        self.sample_idx = sample_idx\n        sample_channels = np.array(np.split(array, array.shape[1], axis=1))\n\n        # Initialize the statistics\n        if self.sample_idx == 0:\n            # Compute the mean and standard deviation\n            self.mean, _ = compute_normalization_stats(array)\n            # Initialize the count and m2 with zero-valued arrays of shape (C,)\n            self.count, self.mean, self.m2 = update_iterative_stats(\n                count=np.zeros(array.shape[1]),\n                mean=self.mean,\n                m2=np.zeros(array.shape[1]),\n                new_values=sample_channels,\n            )\n        else:\n            # Update the statistics\n            self.count, self.mean, self.m2 = update_iterative_stats(\n                count=self.count, mean=self.mean, m2=self.m2, new_values=sample_channels\n            )\n\n        self.sample_idx += 1\n\n    def finalize(self) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"Finalize the Welford statistics.\n\n        Returns\n        -------\n        tuple or numpy arrays\n            Final mean and standard deviation.\n        \"\"\"\n        return finalize_iterative_stats(self.count, self.mean, self.m2)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the Welford statistics.</p> <p>Returns:</p> Type Description <code>tuple or numpy arrays</code> <p>Final mean and standard deviation.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def finalize(self) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"Finalize the Welford statistics.\n\n    Returns\n    -------\n    tuple or numpy arrays\n        Final mean and standard deviation.\n    \"\"\"\n    return finalize_iterative_stats(self.count, self.mean, self.m2)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.WelfordStatistics.update","title":"<code>update(array, sample_idx)</code>","text":"<p>Update the Welford statistics.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>Input array.</p> required <code>sample_idx</code> <code>int</code> <p>Current sample number.</p> required Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def update(self, array: NDArray, sample_idx: int) -&gt; None:\n    \"\"\"Update the Welford statistics.\n\n    Parameters\n    ----------\n    array : NDArray\n        Input array.\n    sample_idx : int\n        Current sample number.\n    \"\"\"\n    self.sample_idx = sample_idx\n    sample_channels = np.array(np.split(array, array.shape[1], axis=1))\n\n    # Initialize the statistics\n    if self.sample_idx == 0:\n        # Compute the mean and standard deviation\n        self.mean, _ = compute_normalization_stats(array)\n        # Initialize the count and m2 with zero-valued arrays of shape (C,)\n        self.count, self.mean, self.m2 = update_iterative_stats(\n            count=np.zeros(array.shape[1]),\n            mean=self.mean,\n            m2=np.zeros(array.shape[1]),\n            new_values=sample_channels,\n        )\n    else:\n        # Update the statistics\n        self.count, self.mean, self.m2 = update_iterative_stats(\n            count=self.count, mean=self.mean, m2=self.m2, new_values=sample_channels\n        )\n\n    self.sample_idx += 1\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.compute_normalization_stats","title":"<code>compute_normalization_stats(image)</code>","text":"<p>Compute mean and standard deviation of an array.</p> <p>Expected input shape is (S, C, (Z), Y, X). The mean and standard deviation are computed per channel.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>tuple of (list of floats, list of floats)</code> <p>Lists of mean and standard deviation values per channel.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def compute_normalization_stats(image: NDArray) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Compute mean and standard deviation of an array.\n\n    Expected input shape is (S, C, (Z), Y, X). The mean and standard deviation are\n    computed per channel.\n\n    Parameters\n    ----------\n    image : NDArray\n        Input array.\n\n    Returns\n    -------\n    tuple of (list of floats, list of floats)\n        Lists of mean and standard deviation values per channel.\n    \"\"\"\n    # Define the list of axes excluding the channel axis\n    axes = tuple(np.delete(np.arange(image.ndim), 1))\n    return np.mean(image, axis=axes), np.std(image, axis=axes)\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.finalize_iterative_stats","title":"<code>finalize_iterative_stats(count, mean, m2)</code>","text":"<p>Finalize the mean and variance computation.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>NDArray</code> <p>Number of elements in the array. Shape: (C,).</p> required <code>mean</code> <code>NDArray</code> <p>Mean of the array. Shape: (C,).</p> required <code>m2</code> <code>NDArray</code> <p>Variance of the array. Shape: (C,).</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>Final channel-wise mean and standard deviation.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def finalize_iterative_stats(\n    count: NDArray, mean: NDArray, m2: NDArray\n) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"Finalize the mean and variance computation.\n\n    Parameters\n    ----------\n    count : NDArray\n        Number of elements in the array. Shape: (C,).\n    mean : NDArray\n        Mean of the array. Shape: (C,).\n    m2 : NDArray\n        Variance of the array. Shape: (C,).\n\n    Returns\n    -------\n    tuple[NDArray, NDArray]\n        Final channel-wise mean and standard deviation.\n    \"\"\"\n    std = np.sqrt(m2 / count)\n    if any(c &lt; 2 for c in count):\n        return np.full(mean.shape, np.nan), np.full(std.shape, np.nan)\n    else:\n        return mean, std\n</code></pre>"},{"location":"reference/careamics/dataset/dataset_utils/running_stats/#careamics.dataset.dataset_utils.running_stats.update_iterative_stats","title":"<code>update_iterative_stats(count, mean, m2, new_values)</code>","text":"<p>Update the mean and variance of an array iteratively.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>NDArray</code> <p>Number of elements in the array. Shape: (C,).</p> required <code>mean</code> <code>NDArray</code> <p>Mean of the array. Shape: (C,).</p> required <code>m2</code> <code>NDArray</code> <p>Variance of the array. Shape: (C,).</p> required <code>new_values</code> <code>NDArray</code> <p>New values to add to the mean and variance. Shape: (C, 1, 1, Z, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray, NDArray]</code> <p>Updated count, mean, and variance.</p> Source code in <code>src/careamics/dataset/dataset_utils/running_stats.py</code> <pre><code>def update_iterative_stats(\n    count: NDArray, mean: NDArray, m2: NDArray, new_values: NDArray\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"Update the mean and variance of an array iteratively.\n\n    Parameters\n    ----------\n    count : NDArray\n        Number of elements in the array. Shape: (C,).\n    mean : NDArray\n        Mean of the array. Shape: (C,).\n    m2 : NDArray\n        Variance of the array. Shape: (C,).\n    new_values : NDArray\n        New values to add to the mean and variance. Shape: (C, 1, 1, Z, Y, X).\n\n    Returns\n    -------\n    tuple[NDArray, NDArray, NDArray]\n        Updated count, mean, and variance.\n    \"\"\"\n    num_channels = len(new_values)\n\n    # --- update channel-wise counts ---\n    count += np.ones_like(count) * np.prod(new_values.shape[1:])\n\n    # --- update channel-wise mean ---\n    # compute (new_values - old_mean) -&gt; shape: (C, Z*Y*X)\n    delta = new_values.reshape(num_channels, -1) - mean.reshape(num_channels, 1)\n    mean += np.sum(delta / count.reshape(num_channels, 1), axis=1)\n\n    # --- update channel-wise SoS ---\n    # compute (new_values - new_mean) -&gt; shape: (C, Z*Y*X)\n    delta2 = new_values.reshape(num_channels, -1) - mean.reshape(num_channels, 1)\n    m2 += np.sum(delta * delta2, axis=1)\n\n    return count, mean, m2\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/","title":"patching","text":"<p>Patching functions.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput","title":"<code>PatchedOutput</code>  <code>dataclass</code>","text":"<p>Dataclass to store patches and statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>@dataclass\nclass PatchedOutput:\n    \"\"\"Dataclass to store patches and statistics.\"\"\"\n\n    patches: Union[NDArray]\n    \"\"\"Image patches.\"\"\"\n\n    targets: Union[NDArray, None]\n    \"\"\"Target patches.\"\"\"\n\n    image_stats: Stats\n    \"\"\"Statistics of the image patches.\"\"\"\n\n    target_stats: Stats\n    \"\"\"Statistics of the target patches.\"\"\"\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.image_stats","title":"<code>image_stats</code>  <code>instance-attribute</code>","text":"<p>Statistics of the image patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.patches","title":"<code>patches</code>  <code>instance-attribute</code>","text":"<p>Image patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.target_stats","title":"<code>target_stats</code>  <code>instance-attribute</code>","text":"<p>Statistics of the target patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.PatchedOutput.targets","title":"<code>targets</code>  <code>instance-attribute</code>","text":"<p>Target patches.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats","title":"<code>Stats</code>  <code>dataclass</code>","text":"<p>Dataclass to store statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>@dataclass\nclass Stats:\n    \"\"\"Dataclass to store statistics.\"\"\"\n\n    means: Union[NDArray, tuple, list, None]\n    \"\"\"Mean of the data across channels.\"\"\"\n\n    stds: Union[NDArray, tuple, list, None]\n    \"\"\"Standard deviation of the data across channels.\"\"\"\n\n    def get_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return the means and standard deviations.\n\n        Returns\n        -------\n        tuple of two lists of floats\n            Means and standard deviations.\n        \"\"\"\n        if self.means is None or self.stds is None:\n            return [], []\n\n        return list(self.means), list(self.stds)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.means","title":"<code>means</code>  <code>instance-attribute</code>","text":"<p>Mean of the data across channels.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.stds","title":"<code>stds</code>  <code>instance-attribute</code>","text":"<p>Standard deviation of the data across channels.</p>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.Stats.get_statistics","title":"<code>get_statistics()</code>","text":"<p>Return the means and standard deviations.</p> <p>Returns:</p> Type Description <code>tuple of two lists of floats</code> <p>Means and standard deviations.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def get_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return the means and standard deviations.\n\n    Returns\n    -------\n    tuple of two lists of floats\n        Means and standard deviations.\n    \"\"\"\n    if self.means is None or self.stds is None:\n        return [], []\n\n    return list(self.means), list(self.stds)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_supervised","title":"<code>prepare_patches_supervised(train_files, target_files, axes, patch_size, read_source_func)</code>","text":"<p>Iterate over data source and create an array of patches and corresponding targets.</p> <p>The lists of Paths should be pre-sorted.</p> <p>Parameters:</p> Name Type Description Default <code>train_files</code> <code>list of pathlib.Path</code> <p>List of paths to training data.</p> required <code>target_files</code> <code>list of pathlib.Path</code> <p>List of paths to target data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read the data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of patches.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_supervised(\n    train_files: list[Path],\n    target_files: list[Path],\n    axes: str,\n    patch_size: Union[list[int], tuple[int, ...]],\n    read_source_func: Callable,\n) -&gt; PatchedOutput:\n    \"\"\"\n    Iterate over data source and create an array of patches and corresponding targets.\n\n    The lists of Paths should be pre-sorted.\n\n    Parameters\n    ----------\n    train_files : list of pathlib.Path\n        List of paths to training data.\n    target_files : list of pathlib.Path\n        List of paths to target data.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n    read_source_func : Callable\n        Function to read the data.\n\n    Returns\n    -------\n    np.ndarray\n        Array of patches.\n    \"\"\"\n    means, stds, num_samples = 0, 0, 0\n    all_patches, all_targets = [], []\n    for train_filename, target_filename in zip(train_files, target_files):\n        try:\n            sample: np.ndarray = read_source_func(train_filename, axes)\n            target: np.ndarray = read_source_func(target_filename, axes)\n            means += sample.mean()\n            stds += sample.std()\n            num_samples += 1\n\n            # reshape array\n            sample = reshape_array(sample, axes)\n            target = reshape_array(target, axes)\n\n            # generate patches, return a generator\n            patches, targets = extract_patches_sequential(\n                sample, patch_size=patch_size, target=target\n            )\n\n            # convert generator to list and add to all_patches\n            all_patches.append(patches)\n\n            # ensure targets are not None (type checking)\n            if targets is not None:\n                all_targets.append(targets)\n            else:\n                raise ValueError(f\"No target found for {target_filename}.\")\n\n        except Exception as e:\n            # emit warning and continue\n            logger.error(f\"Failed to read {train_filename} or {target_filename}: {e}\")\n\n    # raise error if no valid samples found\n    if num_samples == 0 or len(all_patches) == 0:\n        raise ValueError(\n            f\"No valid samples found in the input data: {train_files} and \"\n            f\"{target_files}.\"\n        )\n\n    image_means, image_stds = compute_normalization_stats(np.concatenate(all_patches))\n    target_means, target_stds = compute_normalization_stats(np.concatenate(all_targets))\n\n    patch_array: np.ndarray = np.concatenate(all_patches, axis=0)\n    target_array: np.ndarray = np.concatenate(all_targets, axis=0)\n    logger.info(f\"Extracted {patch_array.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patch_array,\n        target_array,\n        Stats(image_means, image_stds),\n        Stats(target_means, target_stds),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_supervised_array","title":"<code>prepare_patches_supervised_array(data, axes, data_target, patch_size)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method expects an array of shape SC(Z)YX, where S and C can be singleton dimensions.</p> <p>Patches returned are of shape SC(Z)YX, where S is now the patches dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>data_target</code> <code>ndarray</code> <p>Target data array.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding the source and target patches, with their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_supervised_array(\n    data: NDArray,\n    axes: str,\n    data_target: NDArray,\n    patch_size: Union[list[int], tuple[int]],\n) -&gt; PatchedOutput:\n    \"\"\"Iterate over data source and create an array of patches.\n\n    This method expects an array of shape SC(Z)YX, where S and C can be singleton\n    dimensions.\n\n    Patches returned are of shape SC(Z)YX, where S is now the patches dimension.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Input data array.\n    axes : str\n        Axes of the data.\n    data_target : numpy.ndarray\n        Target data array.\n    patch_size : list or tuple of int\n        Size of the patches.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding the source and target patches, with their statistics.\n    \"\"\"\n    # reshape array\n    reshaped_sample = reshape_array(data, axes)\n    reshaped_target = reshape_array(data_target, axes)\n\n    # compute statistics\n    image_means, image_stds = compute_normalization_stats(reshaped_sample)\n    target_means, target_stds = compute_normalization_stats(reshaped_target)\n\n    # generate patches, return a generator\n    patches, patch_targets = extract_patches_sequential(\n        reshaped_sample, patch_size=patch_size, target=reshaped_target\n    )\n\n    if patch_targets is None:\n        raise ValueError(\"No target extracted.\")\n\n    logger.info(f\"Extracted {patches.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patches,\n        patch_targets,\n        Stats(image_means, image_stds),\n        Stats(target_means, target_stds),\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_unsupervised","title":"<code>prepare_patches_unsupervised(train_files, axes, patch_size, read_source_func)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method returns the mean and standard deviation of the image.</p> <p>Parameters:</p> Name Type Description Default <code>train_files</code> <code>list of pathlib.Path</code> <p>List of paths to training data.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read the data.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding patches and their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_unsupervised(\n    train_files: list[Path],\n    axes: str,\n    patch_size: Union[list[int], tuple[int]],\n    read_source_func: Callable,\n) -&gt; PatchedOutput:\n    \"\"\"Iterate over data source and create an array of patches.\n\n    This method returns the mean and standard deviation of the image.\n\n    Parameters\n    ----------\n    train_files : list of pathlib.Path\n        List of paths to training data.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n    read_source_func : Callable\n        Function to read the data.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding patches and their statistics.\n    \"\"\"\n    means, stds, num_samples = 0, 0, 0\n    all_patches = []\n    for filename in train_files:\n        try:\n            sample: np.ndarray = read_source_func(filename, axes)\n            means += sample.mean()\n            stds += sample.std()\n            num_samples += 1\n\n            # reshape array\n            sample = reshape_array(sample, axes)\n\n            # generate patches, return a generator\n            patches, _ = extract_patches_sequential(sample, patch_size=patch_size)\n\n            # convert generator to list and add to all_patches\n            all_patches.append(patches)\n        except Exception as e:\n            # emit warning and continue\n            logger.error(f\"Failed to read {filename}: {e}\")\n\n    # raise error if no valid samples found\n    if num_samples == 0:\n        raise ValueError(f\"No valid samples found in the input data: {train_files}.\")\n\n    image_means, image_stds = compute_normalization_stats(np.concatenate(all_patches))\n\n    patch_array: np.ndarray = np.concatenate(all_patches)\n    logger.info(f\"Extracted {patch_array.shape[0]} patches from input array.\")\n\n    return PatchedOutput(\n        patch_array, None, Stats(image_means, image_stds), Stats((), ())\n    )\n</code></pre>"},{"location":"reference/careamics/dataset/patching/patching/#careamics.dataset.patching.patching.prepare_patches_unsupervised_array","title":"<code>prepare_patches_unsupervised_array(data, axes, patch_size)</code>","text":"<p>Iterate over data source and create an array of patches.</p> <p>This method expects an array of shape SC(Z)YX, where S and C can be singleton dimensions.</p> <p>Patches returned are of shape SC(Z)YX, where S is now the patches dimension.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <code>patch_size</code> <code>list or tuple of int</code> <p>Size of the patches.</p> required <p>Returns:</p> Type Description <code>PatchedOutput</code> <p>Dataclass holding the patches and their statistics.</p> Source code in <code>src/careamics/dataset/patching/patching.py</code> <pre><code>def prepare_patches_unsupervised_array(\n    data: NDArray,\n    axes: str,\n    patch_size: Union[list[int], tuple[int]],\n) -&gt; PatchedOutput:\n    \"\"\"\n    Iterate over data source and create an array of patches.\n\n    This method expects an array of shape SC(Z)YX, where S and C can be singleton\n    dimensions.\n\n    Patches returned are of shape SC(Z)YX, where S is now the patches dimension.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Input data array.\n    axes : str\n        Axes of the data.\n    patch_size : list or tuple of int\n        Size of the patches.\n\n    Returns\n    -------\n    PatchedOutput\n        Dataclass holding the patches and their statistics.\n    \"\"\"\n    # reshape array\n    reshaped_sample = reshape_array(data, axes)\n\n    # calculate mean and std\n    means, stds = compute_normalization_stats(reshaped_sample)\n\n    # generate patches, return a generator\n    patches, _ = extract_patches_sequential(reshaped_sample, patch_size=patch_size)\n\n    return PatchedOutput(patches, None, Stats(means, stds), Stats((), ()))\n</code></pre>"},{"location":"reference/careamics/dataset/patching/random_patching/","title":"random_patching","text":"<p>Random patching utilities.</p>"},{"location":"reference/careamics/dataset/patching/random_patching/#careamics.dataset.patching.random_patching.extract_patches_random","title":"<code>extract_patches_random(arr, patch_size, target=None, seed=None)</code>","text":"<p>Generate patches from an array in a random manner.</p> <p>The method calculates how many patches the image can be divided into and then extracts an equal number of random patches.</p> <p>It returns a generator that yields the following:</p> <ul> <li>patch: np.ndarray, dimension C(Z)YX.</li> <li>target_patch: np.ndarray, dimension C(Z)YX, if the target is present, None     otherwise.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>tuple of int</code> <p>Patch sizes in each dimension.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target array, by default None.</p> <code>None</code> <code>seed</code> <code>int or None</code> <p>Random seed.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>Generator of patches.</p> Source code in <code>src/careamics/dataset/patching/random_patching.py</code> <pre><code>def extract_patches_random(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    target: Optional[np.ndarray] = None,\n    seed: Optional[int] = None,\n) -&gt; Generator[tuple[np.ndarray, Optional[np.ndarray]], None, None]:\n    \"\"\"\n    Generate patches from an array in a random manner.\n\n    The method calculates how many patches the image can be divided into and then\n    extracts an equal number of random patches.\n\n    It returns a generator that yields the following:\n\n    - patch: np.ndarray, dimension C(Z)YX.\n    - target_patch: np.ndarray, dimension C(Z)YX, if the target is present, None\n        otherwise.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : tuple of int\n        Patch sizes in each dimension.\n    target : Optional[np.ndarray], optional\n        Target array, by default None.\n    seed : int or None, default=None\n        Random seed.\n\n    Yields\n    ------\n    Generator[np.ndarray, None, None]\n        Generator of patches.\n    \"\"\"\n    rng = np.random.default_rng(seed=seed)\n\n    is_3d_patch = len(patch_size) == 3\n\n    # patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    # Update patch size to encompass S and C dimensions\n    patch_size = [1, arr.shape[1], *patch_size]\n\n    # iterate over the number of samples (S or T)\n    for sample_idx in range(arr.shape[0]):\n        # get sample array\n        sample: np.ndarray = arr[sample_idx, ...]\n\n        # same for target\n        if target is not None:\n            target_sample: np.ndarray = target[sample_idx, ...]\n\n        # calculate the number of patches\n        n_patches = np.ceil(np.prod(sample.shape) / np.prod(patch_size)).astype(int)\n\n        # iterate over the number of patches\n        for _ in range(n_patches):\n            # get crop coordinates\n            crop_coords = [\n                rng.integers(0, sample.shape[i] - patch_size[1:][i], endpoint=True)\n                for i in range(len(patch_size[1:]))\n            ]\n\n            # extract patch\n            patch = (\n                sample[\n                    (\n                        ...,  # type: ignore\n                        *[  # type: ignore\n                            slice(c, c + patch_size[1:][i])\n                            for i, c in enumerate(crop_coords)\n                        ],\n                    )\n                ]\n                .copy()\n                .astype(np.float32)\n            )\n\n            # same for target\n            if target is not None:\n                target_patch = (\n                    target_sample[\n                        (\n                            ...,  # type: ignore\n                            *[  # type: ignore\n                                slice(c, c + patch_size[1:][i])\n                                for i, c in enumerate(crop_coords)\n                            ],\n                        )\n                    ]\n                    .copy()\n                    .astype(np.float32)\n                )\n                # return patch and target patch\n                yield patch, target_patch\n            else:\n                # return patch\n                yield patch, None\n</code></pre>"},{"location":"reference/careamics/dataset/patching/random_patching/#careamics.dataset.patching.random_patching.extract_patches_random_from_chunks","title":"<code>extract_patches_random_from_chunks(arr, patch_size, chunk_size, chunk_limit=None, seed=None)</code>","text":"<p>Generate patches from an array in a random manner.</p> <p>The method calculates how many patches the image can be divided into and then extracts an equal number of random patches.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Patch sizes in each dimension.</p> required <code>chunk_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Chunk sizes to load from the.</p> required <code>chunk_limit</code> <code>Optional[int]</code> <p>Number of chunks to load, by default None.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>Generator of patches.</p> Source code in <code>src/careamics/dataset/patching/random_patching.py</code> <pre><code>def extract_patches_random_from_chunks(\n    arr: zarr.Array,\n    patch_size: Union[list[int], tuple[int, ...]],\n    chunk_size: Union[list[int], tuple[int, ...]],\n    chunk_limit: Optional[int] = None,\n    seed: Optional[int] = None,\n) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"\n    Generate patches from an array in a random manner.\n\n    The method calculates how many patches the image can be divided into and then\n    extracts an equal number of random patches.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : Union[list[int], tuple[int, ...]]\n        Patch sizes in each dimension.\n    chunk_size : Union[list[int], tuple[int, ...]]\n        Chunk sizes to load from the.\n    chunk_limit : Optional[int], optional\n        Number of chunks to load, by default None.\n    seed : Optional[int], optional\n        Random seed, by default None.\n\n    Yields\n    ------\n    Generator[np.ndarray, None, None]\n        Generator of patches.\n    \"\"\"\n    is_3d_patch = len(patch_size) == 3\n\n    # Patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    rng = np.random.default_rng(seed=seed)\n    num_chunks = chunk_limit if chunk_limit else np.prod(arr._cdata_shape)\n\n    # Iterate over num chunks in the array\n    for _ in range(num_chunks):\n        chunk_crop_coords = [\n            rng.integers(0, max(0, arr.shape[i] - chunk_size[i]), endpoint=True)\n            for i in range(len(chunk_size))\n        ]\n        chunk = arr[\n            (\n                ...,\n                *[slice(c, c + chunk_size[i]) for i, c in enumerate(chunk_crop_coords)],\n            )\n        ].squeeze()\n\n        # Add a singleton dimension if the chunk does not have a sample dimension\n        if len(chunk.shape) == len(patch_size):\n            chunk = np.expand_dims(chunk, axis=0)\n\n        # Iterate over num samples (S)\n        for sample_idx in range(chunk.shape[0]):\n            spatial_chunk = chunk[sample_idx]\n            assert len(spatial_chunk.shape) == len(\n                patch_size\n            ), \"Requested chunk shape is not equal to patch size\"\n\n            n_patches = np.ceil(\n                np.prod(spatial_chunk.shape) / np.prod(patch_size)\n            ).astype(int)\n\n            # Iterate over the number of patches\n            for _ in range(n_patches):\n                patch_crop_coords = [\n                    rng.integers(\n                        0, spatial_chunk.shape[i] - patch_size[i], endpoint=True\n                    )\n                    for i in range(len(patch_size))\n                ]\n                patch = (\n                    spatial_chunk[\n                        (\n                            ...,\n                            *[\n                                slice(c, c + patch_size[i])\n                                for i, c in enumerate(patch_crop_coords)\n                            ],\n                        )\n                    ]\n                    .copy()\n                    .astype(np.float32)\n                )\n                yield patch\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/","title":"sequential_patching","text":"<p>Sequential patching functions.</p>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching._compute_number_of_patches","title":"<code>_compute_number_of_patches(arr_shape, patch_sizes)</code>","text":"<p>Compute the number of patches that fit in each dimension.</p> <p>Parameters:</p> Name Type Description Default <code>arr_shape</code> <code>tuple[int, ...]</code> <p>Shape of the input array.</p> required <code>patch_sizes</code> <code>Union[list[int], tuple[int, ...]</code> <p>Shape of the patches.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Number of patches in each dimension.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def _compute_number_of_patches(\n    arr_shape: tuple[int, ...], patch_sizes: Union[list[int], tuple[int, ...]]\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Compute the number of patches that fit in each dimension.\n\n    Parameters\n    ----------\n    arr_shape : tuple[int, ...]\n        Shape of the input array.\n    patch_sizes : Union[list[int], tuple[int, ...]\n        Shape of the patches.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Number of patches in each dimension.\n    \"\"\"\n    if len(arr_shape) != len(patch_sizes):\n        raise ValueError(\n            f\"Array shape {arr_shape} and patch size {patch_sizes} should have the \"\n            f\"same dimension, including singleton dimension for S and equal dimension \"\n            f\"for C.\"\n        )\n\n    try:\n        n_patches = [\n            np.ceil(arr_shape[i] / patch_sizes[i]).astype(int)\n            for i in range(len(patch_sizes))\n        ]\n    except IndexError as e:\n        raise ValueError(\n            f\"Patch size {patch_sizes} is not compatible with array shape {arr_shape}\"\n        ) from e\n\n    return tuple(n_patches)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching._compute_overlap","title":"<code>_compute_overlap(arr_shape, patch_sizes)</code>","text":"<p>Compute the overlap between patches in each dimension.</p> <p>If the array dimensions are divisible by the patch sizes, then the overlap is 0. Otherwise, it is the result of the division rounded to the upper value.</p> <p>Parameters:</p> Name Type Description Default <code>arr_shape</code> <code>tuple[int, ...]</code> <p>Input array shape.</p> required <code>patch_sizes</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Size of the patches.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Overlap between patches in each dimension.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def _compute_overlap(\n    arr_shape: tuple[int, ...], patch_sizes: Union[list[int], tuple[int, ...]]\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Compute the overlap between patches in each dimension.\n\n    If the array dimensions are divisible by the patch sizes, then the overlap is\n    0. Otherwise, it is the result of the division rounded to the upper value.\n\n    Parameters\n    ----------\n    arr_shape : tuple[int, ...]\n        Input array shape.\n    patch_sizes : Union[list[int], tuple[int, ...]]\n        Size of the patches.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Overlap between patches in each dimension.\n    \"\"\"\n    n_patches = _compute_number_of_patches(arr_shape, patch_sizes)\n\n    overlap = [\n        np.ceil(\n            np.clip(n_patches[i] * patch_sizes[i] - arr_shape[i], 0, None)\n            / max(1, (n_patches[i] - 1))\n        ).astype(int)\n        for i in range(len(patch_sizes))\n    ]\n    return tuple(overlap)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching._compute_patch_steps","title":"<code>_compute_patch_steps(patch_sizes, overlaps)</code>","text":"<p>Compute steps between patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch_sizes</code> <code>tuple[int]</code> <p>Size of the patches.</p> required <code>overlaps</code> <code>tuple[int]</code> <p>Overlap between patches.</p> required <p>Returns:</p> Type Description <code>tuple[int]</code> <p>Steps between patches.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def _compute_patch_steps(\n    patch_sizes: Union[list[int], tuple[int, ...]], overlaps: tuple[int, ...]\n) -&gt; tuple[int, ...]:\n    \"\"\"\n    Compute steps between patches.\n\n    Parameters\n    ----------\n    patch_sizes : tuple[int]\n        Size of the patches.\n    overlaps : tuple[int]\n        Overlap between patches.\n\n    Returns\n    -------\n    tuple[int]\n        Steps between patches.\n    \"\"\"\n    steps = [\n        min(patch_sizes[i] - overlaps[i], patch_sizes[i])\n        for i in range(len(patch_sizes))\n    ]\n    return tuple(steps)\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching._compute_patch_views","title":"<code>_compute_patch_views(arr, window_shape, step, output_shape, target=None)</code>","text":"<p>Compute views of an array corresponding to patches.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array from which the views are extracted.</p> required <code>window_shape</code> <code>tuple[int]</code> <p>Shape of the views.</p> required <code>step</code> <code>tuple[int]</code> <p>Steps between views.</p> required <code>output_shape</code> <code>tuple[int]</code> <p>Shape of the output array.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target array, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array with views dimension.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def _compute_patch_views(\n    arr: np.ndarray,\n    window_shape: list[int],\n    step: tuple[int, ...],\n    output_shape: list[int],\n    target: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute views of an array corresponding to patches.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Array from which the views are extracted.\n    window_shape : tuple[int]\n        Shape of the views.\n    step : tuple[int]\n        Steps between views.\n    output_shape : tuple[int]\n        Shape of the output array.\n    target : Optional[np.ndarray], optional\n        Target array, by default None.\n\n    Returns\n    -------\n    np.ndarray\n        Array with views dimension.\n    \"\"\"\n    rng = np.random.default_rng()\n\n    if target is not None:\n        arr = np.stack([arr, target], axis=0)\n        window_shape = [arr.shape[0], *window_shape]\n        step = (arr.shape[0], *step)\n        output_shape = [-1, arr.shape[0], arr.shape[2], *output_shape[2:]]\n\n    patches = view_as_windows(arr, window_shape=window_shape, step=step).reshape(\n        *output_shape\n    )\n    rng.shuffle(patches, axis=0)\n    return patches\n</code></pre>"},{"location":"reference/careamics/dataset/patching/sequential_patching/#careamics.dataset.patching.sequential_patching.extract_patches_sequential","title":"<code>extract_patches_sequential(arr, patch_size, target=None)</code>","text":"<p>Generate patches from an array in a sequential manner.</p> <p>Array dimensions should be SC(Z)YX, where S and C can be singleton dimensions. The patches are generated sequentially and cover the whole array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input image array.</p> required <code>patch_size</code> <code>tuple[int]</code> <p>Patch sizes in each dimension.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target array, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>Patches.</p> Source code in <code>src/careamics/dataset/patching/sequential_patching.py</code> <pre><code>def extract_patches_sequential(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    target: Optional[np.ndarray] = None,\n) -&gt; tuple[np.ndarray, Optional[np.ndarray]]:\n    \"\"\"\n    Generate patches from an array in a sequential manner.\n\n    Array dimensions should be SC(Z)YX, where S and C can be singleton dimensions. The\n    patches are generated sequentially and cover the whole array.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input image array.\n    patch_size : tuple[int]\n        Patch sizes in each dimension.\n    target : Optional[np.ndarray], optional\n        Target array, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        Patches.\n    \"\"\"\n    is_3d_patch = len(patch_size) == 3\n\n    # Patches sanity check\n    validate_patch_dimensions(arr, patch_size, is_3d_patch)\n\n    # Update patch size to encompass S and C dimensions\n    patch_size = [1, arr.shape[1], *patch_size]\n\n    # Compute overlap\n    overlaps = _compute_overlap(arr_shape=arr.shape, patch_sizes=patch_size)\n\n    # Create view window and overlaps\n    window_steps = _compute_patch_steps(patch_sizes=patch_size, overlaps=overlaps)\n\n    output_shape = [\n        -1,\n    ] + patch_size[1:]\n\n    # Generate a view of the input array containing pre-calculated number of patches\n    # in each dimension with overlap.\n    # Resulting array is resized to (n_patches, C, Z, Y, X) or (n_patches, C, Y, X)\n    patches = _compute_patch_views(\n        arr,\n        window_shape=patch_size,\n        step=window_steps,\n        output_shape=output_shape,\n        target=target,\n    )\n\n    if target is not None:\n        # target was concatenated to patches in _compute_reshaped_view\n        return (\n            patches[:, 0, ...],\n            patches[:, 1, ...],\n        )  # TODO  in _compute_reshaped_view?\n    else:\n        return patches, None\n</code></pre>"},{"location":"reference/careamics/dataset/patching/validate_patch_dimension/","title":"validate_patch_dimension","text":"<p>Patch validation functions.</p>"},{"location":"reference/careamics/dataset/patching/validate_patch_dimension/#careamics.dataset.patching.validate_patch_dimension.validate_patch_dimensions","title":"<code>validate_patch_dimensions(arr, patch_size, is_3d_patch)</code>","text":"<p>Check patch size and array compatibility.</p> <p>This method validates the patch sizes with respect to the array dimensions:</p> <ul> <li>Patch must have two dimensions fewer than the array (S and C).</li> <li>Patch sizes are smaller than the corresponding array dimensions.</li> </ul> <p>If one of these conditions is not met, a ValueError is raised.</p> <p>This method should be called after inputs have been resized.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Input array.</p> required <code>patch_size</code> <code>Union[list[int], tuple[int, ...]]</code> <p>Size of the patches along each dimension of the array, except the first.</p> required <code>is_3d_patch</code> <code>bool</code> <p>Whether the patch is 3D or not.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the patch size is not consistent with the array shape (one more array dimension).</p> <code>ValueError</code> <p>If the patch size in Z is larger than the array dimension.</p> <code>ValueError</code> <p>If either of the patch sizes in X or Y is larger than the corresponding array dimension.</p> Source code in <code>src/careamics/dataset/patching/validate_patch_dimension.py</code> <pre><code>def validate_patch_dimensions(\n    arr: np.ndarray,\n    patch_size: Union[list[int], tuple[int, ...]],\n    is_3d_patch: bool,\n) -&gt; None:\n    \"\"\"\n    Check patch size and array compatibility.\n\n    This method validates the patch sizes with respect to the array dimensions:\n\n    - Patch must have two dimensions fewer than the array (S and C).\n    - Patch sizes are smaller than the corresponding array dimensions.\n\n    If one of these conditions is not met, a ValueError is raised.\n\n    This method should be called after inputs have been resized.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Input array.\n    patch_size : Union[list[int], tuple[int, ...]]\n        Size of the patches along each dimension of the array, except the first.\n    is_3d_patch : bool\n        Whether the patch is 3D or not.\n\n    Raises\n    ------\n    ValueError\n        If the patch size is not consistent with the array shape (one more array\n        dimension).\n    ValueError\n        If the patch size in Z is larger than the array dimension.\n    ValueError\n        If either of the patch sizes in X or Y is larger than the corresponding array\n        dimension.\n    \"\"\"\n    if len(patch_size) != len(arr.shape[2:]):\n        raise ValueError(\n            f\"There must be a patch size for each spatial dimensions \"\n            f\"(got {patch_size} patches for dims {arr.shape}). Check the axes order.\"\n        )\n\n    # Sanity checks on patch sizes versus array dimension\n    if is_3d_patch and patch_size[0] &gt; arr.shape[-3]:\n        raise ValueError(\n            f\"Z patch size is inconsistent with image shape \"\n            f\"(got {patch_size[0]} patches for dim {arr.shape[1]}). Check the axes \"\n            f\"order.\"\n        )\n\n    if patch_size[-2] &gt; arr.shape[-2] or patch_size[-1] &gt; arr.shape[-1]:\n        raise ValueError(\n            f\"At least one of YX patch dimensions is larger than the corresponding \"\n            f\"image dimension (got {patch_size} patches for dims {arr.shape[-2:]}). \"\n            f\"Check the axes order.\"\n        )\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/collate_tiles/","title":"collate_tiles","text":"<p>Collate function for tiling.</p>"},{"location":"reference/careamics/dataset/tiling/collate_tiles/#careamics.dataset.tiling.collate_tiles.collate_tiles","title":"<code>collate_tiles(batch)</code>","text":"<p>Collate tiles received from CAREamics prediction dataloader.</p> <p>CAREamics prediction dataloader returns tuples of arrays and TileInformation. In case of non-tiled data, this function will return the arrays. In case of tiled data, it will return the arrays, the last tile flag, the overlap crop coordinates and the stitch coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[tuple[ndarray, TileInformation], ...]</code> <p>Batch of tiles.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Collated batch.</p> Source code in <code>src/careamics/dataset/tiling/collate_tiles.py</code> <pre><code>def collate_tiles(batch: list[tuple[np.ndarray, TileInformation]]) -&gt; Any:\n    \"\"\"\n    Collate tiles received from CAREamics prediction dataloader.\n\n    CAREamics prediction dataloader returns tuples of arrays and TileInformation. In\n    case of non-tiled data, this function will return the arrays. In case of tiled data,\n    it will return the arrays, the last tile flag, the overlap crop coordinates and the\n    stitch coordinates.\n\n    Parameters\n    ----------\n    batch : list[tuple[np.ndarray, TileInformation], ...]\n        Batch of tiles.\n\n    Returns\n    -------\n    Any\n        Collated batch.\n    \"\"\"\n    new_batch = [tile for tile, _ in batch]\n    tiles_batch = [tile_info for _, tile_info in batch]\n\n    return default_collate(new_batch), tiles_batch\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/","title":"lvae_tiled_patching","text":"<p>Functions to reimplement the tiling in the Disentangle repository.</p>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_padding","title":"<code>compute_padding(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate padding to ensure stitched data comes from the center of a tile.</p> <p>Padding is added to an array with shape <code>data_shape</code> so that when tiles are stitched together, the data used always comes from the center of a tile, even for tiles at the boundaries of the array.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of (int, int)</code> <p>A tuple specifying the padding to add in each dimension, each element is a two element tuple specifying the padding to add before and after the data. This can be used as the <code>pad_width</code> argument to <code>numpy.pad</code>.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_padding(\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n) -&gt; tuple[tuple[int, int], ...]:\n    \"\"\"\n    Calculate padding to ensure stitched data comes from the center of a tile.\n\n    Padding is added to an array with shape `data_shape` so that when tiles are\n    stitched together, the data used always comes from the center of a tile, even for\n    tiles at the boundaries of the array.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n    Returns\n    -------\n    tuple of (int, int)\n        A tuple specifying the padding to add in each dimension, each element is a two\n        element tuple specifying the padding to add before and after the data. This\n        can be used as the `pad_width` argument to `numpy.pad`.\n    \"\"\"\n    tile_grid_shape = np.array(compute_tile_grid_shape(data_shape, tile_size, overlaps))\n    covered_shape = (tile_size - overlaps) * tile_grid_shape + overlaps\n\n    pad_before = overlaps // 2\n    pad_after = covered_shape - data_shape[-len(tile_size) :] - pad_before\n\n    return tuple((before, after) for before, after in zip(pad_before, pad_after))\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_grid_shape","title":"<code>compute_tile_grid_shape(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate the number of tiles in each dimension.</p> <p>This can be thought of as a grid of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of int</code> <p>The number of tiles in each direction, ((Z, Y, X)).</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_grid_shape(\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n) -&gt; tuple[int, ...]:\n    \"\"\"Calculate the number of tiles in each dimension.\n\n    This can be thought of as a grid of tiles.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n    Returns\n    -------\n    tuple of int\n        The number of tiles in each direction, ((Z, Y, X)).\n    \"\"\"\n    shape = [0 for _ in range(len(tile_size))]\n    # assume spatial dimension are the last dimensions so iterate backwards\n    for i in range(-1, -len(tile_size) - 1, -1):\n        shape[i] = n_tiles_1d(data_shape[i], tile_size[i], overlaps[i])\n    return tuple(shape)\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_info","title":"<code>compute_tile_info(tile_grid_indices, data_shape, tile_size, overlaps, sample_id=0)</code>","text":"<p>Compute the tile information for a tile with the coordinates <code>tile_grid_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tile_grid_indices</code> <code>1D np.array of int</code> <p>The coordinates of the tile within the tile grid, ((Z), Y, X), i.e. for 2D tiling the coordinates for the second tile in the first row of tiles would be (0, 1).</p> required <code>data_shape</code> <code>1D np.array of int</code> <p>The shape of the data, should be (C, (Z), Y, X) where Z is optional.</p> required <code>tile_size</code> <code>1D np.array of int</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>1D np.array of int</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <code>sample_id</code> <code>int</code> <p>An ID to identify which sample a tile belongs to.</p> <code>0</code> <p>Returns:</p> Type Description <code>TileInformation</code> <p>Information that describes how to crop and stitch a tile to create a full image.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_info(\n    tile_grid_indices: NDArray[np.int_],\n    data_shape: NDArray[np.int_],\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n    sample_id: int = 0,\n) -&gt; TileInformation:\n    \"\"\"\n    Compute the tile information for a tile with the coordinates `tile_grid_indices`.\n\n    Parameters\n    ----------\n    tile_grid_indices : 1D np.array of int\n        The coordinates of the tile within the tile grid, ((Z), Y, X), i.e. for 2D\n        tiling the coordinates for the second tile in the first row of tiles would be\n        (0, 1).\n    data_shape : 1D np.array of int\n        The shape of the data, should be (C, (Z), Y, X) where Z is optional.\n    tile_size : 1D np.array of int\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : 1D np.array of int\n        Overlap values in each dimension, of length 2 or 3.\n    sample_id : int, default=0\n        An ID to identify which sample a tile belongs to.\n\n    Returns\n    -------\n    TileInformation\n        Information that describes how to crop and stitch a tile to create a full image.\n    \"\"\"\n    spatial_dims_shape = data_shape[-len(tile_size) :]\n\n    # The extent of the tile which will make up part of the stitched image.\n    stitch_size = tile_size - overlaps\n    stitch_coords_start = tile_grid_indices * stitch_size\n    stitch_coords_end = stitch_coords_start + stitch_size\n\n    tile_coords_start = stitch_coords_start - overlaps // 2\n\n    # --- replace out of bounds indices\n    out_of_lower_bound = stitch_coords_start &lt; 0\n    out_of_upper_bound = stitch_coords_end &gt; spatial_dims_shape\n    stitch_coords_start[out_of_lower_bound] = 0\n    stitch_coords_end[out_of_upper_bound] = spatial_dims_shape[out_of_upper_bound]\n\n    # --- calculate overlap crop coords\n    overlap_crop_coords_start = stitch_coords_start - tile_coords_start\n    overlap_crop_coords_end = overlap_crop_coords_start + (\n        stitch_coords_end - stitch_coords_start\n    )\n\n    # --- combine start and end\n    stitch_coords = tuple(\n        (start, end) for start, end in zip(stitch_coords_start, stitch_coords_end)\n    )\n    overlap_crop_coords = tuple(\n        (start, end)\n        for start, end in zip(overlap_crop_coords_start, overlap_crop_coords_end)\n    )\n\n    # --- Check if last tile\n    tile_grid_shape = np.array(compute_tile_grid_shape(data_shape, tile_size, overlaps))\n    last_tile = (tile_grid_indices == (tile_grid_shape - 1)).all()\n\n    tile_info = TileInformation(\n        array_shape=data_shape,\n        last_tile=last_tile,\n        overlap_crop_coords=overlap_crop_coords,\n        stitch_coords=stitch_coords,\n        sample_id=sample_id,\n    )\n    return tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.compute_tile_info_legacy","title":"<code>compute_tile_info_legacy(grid_index_manager, index)</code>","text":"<p>Compute the tile information for a tile at a given dataset index.</p> <p>Parameters:</p> Name Type Description Default <code>grid_index_manager</code> <code>GridIndexManager</code> <p>The grid index manager that keeps track of tile locations.</p> required <code>index</code> <code>int</code> <p>The dataset index.</p> required <p>Returns:</p> Type Description <code>TileInformation</code> <p>Information that describes how to crop and stitch a tile to create a full image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>grid_index_manager.data_shape</code> does not have 4 or 5 dimensions.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def compute_tile_info_legacy(\n    grid_index_manager: GridIndexManager, index: int\n) -&gt; TileInformation:\n    \"\"\"\n    Compute the tile information for a tile at a given dataset index.\n\n    Parameters\n    ----------\n    grid_index_manager : GridIndexManager\n        The grid index manager that keeps track of tile locations.\n    index : int\n        The dataset index.\n\n    Returns\n    -------\n    TileInformation\n        Information that describes how to crop and stitch a tile to create a full image.\n\n    Raises\n    ------\n    ValueError\n        If `grid_index_manager.data_shape` does not have 4 or 5 dimensions.\n    \"\"\"\n    data_shape = np.array(grid_index_manager.data_shape)\n    if len(data_shape) == 5:\n        n_spatial_dims = 3\n    elif len(data_shape) == 4:\n        n_spatial_dims = 2\n    else:\n        raise ValueError(\"Data shape must have 4 or 5 dimensions, equating to SC(Z)YX.\")\n\n    stitch_coords_start = np.array(\n        grid_index_manager.get_location_from_dataset_idx(index)\n    )\n    stitch_coords_end = stitch_coords_start + np.array(grid_index_manager.grid_shape)\n\n    tile_coords_start = stitch_coords_start - grid_index_manager.patch_offset()\n\n    # --- replace out of bounds indices\n    out_of_lower_bound = stitch_coords_start &lt; 0\n    out_of_upper_bound = stitch_coords_end &gt; data_shape\n    stitch_coords_start[out_of_lower_bound] = 0\n    stitch_coords_end[out_of_upper_bound] = data_shape[out_of_upper_bound]\n\n    # TODO: TilingMode not in current version\n    # if grid_index_manager.tiling_mode == TilingMode.ShiftBoundary:\n    #     for dim in range(len(stitch_coords_start)):\n    #         if tile_coords_start[dim] == 0:\n    #             stitch_coords_start[dim] = 0\n    #         if tile_coords_end[dim] == grid_index_manager.data_shape[dim]:\n    #             tile_coords_end [dim]= grid_index_manager.data_shape[dim]\n\n    # --- calculate overlap crop coords\n    overlap_crop_coords_start = stitch_coords_start - tile_coords_start\n    overlap_crop_coords_end = overlap_crop_coords_start + (\n        stitch_coords_end - stitch_coords_start\n    )\n\n    last_tile = index == grid_index_manager.total_grid_count() - 1\n\n    # --- combine start and end\n    stitch_coords = tuple(\n        (start, end) for start, end in zip(stitch_coords_start, stitch_coords_end)\n    )\n    overlap_crop_coords = tuple(\n        (start, end)\n        for start, end in zip(overlap_crop_coords_start, overlap_crop_coords_end)\n    )\n\n    tile_info = TileInformation(\n        array_shape=data_shape[1:],  # remove S dim\n        last_tile=last_tile,\n        overlap_crop_coords=overlap_crop_coords[-n_spatial_dims:],\n        stitch_coords=stitch_coords[-n_spatial_dims:],\n        sample_id=0,\n    )\n    return tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.extract_tiles","title":"<code>extract_tiles(arr, tile_size, overlaps, padding_kwargs=None)</code>","text":"<p>Generate tiles from the input array with specified overlap.</p> <p>The tiles cover the whole array; which will be additionally padded, to ensure that the section of the tile that contributes to the final image comes from the center of the tile.</p> <p>The method returns a generator that yields tuples of array and tile information, the latter includes whether the tile is the last one, the coordinates of the overlap crop, and the coordinates of the stitched tile.</p> <p>Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX, where C can be a singleton.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array of shape (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.ndarray of tuple</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>1D numpy.ndarray of tuple</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <code>padding_kwargs</code> <code>dict</code> <p>The arguments of <code>np.pad</code> after the first two arguments, <code>array</code> and <code>pad_width</code>. If not specified the default will be <code>{\"mode\": \"reflect\"}</code>. See <code>numpy.pad</code> docs: https://numpy.org/doc/stable/reference/generated/numpy.pad.html.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[Tuple[ndarray, TileInformation], None, None]</code> <p>Tile generator, yields the tile and additional information.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def extract_tiles(\n    arr: NDArray,\n    tile_size: NDArray[np.int_],\n    overlaps: NDArray[np.int_],\n    padding_kwargs: Optional[dict[str, Any]] = None,\n) -&gt; Generator[tuple[NDArray, TileInformation], None, None]:\n    \"\"\"Generate tiles from the input array with specified overlap.\n\n    The tiles cover the whole array; which will be additionally padded, to ensure that\n    the section of the tile that contributes to the final image comes from the center\n    of the tile.\n\n    The method returns a generator that yields tuples of array and tile information,\n    the latter includes whether the tile is the last one, the coordinates of the\n    overlap crop, and the coordinates of the stitched tile.\n\n    Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX,\n    where C can be a singleton.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Array of shape (S, C, (Z), Y, X).\n    tile_size : 1D numpy.ndarray of tuple\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : 1D numpy.ndarray of tuple\n        Overlap values in each dimension, of length 2 or 3.\n    padding_kwargs : dict, optional\n        The arguments of `np.pad` after the first two arguments, `array` and\n        `pad_width`. If not specified the default will be `{\"mode\": \"reflect\"}`. See\n        `numpy.pad` docs:\n        https://numpy.org/doc/stable/reference/generated/numpy.pad.html.\n\n    Yields\n    ------\n    Generator[Tuple[np.ndarray, TileInformation], None, None]\n        Tile generator, yields the tile and additional information.\n    \"\"\"\n    if padding_kwargs is None:\n        padding_kwargs = {\"mode\": \"reflect\"}\n\n    # Iterate over num samples (S)\n    for sample_idx in range(arr.shape[0]):\n        sample = arr[sample_idx, ...]\n        data_shape = np.array(sample.shape)\n\n        # add padding to ensure evenly spaced &amp; overlapping tiles.\n        spatial_padding = compute_padding(data_shape, tile_size, overlaps)\n        padding = ((0, 0), *spatial_padding)\n        sample = np.pad(sample, padding, **padding_kwargs)\n\n        # The number of tiles in each dimension, should be of length 2 or 3\n        tile_grid_shape = compute_tile_grid_shape(data_shape, tile_size, overlaps)\n        # itertools.product is equivalent of nested loops\n\n        stitch_size = tile_size - overlaps\n        for tile_grid_indices in itertools.product(\n            *[range(n) for n in tile_grid_shape]\n        ):\n\n            # calculate crop coordinates\n            crop_coords_start = np.array(tile_grid_indices) * stitch_size\n            crop_slices: tuple[Union[builtins.ellipsis, slice], ...] = (\n                ...,\n                *[\n                    slice(coords, coords + extent)\n                    for coords, extent in zip(crop_coords_start, tile_size)\n                ],\n            )\n            tile = sample[crop_slices]\n\n            tile_info = compute_tile_info(\n                np.array(tile_grid_indices),\n                np.array(data_shape),\n                np.array(tile_size),\n                np.array(overlaps),\n                sample_idx,\n            )\n            # TODO: kinda weird this is a generator,\n            #   -&gt; doesn't really save memory ? Don't think there are any places the\n            #    tiles are not exracted all at the same time.\n            #   Although I guess it would make sense for a zarr tile extractor.\n            yield tile, tile_info\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.n_tiles_1d","title":"<code>n_tiles_1d(axis_size, tile_size, overlap)</code>","text":"<p>Calculate the number of tiles in a specific dimension.</p> <p>Parameters:</p> Name Type Description Default <code>axis_size</code> <code>int</code> <p>The length of the data for in a specific dimension.</p> required <code>tile_size</code> <code>int</code> <p>The length of the tiles in a specific dimension.</p> required <code>overlap</code> <code>int</code> <p>The tile overlap in a specific dimension.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of tiles that fit in one dimension given the arguments.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def n_tiles_1d(axis_size: int, tile_size: int, overlap: int) -&gt; int:\n    \"\"\"Calculate the number of tiles in a specific dimension.\n\n    Parameters\n    ----------\n    axis_size : int\n        The length of the data for in a specific dimension.\n    tile_size : int\n        The length of the tiles in a specific dimension.\n    overlap : int\n        The tile overlap in a specific dimension.\n\n    Returns\n    -------\n    int\n        The number of tiles that fit in one dimension given the arguments.\n    \"\"\"\n    return int(np.ceil(axis_size / (tile_size - overlap)))\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/lvae_tiled_patching/#careamics.dataset.tiling.lvae_tiled_patching.total_n_tiles","title":"<code>total_n_tiles(data_shape, tile_size, overlaps)</code>","text":"<p>Calculate The total number of tiles over all dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>data_shape</code> <code>1D numpy.array of int</code> <p>The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>1D numpy.array of int</code> <p>The tile size in each dimension, ((Z), Y, X).</p> required <code>overlaps</code> <code>1D numpy.array of int</code> <p>The tile overlap in each dimension, ((Z), Y, X).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The total number of tiles over all dimensions.</p> Source code in <code>src/careamics/dataset/tiling/lvae_tiled_patching.py</code> <pre><code>def total_n_tiles(\n    data_shape: tuple[int, ...], tile_size: tuple[int, ...], overlaps: tuple[int, ...]\n) -&gt; int:\n    \"\"\"Calculate The total number of tiles over all dimensions.\n\n    Parameters\n    ----------\n    data_shape : 1D numpy.array of int\n        The shape of the data to be tiled and stitched together, (S, C, (Z), Y, X).\n    tile_size : 1D numpy.array of int\n        The tile size in each dimension, ((Z), Y, X).\n    overlaps : 1D numpy.array of int\n        The tile overlap in each dimension, ((Z), Y, X).\n\n\n    Returns\n    -------\n    int\n        The total number of tiles over all dimensions.\n    \"\"\"\n    result = 1\n    # assume spatial dimension are the last dimensions so iterate backwards\n    for i in range(-1, -len(tile_size) - 1, -1):\n        result = result * n_tiles_1d(data_shape[i], tile_size[i], overlaps[i])\n\n    return result\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/tiled_patching/","title":"tiled_patching","text":"<p>Tiled patching utilities.</p>"},{"location":"reference/careamics/dataset/tiling/tiled_patching/#careamics.dataset.tiling.tiled_patching._compute_crop_and_stitch_coords_1d","title":"<code>_compute_crop_and_stitch_coords_1d(axis_size, tile_size, overlap)</code>","text":"<p>Compute the coordinates of each tile along an axis, given the overlap.</p> <p>Parameters:</p> Name Type Description Default <code>axis_size</code> <code>int</code> <p>Length of the axis.</p> required <code>tile_size</code> <code>int</code> <p>Size of the tile for the given axis.</p> required <code>overlap</code> <code>int</code> <p>Size of the overlap for the given axis.</p> required <p>Returns:</p> Type Description <code>tuple[tuple[int, ...], ...]</code> <p>tuple of all coordinates for given axis.</p> Source code in <code>src/careamics/dataset/tiling/tiled_patching.py</code> <pre><code>def _compute_crop_and_stitch_coords_1d(\n    axis_size: int, tile_size: int, overlap: int\n) -&gt; tuple[list[tuple[int, int]], list[tuple[int, int]], list[tuple[int, int]]]:\n    \"\"\"\n    Compute the coordinates of each tile along an axis, given the overlap.\n\n    Parameters\n    ----------\n    axis_size : int\n        Length of the axis.\n    tile_size : int\n        Size of the tile for the given axis.\n    overlap : int\n        Size of the overlap for the given axis.\n\n    Returns\n    -------\n    tuple[tuple[int, ...], ...]\n        tuple of all coordinates for given axis.\n    \"\"\"\n    # Compute the step between tiles\n    step = tile_size - overlap\n    crop_coords = []\n    stitch_coords = []\n    overlap_crop_coords = []\n\n    # Iterate over the axis with step\n    for i in range(0, max(1, axis_size - overlap), step):\n        # Check if the tile fits within the axis\n        if i + tile_size &lt;= axis_size:\n            # Add the coordinates to crop one tile\n            crop_coords.append((i, i + tile_size))\n\n            # Add the pixel coordinates of the cropped tile in the original image space\n            stitch_coords.append(\n                (\n                    i + overlap // 2 if i &gt; 0 else 0,\n                    (\n                        i + tile_size - overlap // 2\n                        if crop_coords[-1][1] &lt; axis_size\n                        else axis_size\n                    ),\n                )\n            )\n\n            # Add the coordinates to crop the overlap from the prediction.\n            overlap_crop_coords.append(\n                (\n                    overlap // 2 if i &gt; 0 else 0,\n                    (\n                        tile_size - overlap // 2\n                        if crop_coords[-1][1] &lt; axis_size\n                        else tile_size\n                    ),\n                )\n            )\n\n        # If the tile does not fit within the axis, perform the abovementioned\n        # operations starting from the end of the axis\n        else:\n            # if (axis_size - tile_size, axis_size) not in crop_coords:\n            crop_coords.append((max(0, axis_size - tile_size), axis_size))\n            last_tile_end_coord = stitch_coords[-1][1] if stitch_coords else 1\n            stitch_coords.append((last_tile_end_coord, axis_size))\n            overlap_crop_coords.append(\n                (tile_size - (axis_size - last_tile_end_coord), tile_size)\n            )\n            break\n    return crop_coords, stitch_coords, overlap_crop_coords\n</code></pre>"},{"location":"reference/careamics/dataset/tiling/tiled_patching/#careamics.dataset.tiling.tiled_patching.extract_tiles","title":"<code>extract_tiles(arr, tile_size, overlaps)</code>","text":"<p>Generate tiles from the input array with specified overlap.</p> <p>The tiles cover the whole array. The method returns a generator that yields tuples of array and tile information, the latter includes whether the tile is the last one, the coordinates of the overlap crop, and the coordinates of the stitched tile.</p> <p>Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX, where C can be a singleton.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>Array of shape (S, C, (Z), Y, X).</p> required <code>tile_size</code> <code>Union[list[int], tuple[int]]</code> <p>Tile sizes in each dimension, of length 2 or 3.</p> required <code>overlaps</code> <code>Union[list[int], tuple[int]]</code> <p>Overlap values in each dimension, of length 2 or 3.</p> required <p>Yields:</p> Type Description <code>Generator[tuple[ndarray, TileInformation], None, None]</code> <p>Tile generator, yields the tile and additional information.</p> Source code in <code>src/careamics/dataset/tiling/tiled_patching.py</code> <pre><code>def extract_tiles(\n    arr: np.ndarray,\n    tile_size: Union[list[int], tuple[int, ...]],\n    overlaps: Union[list[int], tuple[int, ...]],\n) -&gt; Generator[tuple[np.ndarray, TileInformation], None, None]:\n    \"\"\"Generate tiles from the input array with specified overlap.\n\n    The tiles cover the whole array. The method returns a generator that yields\n    tuples of array and tile information, the latter includes whether\n    the tile is the last one, the coordinates of the overlap crop, and the coordinates\n    of the stitched tile.\n\n    Input array should have shape SC(Z)YX, while the returned tiles have shape C(Z)YX,\n    where C can be a singleton.\n\n    Parameters\n    ----------\n    arr : np.ndarray\n        Array of shape (S, C, (Z), Y, X).\n    tile_size : Union[list[int], tuple[int]]\n        Tile sizes in each dimension, of length 2 or 3.\n    overlaps : Union[list[int], tuple[int]]\n        Overlap values in each dimension, of length 2 or 3.\n\n    Yields\n    ------\n    Generator[tuple[np.ndarray, TileInformation], None, None]\n        Tile generator, yields the tile and additional information.\n    \"\"\"\n    # Iterate over num samples (S)\n    for sample_idx in range(arr.shape[0]):\n        sample: np.ndarray = arr[sample_idx, ...]\n\n        # Create a list of coordinates for cropping and stitching all axes.\n        # [crop coordinates, stitching coordinates, overlap crop coordinates]\n        # For axis of size 35 and patch size of 32 compute_crop_and_stitch_coords_1d\n        # will output ([(0, 32), (3, 35)], [(0, 20), (20, 35)], [(0, 20), (17, 32)])\n        crop_and_stitch_coords_list = [\n            _compute_crop_and_stitch_coords_1d(\n                sample.shape[i + 1], tile_size[i], overlaps[i]\n            )\n            for i in range(len(tile_size))\n        ]\n\n        # Rearrange crop coordinates from a list of coordinate pairs per axis to a list\n        # grouped by type.\n        all_crop_coords, all_stitch_coords, all_overlap_crop_coords = zip(\n            *crop_and_stitch_coords_list\n        )\n\n        # Maximum tile index\n        max_tile_idx = np.prod([len(axis) for axis in all_crop_coords]) - 1\n\n        # Iterate over generated coordinate pairs:\n        for tile_idx, (crop_coords, stitch_coords, overlap_crop_coords) in enumerate(\n            zip(\n                itertools.product(*all_crop_coords),\n                itertools.product(*all_stitch_coords),\n                itertools.product(*all_overlap_crop_coords),\n            )\n        ):\n            # Extract tile from the sample\n            tile: np.ndarray = sample[\n                (..., *[slice(c[0], c[1]) for c in list(crop_coords)])  # type: ignore\n            ]\n\n            # Check if we are at the end of the sample by computing the length of the\n            # array that contains all the tiles\n            if tile_idx == max_tile_idx:\n                last_tile = True\n            else:\n                last_tile = False\n\n            # create tile information\n            tile_info = TileInformation(\n                array_shape=sample.shape,\n                last_tile=last_tile,\n                overlap_crop_coords=overlap_crop_coords,\n                stitch_coords=stitch_coords,\n                sample_id=sample_idx,\n            )\n\n            yield tile, tile_info\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/","title":"get_func","text":"<p>Module to get read functions.</p>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.ReadFunc","title":"<code>ReadFunc</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for type hinting read functions.</p> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>class ReadFunc(Protocol):\n    \"\"\"Protocol for type hinting read functions.\"\"\"\n\n    def __call__(self, file_path: Path, *args, **kwargs) -&gt; NDArray:\n        \"\"\"\n        Type hinted callables must match this function signature (not including self).\n\n        Parameters\n        ----------\n        file_path : pathlib.Path\n            Path to file.\n        *args\n            Other positional arguments.\n        **kwargs\n            Other keyword arguments.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.ReadFunc.__call__","title":"<code>__call__(file_path, *args, **kwargs)</code>","text":"<p>Type hinted callables must match this function signature (not including self).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>*args</code> <p>Other positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>def __call__(self, file_path: Path, *args, **kwargs) -&gt; NDArray:\n    \"\"\"\n    Type hinted callables must match this function signature (not including self).\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    *args\n        Other positional arguments.\n    **kwargs\n        Other keyword arguments.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/read/get_func/#careamics.file_io.read.get_func.get_read_func","title":"<code>get_read_func(data_type)</code>","text":"<p>Get the read function for the data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>SupportedData</code> <p>Data type.</p> required <p>Returns:</p> Type Description <code>callable</code> <p>Read function.</p> Source code in <code>src/careamics/file_io/read/get_func.py</code> <pre><code>def get_read_func(data_type: Union[str, SupportedData]) -&gt; Callable:\n    \"\"\"\n    Get the read function for the data type.\n\n    Parameters\n    ----------\n    data_type : SupportedData\n        Data type.\n\n    Returns\n    -------\n    callable\n        Read function.\n    \"\"\"\n    if data_type in READ_FUNCS:\n        data_type = SupportedData(data_type)  # mypy complaining about dict key type\n        return READ_FUNCS[data_type]\n    else:\n        raise NotImplementedError(f\"Data type '{data_type}' is not supported.\")\n</code></pre>"},{"location":"reference/careamics/file_io/read/tiff/","title":"tiff","text":"<p>Functions to read tiff images.</p>"},{"location":"reference/careamics/file_io/read/tiff/#careamics.file_io.read.tiff.read_tiff","title":"<code>read_tiff(file_path, *args, **kwargs)</code>","text":"<p>Read a tiff file and return a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to a file.</p> required <code>*args</code> <code>list</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Resulting array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file failed to open.</p> <code>OSError</code> <p>If the file failed to open.</p> <code>ValueError</code> <p>If the file is not a valid tiff.</p> <code>ValueError</code> <p>If the data dimensions are incorrect.</p> <code>ValueError</code> <p>If the axes length is incorrect.</p> Source code in <code>src/careamics/file_io/read/tiff.py</code> <pre><code>def read_tiff(file_path: Path, *args: list, **kwargs: dict) -&gt; np.ndarray:\n    \"\"\"\n    Read a tiff file and return a numpy array.\n\n    Parameters\n    ----------\n    file_path : Path\n        Path to a file.\n    *args : list\n        Additional arguments.\n    **kwargs : dict\n        Additional keyword arguments.\n\n    Returns\n    -------\n    np.ndarray\n        Resulting array.\n\n    Raises\n    ------\n    ValueError\n        If the file failed to open.\n    OSError\n        If the file failed to open.\n    ValueError\n        If the file is not a valid tiff.\n    ValueError\n        If the data dimensions are incorrect.\n    ValueError\n        If the axes length is incorrect.\n    \"\"\"\n    if fnmatch(\n        file_path.suffix, SupportedData.get_extension_pattern(SupportedData.TIFF)\n    ):\n        try:\n            array = tifffile.imread(file_path)\n        except (ValueError, OSError) as e:\n            logging.exception(f\"Exception in file {file_path}: {e}, skipping it.\")\n            raise e\n    else:\n        raise ValueError(f\"File {file_path} is not a valid tiff.\")\n\n    return array\n</code></pre>"},{"location":"reference/careamics/file_io/read/zarr/","title":"zarr","text":"<p>Function to read zarr images.</p>"},{"location":"reference/careamics/file_io/read/zarr/#careamics.file_io.read.zarr.read_zarr","title":"<code>read_zarr(zarr_source, axes)</code>","text":"<p>Read a file and returns a pointer.</p> <p>Parameters:</p> Name Type Description Default <code>zarr_source</code> <code>Group</code> <p>Zarr storage.</p> required <code>axes</code> <code>str</code> <p>Axes of the data.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Pointer to zarr storage.</p> <p>Raises:</p> Type Description <code>(ValueError, OSError)</code> <p>if a file is not a valid tiff or damaged.</p> <code>ValueError</code> <p>if data dimensions are not 2, 3 or 4.</p> <code>ValueError</code> <p>if axes parameter from config is not consistent with data dimensions.</p> Source code in <code>src/careamics/file_io/read/zarr.py</code> <pre><code>def read_zarr(\n    zarr_source: Group, axes: str\n) -&gt; Union[core.Array, storage.DirectoryStore, hierarchy.Group]:\n    \"\"\"Read a file and returns a pointer.\n\n    Parameters\n    ----------\n    zarr_source : Group\n        Zarr storage.\n    axes : str\n        Axes of the data.\n\n    Returns\n    -------\n    np.ndarray\n        Pointer to zarr storage.\n\n    Raises\n    ------\n    ValueError, OSError\n        if a file is not a valid tiff or damaged.\n    ValueError\n        if data dimensions are not 2, 3 or 4.\n    ValueError\n        if axes parameter from config is not consistent with data dimensions.\n    \"\"\"\n    if isinstance(zarr_source, hierarchy.Group):\n        array = zarr_source[0]\n\n    elif isinstance(zarr_source, storage.DirectoryStore):\n        raise NotImplementedError(\"DirectoryStore not supported yet\")\n\n    elif isinstance(zarr_source, core.Array):\n        # array should be of shape (S, (C), (Z), Y, X), iterating over S ?\n        if zarr_source.dtype == \"O\":\n            raise NotImplementedError(\"Object type not supported yet\")\n        else:\n            array = zarr_source\n    else:\n        raise ValueError(f\"Unsupported zarr object type {type(zarr_source)}\")\n\n    # sanity check on dimensions\n    if len(array.shape) &lt; 2 or len(array.shape) &gt; 4:\n        raise ValueError(\n            f\"Incorrect data dimensions. Must be 2, 3 or 4 (got {array.shape}).\"\n        )\n\n    # sanity check on axes length\n    if len(axes) != len(array.shape):\n        raise ValueError(f\"Incorrect axes length (got {axes}).\")\n\n    # arr = fix_axes(arr, axes)\n    return array\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/","title":"get_func","text":"<p>Module to get write functions.</p>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.WriteFunc","title":"<code>WriteFunc</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for type hinting write functions.</p> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>class WriteFunc(Protocol):\n    \"\"\"Protocol for type hinting write functions.\"\"\"\n\n    def __call__(self, file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n        \"\"\"\n        Type hinted callables must match this function signature (not including self).\n\n        Parameters\n        ----------\n        file_path : pathlib.Path\n            Path to file.\n        img : numpy.ndarray\n            Image data to save.\n        *args\n            Other positional arguments.\n        **kwargs\n            Other keyword arguments.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.WriteFunc.__call__","title":"<code>__call__(file_path, img, *args, **kwargs)</code>","text":"<p>Type hinted callables must match this function signature (not including self).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>img</code> <code>ndarray</code> <p>Image data to save.</p> required <code>*args</code> <p>Other positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Other keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>def __call__(self, file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n    \"\"\"\n    Type hinted callables must match this function signature (not including self).\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    img : numpy.ndarray\n        Image data to save.\n    *args\n        Other positional arguments.\n    **kwargs\n        Other keyword arguments.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/file_io/write/get_func/#careamics.file_io.write.get_func.get_write_func","title":"<code>get_write_func(data_type)</code>","text":"<p>Get the write function for the data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>(tiff, custom)</code> <p>Data type.</p> <code>\"tiff\"</code> <p>Returns:</p> Type Description <code>callable</code> <p>Write function.</p> Source code in <code>src/careamics/file_io/write/get_func.py</code> <pre><code>def get_write_func(data_type: SupportedWriteType) -&gt; WriteFunc:\n    \"\"\"\n    Get the write function for the data type.\n\n    Parameters\n    ----------\n    data_type : {\"tiff\", \"custom\"}\n        Data type.\n\n    Returns\n    -------\n    callable\n        Write function.\n    \"\"\"\n    # error raised here if not supported\n    data_type_ = SupportedData(data_type)  # new variable for mypy\n    # error if no write func.\n    if data_type_ not in WRITE_FUNCS:\n        raise NotImplementedError(f\"No write function for data type '{data_type}'.\")\n\n    return WRITE_FUNCS[data_type_]\n</code></pre>"},{"location":"reference/careamics/file_io/write/tiff/","title":"tiff","text":"<p>Write tiff function.</p>"},{"location":"reference/careamics/file_io/write/tiff/#careamics.file_io.write.tiff.write_tiff","title":"<code>write_tiff(file_path, img, *args, **kwargs)</code>","text":"<p>Write tiff files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to file.</p> required <code>img</code> <code>ndarray</code> <p>Image data to save.</p> required <code>*args</code> <p>Positional arguments passed to <code>tifffile.imwrite</code>.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to <code>tifffile.imwrite</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file extension of <code>file_path</code> does not match the Unix shell-style pattern '.tif'.</p> Source code in <code>src/careamics/file_io/write/tiff.py</code> <pre><code>def write_tiff(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None:\n    # TODO: add link to tiffile docs for args kwrgs?\n    \"\"\"\n    Write tiff files.\n\n    Parameters\n    ----------\n    file_path : pathlib.Path\n        Path to file.\n    img : numpy.ndarray\n        Image data to save.\n    *args\n        Positional arguments passed to `tifffile.imwrite`.\n    **kwargs\n        Keyword arguments passed to `tifffile.imwrite`.\n\n    Raises\n    ------\n    ValueError\n        When the file extension of `file_path` does not match the Unix shell-style\n        pattern '*.tif*'.\n    \"\"\"\n    if not fnmatch(\n        file_path.suffix, SupportedData.get_extension_pattern(SupportedData.TIFF)\n    ):\n        raise ValueError(\n            f\"Unexpected extension '{file_path.suffix}' for save file type 'tiff'.\"\n        )\n    tifffile.imwrite(file_path, img, *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/","title":"lightning_module","text":"<p>CAREamics Lightning module.</p>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule","title":"<code>FCNModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CAREamics Lightning module.</p> <p>This class encapsulates the PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>AlgorithmModel or dict</code> <p>Algorithm configuration.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>PyTorch model.</p> <code>loss_func</code> <code>Module</code> <p>Loss function.</p> <code>optimizer_name</code> <code>str</code> <p>Optimizer name.</p> <code>optimizer_params</code> <code>dict</code> <p>Optimizer parameters.</p> <code>lr_scheduler_name</code> <code>str</code> <p>Learning rate scheduler name.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>class FCNModule(L.LightningModule):\n    \"\"\"\n    CAREamics Lightning module.\n\n    This class encapsulates the PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : AlgorithmModel or dict\n        Algorithm configuration.\n\n    Attributes\n    ----------\n    model : torch.nn.Module\n        PyTorch model.\n    loss_func : torch.nn.Module\n        Loss function.\n    optimizer_name : str\n        Optimizer name.\n    optimizer_params : dict\n        Optimizer parameters.\n    lr_scheduler_name : str\n        Learning rate scheduler name.\n    \"\"\"\n\n    def __init__(self, algorithm_config: Union[UNetBasedAlgorithm, dict]) -&gt; None:\n        \"\"\"Lightning module for CAREamics.\n\n        This class encapsulates the a PyTorch model along with the training, validation,\n        and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n        Parameters\n        ----------\n        algorithm_config : AlgorithmModel or dict\n            Algorithm configuration.\n        \"\"\"\n        super().__init__()\n\n        if isinstance(algorithm_config, dict):\n            algorithm_config = algorithm_factory(algorithm_config)\n\n        # create preprocessing, model and loss function\n        if isinstance(algorithm_config, N2VAlgorithm):\n            self.use_n2v = True\n            self.n2v_preprocess: Optional[N2VManipulateTorch] = N2VManipulateTorch(\n                n2v_manipulate_config=algorithm_config.n2v_config\n            )\n        else:\n            self.use_n2v = False\n            self.n2v_preprocess = None\n\n        self.algorithm = algorithm_config.algorithm\n        self.model: nn.Module = model_factory(algorithm_config.model)\n        self.loss_func = loss_factory(algorithm_config.loss)\n\n        # save optimizer and lr_scheduler names and parameters\n        self.optimizer_name = algorithm_config.optimizer.name\n        self.optimizer_params = algorithm_config.optimizer.parameters\n        self.lr_scheduler_name = algorithm_config.lr_scheduler.name\n        self.lr_scheduler_params = algorithm_config.lr_scheduler.parameters\n\n    def forward(self, x: Any) -&gt; Any:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : Any\n            Input tensor.\n\n        Returns\n        -------\n        Any\n            Output tensor.\n        \"\"\"\n        return self.model(x)\n\n    def training_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Training step.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Loss value.\n        \"\"\"\n        x, *targets = batch\n        if self.use_n2v and self.n2v_preprocess is not None:\n            x_preprocessed, *aux = self.n2v_preprocess(x)\n        else:\n            x_preprocessed = x\n            aux = []\n\n        out = self.model(x_preprocessed)\n        loss = self.loss_func(out, *aux, *targets)\n        self.log(\n            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss\n\n    def validation_step(self, batch: Tensor, batch_idx: Any) -&gt; None:\n        \"\"\"Validation step.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n        \"\"\"\n        x, *targets = batch\n        if self.use_n2v and self.n2v_preprocess is not None:\n            x_preprocessed, *aux = self.n2v_preprocess(x)\n        else:\n            x_preprocessed = x\n            aux = []\n\n        out = self.model(x_preprocessed)\n        val_loss = self.loss_func(out, *aux, *targets)\n\n        # log validation loss\n        self.log(\n            \"val_loss\",\n            val_loss,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n\n    def predict_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Prediction step.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Model output.\n        \"\"\"\n        # TODO refactor when redoing datasets\n        # hacky way to determine if it is PredictDataModule, otherwise there is a\n        # circular import to solve with isinstance\n        from_prediction = hasattr(self._trainer.datamodule, \"tiled\")\n        is_tiled = (\n            len(batch) &gt; 1\n            and isinstance(batch[1], list)\n            and isinstance(batch[1][0], TileInformation)\n        )\n\n        # TODO add explanations for what is happening here\n        if is_tiled:\n            x, *aux = batch\n            if type(x) in [list, tuple]:\n                x = x[0]\n        else:\n            if type(batch) in [list, tuple]:\n                x = batch[0]  # TODO change, ugly way to deal with n2v refac\n            else:\n                x = batch\n            aux = []\n\n        # apply test-time augmentation if available\n        # TODO: probably wont work with batch size &gt; 1\n        if (\n            from_prediction\n            and self._trainer.datamodule.prediction_config.tta_transforms\n        ):\n            tta = ImageRestorationTTA()\n            augmented_batch = tta.forward(x)  # list of augmented tensors\n            augmented_output = []\n            for augmented in augmented_batch:\n                augmented_pred = self.model(augmented)\n                augmented_output.append(augmented_pred)\n            output = tta.backward(augmented_output)\n        else:\n            output = self.model(x)\n\n        # Denormalize the output\n        # TODO incompatible API between predict and train datasets\n        denorm = Denormalize(\n            image_means=(\n                self._trainer.datamodule.predict_dataset.image_means\n                if from_prediction\n                else self._trainer.datamodule.train_dataset.image_stats.means\n            ),\n            image_stds=(\n                self._trainer.datamodule.predict_dataset.image_stds\n                if from_prediction\n                else self._trainer.datamodule.train_dataset.image_stats.stds\n            ),\n        )\n        denormalized_output = denorm(patch=output.cpu().numpy())\n\n        if len(aux) &gt; 0:  # aux can be tiling information\n            return denormalized_output, *aux\n        else:\n            return denormalized_output\n\n    def configure_optimizers(self) -&gt; Any:\n        \"\"\"Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Any\n            Optimizer and learning rate scheduler.\n        \"\"\"\n        # instantiate optimizer\n        optimizer_func = get_optimizer(self.optimizer_name)\n        optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n        # and scheduler\n        scheduler_func = get_scheduler(self.lr_scheduler_name)\n        scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n        }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Lightning module for CAREamics.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>AlgorithmModel or dict</code> <p>Algorithm configuration.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def __init__(self, algorithm_config: Union[UNetBasedAlgorithm, dict]) -&gt; None:\n    \"\"\"Lightning module for CAREamics.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : AlgorithmModel or dict\n        Algorithm configuration.\n    \"\"\"\n    super().__init__()\n\n    if isinstance(algorithm_config, dict):\n        algorithm_config = algorithm_factory(algorithm_config)\n\n    # create preprocessing, model and loss function\n    if isinstance(algorithm_config, N2VAlgorithm):\n        self.use_n2v = True\n        self.n2v_preprocess: Optional[N2VManipulateTorch] = N2VManipulateTorch(\n            n2v_manipulate_config=algorithm_config.n2v_config\n        )\n    else:\n        self.use_n2v = False\n        self.n2v_preprocess = None\n\n    self.algorithm = algorithm_config.algorithm\n    self.model: nn.Module = model_factory(algorithm_config.model)\n    self.loss_func = loss_factory(algorithm_config.loss)\n\n    # save optimizer and lr_scheduler names and parameters\n    self.optimizer_name = algorithm_config.optimizer.name\n    self.optimizer_params = algorithm_config.optimizer.parameters\n    self.lr_scheduler_name = algorithm_config.lr_scheduler.name\n    self.lr_scheduler_params = algorithm_config.lr_scheduler.parameters\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Optimizer and learning rate scheduler.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Any:\n    \"\"\"Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Any\n        Optimizer and learning rate scheduler.\n    \"\"\"\n    # instantiate optimizer\n    optimizer_func = get_optimizer(self.optimizer_name)\n    optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n    # and scheduler\n    scheduler_func = get_scheduler(self.lr_scheduler_name)\n    scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n        \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n    }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Output tensor.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def forward(self, x: Any) -&gt; Any:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : Any\n        Input tensor.\n\n    Returns\n    -------\n    Any\n        Output tensor.\n    \"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def predict_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Prediction step.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Model output.\n    \"\"\"\n    # TODO refactor when redoing datasets\n    # hacky way to determine if it is PredictDataModule, otherwise there is a\n    # circular import to solve with isinstance\n    from_prediction = hasattr(self._trainer.datamodule, \"tiled\")\n    is_tiled = (\n        len(batch) &gt; 1\n        and isinstance(batch[1], list)\n        and isinstance(batch[1][0], TileInformation)\n    )\n\n    # TODO add explanations for what is happening here\n    if is_tiled:\n        x, *aux = batch\n        if type(x) in [list, tuple]:\n            x = x[0]\n    else:\n        if type(batch) in [list, tuple]:\n            x = batch[0]  # TODO change, ugly way to deal with n2v refac\n        else:\n            x = batch\n        aux = []\n\n    # apply test-time augmentation if available\n    # TODO: probably wont work with batch size &gt; 1\n    if (\n        from_prediction\n        and self._trainer.datamodule.prediction_config.tta_transforms\n    ):\n        tta = ImageRestorationTTA()\n        augmented_batch = tta.forward(x)  # list of augmented tensors\n        augmented_output = []\n        for augmented in augmented_batch:\n            augmented_pred = self.model(augmented)\n            augmented_output.append(augmented_pred)\n        output = tta.backward(augmented_output)\n    else:\n        output = self.model(x)\n\n    # Denormalize the output\n    # TODO incompatible API between predict and train datasets\n    denorm = Denormalize(\n        image_means=(\n            self._trainer.datamodule.predict_dataset.image_means\n            if from_prediction\n            else self._trainer.datamodule.train_dataset.image_stats.means\n        ),\n        image_stds=(\n            self._trainer.datamodule.predict_dataset.image_stds\n            if from_prediction\n            else self._trainer.datamodule.train_dataset.image_stats.stds\n        ),\n    )\n    denormalized_output = denorm(patch=output.cpu().numpy())\n\n    if len(aux) &gt; 0:  # aux can be tiling information\n        return denormalized_output, *aux\n    else:\n        return denormalized_output\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loss value.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def training_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Training step.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Loss value.\n    \"\"\"\n    x, *targets = batch\n    if self.use_n2v and self.n2v_preprocess is not None:\n        x_preprocessed, *aux = self.n2v_preprocess(x)\n    else:\n        x_preprocessed = x\n        aux = []\n\n    out = self.model(x_preprocessed)\n    loss = self.loss_func(out, *aux, *targets)\n    self.log(\n        \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n    )\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.FCNModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def validation_step(self, batch: Tensor, batch_idx: Any) -&gt; None:\n    \"\"\"Validation step.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n    \"\"\"\n    x, *targets = batch\n    if self.use_n2v and self.n2v_preprocess is not None:\n        x_preprocessed, *aux = self.n2v_preprocess(x)\n    else:\n        x_preprocessed = x\n        aux = []\n\n    out = self.model(x_preprocessed)\n    val_loss = self.loss_func(out, *aux, *targets)\n\n    # log validation loss\n    self.log(\n        \"val_loss\",\n        val_loss,\n        on_step=False,\n        on_epoch=True,\n        prog_bar=True,\n        logger=True,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule","title":"<code>VAEModule</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>CAREamics Lightning module.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>Union[VAEAlgorithmConfig, dict]</code> <p>Algorithm configuration.</p> required <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>PyTorch model.</p> <code>loss_func</code> <code>Module</code> <p>Loss function.</p> <code>optimizer_name</code> <code>str</code> <p>Optimizer name.</p> <code>optimizer_params</code> <code>dict</code> <p>Optimizer parameters.</p> <code>lr_scheduler_name</code> <code>str</code> <p>Learning rate scheduler name.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>class VAEModule(L.LightningModule):\n    \"\"\"\n    CAREamics Lightning module.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : Union[VAEAlgorithmConfig, dict]\n        Algorithm configuration.\n\n    Attributes\n    ----------\n    model : nn.Module\n        PyTorch model.\n    loss_func : nn.Module\n        Loss function.\n    optimizer_name : str\n        Optimizer name.\n    optimizer_params : dict\n        Optimizer parameters.\n    lr_scheduler_name : str\n        Learning rate scheduler name.\n    \"\"\"\n\n    def __init__(self, algorithm_config: Union[VAEBasedAlgorithm, dict]) -&gt; None:\n        \"\"\"Lightning module for CAREamics.\n\n        This class encapsulates the a PyTorch model along with the training, validation,\n        and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n        Parameters\n        ----------\n        algorithm_config : Union[AlgorithmModel, dict]\n            Algorithm configuration.\n        \"\"\"\n        super().__init__()\n        # if loading from a checkpoint, AlgorithmModel needs to be instantiated\n        self.algorithm_config = (\n            VAEBasedAlgorithm(**algorithm_config)\n            if isinstance(algorithm_config, dict)\n            else algorithm_config\n        )\n\n        # TODO: log algorithm config\n        # self.save_hyperparameters(self.algorithm_config.model_dump())\n\n        # create model\n        self.model: nn.Module = model_factory(self.algorithm_config.model)\n\n        # create loss function\n        self.noise_model: Optional[NoiseModel] = noise_model_factory(\n            self.algorithm_config.noise_model\n        )\n\n        self.noise_model_likelihood: Optional[NoiseModelLikelihood] = (\n            likelihood_factory(\n                config=self.algorithm_config.noise_model_likelihood,\n                noise_model=self.noise_model,\n            )\n        )\n\n        self.gaussian_likelihood: Optional[GaussianLikelihood] = likelihood_factory(\n            self.algorithm_config.gaussian_likelihood\n        )\n\n        self.loss_parameters = self.algorithm_config.loss\n        self.loss_func = loss_factory(self.algorithm_config.loss.loss_type)\n\n        # save optimizer and lr_scheduler names and parameters\n        self.optimizer_name = self.algorithm_config.optimizer.name\n        self.optimizer_params = self.algorithm_config.optimizer.parameters\n        self.lr_scheduler_name = self.algorithm_config.lr_scheduler.name\n        self.lr_scheduler_params = self.algorithm_config.lr_scheduler.parameters\n\n        # initialize running PSNR\n        self.running_psnr = [\n            RunningPSNR() for _ in range(self.algorithm_config.model.output_channels)\n        ]\n\n    def forward(self, x: Tensor) -&gt; tuple[Tensor, dict[str, Any]]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : Tensor\n            Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs.\n\n        Returns\n        -------\n        tuple[Tensor, dict[str, Any]]\n            A tuple with the output tensor and additional data from the top-down pass.\n        \"\"\"\n        return self.model(x)  # TODO Different model can have more than one output\n\n    def training_step(\n        self, batch: tuple[Tensor, Tensor], batch_idx: Any\n    ) -&gt; Optional[dict[str, Tensor]]:\n        \"\"\"Training step.\n\n        Parameters\n        ----------\n        batch : tuple[Tensor, Tensor]\n            Input batch. It is a tuple with the input tensor and the target tensor.\n            The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n            where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n            muSplit/denoiSplit).\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Loss value.\n        \"\"\"\n        x, target = batch\n\n        # Forward pass\n        out = self.model(x)\n\n        # Update loss parameters\n        self.loss_parameters.kl_params.current_epoch = self.current_epoch\n\n        # Compute loss\n        loss = self.loss_func(\n            model_outputs=out,\n            targets=target,\n            config=self.loss_parameters,\n            gaussian_likelihood=self.gaussian_likelihood,\n            noise_model_likelihood=self.noise_model_likelihood,\n        )\n\n        # Logging\n        # TODO: implement a separate logging method?\n        self.log_dict(loss, on_step=True, on_epoch=True)\n        # self.log(\"lr\", self, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch: tuple[Tensor, Tensor], batch_idx: Any) -&gt; None:\n        \"\"\"Validation step.\n\n        Parameters\n        ----------\n        batch : tuple[Tensor, Tensor]\n            Input batch. It is a tuple with the input tensor and the target tensor.\n            The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n            number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n            where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n            muSplit/denoiSplit).\n        batch_idx : Any\n            Batch index.\n        \"\"\"\n        x, target = batch\n\n        # Forward pass\n        out = self.model(x)\n\n        # Compute loss\n        loss = self.loss_func(\n            model_outputs=out,\n            targets=target,\n            config=self.loss_parameters,\n            gaussian_likelihood=self.gaussian_likelihood,\n            noise_model_likelihood=self.noise_model_likelihood,\n        )\n\n        # Logging\n        # Rename val_loss dict\n        loss = {\"_\".join([\"val\", k]): v for k, v in loss.items()}\n        self.log_dict(loss, on_epoch=True, prog_bar=True)\n        curr_psnr = self.compute_val_psnr(out, target)\n        for i, psnr in enumerate(curr_psnr):\n            self.log(f\"val_psnr_ch{i+1}_batch\", psnr, on_epoch=True)\n\n    def on_validation_epoch_end(self) -&gt; None:\n        \"\"\"Validation epoch end.\"\"\"\n        psnr_ = self.reduce_running_psnr()\n        if psnr_ is not None:\n            self.log(\"val_psnr\", psnr_, on_epoch=True, prog_bar=True)\n        else:\n            self.log(\"val_psnr\", 0.0, on_epoch=True, prog_bar=True)\n\n    def predict_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n        \"\"\"Prediction step.\n\n        Parameters\n        ----------\n        batch : Tensor\n            Input batch.\n        batch_idx : Any\n            Batch index.\n\n        Returns\n        -------\n        Any\n            Model output.\n        \"\"\"\n        if self._trainer.datamodule.tiled:\n            x, *aux = batch\n        else:\n            x = batch\n            aux = []\n\n        # apply test-time augmentation if available\n        # TODO: probably wont work with batch size &gt; 1\n        if self._trainer.datamodule.prediction_config.tta_transforms:\n            tta = ImageRestorationTTA()\n            augmented_batch = tta.forward(x)  # list of augmented tensors\n            augmented_output = []\n            for augmented in augmented_batch:\n                augmented_pred = self.model(augmented)\n                augmented_output.append(augmented_pred)\n            output = tta.backward(augmented_output)\n        else:\n            output = self.model(x)\n\n        # Denormalize the output\n        denorm = Denormalize(\n            image_means=self._trainer.datamodule.predict_dataset.image_means,\n            image_stds=self._trainer.datamodule.predict_dataset.image_stds,\n        )\n        denormalized_output = denorm(patch=output.cpu().numpy())\n\n        if len(aux) &gt; 0:  # aux can be tiling information\n            return denormalized_output, *aux\n        else:\n            return denormalized_output\n\n    def configure_optimizers(self) -&gt; Any:\n        \"\"\"Configure optimizers and learning rate schedulers.\n\n        Returns\n        -------\n        Any\n            Optimizer and learning rate scheduler.\n        \"\"\"\n        # instantiate optimizer\n        optimizer_func = get_optimizer(self.optimizer_name)\n        optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n        # and scheduler\n        scheduler_func = get_scheduler(self.lr_scheduler_name)\n        scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n        }\n\n    # TODO: find a way to move the following methods to a separate module\n    # TODO: this same operation is done in many other places, like in loss_func\n    # should we refactor LadderVAE so that it already outputs\n    # tuple(`mean`, `logvar`, `td_data`)?\n    def get_reconstructed_tensor(\n        self, model_outputs: tuple[Tensor, dict[str, Any]]\n    ) -&gt; Tensor:\n        \"\"\"Get the reconstructed tensor from the LVAE model outputs.\n\n        Parameters\n        ----------\n        model_outputs : tuple[Tensor, dict[str, Any]]\n            Model outputs. It is a tuple with a tensor representing the predicted mean\n            and (optionally) logvar, and the top-down data dictionary.\n\n        Returns\n        -------\n        Tensor\n            Reconstructed tensor, i.e., the predicted mean.\n        \"\"\"\n        predictions, _ = model_outputs\n        if self.model.predict_logvar is None:\n            return predictions\n        elif self.model.predict_logvar == \"pixelwise\":\n            return predictions.chunk(2, dim=1)[0]\n\n    def compute_val_psnr(\n        self,\n        model_output: tuple[Tensor, dict[str, Any]],\n        target: Tensor,\n        psnr_func: Callable = scale_invariant_psnr,\n    ) -&gt; list[float]:\n        \"\"\"Compute the PSNR for the current validation batch.\n\n        Parameters\n        ----------\n        model_output : tuple[Tensor, dict[str, Any]]\n            Model output, a tuple with the predicted mean and (optionally) logvar,\n            and the top-down data dictionary.\n        target : Tensor\n            Target tensor.\n        psnr_func : Callable, optional\n            PSNR function to use, by default `scale_invariant_psnr`.\n\n        Returns\n        -------\n        list[float]\n            PSNR for each channel in the current batch.\n        \"\"\"\n        out_channels = target.shape[1]\n\n        # get the reconstructed image\n        recons_img = self.get_reconstructed_tensor(model_output)\n\n        # update running psnr\n        for i in range(out_channels):\n            self.running_psnr[i].update(rec=recons_img[:, i], tar=target[:, i])\n\n        # compute psnr for each channel in the current batch\n        # TODO: this doesn't need do be a method of this class\n        # and hence can be moved to a separate module\n        return [\n            psnr_func(\n                gt=target[:, i].clone().detach().cpu().numpy(),\n                pred=recons_img[:, i].clone().detach().cpu().numpy(),\n            )\n            for i in range(out_channels)\n        ]\n\n    def reduce_running_psnr(self) -&gt; Optional[float]:\n        \"\"\"Reduce the running PSNR statistics and reset the running PSNR.\n\n        Returns\n        -------\n        Optional[float]\n            Running PSNR averaged over the different output channels.\n        \"\"\"\n        psnr_arr = []  # type: ignore\n        for i in range(len(self.running_psnr)):\n            psnr = self.running_psnr[i].get()\n            if psnr is None:\n                psnr_arr = None  # type: ignore\n                break\n            psnr_arr.append(psnr.cpu().numpy())\n            self.running_psnr[i].reset()\n            # TODO: this line forces it to be a method of this class\n            # alternative is returning also the reset `running_psnr`\n        if psnr_arr is not None:\n            psnr = np.mean(psnr_arr)\n        return psnr\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.__init__","title":"<code>__init__(algorithm_config)</code>","text":"<p>Lightning module for CAREamics.</p> <p>This class encapsulates the a PyTorch model along with the training, validation, and testing logic. It is configured using an <code>AlgorithmModel</code> Pydantic class.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm_config</code> <code>Union[AlgorithmModel, dict]</code> <p>Algorithm configuration.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def __init__(self, algorithm_config: Union[VAEBasedAlgorithm, dict]) -&gt; None:\n    \"\"\"Lightning module for CAREamics.\n\n    This class encapsulates the a PyTorch model along with the training, validation,\n    and testing logic. It is configured using an `AlgorithmModel` Pydantic class.\n\n    Parameters\n    ----------\n    algorithm_config : Union[AlgorithmModel, dict]\n        Algorithm configuration.\n    \"\"\"\n    super().__init__()\n    # if loading from a checkpoint, AlgorithmModel needs to be instantiated\n    self.algorithm_config = (\n        VAEBasedAlgorithm(**algorithm_config)\n        if isinstance(algorithm_config, dict)\n        else algorithm_config\n    )\n\n    # TODO: log algorithm config\n    # self.save_hyperparameters(self.algorithm_config.model_dump())\n\n    # create model\n    self.model: nn.Module = model_factory(self.algorithm_config.model)\n\n    # create loss function\n    self.noise_model: Optional[NoiseModel] = noise_model_factory(\n        self.algorithm_config.noise_model\n    )\n\n    self.noise_model_likelihood: Optional[NoiseModelLikelihood] = (\n        likelihood_factory(\n            config=self.algorithm_config.noise_model_likelihood,\n            noise_model=self.noise_model,\n        )\n    )\n\n    self.gaussian_likelihood: Optional[GaussianLikelihood] = likelihood_factory(\n        self.algorithm_config.gaussian_likelihood\n    )\n\n    self.loss_parameters = self.algorithm_config.loss\n    self.loss_func = loss_factory(self.algorithm_config.loss.loss_type)\n\n    # save optimizer and lr_scheduler names and parameters\n    self.optimizer_name = self.algorithm_config.optimizer.name\n    self.optimizer_params = self.algorithm_config.optimizer.parameters\n    self.lr_scheduler_name = self.algorithm_config.lr_scheduler.name\n    self.lr_scheduler_params = self.algorithm_config.lr_scheduler.parameters\n\n    # initialize running PSNR\n    self.running_psnr = [\n        RunningPSNR() for _ in range(self.algorithm_config.model.output_channels)\n    ]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.compute_val_psnr","title":"<code>compute_val_psnr(model_output, target, psnr_func=scale_invariant_psnr)</code>","text":"<p>Compute the PSNR for the current validation batch.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Model output, a tuple with the predicted mean and (optionally) logvar, and the top-down data dictionary.</p> required <code>target</code> <code>Tensor</code> <p>Target tensor.</p> required <code>psnr_func</code> <code>Callable</code> <p>PSNR function to use, by default <code>scale_invariant_psnr</code>.</p> <code>scale_invariant_psnr</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>PSNR for each channel in the current batch.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def compute_val_psnr(\n    self,\n    model_output: tuple[Tensor, dict[str, Any]],\n    target: Tensor,\n    psnr_func: Callable = scale_invariant_psnr,\n) -&gt; list[float]:\n    \"\"\"Compute the PSNR for the current validation batch.\n\n    Parameters\n    ----------\n    model_output : tuple[Tensor, dict[str, Any]]\n        Model output, a tuple with the predicted mean and (optionally) logvar,\n        and the top-down data dictionary.\n    target : Tensor\n        Target tensor.\n    psnr_func : Callable, optional\n        PSNR function to use, by default `scale_invariant_psnr`.\n\n    Returns\n    -------\n    list[float]\n        PSNR for each channel in the current batch.\n    \"\"\"\n    out_channels = target.shape[1]\n\n    # get the reconstructed image\n    recons_img = self.get_reconstructed_tensor(model_output)\n\n    # update running psnr\n    for i in range(out_channels):\n        self.running_psnr[i].update(rec=recons_img[:, i], tar=target[:, i])\n\n    # compute psnr for each channel in the current batch\n    # TODO: this doesn't need do be a method of this class\n    # and hence can be moved to a separate module\n    return [\n        psnr_func(\n            gt=target[:, i].clone().detach().cpu().numpy(),\n            pred=recons_img[:, i].clone().detach().cpu().numpy(),\n        )\n        for i in range(out_channels)\n    ]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimizers and learning rate schedulers.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Optimizer and learning rate scheduler.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Any:\n    \"\"\"Configure optimizers and learning rate schedulers.\n\n    Returns\n    -------\n    Any\n        Optimizer and learning rate scheduler.\n    \"\"\"\n    # instantiate optimizer\n    optimizer_func = get_optimizer(self.optimizer_name)\n    optimizer = optimizer_func(self.model.parameters(), **self.optimizer_params)\n\n    # and scheduler\n    scheduler_func = get_scheduler(self.lr_scheduler_name)\n    scheduler = scheduler_func(optimizer, **self.lr_scheduler_params)\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": scheduler,\n        \"monitor\": \"val_loss\",  # otherwise triggers MisconfigurationException\n    }\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Any]]</code> <p>A tuple with the output tensor and additional data from the top-down pass.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def forward(self, x: Tensor) -&gt; tuple[Tensor, dict[str, Any]]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : Tensor\n        Input tensor of shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs.\n\n    Returns\n    -------\n    tuple[Tensor, dict[str, Any]]\n        A tuple with the output tensor and additional data from the top-down pass.\n    \"\"\"\n    return self.model(x)  # TODO Different model can have more than one output\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.get_reconstructed_tensor","title":"<code>get_reconstructed_tensor(model_outputs)</code>","text":"<p>Get the reconstructed tensor from the LVAE model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Model outputs. It is a tuple with a tensor representing the predicted mean and (optionally) logvar, and the top-down data dictionary.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor, i.e., the predicted mean.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def get_reconstructed_tensor(\n    self, model_outputs: tuple[Tensor, dict[str, Any]]\n) -&gt; Tensor:\n    \"\"\"Get the reconstructed tensor from the LVAE model outputs.\n\n    Parameters\n    ----------\n    model_outputs : tuple[Tensor, dict[str, Any]]\n        Model outputs. It is a tuple with a tensor representing the predicted mean\n        and (optionally) logvar, and the top-down data dictionary.\n\n    Returns\n    -------\n    Tensor\n        Reconstructed tensor, i.e., the predicted mean.\n    \"\"\"\n    predictions, _ = model_outputs\n    if self.model.predict_logvar is None:\n        return predictions\n    elif self.model.predict_logvar == \"pixelwise\":\n        return predictions.chunk(2, dim=1)[0]\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.on_validation_epoch_end","title":"<code>on_validation_epoch_end()</code>","text":"<p>Validation epoch end.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def on_validation_epoch_end(self) -&gt; None:\n    \"\"\"Validation epoch end.\"\"\"\n    psnr_ = self.reduce_running_psnr()\n    if psnr_ is not None:\n        self.log(\"val_psnr\", psnr_, on_epoch=True, prog_bar=True)\n    else:\n        self.log(\"val_psnr\", 0.0, on_epoch=True, prog_bar=True)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.predict_step","title":"<code>predict_step(batch, batch_idx)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Input batch.</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Model output.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def predict_step(self, batch: Tensor, batch_idx: Any) -&gt; Any:\n    \"\"\"Prediction step.\n\n    Parameters\n    ----------\n    batch : Tensor\n        Input batch.\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Model output.\n    \"\"\"\n    if self._trainer.datamodule.tiled:\n        x, *aux = batch\n    else:\n        x = batch\n        aux = []\n\n    # apply test-time augmentation if available\n    # TODO: probably wont work with batch size &gt; 1\n    if self._trainer.datamodule.prediction_config.tta_transforms:\n        tta = ImageRestorationTTA()\n        augmented_batch = tta.forward(x)  # list of augmented tensors\n        augmented_output = []\n        for augmented in augmented_batch:\n            augmented_pred = self.model(augmented)\n            augmented_output.append(augmented_pred)\n        output = tta.backward(augmented_output)\n    else:\n        output = self.model(x)\n\n    # Denormalize the output\n    denorm = Denormalize(\n        image_means=self._trainer.datamodule.predict_dataset.image_means,\n        image_stds=self._trainer.datamodule.predict_dataset.image_stds,\n    )\n    denormalized_output = denorm(patch=output.cpu().numpy())\n\n    if len(aux) &gt; 0:  # aux can be tiling information\n        return denormalized_output, *aux\n    else:\n        return denormalized_output\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.reduce_running_psnr","title":"<code>reduce_running_psnr()</code>","text":"<p>Reduce the running PSNR statistics and reset the running PSNR.</p> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Running PSNR averaged over the different output channels.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def reduce_running_psnr(self) -&gt; Optional[float]:\n    \"\"\"Reduce the running PSNR statistics and reset the running PSNR.\n\n    Returns\n    -------\n    Optional[float]\n        Running PSNR averaged over the different output channels.\n    \"\"\"\n    psnr_arr = []  # type: ignore\n    for i in range(len(self.running_psnr)):\n        psnr = self.running_psnr[i].get()\n        if psnr is None:\n            psnr_arr = None  # type: ignore\n            break\n        psnr_arr.append(psnr.cpu().numpy())\n        self.running_psnr[i].reset()\n        # TODO: this line forces it to be a method of this class\n        # alternative is returning also the reset `running_psnr`\n    if psnr_arr is not None:\n        psnr = np.mean(psnr_arr)\n    return psnr\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple[Tensor, Tensor]</code> <p>Input batch. It is a tuple with the input tensor and the target tensor. The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X), where C is the number of target channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Loss value.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def training_step(\n    self, batch: tuple[Tensor, Tensor], batch_idx: Any\n) -&gt; Optional[dict[str, Tensor]]:\n    \"\"\"Training step.\n\n    Parameters\n    ----------\n    batch : tuple[Tensor, Tensor]\n        Input batch. It is a tuple with the input tensor and the target tensor.\n        The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n        where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n        muSplit/denoiSplit).\n    batch_idx : Any\n        Batch index.\n\n    Returns\n    -------\n    Any\n        Loss value.\n    \"\"\"\n    x, target = batch\n\n    # Forward pass\n    out = self.model(x)\n\n    # Update loss parameters\n    self.loss_parameters.kl_params.current_epoch = self.current_epoch\n\n    # Compute loss\n    loss = self.loss_func(\n        model_outputs=out,\n        targets=target,\n        config=self.loss_parameters,\n        gaussian_likelihood=self.gaussian_likelihood,\n        noise_model_likelihood=self.noise_model_likelihood,\n    )\n\n    # Logging\n    # TODO: implement a separate logging method?\n    self.log_dict(loss, on_step=True, on_epoch=True)\n    # self.log(\"lr\", self, on_epoch=True)\n    return loss\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.VAEModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple[Tensor, Tensor]</code> <p>Input batch. It is a tuple with the input tensor and the target tensor. The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X), where C is the number of target channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>batch_idx</code> <code>Any</code> <p>Batch index.</p> required Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def validation_step(self, batch: tuple[Tensor, Tensor], batch_idx: Any) -&gt; None:\n    \"\"\"Validation step.\n\n    Parameters\n    ----------\n    batch : tuple[Tensor, Tensor]\n        Input batch. It is a tuple with the input tensor and the target tensor.\n        The input tensor has shape (B, (1 + n_LC), [Z], Y, X), where n_LC is the\n        number of lateral inputs. The target tensor has shape (B, C, [Z], Y, X),\n        where C is the number of target channels (e.g., 1 in HDN, &gt;1 in\n        muSplit/denoiSplit).\n    batch_idx : Any\n        Batch index.\n    \"\"\"\n    x, target = batch\n\n    # Forward pass\n    out = self.model(x)\n\n    # Compute loss\n    loss = self.loss_func(\n        model_outputs=out,\n        targets=target,\n        config=self.loss_parameters,\n        gaussian_likelihood=self.gaussian_likelihood,\n        noise_model_likelihood=self.noise_model_likelihood,\n    )\n\n    # Logging\n    # Rename val_loss dict\n    loss = {\"_\".join([\"val\", k]): v for k, v in loss.items()}\n    self.log_dict(loss, on_epoch=True, prog_bar=True)\n    curr_psnr = self.compute_val_psnr(out, target)\n    for i, psnr in enumerate(curr_psnr):\n        self.log(f\"val_psnr_ch{i+1}_batch\", psnr, on_epoch=True)\n</code></pre>"},{"location":"reference/careamics/lightning/lightning_module/#careamics.lightning.lightning_module.create_careamics_module","title":"<code>create_careamics_module(algorithm, loss, architecture, use_n2v2=False, struct_n2v_axis='none', struct_n2v_span=5, model_parameters=None, optimizer='Adam', optimizer_parameters=None, lr_scheduler='ReduceLROnPlateau', lr_scheduler_parameters=None)</code>","text":"<p>Create a CAREamics Lightning module.</p> <p>This function exposes parameters used to create an AlgorithmModel instance, triggering parameters validation.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>SupportedAlgorithm or str</code> <p>Algorithm to use for training (see SupportedAlgorithm).</p> required <code>loss</code> <code>SupportedLoss or str</code> <p>Loss function to use for training (see SupportedLoss).</p> required <code>architecture</code> <code>SupportedArchitecture or str</code> <p>Model architecture to use for training (see SupportedArchitecture).</p> required <code>use_n2v2</code> <code>bool</code> <p>Whether to use N2V2 or Noise2Void.</p> <code>False</code> <code>struct_n2v_axis</code> <code>\"horizontal\", \"vertical\", or \"none\"</code> <p>Axis of the StructN2V mask.</p> <code>\"none\"</code> <code>struct_n2v_span</code> <code>int</code> <p>Span of the StructN2V mask.</p> <code>5</code> <code>model_parameters</code> <code>dict</code> <p>Model parameters to use for training, by default {}. Model parameters are defined in the relevant <code>torch.nn.Module</code> class, or Pyddantic model (see <code>careamics.config.architectures</code>).</p> <code>None</code> <code>optimizer</code> <code>SupportedOptimizer or str</code> <p>Optimizer to use for training, by default \"Adam\" (see SupportedOptimizer).</p> <code>'Adam'</code> <code>optimizer_parameters</code> <code>dict</code> <p>Optimizer parameters to use for training, as defined in <code>torch.optim</code>, by default {}.</p> <code>None</code> <code>lr_scheduler</code> <code>SupportedScheduler or str</code> <p>Learning rate scheduler to use for training, by default \"ReduceLROnPlateau\" (see SupportedScheduler).</p> <code>'ReduceLROnPlateau'</code> <code>lr_scheduler_parameters</code> <code>dict</code> <p>Learning rate scheduler parameters to use for training, as defined in <code>torch.optim</code>, by default {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>CAREamicsModule</code> <p>CAREamics Lightning module.</p> Source code in <code>src/careamics/lightning/lightning_module.py</code> <pre><code>def create_careamics_module(\n    algorithm: Union[SupportedAlgorithm, str],\n    loss: Union[SupportedLoss, str],\n    architecture: Union[SupportedArchitecture, str],\n    use_n2v2: bool = False,\n    struct_n2v_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_n2v_span: int = 5,\n    model_parameters: Optional[dict] = None,\n    optimizer: Union[SupportedOptimizer, str] = \"Adam\",\n    optimizer_parameters: Optional[dict] = None,\n    lr_scheduler: Union[SupportedScheduler, str] = \"ReduceLROnPlateau\",\n    lr_scheduler_parameters: Optional[dict] = None,\n) -&gt; Union[FCNModule, VAEModule]:\n    \"\"\"Create a CAREamics Lightning module.\n\n    This function exposes parameters used to create an AlgorithmModel instance,\n    triggering parameters validation.\n\n    Parameters\n    ----------\n    algorithm : SupportedAlgorithm or str\n        Algorithm to use for training (see SupportedAlgorithm).\n    loss : SupportedLoss or str\n        Loss function to use for training (see SupportedLoss).\n    architecture : SupportedArchitecture or str\n        Model architecture to use for training (see SupportedArchitecture).\n    use_n2v2 : bool, default=False\n        Whether to use N2V2 or Noise2Void.\n    struct_n2v_axis : \"horizontal\", \"vertical\", or \"none\", default=\"none\"\n        Axis of the StructN2V mask.\n    struct_n2v_span : int, default=5\n        Span of the StructN2V mask.\n    model_parameters : dict, optional\n        Model parameters to use for training, by default {}. Model parameters are\n        defined in the relevant `torch.nn.Module` class, or Pyddantic model (see\n        `careamics.config.architectures`).\n    optimizer : SupportedOptimizer or str, optional\n        Optimizer to use for training, by default \"Adam\" (see SupportedOptimizer).\n    optimizer_parameters : dict, optional\n        Optimizer parameters to use for training, as defined in `torch.optim`, by\n        default {}.\n    lr_scheduler : SupportedScheduler or str, optional\n        Learning rate scheduler to use for training, by default \"ReduceLROnPlateau\"\n        (see SupportedScheduler).\n    lr_scheduler_parameters : dict, optional\n        Learning rate scheduler parameters to use for training, as defined in\n        `torch.optim`, by default {}.\n\n    Returns\n    -------\n    CAREamicsModule\n        CAREamics Lightning module.\n    \"\"\"\n    # TODO should use the same functions are in configuration_factory.py\n    # create an AlgorithmModel compatible dictionary\n    if lr_scheduler_parameters is None:\n        lr_scheduler_parameters = {}\n    if optimizer_parameters is None:\n        optimizer_parameters = {}\n    if model_parameters is None:\n        model_parameters = {}\n    algorithm_dict: dict[str, Any] = {\n        \"algorithm\": algorithm,\n        \"loss\": loss,\n        \"optimizer\": {\n            \"name\": optimizer,\n            \"parameters\": optimizer_parameters,\n        },\n        \"lr_scheduler\": {\n            \"name\": lr_scheduler,\n            \"parameters\": lr_scheduler_parameters,\n        },\n    }\n\n    model_dict = {\"architecture\": architecture}\n    model_dict.update(model_parameters)\n\n    # add model parameters to algorithm configuration\n    algorithm_dict[\"model\"] = model_dict\n\n    which_algo = algorithm_dict[\"algorithm\"]\n    if which_algo in UNetBasedAlgorithm.get_compatible_algorithms():\n        algorithm_cfg = algorithm_factory(algorithm_dict)\n\n        # if use N2V\n        if isinstance(algorithm_cfg, N2VAlgorithm):\n            algorithm_cfg.n2v_config.struct_mask_axis = struct_n2v_axis\n            algorithm_cfg.n2v_config.struct_mask_span = struct_n2v_span\n            algorithm_cfg.set_n2v2(use_n2v2)\n\n        return FCNModule(algorithm_cfg)\n    else:\n        raise NotImplementedError(\n            f\"Algorithm {which_algo} is not implemented or unknown.\"\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/","title":"predict_data_module","text":"<p>Prediction Lightning data modules.</p>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule","title":"<code>PredictDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>CAREamics Lightning prediction data module.</p> <p>The data module can be used with Path, str or numpy arrays. The data can be either a folder containing images or a single file.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>InferenceModel</code> <p>Pydantic model for CAREamics prediction configuration.</p> required <code>pred_data</code> <code>Path or str or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters, by default {}.</p> <code>None</code> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>class PredictDataModule(L.LightningDataModule):\n    \"\"\"\n    CAREamics Lightning prediction data module.\n\n    The data module can be used with Path, str or numpy arrays. The data can be either\n    a folder containing images or a single file.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    pred_config : InferenceModel\n        Pydantic model for CAREamics prediction configuration.\n    pred_data : pathlib.Path or str or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types, by default None.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types, by default \"\".\n    dataloader_params : dict, optional\n        Dataloader parameters, by default {}.\n    \"\"\"\n\n    def __init__(\n        self,\n        pred_config: InferenceConfig,\n        pred_data: Union[Path, str, NDArray],\n        read_source_func: Optional[Callable] = None,\n        extension_filter: str = \"\",\n        dataloader_params: Optional[dict] = None,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        The data module can be used with Path, str or numpy arrays. The data can be\n        either a folder containing images or a single file.\n\n        To read custom data types, you can set `data_type` to `custom` in `data_config`\n        and provide a function that returns a numpy array from a path as\n        `read_source_func` parameter. The function will receive a Path object and\n        an axies string as arguments, the axes being derived from the `data_config`.\n\n        You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n        \"*.czi\") to filter the files extension using `extension_filter`.\n\n        Parameters\n        ----------\n        pred_config : InferenceModel\n            Pydantic model for CAREamics prediction configuration.\n        pred_data : pathlib.Path or str or numpy.ndarray\n            Prediction data, can be a path to a folder, a file or a numpy array.\n        read_source_func : Callable, optional\n            Function to read custom types, by default None.\n        extension_filter : str, optional\n            Filter to filter file extensions for custom types, by default \"\".\n        dataloader_params : dict, optional\n            Dataloader parameters, by default {}.\n\n        Raises\n        ------\n        ValueError\n            If the data type is `custom` and no `read_source_func` is provided.\n        ValueError\n            If the data type is `array` and the input is not a numpy array.\n        ValueError\n            If the data type is `tiff` and the input is neither a Path nor a str.\n        \"\"\"\n        if dataloader_params is None:\n            dataloader_params = {}\n        if dataloader_params is None:\n            dataloader_params = {}\n        super().__init__()\n\n        # check that a read source function is provided for custom types\n        if pred_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n            raise ValueError(\n                f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n                f\"specifying a `read_source_func` and an `extension_filer`.\"\n            )\n\n        # check correct input type\n        if (\n            isinstance(pred_data, np.ndarray)\n            and pred_config.data_type != SupportedData.ARRAY\n        ):\n            raise ValueError(\n                f\"Received a numpy array as input, but the data type was set to \"\n                f\"{pred_config.data_type}. Set the data type \"\n                f\"to {SupportedData.ARRAY} to predict on numpy arrays.\"\n            )\n\n        # and that Path or str are passed, if tiff file type specified\n        elif (isinstance(pred_data, Path) or isinstance(pred_config, str)) and (\n            pred_config.data_type != SupportedData.TIFF\n            and pred_config.data_type != SupportedData.CUSTOM\n        ):\n            raise ValueError(\n                f\"Received a path as input, but the data type was neither set to \"\n                f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n                f\" to {SupportedData.TIFF} or \"\n                f\"{SupportedData.CUSTOM} to predict on files.\"\n            )\n\n        # configuration data\n        self.prediction_config = pred_config\n        self.data_type = pred_config.data_type\n        self.batch_size = pred_config.batch_size\n        self.dataloader_params = dataloader_params\n\n        self.pred_data = pred_data\n        self.tile_size = pred_config.tile_size\n        self.tile_overlap = pred_config.tile_overlap\n\n        # check if it is tiled\n        self.tiled = self.tile_size is not None and self.tile_overlap is not None\n\n        # read source function\n        if pred_config.data_type == SupportedData.CUSTOM:\n            # mypy check\n            assert read_source_func is not None\n\n            self.read_source_func: Callable = read_source_func\n        elif pred_config.data_type != SupportedData.ARRAY:\n            self.read_source_func = get_read_func(pred_config.data_type)\n\n        self.extension_filter = extension_filter\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n        # if the data is a Path or a str\n        if not isinstance(self.pred_data, np.ndarray):\n            self.pred_files = list_files(\n                self.pred_data, self.data_type, self.extension_filter\n            )\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Hook called at the beginning of predict.\n\n        Parameters\n        ----------\n        stage : Optional[str], optional\n            Stage, by default None.\n        \"\"\"\n        # if numpy array\n        if self.data_type == SupportedData.ARRAY:\n            if self.tiled:\n                self.predict_dataset: PredictDatasetType = InMemoryTiledPredDataset(\n                    prediction_config=self.prediction_config,\n                    inputs=self.pred_data,\n                )\n            else:\n                self.predict_dataset = InMemoryPredDataset(\n                    prediction_config=self.prediction_config,\n                    inputs=self.pred_data,\n                )\n        else:\n            if self.tiled:\n                self.predict_dataset = IterableTiledPredDataset(\n                    prediction_config=self.prediction_config,\n                    src_files=self.pred_files,\n                    read_source_func=self.read_source_func,\n                )\n            else:\n                self.predict_dataset = IterablePredDataset(\n                    prediction_config=self.prediction_config,\n                    src_files=self.pred_files,\n                    read_source_func=self.read_source_func,\n                )\n\n    def predict_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create a dataloader for prediction.\n\n        Returns\n        -------\n        DataLoader\n            Prediction dataloader.\n        \"\"\"\n        return DataLoader(\n            self.predict_dataset,\n            batch_size=self.batch_size,\n            collate_fn=collate_tiles if self.tiled else None,\n            **self.dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.__init__","title":"<code>__init__(pred_config, pred_data, read_source_func=None, extension_filter='', dataloader_params=None)</code>","text":"<p>Constructor.</p> <p>The data module can be used with Path, str or numpy arrays. The data can be either a folder containing images or a single file.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred_config</code> <code>InferenceModel</code> <p>Pydantic model for CAREamics prediction configuration.</p> required <code>pred_data</code> <code>Path or str or ndarray</code> <p>Prediction data, can be a path to a folder, a file or a numpy array.</p> required <code>read_source_func</code> <code>Callable</code> <p>Function to read custom types, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter to filter file extensions for custom types, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Dataloader parameters, by default {}.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data type is <code>custom</code> and no <code>read_source_func</code> is provided.</p> <code>ValueError</code> <p>If the data type is <code>array</code> and the input is not a numpy array.</p> <code>ValueError</code> <p>If the data type is <code>tiff</code> and the input is neither a Path nor a str.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def __init__(\n    self,\n    pred_config: InferenceConfig,\n    pred_data: Union[Path, str, NDArray],\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    dataloader_params: Optional[dict] = None,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    The data module can be used with Path, str or numpy arrays. The data can be\n    either a folder containing images or a single file.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    pred_config : InferenceModel\n        Pydantic model for CAREamics prediction configuration.\n    pred_data : pathlib.Path or str or numpy.ndarray\n        Prediction data, can be a path to a folder, a file or a numpy array.\n    read_source_func : Callable, optional\n        Function to read custom types, by default None.\n    extension_filter : str, optional\n        Filter to filter file extensions for custom types, by default \"\".\n    dataloader_params : dict, optional\n        Dataloader parameters, by default {}.\n\n    Raises\n    ------\n    ValueError\n        If the data type is `custom` and no `read_source_func` is provided.\n    ValueError\n        If the data type is `array` and the input is not a numpy array.\n    ValueError\n        If the data type is `tiff` and the input is neither a Path nor a str.\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n    if dataloader_params is None:\n        dataloader_params = {}\n    super().__init__()\n\n    # check that a read source function is provided for custom types\n    if pred_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n        raise ValueError(\n            f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n            f\"specifying a `read_source_func` and an `extension_filer`.\"\n        )\n\n    # check correct input type\n    if (\n        isinstance(pred_data, np.ndarray)\n        and pred_config.data_type != SupportedData.ARRAY\n    ):\n        raise ValueError(\n            f\"Received a numpy array as input, but the data type was set to \"\n            f\"{pred_config.data_type}. Set the data type \"\n            f\"to {SupportedData.ARRAY} to predict on numpy arrays.\"\n        )\n\n    # and that Path or str are passed, if tiff file type specified\n    elif (isinstance(pred_data, Path) or isinstance(pred_config, str)) and (\n        pred_config.data_type != SupportedData.TIFF\n        and pred_config.data_type != SupportedData.CUSTOM\n    ):\n        raise ValueError(\n            f\"Received a path as input, but the data type was neither set to \"\n            f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n            f\" to {SupportedData.TIFF} or \"\n            f\"{SupportedData.CUSTOM} to predict on files.\"\n        )\n\n    # configuration data\n    self.prediction_config = pred_config\n    self.data_type = pred_config.data_type\n    self.batch_size = pred_config.batch_size\n    self.dataloader_params = dataloader_params\n\n    self.pred_data = pred_data\n    self.tile_size = pred_config.tile_size\n    self.tile_overlap = pred_config.tile_overlap\n\n    # check if it is tiled\n    self.tiled = self.tile_size is not None and self.tile_overlap is not None\n\n    # read source function\n    if pred_config.data_type == SupportedData.CUSTOM:\n        # mypy check\n        assert read_source_func is not None\n\n        self.read_source_func: Callable = read_source_func\n    elif pred_config.data_type != SupportedData.ARRAY:\n        self.read_source_func = get_read_func(pred_config.data_type)\n\n    self.extension_filter = extension_filter\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create a dataloader for prediction.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Prediction dataloader.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create a dataloader for prediction.\n\n    Returns\n    -------\n    DataLoader\n        Prediction dataloader.\n    \"\"\"\n    return DataLoader(\n        self.predict_dataset,\n        batch_size=self.batch_size,\n        collate_fn=collate_tiles if self.tiled else None,\n        **self.dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Hook used to prepare the data before calling <code>setup</code>.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"Hook used to prepare the data before calling `setup`.\"\"\"\n    # if the data is a Path or a str\n    if not isinstance(self.pred_data, np.ndarray):\n        self.pred_files = list_files(\n            self.pred_data, self.data_type, self.extension_filter\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.PredictDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Hook called at the beginning of predict.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>Stage, by default None.</p> <code>None</code> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Hook called at the beginning of predict.\n\n    Parameters\n    ----------\n    stage : Optional[str], optional\n        Stage, by default None.\n    \"\"\"\n    # if numpy array\n    if self.data_type == SupportedData.ARRAY:\n        if self.tiled:\n            self.predict_dataset: PredictDatasetType = InMemoryTiledPredDataset(\n                prediction_config=self.prediction_config,\n                inputs=self.pred_data,\n            )\n        else:\n            self.predict_dataset = InMemoryPredDataset(\n                prediction_config=self.prediction_config,\n                inputs=self.pred_data,\n            )\n    else:\n        if self.tiled:\n            self.predict_dataset = IterableTiledPredDataset(\n                prediction_config=self.prediction_config,\n                src_files=self.pred_files,\n                read_source_func=self.read_source_func,\n            )\n        else:\n            self.predict_dataset = IterablePredDataset(\n                prediction_config=self.prediction_config,\n                src_files=self.pred_files,\n                read_source_func=self.read_source_func,\n            )\n</code></pre>"},{"location":"reference/careamics/lightning/predict_data_module/#careamics.lightning.predict_data_module.create_predict_datamodule","title":"<code>create_predict_datamodule(pred_data, data_type, axes, image_means, image_stds, tile_size=None, tile_overlap=None, batch_size=1, tta_transforms=True, read_source_func=None, extension_filter='', dataloader_params=None)</code>","text":"<p>Create a CAREamics prediction Lightning datamodule.</p> <p>This function is used to explicitly pass the parameters usually contained in an <code>inference_model</code> configuration.</p> <p>Since the lightning datamodule has no access to the model, make sure that the parameters passed to the datamodule are consistent with the model's requirements and are coherent. This can be done by creating a <code>Configuration</code> object beforehand and passing its parameters to the different Lightning modules.</p> <p>The data module can be used with Path, str or numpy arrays. To use array data, set <code>data_type</code> to <code>array</code> and pass a numpy array to <code>train_data</code>.</p> <p>By default, CAREamics only supports types defined in <code>careamics.config.support.SupportedData</code>. To read custom data types, you can set <code>data_type</code> to <code>custom</code> and provide a function that returns a numpy array from a path. Additionally, pass a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.jpeg\") to filter the files extension using <code>extension_filter</code>.</p> <p>In <code>dataloader_params</code>, you can pass any parameter accepted by PyTorch dataloaders, except for <code>batch_size</code>, which is set by the <code>batch_size</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>pred_data</code> <code>str or Path or ndarray</code> <p>Prediction data.</p> required <code>data_type</code> <code>(array, tiff, custom)</code> <p>Data type, see <code>SupportedData</code> for available options.</p> <code>\"array\"</code> <code>axes</code> <code>str</code> <p>Axes of the data, chosen among SCZYX.</p> required <code>image_means</code> <code>list of float</code> <p>Mean values for normalization, only used if Normalization is defined.</p> required <code>image_stds</code> <code>list of float</code> <p>Std values for normalization, only used if Normalization is defined.</p> required <code>tile_size</code> <code>tuple of int</code> <p>Tile size, 2D or 3D tile size.</p> <code>None</code> <code>tile_overlap</code> <code>tuple of int</code> <p>Tile overlap, 2D or 3D tile overlap.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>1</code> <code>tta_transforms</code> <code>bool</code> <p>Use test time augmentation, by default True.</p> <code>True</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>, by default \"\".</p> <code>''</code> <code>dataloader_params</code> <code>dict</code> <p>Pytorch dataloader parameters, by default {}.</p> <code>None</code> <p>Returns:</p> Type Description <code>PredictDataModule</code> <p>CAREamics prediction datamodule.</p> Notes <p>If you are using a UNet model and tiling, the tile size must be divisible in every dimension by 2**d, where d is the depth of the model. This avoids artefacts arising from the broken shift invariance induced by the pooling layers of the UNet. If your image has less dimensions, as it may happen in the Z dimension, consider padding your image.</p> Source code in <code>src/careamics/lightning/predict_data_module.py</code> <pre><code>def create_predict_datamodule(\n    pred_data: Union[str, Path, NDArray],\n    data_type: Union[Literal[\"array\", \"tiff\", \"custom\"], SupportedData],\n    axes: str,\n    image_means: list[float],\n    image_stds: list[float],\n    tile_size: Optional[tuple[int, ...]] = None,\n    tile_overlap: Optional[tuple[int, ...]] = None,\n    batch_size: int = 1,\n    tta_transforms: bool = True,\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    dataloader_params: Optional[dict] = None,\n) -&gt; PredictDataModule:\n    \"\"\"Create a CAREamics prediction Lightning datamodule.\n\n    This function is used to explicitly pass the parameters usually contained in an\n    `inference_model` configuration.\n\n    Since the lightning datamodule has no access to the model, make sure that the\n    parameters passed to the datamodule are consistent with the model's requirements\n    and are coherent. This can be done by creating a `Configuration` object beforehand\n    and passing its parameters to the different Lightning modules.\n\n    The data module can be used with Path, str or numpy arrays. To use array data, set\n    `data_type` to `array` and pass a numpy array to `train_data`.\n\n    By default, CAREamics only supports types defined in\n    `careamics.config.support.SupportedData`. To read custom data types, you can set\n    `data_type` to `custom` and provide a function that returns a numpy array from a\n    path. Additionally, pass a `fnmatch` and `Path.rglob` compatible expression\n    (e.g. \"*.jpeg\") to filter the files extension using `extension_filter`.\n\n    In `dataloader_params`, you can pass any parameter accepted by PyTorch\n    dataloaders, except for `batch_size`, which is set by the `batch_size`\n    parameter.\n\n    Parameters\n    ----------\n    pred_data : str or pathlib.Path or numpy.ndarray\n        Prediction data.\n    data_type : {\"array\", \"tiff\", \"custom\"}\n        Data type, see `SupportedData` for available options.\n    axes : str\n        Axes of the data, chosen among SCZYX.\n    image_means : list of float\n        Mean values for normalization, only used if Normalization is defined.\n    image_stds : list of float\n        Std values for normalization, only used if Normalization is defined.\n    tile_size : tuple of int, optional\n        Tile size, 2D or 3D tile size.\n    tile_overlap : tuple of int, optional\n        Tile overlap, 2D or 3D tile overlap.\n    batch_size : int\n        Batch size.\n    tta_transforms : bool, optional\n        Use test time augmentation, by default True.\n    read_source_func : Callable, optional\n        Function to read the source data, used if `data_type` is `custom`, by\n        default None.\n    extension_filter : str, optional\n        Filter for file extensions, used if `data_type` is `custom`, by default \"\".\n    dataloader_params : dict, optional\n        Pytorch dataloader parameters, by default {}.\n\n    Returns\n    -------\n    PredictDataModule\n        CAREamics prediction datamodule.\n\n    Notes\n    -----\n    If you are using a UNet model and tiling, the tile size must be\n    divisible in every dimension by 2**d, where d is the depth of the model. This\n    avoids artefacts arising from the broken shift invariance induced by the\n    pooling layers of the UNet. If your image has less dimensions, as it may\n    happen in the Z dimension, consider padding your image.\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n\n    prediction_dict: dict[str, Any] = {\n        \"data_type\": data_type,\n        \"tile_size\": tile_size,\n        \"tile_overlap\": tile_overlap,\n        \"axes\": axes,\n        \"image_means\": image_means,\n        \"image_stds\": image_stds,\n        \"tta_transforms\": tta_transforms,\n        \"batch_size\": batch_size,\n    }\n\n    # validate configuration\n    prediction_config = InferenceConfig(**prediction_dict)\n\n    # sanity check on the dataloader parameters\n    if \"batch_size\" in dataloader_params:\n        # remove it\n        del dataloader_params[\"batch_size\"]\n\n    return PredictDataModule(\n        pred_config=prediction_config,\n        pred_data=pred_data,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        dataloader_params=dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/","title":"train_data_module","text":"<p>Training and validation Lightning data modules.</p>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule","title":"<code>TrainDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>CAREamics Ligthning training and validation data module.</p> <p>The data module can be used with Path, str or numpy arrays. In the case of numpy arrays, it loads and computes all the patches in memory. For Path and str inputs, it calculates the total file size and estimate whether it can fit in memory. If it does not, it iterates through the files. This behaviour can be deactivated by setting <code>use_in_memory</code> to False, in which case it will always use the iterating dataset to train on a Path or str.</p> <p>The data can be either a folder containing images or a single file.</p> <p>Validation can be omitted, in which case the validation data is extracted from the training data. The percentage of the training data to use for validation, as well as the minimum number of patches or files to split from the training data can be set using <code>val_percentage</code> and <code>val_minimum_split</code>, respectively.</p> <p>To read custom data types, you can set <code>data_type</code> to <code>custom</code> in <code>data_config</code> and provide a function that returns a numpy array from a path as <code>read_source_func</code> parameter. The function will receive a Path object and an axies string as arguments, the axes being derived from the <code>data_config</code>.</p> <p>You can also provide a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.czi\") to filter the files extension using <code>extension_filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataModel</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data, can be a path to a folder, a file or a numpy array.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>train_data_target</code> <code>Path or str or ndarray</code> <p>Training target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>val_data_target</code> <code>Path or str or ndarray</code> <p>Validation target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, by default None. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\". Only used for <code>custom</code> data types (see DataModel).</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, by default 0.1. Only used if <code>val_data</code> is None.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, by default 5. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>data_config</code> <code>DataModel</code> <p>CAREamics data configuration.</p> <code>data_type</code> <code>SupportedData</code> <p>Expected data type, one of \"tiff\", \"array\" or \"custom\".</p> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>use_in_memory</code> <code>bool</code> <p>Whether to use in memory dataset if possible.</p> <code>train_data</code> <code>Path or ndarray</code> <p>Training data.</p> <code>val_data</code> <code>Path or ndarray</code> <p>Validation data.</p> <code>train_data_target</code> <code>Path or ndarray</code> <p>Training target data.</p> <code>val_data_target</code> <code>Path or ndarray</code> <p>Validation target data.</p> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, if no validation data is provided.</p> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, if no validation data is provided.</p> <code>read_source_func</code> <code>Optional[Callable]</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>.</p> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>class TrainDataModule(L.LightningDataModule):\n    \"\"\"\n    CAREamics Ligthning training and validation data module.\n\n    The data module can be used with Path, str or numpy arrays. In the case of\n    numpy arrays, it loads and computes all the patches in memory. For Path and str\n    inputs, it calculates the total file size and estimate whether it can fit in\n    memory. If it does not, it iterates through the files. This behaviour can be\n    deactivated by setting `use_in_memory` to False, in which case it will\n    always use the iterating dataset to train on a Path or str.\n\n    The data can be either a folder containing images or a single file.\n\n    Validation can be omitted, in which case the validation data is extracted from\n    the training data. The percentage of the training data to use for validation,\n    as well as the minimum number of patches or files to split from the training\n    data can be set using `val_percentage` and `val_minimum_split`, respectively.\n\n    To read custom data types, you can set `data_type` to `custom` in `data_config`\n    and provide a function that returns a numpy array from a path as\n    `read_source_func` parameter. The function will receive a Path object and\n    an axies string as arguments, the axes being derived from the `data_config`.\n\n    You can also provide a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.czi\") to filter the files extension using `extension_filter`.\n\n    Parameters\n    ----------\n    data_config : DataModel\n        Pydantic model for CAREamics data configuration.\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data, can be a path to a folder, a file or a numpy array.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    train_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    val_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, can be a path to a folder, a file or a numpy array,\n        by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, by default None. Only used for `custom`\n        data type (see DataModel).\n    extension_filter : str, optional\n        Filter for file extensions, by default \"\". Only used for `custom` data types\n        (see DataModel).\n    val_percentage : float, optional\n        Percentage of the training data to use for validation, by default 0.1. Only\n        used if `val_data` is None.\n    val_minimum_split : int, optional\n        Minimum number of patches or files to split from the training data for\n        validation, by default 5. Only used if `val_data` is None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Attributes\n    ----------\n    data_config : DataModel\n        CAREamics data configuration.\n    data_type : SupportedData\n        Expected data type, one of \"tiff\", \"array\" or \"custom\".\n    batch_size : int\n        Batch size.\n    use_in_memory : bool\n        Whether to use in memory dataset if possible.\n    train_data : pathlib.Path or numpy.ndarray\n        Training data.\n    val_data : pathlib.Path or numpy.ndarray\n        Validation data.\n    train_data_target : pathlib.Path or numpy.ndarray\n        Training target data.\n    val_data_target : pathlib.Path or numpy.ndarray\n        Validation target data.\n    val_percentage : float\n        Percentage of the training data to use for validation, if no validation data is\n        provided.\n    val_minimum_split : int\n        Minimum number of patches or files to split from the training data for\n        validation, if no validation data is provided.\n    read_source_func : Optional[Callable]\n        Function to read the source data, used if `data_type` is `custom`.\n    extension_filter : str\n        Filter for file extensions, used if `data_type` is `custom`.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DataConfig,\n        train_data: Union[Path, str, NDArray],\n        val_data: Optional[Union[Path, str, NDArray]] = None,\n        train_data_target: Optional[Union[Path, str, NDArray]] = None,\n        val_data_target: Optional[Union[Path, str, NDArray]] = None,\n        read_source_func: Optional[Callable] = None,\n        extension_filter: str = \"\",\n        val_percentage: float = 0.1,\n        val_minimum_split: int = 5,\n        use_in_memory: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        data_config : DataModel\n            Pydantic model for CAREamics data configuration.\n        train_data : pathlib.Path or str or numpy.ndarray\n            Training data, can be a path to a folder, a file or a numpy array.\n        val_data : pathlib.Path or str or numpy.ndarray, optional\n            Validation data, can be a path to a folder, a file or a numpy array, by\n            default None.\n        train_data_target : pathlib.Path or str or numpy.ndarray, optional\n            Training target data, can be a path to a folder, a file or a numpy array, by\n            default None.\n        val_data_target : pathlib.Path or str or numpy.ndarray, optional\n            Validation target data, can be a path to a folder, a file or a numpy array,\n            by default None.\n        read_source_func : Callable, optional\n            Function to read the source data, by default None. Only used for `custom`\n            data type (see DataModel).\n        extension_filter : str, optional\n            Filter for file extensions, by default \"\". Only used for `custom` data types\n            (see DataModel).\n        val_percentage : float, optional\n            Percentage of the training data to use for validation, by default 0.1. Only\n            used if `val_data` is None.\n        val_minimum_split : int, optional\n            Minimum number of patches or files to split from the training data for\n            validation, by default 5. Only used if `val_data` is None.\n        use_in_memory : bool, optional\n            Use in memory dataset if possible, by default True.\n\n        Raises\n        ------\n        NotImplementedError\n            Raised if target data is provided.\n        ValueError\n            If the input types are mixed (e.g. Path and numpy.ndarray).\n        ValueError\n            If the data type is `custom` and no `read_source_func` is provided.\n        ValueError\n            If the data type is `array` and the input is not a numpy array.\n        ValueError\n            If the data type is `tiff` and the input is neither a Path nor a str.\n        \"\"\"\n        super().__init__()\n\n        # check input types coherence (no mixed types)\n        inputs = [train_data, val_data, train_data_target, val_data_target]\n        types_set = {type(i) for i in inputs}\n        if len(types_set) &gt; 2:  # None + expected type\n            raise ValueError(\n                f\"Inputs for `train_data`, `val_data`, `train_data_target` and \"\n                f\"`val_data_target` must be of the same type or None. Got \"\n                f\"{types_set}.\"\n            )\n\n        # check that a read source function is provided for custom types\n        if data_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n            raise ValueError(\n                f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n                f\"specifying a `read_source_func` and an `extension_filer`.\"\n            )\n\n        # check correct input type\n        if (\n            isinstance(train_data, np.ndarray)\n            and data_config.data_type != SupportedData.ARRAY\n        ):\n            raise ValueError(\n                f\"Received a numpy array as input, but the data type was set to \"\n                f\"{data_config.data_type}. Set the data type in the configuration \"\n                f\"to {SupportedData.ARRAY} to train on numpy arrays.\"\n            )\n\n        # and that Path or str are passed, if tiff file type specified\n        elif (isinstance(train_data, Path) or isinstance(train_data, str)) and (\n            data_config.data_type != SupportedData.TIFF\n            and data_config.data_type != SupportedData.CUSTOM\n        ):\n            raise ValueError(\n                f\"Received a path as input, but the data type was neither set to \"\n                f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n                f\"in the configuration to {SupportedData.TIFF} or \"\n                f\"{SupportedData.CUSTOM} to train on files.\"\n            )\n\n        # configuration\n        self.data_config: DataConfig = data_config\n        self.data_type: str = data_config.data_type\n        self.batch_size: int = data_config.batch_size\n        self.use_in_memory: bool = use_in_memory\n\n        # data: make data Path or np.ndarray, use type annotations for mypy\n        self.train_data: Union[Path, NDArray] = (\n            Path(train_data) if isinstance(train_data, str) else train_data\n        )\n\n        self.val_data: Union[Path, NDArray] = (\n            Path(val_data) if isinstance(val_data, str) else val_data\n        )\n\n        self.train_data_target: Union[Path, NDArray] = (\n            Path(train_data_target)\n            if isinstance(train_data_target, str)\n            else train_data_target\n        )\n\n        self.val_data_target: Union[Path, NDArray] = (\n            Path(val_data_target)\n            if isinstance(val_data_target, str)\n            else val_data_target\n        )\n\n        # validation split\n        self.val_percentage = val_percentage\n        self.val_minimum_split = val_minimum_split\n\n        # read source function corresponding to the requested type\n        if data_config.data_type == SupportedData.CUSTOM.value:\n            # mypy check\n            assert read_source_func is not None\n\n            self.read_source_func: Callable = read_source_func\n\n        elif data_config.data_type != SupportedData.ARRAY:\n            self.read_source_func = get_read_func(data_config.data_type)\n\n        self.extension_filter: str = extension_filter\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Hook used to prepare the data before calling `setup`.\n\n        Here, we only need to examine the data if it was provided as a str or a Path.\n\n        TODO: from lightning doc:\n        prepare_data is called from the main process. It is not recommended to assign\n        state here (e.g. self.x = y) since it is called on a single process and if you\n        assign states here then they won't be available for other processes.\n\n        https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n        \"\"\"\n        # if the data is a Path or a str\n        if (\n            not isinstance(self.train_data, np.ndarray)\n            and not isinstance(self.val_data, np.ndarray)\n            and not isinstance(self.train_data_target, np.ndarray)\n            and not isinstance(self.val_data_target, np.ndarray)\n        ):\n            # list training files\n            self.train_files = list_files(\n                self.train_data, self.data_type, self.extension_filter\n            )\n            self.train_files_size = get_files_size(self.train_files)\n\n            # list validation files\n            if self.val_data is not None:\n                self.val_files = list_files(\n                    self.val_data, self.data_type, self.extension_filter\n                )\n\n            # same for target data\n            if self.train_data_target is not None:\n                self.train_target_files: list[Path] = list_files(\n                    self.train_data_target, self.data_type, self.extension_filter\n                )\n\n                # verify that they match the training data\n                validate_source_target_files(self.train_files, self.train_target_files)\n\n            if self.val_data_target is not None:\n                self.val_target_files = list_files(\n                    self.val_data_target, self.data_type, self.extension_filter\n                )\n\n                # verify that they match the validation data\n                validate_source_target_files(self.val_files, self.val_target_files)\n\n    def setup(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Hook called at the beginning of fit, validate, or predict.\n\n        Parameters\n        ----------\n        *args : Any\n            Unused.\n        **kwargs : Any\n            Unused.\n        \"\"\"\n        # if numpy array\n        if self.data_type == SupportedData.ARRAY:\n            # mypy checks\n            assert isinstance(self.train_data, np.ndarray)\n            if self.train_data_target is not None:\n                assert isinstance(self.train_data_target, np.ndarray)\n\n            # train dataset\n            self.train_dataset: DatasetType = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.train_data,\n                input_target=self.train_data_target,\n            )\n\n            # validation dataset\n            if self.val_data is not None:\n                # mypy checks\n                assert isinstance(self.val_data, np.ndarray)\n                if self.val_data_target is not None:\n                    assert isinstance(self.val_data_target, np.ndarray)\n\n                # create its own dataset\n                self.val_dataset: DatasetType = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.val_data,\n                    input_target=self.val_data_target,\n                )\n            else:\n                # extract validation from the training patches\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_patches=self.val_minimum_split,\n                )\n\n        # else we read files\n        else:\n            # Heuristics, if the file size is smaller than 80% of the RAM,\n            # we run the training in memory, otherwise we switch to iterable dataset\n            # The switch is deactivated if use_in_memory is False\n            if self.use_in_memory and self.train_files_size &lt; get_ram_size() * 0.8:\n                # train dataset\n                self.train_dataset = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.train_files,\n                    input_target=(\n                        self.train_target_files if self.train_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n\n                # validation dataset\n                if self.val_data is not None:\n                    self.val_dataset = InMemoryDataset(\n                        data_config=self.data_config,\n                        inputs=self.val_files,\n                        input_target=(\n                            self.val_target_files if self.val_data_target else None\n                        ),\n                        read_source_func=self.read_source_func,\n                    )\n                else:\n                    # split dataset\n                    self.val_dataset = self.train_dataset.split_dataset(\n                        percentage=self.val_percentage,\n                        minimum_patches=self.val_minimum_split,\n                    )\n\n            # else if the data is too large, load file by file during training\n            else:\n                # create training dataset\n                self.train_dataset = PathIterableDataset(\n                    data_config=self.data_config,\n                    src_files=self.train_files,\n                    target_files=(\n                        self.train_target_files if self.train_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n\n                # create validation dataset\n                if self.val_data is not None:\n                    # create its own dataset\n                    self.val_dataset = PathIterableDataset(\n                        data_config=self.data_config,\n                        src_files=self.val_files,\n                        target_files=(\n                            self.val_target_files if self.val_data_target else None\n                        ),\n                        read_source_func=self.read_source_func,\n                    )\n                elif len(self.train_files) &lt;= self.val_minimum_split:\n                    raise ValueError(\n                        f\"Not enough files to split a minimum of \"\n                        f\"{self.val_minimum_split} files, got {len(self.train_files)} \"\n                        f\"files.\"\n                    )\n                else:\n                    # extract validation from the training patches\n                    self.val_dataset = self.train_dataset.split_dataset(\n                        percentage=self.val_percentage,\n                        minimum_number=self.val_minimum_split,\n                    )\n\n    def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n        \"\"\"Return training data statistics.\n\n        Returns\n        -------\n        tuple of list\n            Means and standard deviations across channels of the training data.\n        \"\"\"\n        return self.train_dataset.get_data_statistics()\n\n    def train_dataloader(self) -&gt; Any:\n        \"\"\"\n        Create a dataloader for training.\n\n        Returns\n        -------\n        Any\n            Training dataloader.\n        \"\"\"\n        train_dataloader_params = self.data_config.train_dataloader_params.copy()\n\n        # NOTE: When next-gen datasets are completed this can be removed\n        # iterable dataset cannot be shuffled\n        if isinstance(self.train_dataset, IterableDataset):\n            del train_dataloader_params[\"shuffle\"]\n\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            **train_dataloader_params,\n        )\n\n    def val_dataloader(self) -&gt; Any:\n        \"\"\"\n        Create a dataloader for validation.\n\n        Returns\n        -------\n        Any\n            Validation dataloader.\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            **self.data_config.val_dataloader_params,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.__init__","title":"<code>__init__(data_config, train_data, val_data=None, train_data_target=None, val_data_target=None, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_split=5, use_in_memory=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data_config</code> <code>DataModel</code> <p>Pydantic model for CAREamics data configuration.</p> required <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data, can be a path to a folder, a file or a numpy array.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>train_data_target</code> <code>Path or str or ndarray</code> <p>Training target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>val_data_target</code> <code>Path or str or ndarray</code> <p>Validation target data, can be a path to a folder, a file or a numpy array, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, by default None. Only used for <code>custom</code> data type (see DataModel).</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, by default \"\". Only used for <code>custom</code> data types (see DataModel).</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation, by default 0.1. Only used if <code>val_data</code> is None.</p> <code>0.1</code> <code>val_minimum_split</code> <code>int</code> <p>Minimum number of patches or files to split from the training data for validation, by default 5. Only used if <code>val_data</code> is None.</p> <code>5</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Raised if target data is provided.</p> <code>ValueError</code> <p>If the input types are mixed (e.g. Path and numpy.ndarray).</p> <code>ValueError</code> <p>If the data type is <code>custom</code> and no <code>read_source_func</code> is provided.</p> <code>ValueError</code> <p>If the data type is <code>array</code> and the input is not a numpy array.</p> <code>ValueError</code> <p>If the data type is <code>tiff</code> and the input is neither a Path nor a str.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def __init__(\n    self,\n    data_config: DataConfig,\n    train_data: Union[Path, str, NDArray],\n    val_data: Optional[Union[Path, str, NDArray]] = None,\n    train_data_target: Optional[Union[Path, str, NDArray]] = None,\n    val_data_target: Optional[Union[Path, str, NDArray]] = None,\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_split: int = 5,\n    use_in_memory: bool = True,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    data_config : DataModel\n        Pydantic model for CAREamics data configuration.\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data, can be a path to a folder, a file or a numpy array.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    train_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, can be a path to a folder, a file or a numpy array, by\n        default None.\n    val_data_target : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, can be a path to a folder, a file or a numpy array,\n        by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, by default None. Only used for `custom`\n        data type (see DataModel).\n    extension_filter : str, optional\n        Filter for file extensions, by default \"\". Only used for `custom` data types\n        (see DataModel).\n    val_percentage : float, optional\n        Percentage of the training data to use for validation, by default 0.1. Only\n        used if `val_data` is None.\n    val_minimum_split : int, optional\n        Minimum number of patches or files to split from the training data for\n        validation, by default 5. Only used if `val_data` is None.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Raises\n    ------\n    NotImplementedError\n        Raised if target data is provided.\n    ValueError\n        If the input types are mixed (e.g. Path and numpy.ndarray).\n    ValueError\n        If the data type is `custom` and no `read_source_func` is provided.\n    ValueError\n        If the data type is `array` and the input is not a numpy array.\n    ValueError\n        If the data type is `tiff` and the input is neither a Path nor a str.\n    \"\"\"\n    super().__init__()\n\n    # check input types coherence (no mixed types)\n    inputs = [train_data, val_data, train_data_target, val_data_target]\n    types_set = {type(i) for i in inputs}\n    if len(types_set) &gt; 2:  # None + expected type\n        raise ValueError(\n            f\"Inputs for `train_data`, `val_data`, `train_data_target` and \"\n            f\"`val_data_target` must be of the same type or None. Got \"\n            f\"{types_set}.\"\n        )\n\n    # check that a read source function is provided for custom types\n    if data_config.data_type == SupportedData.CUSTOM and read_source_func is None:\n        raise ValueError(\n            f\"Data type {SupportedData.CUSTOM} is not allowed without \"\n            f\"specifying a `read_source_func` and an `extension_filer`.\"\n        )\n\n    # check correct input type\n    if (\n        isinstance(train_data, np.ndarray)\n        and data_config.data_type != SupportedData.ARRAY\n    ):\n        raise ValueError(\n            f\"Received a numpy array as input, but the data type was set to \"\n            f\"{data_config.data_type}. Set the data type in the configuration \"\n            f\"to {SupportedData.ARRAY} to train on numpy arrays.\"\n        )\n\n    # and that Path or str are passed, if tiff file type specified\n    elif (isinstance(train_data, Path) or isinstance(train_data, str)) and (\n        data_config.data_type != SupportedData.TIFF\n        and data_config.data_type != SupportedData.CUSTOM\n    ):\n        raise ValueError(\n            f\"Received a path as input, but the data type was neither set to \"\n            f\"{SupportedData.TIFF} nor {SupportedData.CUSTOM}. Set the data type \"\n            f\"in the configuration to {SupportedData.TIFF} or \"\n            f\"{SupportedData.CUSTOM} to train on files.\"\n        )\n\n    # configuration\n    self.data_config: DataConfig = data_config\n    self.data_type: str = data_config.data_type\n    self.batch_size: int = data_config.batch_size\n    self.use_in_memory: bool = use_in_memory\n\n    # data: make data Path or np.ndarray, use type annotations for mypy\n    self.train_data: Union[Path, NDArray] = (\n        Path(train_data) if isinstance(train_data, str) else train_data\n    )\n\n    self.val_data: Union[Path, NDArray] = (\n        Path(val_data) if isinstance(val_data, str) else val_data\n    )\n\n    self.train_data_target: Union[Path, NDArray] = (\n        Path(train_data_target)\n        if isinstance(train_data_target, str)\n        else train_data_target\n    )\n\n    self.val_data_target: Union[Path, NDArray] = (\n        Path(val_data_target)\n        if isinstance(val_data_target, str)\n        else val_data_target\n    )\n\n    # validation split\n    self.val_percentage = val_percentage\n    self.val_minimum_split = val_minimum_split\n\n    # read source function corresponding to the requested type\n    if data_config.data_type == SupportedData.CUSTOM.value:\n        # mypy check\n        assert read_source_func is not None\n\n        self.read_source_func: Callable = read_source_func\n\n    elif data_config.data_type != SupportedData.ARRAY:\n        self.read_source_func = get_read_func(data_config.data_type)\n\n    self.extension_filter: str = extension_filter\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.get_data_statistics","title":"<code>get_data_statistics()</code>","text":"<p>Return training data statistics.</p> <p>Returns:</p> Type Description <code>tuple of list</code> <p>Means and standard deviations across channels of the training data.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def get_data_statistics(self) -&gt; tuple[list[float], list[float]]:\n    \"\"\"Return training data statistics.\n\n    Returns\n    -------\n    tuple of list\n        Means and standard deviations across channels of the training data.\n    \"\"\"\n    return self.train_dataset.get_data_statistics()\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Hook used to prepare the data before calling <code>setup</code>.</p> <p>Here, we only need to examine the data if it was provided as a str or a Path.</p> <p>TODO: from lightning doc: prepare_data is called from the main process. It is not recommended to assign state here (e.g. self.x = y) since it is called on a single process and if you assign states here then they won't be available for other processes.</p> <p>https://lightning.ai/docs/pytorch/stable/data/datamodule.html</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Hook used to prepare the data before calling `setup`.\n\n    Here, we only need to examine the data if it was provided as a str or a Path.\n\n    TODO: from lightning doc:\n    prepare_data is called from the main process. It is not recommended to assign\n    state here (e.g. self.x = y) since it is called on a single process and if you\n    assign states here then they won't be available for other processes.\n\n    https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n    \"\"\"\n    # if the data is a Path or a str\n    if (\n        not isinstance(self.train_data, np.ndarray)\n        and not isinstance(self.val_data, np.ndarray)\n        and not isinstance(self.train_data_target, np.ndarray)\n        and not isinstance(self.val_data_target, np.ndarray)\n    ):\n        # list training files\n        self.train_files = list_files(\n            self.train_data, self.data_type, self.extension_filter\n        )\n        self.train_files_size = get_files_size(self.train_files)\n\n        # list validation files\n        if self.val_data is not None:\n            self.val_files = list_files(\n                self.val_data, self.data_type, self.extension_filter\n            )\n\n        # same for target data\n        if self.train_data_target is not None:\n            self.train_target_files: list[Path] = list_files(\n                self.train_data_target, self.data_type, self.extension_filter\n            )\n\n            # verify that they match the training data\n            validate_source_target_files(self.train_files, self.train_target_files)\n\n        if self.val_data_target is not None:\n            self.val_target_files = list_files(\n                self.val_data_target, self.data_type, self.extension_filter\n            )\n\n            # verify that they match the validation data\n            validate_source_target_files(self.val_files, self.val_target_files)\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.setup","title":"<code>setup(*args, **kwargs)</code>","text":"<p>Hook called at the beginning of fit, validate, or predict.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Unused.</p> <code>{}</code> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def setup(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Hook called at the beginning of fit, validate, or predict.\n\n    Parameters\n    ----------\n    *args : Any\n        Unused.\n    **kwargs : Any\n        Unused.\n    \"\"\"\n    # if numpy array\n    if self.data_type == SupportedData.ARRAY:\n        # mypy checks\n        assert isinstance(self.train_data, np.ndarray)\n        if self.train_data_target is not None:\n            assert isinstance(self.train_data_target, np.ndarray)\n\n        # train dataset\n        self.train_dataset: DatasetType = InMemoryDataset(\n            data_config=self.data_config,\n            inputs=self.train_data,\n            input_target=self.train_data_target,\n        )\n\n        # validation dataset\n        if self.val_data is not None:\n            # mypy checks\n            assert isinstance(self.val_data, np.ndarray)\n            if self.val_data_target is not None:\n                assert isinstance(self.val_data_target, np.ndarray)\n\n            # create its own dataset\n            self.val_dataset: DatasetType = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.val_data,\n                input_target=self.val_data_target,\n            )\n        else:\n            # extract validation from the training patches\n            self.val_dataset = self.train_dataset.split_dataset(\n                percentage=self.val_percentage,\n                minimum_patches=self.val_minimum_split,\n            )\n\n    # else we read files\n    else:\n        # Heuristics, if the file size is smaller than 80% of the RAM,\n        # we run the training in memory, otherwise we switch to iterable dataset\n        # The switch is deactivated if use_in_memory is False\n        if self.use_in_memory and self.train_files_size &lt; get_ram_size() * 0.8:\n            # train dataset\n            self.train_dataset = InMemoryDataset(\n                data_config=self.data_config,\n                inputs=self.train_files,\n                input_target=(\n                    self.train_target_files if self.train_data_target else None\n                ),\n                read_source_func=self.read_source_func,\n            )\n\n            # validation dataset\n            if self.val_data is not None:\n                self.val_dataset = InMemoryDataset(\n                    data_config=self.data_config,\n                    inputs=self.val_files,\n                    input_target=(\n                        self.val_target_files if self.val_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n            else:\n                # split dataset\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_patches=self.val_minimum_split,\n                )\n\n        # else if the data is too large, load file by file during training\n        else:\n            # create training dataset\n            self.train_dataset = PathIterableDataset(\n                data_config=self.data_config,\n                src_files=self.train_files,\n                target_files=(\n                    self.train_target_files if self.train_data_target else None\n                ),\n                read_source_func=self.read_source_func,\n            )\n\n            # create validation dataset\n            if self.val_data is not None:\n                # create its own dataset\n                self.val_dataset = PathIterableDataset(\n                    data_config=self.data_config,\n                    src_files=self.val_files,\n                    target_files=(\n                        self.val_target_files if self.val_data_target else None\n                    ),\n                    read_source_func=self.read_source_func,\n                )\n            elif len(self.train_files) &lt;= self.val_minimum_split:\n                raise ValueError(\n                    f\"Not enough files to split a minimum of \"\n                    f\"{self.val_minimum_split} files, got {len(self.train_files)} \"\n                    f\"files.\"\n                )\n            else:\n                # extract validation from the training patches\n                self.val_dataset = self.train_dataset.split_dataset(\n                    percentage=self.val_percentage,\n                    minimum_number=self.val_minimum_split,\n                )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create a dataloader for training.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Training dataloader.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def train_dataloader(self) -&gt; Any:\n    \"\"\"\n    Create a dataloader for training.\n\n    Returns\n    -------\n    Any\n        Training dataloader.\n    \"\"\"\n    train_dataloader_params = self.data_config.train_dataloader_params.copy()\n\n    # NOTE: When next-gen datasets are completed this can be removed\n    # iterable dataset cannot be shuffled\n    if isinstance(self.train_dataset, IterableDataset):\n        del train_dataloader_params[\"shuffle\"]\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        **train_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.TrainDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create a dataloader for validation.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Validation dataloader.</p> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def val_dataloader(self) -&gt; Any:\n    \"\"\"\n    Create a dataloader for validation.\n\n    Returns\n    -------\n    Any\n        Validation dataloader.\n    \"\"\"\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        **self.data_config.val_dataloader_params,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/train_data_module/#careamics.lightning.train_data_module.create_train_datamodule","title":"<code>create_train_datamodule(train_data, data_type, patch_size, axes, batch_size, val_data=None, transforms=None, train_target_data=None, val_target_data=None, read_source_func=None, extension_filter='', val_percentage=0.1, val_minimum_patches=5, dataloader_params=None, use_in_memory=True)</code>","text":"<p>Create a TrainDataModule.</p> <p>This function is used to explicitly pass the parameters usually contained in a <code>GenericDataConfig</code> to a TrainDataModule.</p> <p>Since the lightning datamodule has no access to the model, make sure that the parameters passed to the datamodule are consistent with the model's requirements and are coherent.</p> <p>The default augmentations are XY flip and XY rotation. To use a different set of transformations, you can pass a list of transforms to <code>transforms</code>.</p> <p>The data module can be used with Path, str or numpy arrays. In the case of numpy arrays, it loads and computes all the patches in memory. For Path and str inputs, it calculates the total file size and estimate whether it can fit in memory. If it does not, it iterates through the files. This behaviour can be deactivated by setting <code>use_in_memory</code> to False, in which case it will always use the iterating dataset to train on a Path or str.</p> <p>To use array data, set <code>data_type</code> to <code>array</code> and pass a numpy array to <code>train_data</code>.</p> <p>By default, CAREamics only supports types defined in <code>careamics.config.support.SupportedData</code>. To read custom data types, you can set <code>data_type</code> to <code>custom</code> and provide a function that returns a numpy array from a path. Additionally, pass a <code>fnmatch</code> and <code>Path.rglob</code> compatible expression (e.g. \"*.jpeg\") to filter the files extension using <code>extension_filter</code>.</p> <p>In the absence of validation data, the validation data is extracted from the training data. The percentage of the training data to use for validation, as well as the minimum number of patches to split from the training data for validation can be set using <code>val_percentage</code> and <code>val_minimum_patches</code>, respectively.</p> <p>In <code>dataloader_params</code>, you can pass any parameter accepted by PyTorch dataloaders, except for <code>batch_size</code>, which is set by the <code>batch_size</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Path or str or ndarray</code> <p>Training data.</p> required <code>data_type</code> <code>(array, tiff, custom)</code> <p>Data type, see <code>SupportedData</code> for available options.</p> <code>\"array\"</code> <code>patch_size</code> <code>list of int</code> <p>Patch size, 2D or 3D patch size.</p> required <code>axes</code> <code>str</code> <p>Axes of the data, chosen amongst SCZYX.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <code>val_data</code> <code>Path or str or ndarray</code> <p>Validation data, by default None.</p> <code>None</code> <code>transforms</code> <code>list of Transforms</code> <p>List of transforms to apply to training patches. If None, default transforms are applied.</p> <code>None</code> <code>train_target_data</code> <code>Path or str or ndarray</code> <p>Training target data, by default None.</p> <code>None</code> <code>val_target_data</code> <code>Path or str or ndarray</code> <p>Validation target data, by default None.</p> <code>None</code> <code>read_source_func</code> <code>Callable</code> <p>Function to read the source data, used if <code>data_type</code> is <code>custom</code>, by default None.</p> <code>None</code> <code>extension_filter</code> <code>str</code> <p>Filter for file extensions, used if <code>data_type</code> is <code>custom</code>, by default \"\".</p> <code>''</code> <code>val_percentage</code> <code>float</code> <p>Percentage of the training data to use for validation if no validation data is given, by default 0.1.</p> <code>0.1</code> <code>val_minimum_patches</code> <code>int</code> <p>Minimum number of patches to split from the training data for validation if no validation data is given, by default 5.</p> <code>5</code> <code>dataloader_params</code> <code>dict</code> <p>Pytorch dataloader parameters, by default {}.</p> <code>None</code> <code>use_in_memory</code> <code>bool</code> <p>Use in memory dataset if possible, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>TrainDataModule</code> <p>CAREamics training Lightning data module.</p> <p>Examples:</p> <p>Create a TrainingDataModule with default transforms with a numpy array:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=my_array,\n...     data_type=\"array\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n... )\n</code></pre> <p>For custom data types (those not supported by CAREamics), then one can pass a read function and a filter for the files extension:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt;\n&gt;&gt;&gt; def read_npy(path):\n...     return np.load(path)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=\"path/to/data\",\n...     data_type=\"custom\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n...     read_source_func=read_npy,\n...     extension_filter=\"*.npy\",\n... )\n</code></pre> <p>If you want to use a different set of transformations, you can pass a list of transforms:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n&gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n&gt;&gt;&gt; from careamics.config.support import SupportedTransform\n&gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n&gt;&gt;&gt; my_transforms = [\n...     XYFlipModel(flip_y=False),\n... ]\n&gt;&gt;&gt; data_module = create_train_datamodule(\n...     train_data=my_array,\n...     data_type=\"array\",\n...     patch_size=(8, 8),\n...     axes='YX',\n...     batch_size=2,\n...     transforms=my_transforms,\n... )\n</code></pre> Source code in <code>src/careamics/lightning/train_data_module.py</code> <pre><code>def create_train_datamodule(\n    train_data: Union[str, Path, NDArray],\n    data_type: Union[Literal[\"array\", \"tiff\", \"custom\"], SupportedData],\n    patch_size: list[int],\n    axes: str,\n    batch_size: int,\n    val_data: Optional[Union[str, Path, NDArray]] = None,\n    transforms: Optional[list[TransformModel]] = None,\n    train_target_data: Optional[Union[str, Path, NDArray]] = None,\n    val_target_data: Optional[Union[str, Path, NDArray]] = None,\n    read_source_func: Optional[Callable] = None,\n    extension_filter: str = \"\",\n    val_percentage: float = 0.1,\n    val_minimum_patches: int = 5,\n    dataloader_params: Optional[dict] = None,\n    use_in_memory: bool = True,\n) -&gt; TrainDataModule:\n    \"\"\"Create a TrainDataModule.\n\n    This function is used to explicitly pass the parameters usually contained in a\n    `GenericDataConfig` to a TrainDataModule.\n\n    Since the lightning datamodule has no access to the model, make sure that the\n    parameters passed to the datamodule are consistent with the model's requirements and\n    are coherent.\n\n    The default augmentations are XY flip and XY rotation. To use a different set of\n    transformations, you can pass a list of transforms to `transforms`.\n\n    The data module can be used with Path, str or numpy arrays. In the case of\n    numpy arrays, it loads and computes all the patches in memory. For Path and str\n    inputs, it calculates the total file size and estimate whether it can fit in\n    memory. If it does not, it iterates through the files. This behaviour can be\n    deactivated by setting `use_in_memory` to False, in which case it will\n    always use the iterating dataset to train on a Path or str.\n\n    To use array data, set `data_type` to `array` and pass a numpy array to\n    `train_data`.\n\n    By default, CAREamics only supports types defined in\n    `careamics.config.support.SupportedData`. To read custom data types, you can set\n    `data_type` to `custom` and provide a function that returns a numpy array from a\n    path. Additionally, pass a `fnmatch` and `Path.rglob` compatible expression (e.g.\n    \"*.jpeg\") to filter the files extension using `extension_filter`.\n\n    In the absence of validation data, the validation data is extracted from the\n    training data. The percentage of the training data to use for validation, as well as\n    the minimum number of patches to split from the training data for validation can be\n    set using `val_percentage` and `val_minimum_patches`, respectively.\n\n    In `dataloader_params`, you can pass any parameter accepted by PyTorch dataloaders,\n    except for `batch_size`, which is set by the `batch_size` parameter.\n\n    Parameters\n    ----------\n    train_data : pathlib.Path or str or numpy.ndarray\n        Training data.\n    data_type : {\"array\", \"tiff\", \"custom\"}\n        Data type, see `SupportedData` for available options.\n    patch_size : list of int\n        Patch size, 2D or 3D patch size.\n    axes : str\n        Axes of the data, chosen amongst SCZYX.\n    batch_size : int\n        Batch size.\n    val_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation data, by default None.\n    transforms : list of Transforms, optional\n        List of transforms to apply to training patches. If None, default transforms\n        are applied.\n    train_target_data : pathlib.Path or str or numpy.ndarray, optional\n        Training target data, by default None.\n    val_target_data : pathlib.Path or str or numpy.ndarray, optional\n        Validation target data, by default None.\n    read_source_func : Callable, optional\n        Function to read the source data, used if `data_type` is `custom`, by\n        default None.\n    extension_filter : str, optional\n        Filter for file extensions, used if `data_type` is `custom`, by default \"\".\n    val_percentage : float, optional\n        Percentage of the training data to use for validation if no validation data\n        is given, by default 0.1.\n    val_minimum_patches : int, optional\n        Minimum number of patches to split from the training data for validation if\n        no validation data is given, by default 5.\n    dataloader_params : dict, optional\n        Pytorch dataloader parameters, by default {}.\n    use_in_memory : bool, optional\n        Use in memory dataset if possible, by default True.\n\n    Returns\n    -------\n    TrainDataModule\n        CAREamics training Lightning data module.\n\n    Examples\n    --------\n    Create a TrainingDataModule with default transforms with a numpy array:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=my_array,\n    ...     data_type=\"array\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ... )\n\n    For custom data types (those not supported by CAREamics), then one can pass a read\n    function and a filter for the files extension:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; def read_npy(path):\n    ...     return np.load(path)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=\"path/to/data\",\n    ...     data_type=\"custom\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ...     read_source_func=read_npy,\n    ...     extension_filter=\"*.npy\",\n    ... )\n\n    If you want to use a different set of transformations, you can pass a list of\n    transforms:\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from careamics.lightning import create_train_datamodule\n    &gt;&gt;&gt; from careamics.config.transformations import XYFlipModel\n    &gt;&gt;&gt; from careamics.config.support import SupportedTransform\n    &gt;&gt;&gt; my_array = np.arange(256).reshape(16, 16)\n    &gt;&gt;&gt; my_transforms = [\n    ...     XYFlipModel(flip_y=False),\n    ... ]\n    &gt;&gt;&gt; data_module = create_train_datamodule(\n    ...     train_data=my_array,\n    ...     data_type=\"array\",\n    ...     patch_size=(8, 8),\n    ...     axes='YX',\n    ...     batch_size=2,\n    ...     transforms=my_transforms,\n    ... )\n    \"\"\"\n    if dataloader_params is None:\n        dataloader_params = {}\n\n    data_dict: dict[str, Any] = {\n        \"mode\": \"train\",\n        \"data_type\": data_type,\n        \"patch_size\": patch_size,\n        \"axes\": axes,\n        \"batch_size\": batch_size,\n        \"dataloader_params\": dataloader_params,\n    }\n\n    # if transforms are passed (otherwise it will use the default ones)\n    if transforms is not None:\n        data_dict[\"transforms\"] = transforms\n\n    # instantiate data configuration\n    data_config = DataConfig(**data_dict)\n\n    # sanity check on the dataloader parameters\n    if \"batch_size\" in dataloader_params:\n        # remove it\n        del dataloader_params[\"batch_size\"]\n\n    return TrainDataModule(\n        data_config=data_config,\n        train_data=train_data,\n        val_data=val_data,\n        train_data_target=train_target_data,\n        val_data_target=val_target_data,\n        read_source_func=read_source_func,\n        extension_filter=extension_filter,\n        val_percentage=val_percentage,\n        val_minimum_split=val_minimum_patches,\n        use_in_memory=use_in_memory,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/","title":"hyperparameters_callback","text":"<p>Callback saving CAREamics configuration as hyperparameters in the model.</p>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback","title":"<code>HyperParametersCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback allowing saving CAREamics configuration as hyperparameters in the model.</p> <p>This allows saving the configuration as dictionary in the checkpoints, and loading it subsequently in a CAREamist instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>class HyperParametersCallback(Callback):\n    \"\"\"\n    Callback allowing saving CAREamics configuration as hyperparameters in the model.\n\n    This allows saving the configuration as dictionary in the checkpoints, and\n    loading it subsequently in a CAREamist instance.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n\n    Attributes\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n    \"\"\"\n\n    def __init__(self, config: Configuration) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        config : Configuration\n            CAREamics configuration to be saved as hyperparameter in the model.\n        \"\"\"\n        self.config = config\n\n    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"\n        Update the hyperparameters of the model with the configuration on train start.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer, unused.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        pl_module.hparams.update(self.config.model_dump())\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback.__init__","title":"<code>__init__(config)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration to be saved as hyperparameter in the model.</p> required Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>def __init__(self, config: Configuration) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration to be saved as hyperparameter in the model.\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/hyperparameters_callback/#careamics.lightning.callbacks.hyperparameters_callback.HyperParametersCallback.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Update the hyperparameters of the model with the configuration on train start.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer, unused.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics/lightning/callbacks/hyperparameters_callback.py</code> <pre><code>def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"\n    Update the hyperparameters of the model with the configuration on train start.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer, unused.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    pl_module.hparams.update(self.config.model_dump())\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/","title":"progress_bar_callback","text":"<p>Progressbar callback.</p>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback","title":"<code>ProgressBarCallback</code>","text":"<p>               Bases: <code>TQDMProgressBar</code></p> <p>Progress bar for training and validation steps.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>class ProgressBarCallback(TQDMProgressBar):\n    \"\"\"Progress bar for training and validation steps.\"\"\"\n\n    def init_train_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for training.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        bar = tqdm(\n            desc=\"Training\",\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=True,\n            file=sys.stdout,\n            smoothing=0,\n        )\n        return bar\n\n    def init_validation_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for validation.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        # The main progress bar doesn't exist in `trainer.validate()`\n        has_main_bar = self.train_progress_bar is not None\n        bar = tqdm(\n            desc=\"Validating\",\n            position=(2 * self.process_position + has_main_bar),\n            disable=self.is_disabled,\n            leave=False,\n            dynamic_ncols=True,\n            file=sys.stdout,\n        )\n        return bar\n\n    def init_test_tqdm(self) -&gt; tqdm:\n        \"\"\"Override this to customize the tqdm bar for testing.\n\n        Returns\n        -------\n        tqdm\n            A tqdm bar.\n        \"\"\"\n        bar = tqdm(\n            desc=\"Testing\",\n            position=(2 * self.process_position),\n            disable=self.is_disabled,\n            leave=True,\n            dynamic_ncols=False,\n            ncols=100,\n            file=sys.stdout,\n        )\n        return bar\n\n    def get_metrics(\n        self, trainer: Trainer, pl_module: LightningModule\n    ) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n        \"\"\"Override this to customize the metrics displayed in the progress bar.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            The trainer object.\n        pl_module : LightningModule\n            The LightningModule object, unused.\n\n        Returns\n        -------\n        dict\n            A dictionary with the metrics to display in the progress bar.\n        \"\"\"\n        pbar_metrics = trainer.progress_bar_metrics\n        return {**pbar_metrics}\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.get_metrics","title":"<code>get_metrics(trainer, pl_module)</code>","text":"<p>Override this to customize the metrics displayed in the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>The trainer object.</p> required <code>pl_module</code> <code>LightningModule</code> <p>The LightningModule object, unused.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with the metrics to display in the progress bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def get_metrics(\n    self, trainer: Trainer, pl_module: LightningModule\n) -&gt; dict[str, Union[int, str, float, dict[str, float]]]:\n    \"\"\"Override this to customize the metrics displayed in the progress bar.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        The trainer object.\n    pl_module : LightningModule\n        The LightningModule object, unused.\n\n    Returns\n    -------\n    dict\n        A dictionary with the metrics to display in the progress bar.\n    \"\"\"\n    pbar_metrics = trainer.progress_bar_metrics\n    return {**pbar_metrics}\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_test_tqdm","title":"<code>init_test_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for testing.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_test_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for testing.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    bar = tqdm(\n        desc=\"Testing\",\n        position=(2 * self.process_position),\n        disable=self.is_disabled,\n        leave=True,\n        dynamic_ncols=False,\n        ncols=100,\n        file=sys.stdout,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_train_tqdm","title":"<code>init_train_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for training.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_train_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for training.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    bar = tqdm(\n        desc=\"Training\",\n        position=(2 * self.process_position),\n        disable=self.is_disabled,\n        leave=True,\n        dynamic_ncols=True,\n        file=sys.stdout,\n        smoothing=0,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/progress_bar_callback/#careamics.lightning.callbacks.progress_bar_callback.ProgressBarCallback.init_validation_tqdm","title":"<code>init_validation_tqdm()</code>","text":"<p>Override this to customize the tqdm bar for validation.</p> <p>Returns:</p> Type Description <code>tqdm</code> <p>A tqdm bar.</p> Source code in <code>src/careamics/lightning/callbacks/progress_bar_callback.py</code> <pre><code>def init_validation_tqdm(self) -&gt; tqdm:\n    \"\"\"Override this to customize the tqdm bar for validation.\n\n    Returns\n    -------\n    tqdm\n        A tqdm bar.\n    \"\"\"\n    # The main progress bar doesn't exist in `trainer.validate()`\n    has_main_bar = self.train_progress_bar is not None\n    bar = tqdm(\n        desc=\"Validating\",\n        position=(2 * self.process_position + has_main_bar),\n        disable=self.is_disabled,\n        leave=False,\n        dynamic_ncols=True,\n        file=sys.stdout,\n    )\n    return bar\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/","title":"file_path_utils","text":"<p>Module containing file path utilities for <code>WriteStrategy</code> to use.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/#careamics.lightning.callbacks.prediction_writer_callback.file_path_utils.create_write_file_path","title":"<code>create_write_file_path(dirpath, file_path, write_extension)</code>","text":"<p>Create the file name for the output file.</p> <p>Takes the original file path, changes the directory to <code>dirpath</code> and changes the extension to <code>write_extension</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>Path</code> <p>The output directory to write file to.</p> required <code>file_path</code> <code>Path</code> <p>The original file path.</p> required <code>write_extension</code> <code>str</code> <p>The extension that output files should have.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The output file path.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils.py</code> <pre><code>def create_write_file_path(\n    dirpath: Path, file_path: Path, write_extension: str\n) -&gt; Path:\n    \"\"\"\n    Create the file name for the output file.\n\n    Takes the original file path, changes the directory to `dirpath` and changes\n    the extension to `write_extension`.\n\n    Parameters\n    ----------\n    dirpath : pathlib.Path\n        The output directory to write file to.\n    file_path : pathlib.Path\n        The original file path.\n    write_extension : str\n        The extension that output files should have.\n\n    Returns\n    -------\n    Path\n        The output file path.\n    \"\"\"\n    file_name = Path(file_path.stem).with_suffix(write_extension)\n    file_path = dirpath / file_name\n    return file_path\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils/#careamics.lightning.callbacks.prediction_writer_callback.file_path_utils.get_sample_file_path","title":"<code>get_sample_file_path(dataset, sample_id)</code>","text":"<p>Get the file path for a particular sample.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>IterableTiledPredDataset or IterablePredDataset</code> <p>Dataset.</p> required <code>sample_id</code> <code>int</code> <p>Sample ID, the index of the file in the dataset <code>dataset</code>.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The file path corresponding to the sample with the ID <code>sample_id</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/file_path_utils.py</code> <pre><code>def get_sample_file_path(\n    dataset: Union[IterableTiledPredDataset, IterablePredDataset], sample_id: int\n) -&gt; Path:\n    \"\"\"\n    Get the file path for a particular sample.\n\n    Parameters\n    ----------\n    dataset : IterableTiledPredDataset or IterablePredDataset\n        Dataset.\n    sample_id : int\n        Sample ID, the index of the file in the dataset `dataset`.\n\n    Returns\n    -------\n    Path\n        The file path corresponding to the sample with the ID `sample_id`.\n    \"\"\"\n    return dataset.data_files[sample_id]\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/","title":"prediction_writer_callback","text":"<p>Module containing <code>PredictionWriterCallback</code> class.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback","title":"<code>PredictionWriterCallback</code>","text":"<p>               Bases: <code>BasePredictionWriter</code></p> <p>A PyTorch Lightning callback to save predictions.</p> <p>Parameters:</p> Name Type Description Default <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> required <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> <p>Attributes:</p> Name Type Description <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> <code>dirpath</code> <code>pathlib.Path, default=\"predictions\"</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>writing_predictions</code> <code>bool</code> <p>If writing predictions is turned on or off.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>class PredictionWriterCallback(BasePredictionWriter):\n    \"\"\"\n    A PyTorch Lightning callback to save predictions.\n\n    Parameters\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n\n    Attributes\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : pathlib.Path, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n    writing_predictions : bool\n        If writing predictions is turned on or off.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_strategy: WriteStrategy,\n        dirpath: Union[Path, str] = \"predictions\",\n    ):\n        \"\"\"\n        A PyTorch Lightning callback to save predictions.\n\n        Parameters\n        ----------\n        write_strategy : WriteStrategy\n            A strategy for writing predictions.\n        dirpath : pathlib.Path or str, default=\"predictions\"\n            The path to the directory where prediction outputs will be saved. If\n            `dirpath` is not absolute it is assumed to be relative to current working\n            directory.\n        \"\"\"\n        super().__init__(write_interval=\"batch\")\n\n        # Toggle for CAREamist to switch off saving if desired\n        self.writing_predictions: bool = True\n\n        self.write_strategy: WriteStrategy = write_strategy\n\n        # forward declaration\n        self.dirpath: Path\n        # attribute initialisation\n        self._init_dirpath(dirpath)\n\n    @classmethod\n    def from_write_func_params(\n        cls,\n        write_type: SupportedWriteType,\n        tiled: bool,\n        write_func: Optional[WriteFunc] = None,\n        write_extension: Optional[str] = None,\n        write_func_kwargs: Optional[dict[str, Any]] = None,\n        dirpath: Union[Path, str] = \"predictions\",\n    ) -&gt; PredictionWriterCallback:  # TODO: change type hint to self (find out how)\n        \"\"\"\n        Initialize a `PredictionWriterCallback` from write function parameters.\n\n        This will automatically create a `WriteStrategy` to be passed to the\n        initialization of `PredictionWriterCallback`.\n\n        Parameters\n        ----------\n        write_type : {\"tiff\", \"custom\"}\n            The data type to save as, includes custom.\n        tiled : bool\n            Whether the prediction will be tiled or not.\n        write_func : WriteFunc, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` a function to save the data must be passed. See notes below.\n        write_extension : str, optional\n            If a known `write_type` is selected this argument is ignored. For a custom\n            `write_type` an extension to save the data with must be passed.\n        write_func_kwargs : dict of {{str: any}}, optional\n            Additional keyword arguments to be passed to the save function.\n        dirpath : pathlib.Path or str, default=\"predictions\"\n            The path to the directory where prediction outputs will be saved. If\n            `dirpath` is not absolute it is assumed to be relative to current working\n            directory.\n\n        Returns\n        -------\n        PredictionWriterCallback\n            Callback for writing predictions.\n        \"\"\"\n        write_strategy = create_write_strategy(\n            write_type=write_type,\n            tiled=tiled,\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n        return cls(write_strategy=write_strategy, dirpath=dirpath)\n\n    def _init_dirpath(self, dirpath):\n        \"\"\"\n        Initialize directory path. Should only be called from `__init__`.\n\n        Parameters\n        ----------\n        dirpath : pathlib.Path\n            See `__init__` description.\n        \"\"\"\n        dirpath = Path(dirpath)\n        if not dirpath.is_absolute():\n            dirpath = Path.cwd() / dirpath\n            logger.warning(\n                \"Prediction output directory is not absolute, absolute path assumed to\"\n                f\"be '{dirpath}'\"\n            )\n        self.dirpath = dirpath\n\n    def setup(self, trainer: Trainer, pl_module: LightningModule, stage: str) -&gt; None:\n        \"\"\"\n        Create the prediction output directory when predict begins.\n\n        Called when fit, validate, test, predict, or tune begins.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        stage : str\n            Stage of training e.g. 'predict', 'fit', 'validate'.\n        \"\"\"\n        super().setup(trainer, pl_module, stage)\n        if stage == \"predict\":\n            # make prediction output directory\n            logger.info(\"Making prediction output directory.\")\n            self.dirpath.mkdir(parents=True, exist_ok=True)\n\n    def write_on_batch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,  # TODO: change to expected type\n        batch_indices: Optional[Sequence[int]],\n        batch: Any,  # TODO: change to expected type\n        batch_idx: int,\n        dataloader_idx: int,\n    ) -&gt; None:\n        \"\"\"\n        Write predictions at the end of a batch.\n\n        The method of prediction is determined by the attribute `write_strategy`.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        prediction : Any\n            Prediction outputs of `batch`.\n        batch_indices : sequence of Any, optional\n            Batch indices.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        \"\"\"\n        # if writing prediction is turned off\n        if not self.writing_predictions:\n            return\n\n        dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dataloader: DataLoader = (\n            dataloaders[dataloader_idx]\n            if isinstance(dataloaders, list)\n            else dataloaders\n        )\n        dataset: ValidPredDatasets = dataloader.dataset\n        if not (\n            isinstance(dataset, IterablePredDataset)\n            or isinstance(dataset, IterableTiledPredDataset)\n        ):\n            # Note: Error will be raised before here from the source type\n            # This is for extra redundancy of errors.\n            raise TypeError(\n                \"Prediction dataset has to be `IterableTiledPredDataset` or \"\n                \"`IterablePredDataset`. Cannot be `InMemoryPredDataset` because \"\n                \"filenames are taken from the original file.\"\n            )\n\n        self.write_strategy.write_batch(\n            trainer=trainer,\n            pl_module=pl_module,\n            prediction=prediction,\n            batch_indices=batch_indices,\n            batch=batch,\n            batch_idx=batch_idx,\n            dataloader_idx=dataloader_idx,\n            dirpath=self.dirpath,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.__init__","title":"<code>__init__(write_strategy, dirpath='predictions')</code>","text":"<p>A PyTorch Lightning callback to save predictions.</p> <p>Parameters:</p> Name Type Description Default <code>write_strategy</code> <code>WriteStrategy</code> <p>A strategy for writing predictions.</p> required <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def __init__(\n    self,\n    write_strategy: WriteStrategy,\n    dirpath: Union[Path, str] = \"predictions\",\n):\n    \"\"\"\n    A PyTorch Lightning callback to save predictions.\n\n    Parameters\n    ----------\n    write_strategy : WriteStrategy\n        A strategy for writing predictions.\n    dirpath : pathlib.Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n    \"\"\"\n    super().__init__(write_interval=\"batch\")\n\n    # Toggle for CAREamist to switch off saving if desired\n    self.writing_predictions: bool = True\n\n    self.write_strategy: WriteStrategy = write_strategy\n\n    # forward declaration\n    self.dirpath: Path\n    # attribute initialisation\n    self._init_dirpath(dirpath)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback._init_dirpath","title":"<code>_init_dirpath(dirpath)</code>","text":"<p>Initialize directory path. Should only be called from <code>__init__</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dirpath</code> <code>Path</code> <p>See <code>__init__</code> description.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def _init_dirpath(self, dirpath):\n    \"\"\"\n    Initialize directory path. Should only be called from `__init__`.\n\n    Parameters\n    ----------\n    dirpath : pathlib.Path\n        See `__init__` description.\n    \"\"\"\n    dirpath = Path(dirpath)\n    if not dirpath.is_absolute():\n        dirpath = Path.cwd() / dirpath\n        logger.warning(\n            \"Prediction output directory is not absolute, absolute path assumed to\"\n            f\"be '{dirpath}'\"\n        )\n    self.dirpath = dirpath\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.from_write_func_params","title":"<code>from_write_func_params(write_type, tiled, write_func=None, write_extension=None, write_func_kwargs=None, dirpath='predictions')</code>  <code>classmethod</code>","text":"<p>Initialize a <code>PredictionWriterCallback</code> from write function parameters.</p> <p>This will automatically create a <code>WriteStrategy</code> to be passed to the initialization of <code>PredictionWriterCallback</code>.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>('tiff', 'custom')</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>tiled</code> <code>bool</code> <p>Whether the prediction will be tiled or not.</p> required <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {{str: any}}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <code>dirpath</code> <code>Path or str</code> <p>The path to the directory where prediction outputs will be saved. If <code>dirpath</code> is not absolute it is assumed to be relative to current working directory.</p> <code>\"predictions\"</code> <p>Returns:</p> Type Description <code>PredictionWriterCallback</code> <p>Callback for writing predictions.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>@classmethod\ndef from_write_func_params(\n    cls,\n    write_type: SupportedWriteType,\n    tiled: bool,\n    write_func: Optional[WriteFunc] = None,\n    write_extension: Optional[str] = None,\n    write_func_kwargs: Optional[dict[str, Any]] = None,\n    dirpath: Union[Path, str] = \"predictions\",\n) -&gt; PredictionWriterCallback:  # TODO: change type hint to self (find out how)\n    \"\"\"\n    Initialize a `PredictionWriterCallback` from write function parameters.\n\n    This will automatically create a `WriteStrategy` to be passed to the\n    initialization of `PredictionWriterCallback`.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    tiled : bool\n        Whether the prediction will be tiled or not.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func_kwargs : dict of {{str: any}}, optional\n        Additional keyword arguments to be passed to the save function.\n    dirpath : pathlib.Path or str, default=\"predictions\"\n        The path to the directory where prediction outputs will be saved. If\n        `dirpath` is not absolute it is assumed to be relative to current working\n        directory.\n\n    Returns\n    -------\n    PredictionWriterCallback\n        Callback for writing predictions.\n    \"\"\"\n    write_strategy = create_write_strategy(\n        write_type=write_type,\n        tiled=tiled,\n        write_func=write_func,\n        write_extension=write_extension,\n        write_func_kwargs=write_func_kwargs,\n    )\n    return cls(write_strategy=write_strategy, dirpath=dirpath)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.setup","title":"<code>setup(trainer, pl_module, stage)</code>","text":"<p>Create the prediction output directory when predict begins.</p> <p>Called when fit, validate, test, predict, or tune begins.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>stage</code> <code>str</code> <p>Stage of training e.g. 'predict', 'fit', 'validate'.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: str) -&gt; None:\n    \"\"\"\n    Create the prediction output directory when predict begins.\n\n    Called when fit, validate, test, predict, or tune begins.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    stage : str\n        Stage of training e.g. 'predict', 'fit', 'validate'.\n    \"\"\"\n    super().setup(trainer, pl_module, stage)\n    if stage == \"predict\":\n        # make prediction output directory\n        logger.info(\"Making prediction output directory.\")\n        self.dirpath.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback/#careamics.lightning.callbacks.prediction_writer_callback.prediction_writer_callback.PredictionWriterCallback.write_on_batch_end","title":"<code>write_on_batch_end(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx)</code>","text":"<p>Write predictions at the end of a batch.</p> <p>The method of prediction is determined by the attribute <code>write_strategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>prediction</code> <code>Any</code> <p>Prediction outputs of <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of Any</code> <p>Batch indices.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/prediction_writer_callback.py</code> <pre><code>def write_on_batch_end(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,  # TODO: change to expected type\n    batch_indices: Optional[Sequence[int]],\n    batch: Any,  # TODO: change to expected type\n    batch_idx: int,\n    dataloader_idx: int,\n) -&gt; None:\n    \"\"\"\n    Write predictions at the end of a batch.\n\n    The method of prediction is determined by the attribute `write_strategy`.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    prediction : Any\n        Prediction outputs of `batch`.\n    batch_indices : sequence of Any, optional\n        Batch indices.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    \"\"\"\n    # if writing prediction is turned off\n    if not self.writing_predictions:\n        return\n\n    dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dataloader: DataLoader = (\n        dataloaders[dataloader_idx]\n        if isinstance(dataloaders, list)\n        else dataloaders\n    )\n    dataset: ValidPredDatasets = dataloader.dataset\n    if not (\n        isinstance(dataset, IterablePredDataset)\n        or isinstance(dataset, IterableTiledPredDataset)\n    ):\n        # Note: Error will be raised before here from the source type\n        # This is for extra redundancy of errors.\n        raise TypeError(\n            \"Prediction dataset has to be `IterableTiledPredDataset` or \"\n            \"`IterablePredDataset`. Cannot be `InMemoryPredDataset` because \"\n            \"filenames are taken from the original file.\"\n        )\n\n    self.write_strategy.write_batch(\n        trainer=trainer,\n        pl_module=pl_module,\n        prediction=prediction,\n        batch_indices=batch_indices,\n        batch=batch,\n        batch_idx=batch_idx,\n        dataloader_idx=dataloader_idx,\n        dirpath=self.dirpath,\n    )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/","title":"write_strategy","text":"<p>Module containing different strategies for writing predictions.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles","title":"<code>CacheTiles</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>A write strategy that will cache tiles.</p> <p>Tiles are cached until a whole image is predicted on. Then the stitched prediction is saved.</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required <p>Attributes:</p> Name Type Description <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> <code>tile_cache</code> <code>list of numpy.ndarray</code> <p>Tiles cached for stitching prediction.</p> <code>tile_info_cache</code> <code>list of TileInformation</code> <p>Cached tile information for stitching prediction.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class CacheTiles(WriteStrategy):\n    \"\"\"\n    A write strategy that will cache tiles.\n\n    Tiles are cached until a whole image is predicted on. Then the stitched\n    prediction is saved.\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n\n    Attributes\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    tile_cache : list of numpy.ndarray\n        Tiles cached for stitching prediction.\n    tile_info_cache : list of TileInformation\n        Cached tile information for stitching prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_func: WriteFunc,\n        write_extension: str,\n        write_func_kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        A write strategy that will cache tiles.\n\n        Tiles are cached until a whole image is predicted on. Then the stitched\n        prediction is saved.\n\n        Parameters\n        ----------\n        write_func : WriteFunc\n            Function used to save predictions.\n        write_extension : str\n            Extension added to prediction file paths.\n        write_func_kwargs : dict of {str: Any}\n            Extra kwargs to pass to `write_func`.\n        \"\"\"\n        super().__init__()\n\n        self.write_func: WriteFunc = write_func\n        self.write_extension: str = write_extension\n        self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n        # where tiles will be cached until a whole image has been predicted\n        self.tile_cache: list[NDArray] = []\n        self.tile_info_cache: list[TileInformation] = []\n\n    @property\n    def last_tiles(self) -&gt; list[bool]:\n        \"\"\"\n        List of bool to determine whether each tile in the cache is the last tile.\n\n        Returns\n        -------\n        list of bool\n            Whether each tile in the tile cache is the last tile.\n        \"\"\"\n        return [tile_info.last_tile for tile_info in self.tile_info_cache]\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: tuple[NDArray, list[TileInformation]],\n        batch_indices: Optional[Sequence[int]],\n        batch: tuple[NDArray, list[TileInformation]],\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Cache tiles until the last tile is predicted; save the stitched prediction.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n        \"\"\"\n        dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dataloader: DataLoader = (\n            dataloaders[dataloader_idx]\n            if isinstance(dataloaders, list)\n            else dataloaders\n        )\n        dataset: IterableTiledPredDataset = dataloader.dataset\n        if not isinstance(dataset, IterableTiledPredDataset):\n            raise TypeError(\"Prediction dataset is not `IterableTiledPredDataset`.\")\n\n        # cache tiles (batches are split into single samples)\n        self.tile_cache.extend(np.split(prediction[0], prediction[0].shape[0]))\n        self.tile_info_cache.extend(prediction[1])\n\n        # save stitched prediction\n        if self._has_last_tile():\n\n            # get image tiles and remove them from the cache\n            tiles, tile_infos = self._get_image_tiles()\n            self._clear_cache()\n\n            # stitch prediction\n            prediction_image = stitch_prediction_single(\n                tiles=tiles, tile_infos=tile_infos\n            )\n\n            # write prediction\n            sample_id = tile_infos[0].sample_id  # need this to select correct file name\n            input_file_path = get_sample_file_path(dataset=dataset, sample_id=sample_id)\n            file_path = create_write_file_path(\n                dirpath=dirpath,\n                file_path=input_file_path,\n                write_extension=self.write_extension,\n            )\n            self.write_func(\n                file_path=file_path, img=prediction_image[0], **self.write_func_kwargs\n            )\n\n    def _has_last_tile(self) -&gt; bool:\n        \"\"\"\n        Whether a last tile is contained in the cached tiles.\n\n        Returns\n        -------\n        bool\n            Whether a last tile is contained in the cached tiles.\n        \"\"\"\n        return any(self.last_tiles)\n\n    def _clear_cache(self) -&gt; None:\n        \"\"\"Remove the tiles in the cache up to the first last tile.\"\"\"\n        index = self._last_tile_index()\n        self.tile_cache = self.tile_cache[index + 1 :]\n        self.tile_info_cache = self.tile_info_cache[index + 1 :]\n\n    def _last_tile_index(self) -&gt; int:\n        \"\"\"\n        Find the index of the last tile in the tile cache.\n\n        Returns\n        -------\n        int\n            Index of last tile.\n\n        Raises\n        ------\n        ValueError\n            If there is no last tile in the tile cache.\n        \"\"\"\n        last_tiles = self.last_tiles\n        if not any(last_tiles):\n            raise ValueError(\"No last tile in the tile cache.\")\n        index = np.where(last_tiles)[0][0]\n        return index\n\n    def _get_image_tiles(self) -&gt; tuple[list[NDArray], list[TileInformation]]:\n        \"\"\"\n        Get the tiles corresponding to a single image.\n\n        Returns\n        -------\n        tuple of (list of numpy.ndarray, list of TileInformation)\n            Tiles and tile information to stitch together a full image.\n        \"\"\"\n        index = self._last_tile_index()\n        tiles = self.tile_cache[: index + 1]\n        tile_infos = self.tile_info_cache[: index + 1]\n        return tiles, tile_infos\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.last_tiles","title":"<code>last_tiles</code>  <code>property</code>","text":"<p>List of bool to determine whether each tile in the cache is the last tile.</p> <p>Returns:</p> Type Description <code>list of bool</code> <p>Whether each tile in the tile cache is the last tile.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.__init__","title":"<code>__init__(write_func, write_extension, write_func_kwargs)</code>","text":"<p>A write strategy that will cache tiles.</p> <p>Tiles are cached until a whole image is predicted on. Then the stitched prediction is saved.</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def __init__(\n    self,\n    write_func: WriteFunc,\n    write_extension: str,\n    write_func_kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    A write strategy that will cache tiles.\n\n    Tiles are cached until a whole image is predicted on. Then the stitched\n    prediction is saved.\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n    super().__init__()\n\n    self.write_func: WriteFunc = write_func\n    self.write_extension: str = write_extension\n    self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n    # where tiles will be cached until a whole image has been predicted\n    self.tile_cache: list[NDArray] = []\n    self.tile_info_cache: list[TileInformation] = []\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles._clear_cache","title":"<code>_clear_cache()</code>","text":"<p>Remove the tiles in the cache up to the first last tile.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def _clear_cache(self) -&gt; None:\n    \"\"\"Remove the tiles in the cache up to the first last tile.\"\"\"\n    index = self._last_tile_index()\n    self.tile_cache = self.tile_cache[index + 1 :]\n    self.tile_info_cache = self.tile_info_cache[index + 1 :]\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles._get_image_tiles","title":"<code>_get_image_tiles()</code>","text":"<p>Get the tiles corresponding to a single image.</p> <p>Returns:</p> Type Description <code>tuple of (list of numpy.ndarray, list of TileInformation)</code> <p>Tiles and tile information to stitch together a full image.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def _get_image_tiles(self) -&gt; tuple[list[NDArray], list[TileInformation]]:\n    \"\"\"\n    Get the tiles corresponding to a single image.\n\n    Returns\n    -------\n    tuple of (list of numpy.ndarray, list of TileInformation)\n        Tiles and tile information to stitch together a full image.\n    \"\"\"\n    index = self._last_tile_index()\n    tiles = self.tile_cache[: index + 1]\n    tile_infos = self.tile_info_cache[: index + 1]\n    return tiles, tile_infos\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles._has_last_tile","title":"<code>_has_last_tile()</code>","text":"<p>Whether a last tile is contained in the cached tiles.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether a last tile is contained in the cached tiles.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def _has_last_tile(self) -&gt; bool:\n    \"\"\"\n    Whether a last tile is contained in the cached tiles.\n\n    Returns\n    -------\n    bool\n        Whether a last tile is contained in the cached tiles.\n    \"\"\"\n    return any(self.last_tiles)\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles._last_tile_index","title":"<code>_last_tile_index()</code>","text":"<p>Find the index of the last tile in the tile cache.</p> <p>Returns:</p> Type Description <code>int</code> <p>Index of last tile.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there is no last tile in the tile cache.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def _last_tile_index(self) -&gt; int:\n    \"\"\"\n    Find the index of the last tile in the tile cache.\n\n    Returns\n    -------\n    int\n        Index of last tile.\n\n    Raises\n    ------\n    ValueError\n        If there is no last tile in the tile cache.\n    \"\"\"\n    last_tiles = self.last_tiles\n    if not any(last_tiles):\n        raise ValueError(\"No last tile in the tile cache.\")\n    index = np.where(last_tiles)[0][0]\n    return index\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.CacheTiles.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Cache tiles until the last tile is predicted; save the stitched prediction.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: tuple[NDArray, list[TileInformation]],\n    batch_indices: Optional[Sequence[int]],\n    batch: tuple[NDArray, list[TileInformation]],\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Cache tiles until the last tile is predicted; save the stitched prediction.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n    \"\"\"\n    dataloaders: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dataloader: DataLoader = (\n        dataloaders[dataloader_idx]\n        if isinstance(dataloaders, list)\n        else dataloaders\n    )\n    dataset: IterableTiledPredDataset = dataloader.dataset\n    if not isinstance(dataset, IterableTiledPredDataset):\n        raise TypeError(\"Prediction dataset is not `IterableTiledPredDataset`.\")\n\n    # cache tiles (batches are split into single samples)\n    self.tile_cache.extend(np.split(prediction[0], prediction[0].shape[0]))\n    self.tile_info_cache.extend(prediction[1])\n\n    # save stitched prediction\n    if self._has_last_tile():\n\n        # get image tiles and remove them from the cache\n        tiles, tile_infos = self._get_image_tiles()\n        self._clear_cache()\n\n        # stitch prediction\n        prediction_image = stitch_prediction_single(\n            tiles=tiles, tile_infos=tile_infos\n        )\n\n        # write prediction\n        sample_id = tile_infos[0].sample_id  # need this to select correct file name\n        input_file_path = get_sample_file_path(dataset=dataset, sample_id=sample_id)\n        file_path = create_write_file_path(\n            dirpath=dirpath,\n            file_path=input_file_path,\n            write_extension=self.write_extension,\n        )\n        self.write_func(\n            file_path=file_path, img=prediction_image[0], **self.write_func_kwargs\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage","title":"<code>WriteImage</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>A strategy for writing image predictions (i.e. un-tiled predictions).</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required <p>Attributes:</p> Name Type Description <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteImage(WriteStrategy):\n    \"\"\"\n    A strategy for writing image predictions (i.e. un-tiled predictions).\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n\n    Attributes\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n\n    def __init__(\n        self,\n        write_func: WriteFunc,\n        write_extension: str,\n        write_func_kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        A strategy for writing image predictions (i.e. un-tiled predictions).\n\n        Parameters\n        ----------\n        write_func : WriteFunc\n            Function used to save predictions.\n        write_extension : str\n            Extension added to prediction file paths.\n        write_func_kwargs : dict of {str: Any}\n            Extra kwargs to pass to `write_func`.\n        \"\"\"\n        super().__init__()\n\n        self.write_func: WriteFunc = write_func\n        self.write_extension: str = write_extension\n        self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: NDArray,\n        batch_indices: Optional[Sequence[int]],\n        batch: NDArray,\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Save full images.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n\n        Raises\n        ------\n        TypeError\n            If trainer prediction dataset is not `IterablePredDataset`.\n        \"\"\"\n        dls: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n        dl: DataLoader = dls[dataloader_idx] if isinstance(dls, list) else dls\n        ds: IterablePredDataset = dl.dataset\n        if not isinstance(ds, IterablePredDataset):\n            raise TypeError(\"Prediction dataset is not `IterablePredDataset`.\")\n\n        for i in range(prediction.shape[0]):\n            prediction_image = prediction[0]\n            sample_id = batch_idx * dl.batch_size + i\n            input_file_path = get_sample_file_path(dataset=ds, sample_id=sample_id)\n            file_path = create_write_file_path(\n                dirpath=dirpath,\n                file_path=input_file_path,\n                write_extension=self.write_extension,\n            )\n            self.write_func(\n                file_path=file_path, img=prediction_image, **self.write_func_kwargs\n            )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage.__init__","title":"<code>__init__(write_func, write_extension, write_func_kwargs)</code>","text":"<p>A strategy for writing image predictions (i.e. un-tiled predictions).</p> <p>Parameters:</p> Name Type Description Default <code>write_func</code> <code>WriteFunc</code> <p>Function used to save predictions.</p> required <code>write_extension</code> <code>str</code> <p>Extension added to prediction file paths.</p> required <code>write_func_kwargs</code> <code>dict of {str: Any}</code> <p>Extra kwargs to pass to <code>write_func</code>.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def __init__(\n    self,\n    write_func: WriteFunc,\n    write_extension: str,\n    write_func_kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    A strategy for writing image predictions (i.e. un-tiled predictions).\n\n    Parameters\n    ----------\n    write_func : WriteFunc\n        Function used to save predictions.\n    write_extension : str\n        Extension added to prediction file paths.\n    write_func_kwargs : dict of {str: Any}\n        Extra kwargs to pass to `write_func`.\n    \"\"\"\n    super().__init__()\n\n    self.write_func: WriteFunc = write_func\n    self.write_extension: str = write_extension\n    self.write_func_kwargs: dict[str, Any] = write_func_kwargs\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteImage.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Save full images.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If trainer prediction dataset is not <code>IterablePredDataset</code>.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: NDArray,\n    batch_indices: Optional[Sequence[int]],\n    batch: NDArray,\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Save full images.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n\n    Raises\n    ------\n    TypeError\n        If trainer prediction dataset is not `IterablePredDataset`.\n    \"\"\"\n    dls: Union[DataLoader, list[DataLoader]] = trainer.predict_dataloaders\n    dl: DataLoader = dls[dataloader_idx] if isinstance(dls, list) else dls\n    ds: IterablePredDataset = dl.dataset\n    if not isinstance(ds, IterablePredDataset):\n        raise TypeError(\"Prediction dataset is not `IterablePredDataset`.\")\n\n    for i in range(prediction.shape[0]):\n        prediction_image = prediction[0]\n        sample_id = batch_idx * dl.batch_size + i\n        input_file_path = get_sample_file_path(dataset=ds, sample_id=sample_id)\n        file_path = create_write_file_path(\n            dirpath=dirpath,\n            file_path=input_file_path,\n            write_extension=self.write_extension,\n        )\n        self.write_func(\n            file_path=file_path, img=prediction_image, **self.write_func_kwargs\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteStrategy","title":"<code>WriteStrategy</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for write strategy classes.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteStrategy(Protocol):\n    \"\"\"Protocol for write strategy classes.\"\"\"\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,  # TODO: change to expected type\n        batch_indices: Optional[Sequence[int]],\n        batch: Any,  # TODO: change to expected type\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        WriteStrategy subclasses must contain this function to write a batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n        \"\"\"\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteStrategy.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>WriteStrategy subclasses must contain this function to write a batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,  # TODO: change to expected type\n    batch_indices: Optional[Sequence[int]],\n    batch: Any,  # TODO: change to expected type\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    WriteStrategy subclasses must contain this function to write a batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n    \"\"\"\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteTilesZarr","title":"<code>WriteTilesZarr</code>","text":"<p>               Bases: <code>WriteStrategy</code></p> <p>Strategy to write tiles to Zarr file.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>class WriteTilesZarr(WriteStrategy):\n    \"\"\"Strategy to write tiles to Zarr file.\"\"\"\n\n    def write_batch(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        prediction: Any,\n        batch_indices: Optional[Sequence[int]],\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int,\n        dirpath: Path,\n    ) -&gt; None:\n        \"\"\"\n        Write tiles to zarr file.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning Trainer.\n        pl_module : LightningModule\n            PyTorch Lightning LightningModule.\n        prediction : Any\n            Predictions on `batch`.\n        batch_indices : sequence of int\n            Indices identifying the samples in the batch.\n        batch : Any\n            Input batch.\n        batch_idx : int\n            Batch index.\n        dataloader_idx : int\n            Dataloader index.\n        dirpath : Path\n            Path to directory to save predictions to.\n\n        Raises\n        ------\n        NotImplementedError\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy.WriteTilesZarr.write_batch","title":"<code>write_batch(trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx, dirpath)</code>","text":"<p>Write tiles to zarr file.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning Trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning LightningModule.</p> required <code>prediction</code> <code>Any</code> <p>Predictions on <code>batch</code>.</p> required <code>batch_indices</code> <code>sequence of int</code> <p>Indices identifying the samples in the batch.</p> required <code>batch</code> <code>Any</code> <p>Input batch.</p> required <code>batch_idx</code> <code>int</code> <p>Batch index.</p> required <code>dataloader_idx</code> <code>int</code> <p>Dataloader index.</p> required <code>dirpath</code> <code>Path</code> <p>Path to directory to save predictions to.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy.py</code> <pre><code>def write_batch(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    prediction: Any,\n    batch_indices: Optional[Sequence[int]],\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int,\n    dirpath: Path,\n) -&gt; None:\n    \"\"\"\n    Write tiles to zarr file.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning Trainer.\n    pl_module : LightningModule\n        PyTorch Lightning LightningModule.\n    prediction : Any\n        Predictions on `batch`.\n    batch_indices : sequence of int\n        Indices identifying the samples in the batch.\n    batch : Any\n        Input batch.\n    batch_idx : int\n        Batch index.\n    dataloader_idx : int\n        Dataloader index.\n    dirpath : Path\n        Path to directory to save predictions to.\n\n    Raises\n    ------\n    NotImplementedError\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/","title":"write_strategy_factory","text":"<p>Module containing convenience function to create <code>WriteStrategy</code>.</p>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory._create_tiled_write_strategy","title":"<code>_create_tiled_write_strategy(write_type, write_func, write_extension, write_func_kwargs)</code>","text":"<p>Create a tiled write strategy.</p> <p>Either <code>CacheTiles</code> for caching tiles until a whole image is predicted or <code>WriteTilesZarr</code> for writing tiles directly to disk.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> required <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> required <code>write_func_kwargs</code> <code>dict of {str: any}</code> <p>Additional keyword arguments to be passed to the save function.</p> required <p>Returns:</p> Type Description <code>WriteStrategy</code> <p>A strategy for writing tiled predictions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>if `write_type=\"zarr\" is chosen.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def _create_tiled_write_strategy(\n    write_type: SupportedWriteType,\n    write_func: Optional[WriteFunc],\n    write_extension: Optional[str],\n    write_func_kwargs: dict[str, Any],\n) -&gt; WriteStrategy:\n    \"\"\"\n    Create a tiled write strategy.\n\n    Either `CacheTiles` for caching tiles until a whole image is predicted or\n    `WriteTilesZarr` for writing tiles directly to disk.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func_kwargs : dict of {str: any}\n        Additional keyword arguments to be passed to the save function.\n\n    Returns\n    -------\n    WriteStrategy\n        A strategy for writing tiled predictions.\n\n    Raises\n    ------\n    NotImplementedError\n        if `write_type=\"zarr\" is chosen.\n    \"\"\"\n    # if write_type == SupportedData.ZARR:\n    #    create *args, **kwargs\n    #    return WriteTilesZarr(*args, **kwargs)\n    # else:\n    if write_type == \"zarr\":\n        raise NotImplementedError(\"Saving to zarr is not implemented yet.\")\n    else:\n        write_func = select_write_func(write_type=write_type, write_func=write_func)\n        write_extension = select_write_extension(\n            write_type=write_type, write_extension=write_extension\n        )\n        return CacheTiles(\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.create_write_strategy","title":"<code>create_write_strategy(write_type, tiled, write_func=None, write_extension=None, write_func_kwargs=None)</code>","text":"<p>Create a write strategy from convenient parameters.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>tiled</code> <code>bool</code> <p>Whether the prediction will be tiled or not.</p> required <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <code>write_func_kwargs</code> <code>dict of {str: any}</code> <p>Additional keyword arguments to be passed to the save function.</p> <code>None</code> <p>Returns:</p> Type Description <code>WriteStrategy</code> <p>A strategy for writing predicions.</p> Notes <p>The <code>write_func</code> function signature must match that of the example below     <pre><code>write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n</code></pre></p> <p>The <code>write_func_kwargs</code> will be passed to the <code>write_func</code> doing the following:     <pre><code>write_func(file_path=file_path, img=img, **kwargs)\n</code></pre></p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def create_write_strategy(\n    write_type: SupportedWriteType,\n    tiled: bool,\n    write_func: Optional[WriteFunc] = None,\n    write_extension: Optional[str] = None,\n    write_func_kwargs: Optional[dict[str, Any]] = None,\n) -&gt; WriteStrategy:\n    \"\"\"\n    Create a write strategy from convenient parameters.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    tiled : bool\n        Whether the prediction will be tiled or not.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n    write_func_kwargs : dict of {str: any}, optional\n        Additional keyword arguments to be passed to the save function.\n\n    Returns\n    -------\n    WriteStrategy\n        A strategy for writing predicions.\n\n    Notes\n    -----\n    The `write_func` function signature must match that of the example below\n        ```\n        write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n        ```\n\n    The `write_func_kwargs` will be passed to the `write_func` doing the following:\n        ```\n        write_func(file_path=file_path, img=img, **kwargs)\n        ```\n    \"\"\"\n    if write_func_kwargs is None:\n        write_func_kwargs = {}\n\n    write_strategy: WriteStrategy\n    if not tiled:\n        write_func = select_write_func(write_type=write_type, write_func=write_func)\n        write_extension = select_write_extension(\n            write_type=write_type, write_extension=write_extension\n        )\n        write_strategy = WriteImage(\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n    else:\n        # select CacheTiles or WriteTilesZarr (when implemented)\n        write_strategy = _create_tiled_write_strategy(\n            write_type=write_type,\n            write_func=write_func,\n            write_extension=write_extension,\n            write_func_kwargs=write_func_kwargs,\n        )\n\n    return write_strategy\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.select_write_extension","title":"<code>select_write_extension(write_type, write_extension=None)</code>","text":"<p>Return an extension to add to file paths.</p> <p>If <code>write_type</code> is \"custom\" then <code>write_extension</code>, otherwise the known write extension is selected.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_extension</code> <code>str</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> an extension to save the data with must be passed.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The extension to be added to file paths.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>self.save_type=\"custom\"</code> but <code>save_extension</code> has not been given.</p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def select_write_extension(\n    write_type: SupportedWriteType, write_extension: Optional[str] = None\n) -&gt; str:\n    \"\"\"\n    Return an extension to add to file paths.\n\n    If `write_type` is \"custom\" then `write_extension`, otherwise the known\n    write extension is selected.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    write_extension : str, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` an extension to save the data with must be passed.\n\n    Returns\n    -------\n    str\n        The extension to be added to file paths.\n\n    Raises\n    ------\n    ValueError\n        If `self.save_type=\"custom\"` but `save_extension` has not been given.\n    \"\"\"\n    write_type_: SupportedData = SupportedData(write_type)  # new variable for mypy\n    if write_type_ == SupportedData.CUSTOM:\n        if write_extension is None:\n            raise ValueError(\"A save extension must be provided for custom data types.\")\n        else:\n            write_extension = write_extension\n    else:\n        # kind of a weird pattern -&gt; reason to move get_extension from SupportedData\n        write_extension = write_type_.get_extension(write_type_)\n    return write_extension\n</code></pre>"},{"location":"reference/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory/#careamics.lightning.callbacks.prediction_writer_callback.write_strategy_factory.select_write_func","title":"<code>select_write_func(write_type, write_func=None)</code>","text":"<p>Return a function to write images.</p> <p>If <code>write_type</code> is \"custom\" then <code>write_func</code>, otherwise the known write function is selected.</p> <p>Parameters:</p> Name Type Description Default <code>write_type</code> <code>(tiff, custom)</code> <p>The data type to save as, includes custom.</p> <code>\"tiff\"</code> <code>write_func</code> <code>WriteFunc</code> <p>If a known <code>write_type</code> is selected this argument is ignored. For a custom <code>write_type</code> a function to save the data must be passed. See notes below.</p> <code>None</code> <p>Returns:</p> Type Description <code>WriteFunc</code> <p>A function for writing images.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>write_type=\"custom\"</code> but <code>write_func</code> has not been given.</p> Notes <p>The <code>write_func</code> function signature must match that of the example below     <pre><code>write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n</code></pre></p> Source code in <code>src/careamics/lightning/callbacks/prediction_writer_callback/write_strategy_factory.py</code> <pre><code>def select_write_func(\n    write_type: SupportedWriteType, write_func: Optional[WriteFunc] = None\n) -&gt; WriteFunc:\n    \"\"\"\n    Return a function to write images.\n\n    If `write_type` is \"custom\" then `write_func`, otherwise the known write function\n    is selected.\n\n    Parameters\n    ----------\n    write_type : {\"tiff\", \"custom\"}\n        The data type to save as, includes custom.\n    write_func : WriteFunc, optional\n        If a known `write_type` is selected this argument is ignored. For a custom\n        `write_type` a function to save the data must be passed. See notes below.\n\n    Returns\n    -------\n    WriteFunc\n        A function for writing images.\n\n    Raises\n    ------\n    ValueError\n        If `write_type=\"custom\"` but `write_func` has not been given.\n\n    Notes\n    -----\n    The `write_func` function signature must match that of the example below\n        ```\n        write_func(file_path: Path, img: NDArray, *args, **kwargs) -&gt; None: ...\n        ```\n    \"\"\"\n    if write_type == SupportedData.CUSTOM:\n        if write_func is None:\n            raise ValueError(\n                \"A save function must be provided for custom data types.\"\n                # TODO: link to how save functions should be implemented\n            )\n        else:\n            write_func = write_func\n    else:\n        write_func = get_write_func(write_type)\n    return write_func\n</code></pre>"},{"location":"reference/careamics/losses/loss_factory/","title":"loss_factory","text":"<p>Loss factory module.</p> <p>This module contains a factory function for creating loss functions.</p>"},{"location":"reference/careamics/losses/loss_factory/#careamics.losses.loss_factory.FCNLossParameters","title":"<code>FCNLossParameters</code>  <code>dataclass</code>","text":"<p>Dataclass for FCN loss.</p> Source code in <code>src/careamics/losses/loss_factory.py</code> <pre><code>@dataclass\nclass FCNLossParameters:\n    \"\"\"Dataclass for FCN loss.\"\"\"\n\n    # TODO check\n    prediction: tensor\n    targets: tensor\n    mask: tensor\n    current_epoch: int\n    loss_weight: float\n</code></pre>"},{"location":"reference/careamics/losses/loss_factory/#careamics.losses.loss_factory.loss_factory","title":"<code>loss_factory(loss)</code>","text":"<p>Return loss function.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Union[SupportedLoss, str]</code> <p>Requested loss.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Loss function.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the loss is unknown.</p> Source code in <code>src/careamics/losses/loss_factory.py</code> <pre><code>def loss_factory(loss: Union[SupportedLoss, str]) -&gt; Callable:\n    \"\"\"Return loss function.\n\n    Parameters\n    ----------\n    loss : Union[SupportedLoss, str]\n        Requested loss.\n\n    Returns\n    -------\n    Callable\n        Loss function.\n\n    Raises\n    ------\n    NotImplementedError\n        If the loss is unknown.\n    \"\"\"\n    if loss == SupportedLoss.N2V:\n        return n2v_loss\n\n    # elif loss_type == SupportedLoss.PN2V:\n    #     return pn2v_loss\n\n    elif loss == SupportedLoss.MAE:\n        return mae_loss\n\n    elif loss == SupportedLoss.MSE:\n        return mse_loss\n\n    elif loss == SupportedLoss.MUSPLIT:\n        return musplit_loss\n\n    elif loss == SupportedLoss.DENOISPLIT:\n        return denoisplit_loss\n\n    elif loss == SupportedLoss.DENOISPLIT_MUSPLIT:\n        return denoisplit_musplit_loss\n\n    else:\n        raise NotImplementedError(f\"Loss {loss} is not yet supported.\")\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/","title":"losses","text":"<p>Loss submodule.</p> <p>This submodule contains the various losses used in CAREamics.</p>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.mae_loss","title":"<code>mae_loss(samples, labels, *args)</code>","text":"<p>N2N Loss function described in to J Lehtinen et al 2018.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Tensor</code> <p>Raw patches.</p> required <code>labels</code> <code>Tensor</code> <p>Different subset of noisy patches.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def mae_loss(samples: torch.Tensor, labels: torch.Tensor, *args) -&gt; torch.Tensor:\n    \"\"\"\n    N2N Loss function described in to J Lehtinen et al 2018.\n\n    Parameters\n    ----------\n    samples : torch.Tensor\n        Raw patches.\n    labels : torch.Tensor\n        Different subset of noisy patches.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    loss = L1Loss()\n    return loss(samples, labels)\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.mse_loss","title":"<code>mse_loss(source, target, *args)</code>","text":"<p>Mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Tensor</code> <p>Source patches.</p> required <code>target</code> <code>Tensor</code> <p>Target patches.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def mse_loss(source: torch.Tensor, target: torch.Tensor, *args) -&gt; torch.Tensor:\n    \"\"\"\n    Mean squared error loss.\n\n    Parameters\n    ----------\n    source : torch.Tensor\n        Source patches.\n    target : torch.Tensor\n        Target patches.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    loss = MSELoss()\n    return loss(source, target)\n</code></pre>"},{"location":"reference/careamics/losses/fcn/losses/#careamics.losses.fcn.losses.n2v_loss","title":"<code>n2v_loss(manipulated_batch, original_batch, masks, *args)</code>","text":"<p>N2V Loss function described in A Krull et al 2018.</p> <p>Parameters:</p> Name Type Description Default <code>manipulated_batch</code> <code>Tensor</code> <p>Batch after manipulation function applied.</p> required <code>original_batch</code> <code>Tensor</code> <p>Original images.</p> required <code>masks</code> <code>Tensor</code> <p>Coordinates of changed pixels.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>src/careamics/losses/fcn/losses.py</code> <pre><code>def n2v_loss(\n    manipulated_batch: torch.Tensor,\n    original_batch: torch.Tensor,\n    masks: torch.Tensor,\n    *args,\n) -&gt; torch.Tensor:\n    \"\"\"\n    N2V Loss function described in A Krull et al 2018.\n\n    Parameters\n    ----------\n    manipulated_batch : torch.Tensor\n        Batch after manipulation function applied.\n    original_batch : torch.Tensor\n        Original images.\n    masks : torch.Tensor\n        Coordinates of changed pixels.\n    *args : Any\n        Additional arguments.\n\n    Returns\n    -------\n    torch.Tensor\n        Loss value.\n    \"\"\"\n    errors = (original_batch - manipulated_batch) ** 2\n    # Average over pixels and batch\n    loss = torch.sum(errors * masks) / torch.sum(masks)\n    return loss  # TODO change output to dict ?\n</code></pre>"},{"location":"reference/careamics/losses/lvae/loss_utils/","title":"loss_utils","text":""},{"location":"reference/careamics/losses/lvae/loss_utils/#careamics.losses.lvae.loss_utils.free_bits_kl","title":"<code>free_bits_kl(kl, free_bits, batch_average=False, eps=1e-06)</code>","text":"<p>Compute free-bits version of KL divergence.</p> <p>This function ensures that the KL doesn't go to zero for any latent dimension. Hence, it contributes to use latent variables more efficiently, leading to better representation learning.</p> <p>NOTE: Takes in the KL with shape (batch size, layers), returns the KL with free bits (for optimization) with shape (layers,), which is the average free-bits KL per layer in the current batch. If batch_average is False (default), the free bits are per layer and per batch element. Otherwise, the free bits are still per layer, but are assigned on average to the whole batch. In both cases, the batch average is returned, so it's simply a matter of doing mean(clamp(KL)) or clamp(mean(KL)).</p> <p>Parameters:</p> Name Type Description Default <code>kl</code> <code>Tensor</code> <p>The KL divergence tensor with shape (batch size, layers).</p> required <code>free_bits</code> <code>float</code> <p>The free bits value. Set to 0.0 to disable free bits.</p> required <code>batch_average</code> <code>bool</code> <p>Whether to average over the batch before clamping to <code>free_bits</code>.</p> <code>False</code> <code>eps</code> <code>float</code> <p>A small value to avoid numerical instability.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The free-bits version of the KL divergence with shape (layers,).</p> Source code in <code>src/careamics/losses/lvae/loss_utils.py</code> <pre><code>def free_bits_kl(\n    kl: torch.Tensor, free_bits: float, batch_average: bool = False, eps: float = 1e-6\n) -&gt; torch.Tensor:\n    \"\"\"Compute free-bits version of KL divergence.\n\n    This function ensures that the KL doesn't go to zero for any latent dimension.\n    Hence, it contributes to use latent variables more efficiently, leading to\n    better representation learning.\n\n    NOTE:\n    Takes in the KL with shape (batch size, layers), returns the KL with\n    free bits (for optimization) with shape (layers,), which is the average\n    free-bits KL per layer in the current batch.\n    If batch_average is False (default), the free bits are per layer and\n    per batch element. Otherwise, the free bits are still per layer, but\n    are assigned on average to the whole batch. In both cases, the batch\n    average is returned, so it's simply a matter of doing mean(clamp(KL))\n    or clamp(mean(KL)).\n\n    Parameters\n    ----------\n    kl : torch.Tensor\n        The KL divergence tensor with shape (batch size, layers).\n    free_bits : float\n        The free bits value. Set to 0.0 to disable free bits.\n    batch_average : bool\n        Whether to average over the batch before clamping to `free_bits`.\n    eps : float\n        A small value to avoid numerical instability.\n\n    Returns\n    -------\n    torch.Tensor\n        The free-bits version of the KL divergence with shape (layers,).\n    \"\"\"\n    assert kl.dim() == 2\n    if free_bits &lt; eps:\n        return kl.mean(0)\n    if batch_average:\n        return kl.mean(0).clamp(min=free_bits)\n    return kl.clamp(min=free_bits).mean(0)\n</code></pre>"},{"location":"reference/careamics/losses/lvae/loss_utils/#careamics.losses.lvae.loss_utils.get_kl_weight","title":"<code>get_kl_weight(kl_annealing, kl_start, kl_annealtime, kl_weight, current_epoch)</code>","text":"<p>Compute the weight of the KL loss in case of annealing.</p> <p>Parameters:</p> Name Type Description Default <code>kl_annealing</code> <code>bool</code> <p>Whether to use KL annealing.</p> required <code>kl_start</code> <code>int</code> <p>The epoch at which to start</p> required <code>kl_annealtime</code> <code>int</code> <p>The number of epochs for which annealing is applied.</p> required <code>kl_weight</code> <code>float</code> <p>The weight for the KL loss. If <code>None</code>, the weight is computed using annealing, else it is set to a default of 1.</p> required <code>current_epoch</code> <code>int</code> <p>The current epoch.</p> required Source code in <code>src/careamics/losses/lvae/loss_utils.py</code> <pre><code>def get_kl_weight(\n    kl_annealing: bool,\n    kl_start: int,\n    kl_annealtime: int,\n    kl_weight: float,\n    current_epoch: int,\n) -&gt; float:\n    \"\"\"Compute the weight of the KL loss in case of annealing.\n\n    Parameters\n    ----------\n    kl_annealing : bool\n        Whether to use KL annealing.\n    kl_start : int\n        The epoch at which to start\n    kl_annealtime : int\n        The number of epochs for which annealing is applied.\n    kl_weight : float\n        The weight for the KL loss. If `None`, the weight is computed\n        using annealing, else it is set to a default of 1.\n    current_epoch : int\n        The current epoch.\n    \"\"\"\n    if kl_annealing:\n        # calculate relative weight\n        kl_weight = (current_epoch - kl_start) * (1.0 / kl_annealtime)\n        # clamp to [0,1]\n        kl_weight = min(max(0.0, kl_weight), 1.0)\n\n        # if the final weight is given, then apply that weight on top of it\n        if kl_weight is not None:\n            kl_weight = kl_weight * kl_weight\n    elif kl_weight is not None:\n        return kl_weight\n    else:\n        kl_weight = 1.0\n    return kl_weight\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/","title":"losses","text":"<p>Methods for Loss Computation.</p>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses._get_kl_divergence_loss_denoisplit","title":"<code>_get_kl_divergence_loss_denoisplit(topdown_data, img_shape, kl_type)</code>","text":"<p>Compute the KL divergence loss for denoiSplit.</p> <p>Parameters:</p> Name Type Description Default <code>topdown_data</code> <code>dict[str, Tensor]</code> <p>A dictionary containing information computed for each layer during the top-down pass. The dictionary must include the following keys: - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,). - \"z\": The sampled latents for each layer. Shape of each tensor is (B, layers, <code>z_dims[i]</code>, H, W).</p> required <code>img_shape</code> <code>tuple[int]</code> <p>The shape of the input image to the LVAE model. Shape is ([Z], Y, X).</p> required <code>kl_type</code> <code>Literal['kl', 'kl_restricted']</code> <p>The type of KL divergence loss to compute.</p> required <p>Returns:</p> Name Type Description <code>kl_loss</code> <code>Tensor</code> <p>The KL divergence loss for the denoiSplit case. Shape is (1, ).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def _get_kl_divergence_loss_denoisplit(\n    topdown_data: dict[str, torch.Tensor],\n    img_shape: tuple[int],\n    kl_type: Literal[\"kl\", \"kl_restricted\"],\n) -&gt; torch.Tensor:\n    \"\"\"Compute the KL divergence loss for denoiSplit.\n\n    Parameters\n    ----------\n    topdown_data : dict[str, torch.Tensor]\n        A dictionary containing information computed for each layer during the top-down\n        pass. The dictionary must include the following keys:\n        - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,).\n        - \"z\": The sampled latents for each layer. Shape of each tensor is\n        (B, layers, `z_dims[i]`, H, W).\n    img_shape : tuple[int]\n        The shape of the input image to the LVAE model. Shape is ([Z], Y, X).\n    kl_type : Literal[\"kl\", \"kl_restricted\"]\n        The type of KL divergence loss to compute.\n\n    Returns\n    -------\n    kl_loss : torch.Tensor\n        The KL divergence loss for the denoiSplit case. Shape is (1, ).\n    \"\"\"\n    return get_kl_divergence_loss(\n        kl_type=kl_type,\n        topdown_data=topdown_data,\n        rescaling=\"image_dim\",\n        aggregation=\"sum\",\n        free_bits_coeff=1.0,\n        img_shape=img_shape,\n    )\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses._get_kl_divergence_loss_musplit","title":"<code>_get_kl_divergence_loss_musplit(topdown_data, img_shape, kl_type)</code>","text":"<p>Compute the KL divergence loss for muSplit.</p> <p>Parameters:</p> Name Type Description Default <code>topdown_data</code> <code>dict[str, Tensor]</code> <p>A dictionary containing information computed for each layer during the top-down pass. The dictionary must include the following keys: - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,). - \"z\": The sampled latents for each layer. Shape of each tensor is (B, layers, <code>z_dims[i]</code>, H, W).</p> required <code>img_shape</code> <code>tuple[int]</code> <p>The shape of the input image to the LVAE model. Shape is ([Z], Y, X).</p> required <code>kl_type</code> <code>Literal['kl', 'kl_restricted']</code> <p>The type of KL divergence loss to compute.</p> required <p>Returns:</p> Name Type Description <code>kl_loss</code> <code>Tensor</code> <p>The KL divergence loss for the muSplit case. Shape is (1, ).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def _get_kl_divergence_loss_musplit(\n    topdown_data: dict[str, torch.Tensor],\n    img_shape: tuple[int],\n    kl_type: Literal[\"kl\", \"kl_restricted\"],\n) -&gt; torch.Tensor:\n    \"\"\"Compute the KL divergence loss for muSplit.\n\n    Parameters\n    ----------\n    topdown_data : dict[str, torch.Tensor]\n        A dictionary containing information computed for each layer during the top-down\n        pass. The dictionary must include the following keys:\n        - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,).\n        - \"z\": The sampled latents for each layer. Shape of each tensor is\n        (B, layers, `z_dims[i]`, H, W).\n    img_shape : tuple[int]\n        The shape of the input image to the LVAE model. Shape is ([Z], Y, X).\n    kl_type : Literal[\"kl\", \"kl_restricted\"]\n        The type of KL divergence loss to compute.\n\n    Returns\n    -------\n    kl_loss : torch.Tensor\n        The KL divergence loss for the muSplit case. Shape is (1, ).\n    \"\"\"\n    return get_kl_divergence_loss(\n        kl_type=\"kl\",  # TODO: hardcoded, deal in future PR\n        topdown_data=topdown_data,\n        rescaling=\"latent_dim\",\n        aggregation=\"mean\",\n        free_bits_coeff=0.0,\n        img_shape=img_shape,\n    )\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses._reconstruction_loss_musplit_denoisplit","title":"<code>_reconstruction_loss_musplit_denoisplit(predictions, targets, nm_likelihood, gaussian_likelihood, nm_weight, gaussian_weight)</code>","text":"<p>Compute the reconstruction loss for muSplit-denoiSplit loss.</p> <p>The resulting loss is a weighted mean of the noise model likelihood and the Gaussian likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Tensor</code> <p>The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), or (B, 2*C, [Z], Y, X), where C is the number of output channels, and the factor of 2 is for the case of predicted log-variance.</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, C, [Z], Y, X), where C is the number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>nm_likelihood</code> <code>NoiseModelLikelihood</code> <p>A <code>NoiseModelLikelihood</code> object used to compute the noise model likelihood.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>A <code>GaussianLikelihood</code> object used to compute the Gaussian likelihood.</p> required <code>nm_weight</code> <code>float</code> <p>The weight for the noise model likelihood.</p> required <code>gaussian_weight</code> <code>float</code> <p>The weight for the Gaussian likelihood.</p> required <p>Returns:</p> Name Type Description <code>recons_loss</code> <code>Tensor</code> <p>The reconstruction loss. Shape is (1, ).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def _reconstruction_loss_musplit_denoisplit(\n    predictions: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],\n    targets: torch.Tensor,\n    nm_likelihood: NoiseModelLikelihood,\n    gaussian_likelihood: GaussianLikelihood,\n    nm_weight: float,\n    gaussian_weight: float,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the reconstruction loss for muSplit-denoiSplit loss.\n\n    The resulting loss is a weighted mean of the noise model likelihood and the\n    Gaussian likelihood.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), or\n        (B, 2*C, [Z], Y, X), where C is the number of output channels,\n        and the factor of 2 is for the case of predicted log-variance.\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, C, [Z], Y, X), where C is the number of output channels\n        (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).\n    nm_likelihood : NoiseModelLikelihood\n        A `NoiseModelLikelihood` object used to compute the noise model likelihood.\n    gaussian_likelihood : GaussianLikelihood\n        A `GaussianLikelihood` object used to compute the Gaussian likelihood.\n    nm_weight : float\n        The weight for the noise model likelihood.\n    gaussian_weight : float\n        The weight for the Gaussian likelihood.\n\n    Returns\n    -------\n    recons_loss : torch.Tensor\n        The reconstruction loss. Shape is (1, ).\n    \"\"\"\n    if predictions.shape[1] == 2 * targets.shape[1]:\n        # predictions contain both mean and log-variance\n        pred_mean, _ = predictions.chunk(2, dim=1)\n    else:\n        pred_mean = predictions\n\n    recons_loss_nm = get_reconstruction_loss(\n        reconstruction=pred_mean, target=targets, likelihood_obj=nm_likelihood\n    )\n\n    recons_loss_gm = get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=gaussian_likelihood,\n    )\n\n    recons_loss = nm_weight * recons_loss_nm + gaussian_weight * recons_loss_gm\n    return recons_loss\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.denoisplit_loss","title":"<code>denoisplit_loss(model_outputs, targets, config, gaussian_likelihood=None, noise_model_likelihood=None)</code>","text":"<p>Loss function for DenoiSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function containing all loss hyperparameters.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> <code>None</code> <code>noise_model_likelihood</code> <code>NoiseModelLikelihood</code> <p>The noise model likelihood object.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def denoisplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: Optional[GaussianLikelihood] = None,\n    noise_model_likelihood: Optional[NoiseModelLikelihood] = None,\n) -&gt; Optional[dict[str, torch.Tensor]]:\n    \"\"\"Loss function for DenoiSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function containing all loss hyperparameters.\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : NoiseModelLikelihood\n        The noise model likelihood object.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    assert noise_model_likelihood is not None\n\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = config.reconstruction_weight * get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=noise_model_likelihood,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    kl_weight = get_kl_weight(\n        config.kl_params.annealing,\n        config.kl_params.start,\n        config.kl_params.annealtime,\n        config.kl_weight,\n        config.kl_params.current_epoch,\n    )\n    kl_loss = (\n        _get_kl_divergence_loss_denoisplit(\n            topdown_data=td_data,\n            img_shape=targets.shape[2:],\n            kl_type=config.kl_params.loss_type,\n        )\n        * kl_weight\n    )\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.denoisplit_musplit_loss","title":"<code>denoisplit_musplit_loss(model_outputs, targets, config, gaussian_likelihood, noise_model_likelihood)</code>","text":"<p>Loss function for DenoiSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function containing all loss hyperparameters.</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> required <code>noise_model_likelihood</code> <code>NoiseModelLikelihood</code> <p>The noise model likelihood object.</p> required <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def denoisplit_musplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: GaussianLikelihood,\n    noise_model_likelihood: NoiseModelLikelihood,\n) -&gt; Optional[dict[str, torch.Tensor]]:\n    \"\"\"Loss function for DenoiSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function containing all loss hyperparameters.\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : NoiseModelLikelihood\n        The noise model likelihood object.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = _reconstruction_loss_musplit_denoisplit(\n        predictions=predictions,\n        targets=targets,\n        nm_likelihood=noise_model_likelihood,\n        gaussian_likelihood=gaussian_likelihood,\n        nm_weight=config.denoisplit_weight,\n        gaussian_weight=config.musplit_weight,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    # NOTE: 'kl' key stands for the 'kl_samplewise' key in the TopDownLayer class.\n    # The different naming comes from `top_down_pass()` method in the LadderVAE.\n    denoisplit_kl = _get_kl_divergence_loss_denoisplit(\n        topdown_data=td_data,\n        img_shape=targets.shape[2:],\n        kl_type=config.kl_params.loss_type,\n    )\n    musplit_kl = _get_kl_divergence_loss_musplit(\n        topdown_data=td_data,\n        img_shape=targets.shape[2:],\n        kl_type=config.kl_params.loss_type,\n    )\n    kl_loss = (\n        config.denoisplit_weight * denoisplit_kl + config.musplit_weight * musplit_kl\n    )\n    # TODO `kl_weight` is hardcoded (???)\n    kl_loss = config.kl_weight * kl_loss\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.get_kl_divergence_loss","title":"<code>get_kl_divergence_loss(kl_type, topdown_data, rescaling, aggregation, free_bits_coeff, img_shape=None)</code>","text":"<p>Compute the KL divergence loss.</p> <p>NOTE: Description of <code>rescaling</code> methods: - If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). In this way they have the same magnitude across layers. - If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial dimensions. In this way, the lower layers have a larger KL-loss value compared to the higher layers, since the latent space and hence the KL tensor has more entries. Specifically, at hierarchy <code>i</code>, the total KL loss is larger by a factor (128/i**2).</p> <p>NOTE: the type of <code>aggregation</code> determines the magnitude of the KL-loss. Clearly, \"sum\" aggregation results in a larger KL-loss value compared to \"mean\" by a factor of <code>n_layers</code>.</p> <p>NOTE: recall that sample-wise KL is obtained by summing over all dimensions, including Z. Also recall that in current 3D implementation of LVAE, no downsampling is done on Z. Therefore, to avoid emphasizing KL loss too much, we divide it by the Z dimension of input image in every case.</p> <p>Parameters:</p> Name Type Description Default <code>kl_type</code> <code>Literal['kl', 'kl_restricted']</code> <p>The type of KL divergence loss to compute.</p> required <code>topdown_data</code> <code>dict[str, Tensor]</code> <p>A dictionary containing information computed for each layer during the top-down pass. The dictionary must include the following keys: - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,). - \"z\": The sampled latents for each layer. Shape of each tensor is (B, layers, <code>z_dims[i]</code>, H, W).</p> required <code>rescaling</code> <code>Literal['latent_dim', 'image_dim']</code> <p>The rescaling method used for the KL-loss values. If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial dimensions.</p> required <code>aggregation</code> <code>Literal['mean', 'sum']</code> <p>The aggregation method used to combine the KL-loss values across layers. If \"mean\", the KL-loss values are averaged across layers. If \"sum\", the KL-loss values are summed across layers.</p> required <code>free_bits_coeff</code> <code>float</code> <p>The free bits coefficient used for the KL-loss computation.</p> required <code>img_shape</code> <code>Optional[tuple[int]]</code> <p>The shape of the input image to the LVAE model. Shape is ([Z], Y, X).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>kl_loss</code> <code>Tensor</code> <p>The KL divergence loss. Shape is (1, ).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def get_kl_divergence_loss(\n    kl_type: Literal[\"kl\", \"kl_restricted\"],\n    topdown_data: dict[str, torch.Tensor],\n    rescaling: Literal[\"latent_dim\", \"image_dim\"],\n    aggregation: Literal[\"mean\", \"sum\"],\n    free_bits_coeff: float,\n    img_shape: Optional[tuple[int]] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Compute the KL divergence loss.\n\n    NOTE: Description of `rescaling` methods:\n    - If \"latent_dim\", the KL-loss values are rescaled w.r.t. the latent space\n    dimensions (spatial + number of channels, i.e., (C, [Z], Y, X)). In this way they\n    have the same magnitude across layers.\n    - If \"image_dim\", the KL-loss values are rescaled w.r.t. the input image spatial\n    dimensions. In this way, the lower layers have a larger KL-loss value compared to\n    the higher layers, since the latent space and hence the KL tensor has more entries.\n    Specifically, at hierarchy `i`, the total KL loss is larger by a factor (128/i**2).\n\n    NOTE: the type of `aggregation` determines the magnitude of the KL-loss. Clearly,\n    \"sum\" aggregation results in a larger KL-loss value compared to \"mean\" by a factor\n    of `n_layers`.\n\n    NOTE: recall that sample-wise KL is obtained by summing over all dimensions,\n    including Z. Also recall that in current 3D implementation of LVAE, no downsampling\n    is done on Z. Therefore, to avoid emphasizing KL loss too much, we divide it\n    by the Z dimension of input image in every case.\n\n    Parameters\n    ----------\n    kl_type : Literal[\"kl\", \"kl_restricted\"]\n        The type of KL divergence loss to compute.\n    topdown_data : dict[str, torch.Tensor]\n        A dictionary containing information computed for each layer during the top-down\n        pass. The dictionary must include the following keys:\n        - \"kl\": The KL-loss values for each layer. Shape of each tensor is (B,).\n        - \"z\": The sampled latents for each layer. Shape of each tensor is\n        (B, layers, `z_dims[i]`, H, W).\n    rescaling : Literal[\"latent_dim\", \"image_dim\"]\n        The rescaling method used for the KL-loss values. If \"latent_dim\", the KL-loss\n        values are rescaled w.r.t. the latent space dimensions (spatial + number of\n        channels, i.e., (C, [Z], Y, X)). If \"image_dim\", the KL-loss values are\n        rescaled w.r.t. the input image spatial dimensions.\n    aggregation : Literal[\"mean\", \"sum\"]\n        The aggregation method used to combine the KL-loss values across layers. If\n        \"mean\", the KL-loss values are averaged across layers. If \"sum\", the KL-loss\n        values are summed across layers.\n    free_bits_coeff : float\n        The free bits coefficient used for the KL-loss computation.\n    img_shape : Optional[tuple[int]]\n        The shape of the input image to the LVAE model. Shape is ([Z], Y, X).\n\n    Returns\n    -------\n    kl_loss : torch.Tensor\n        The KL divergence loss. Shape is (1, ).\n    \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_data[kl_type]],\n        dim=1,\n    )  # shape: (B, n_layers)\n\n    # Apply free bits (&amp; batch average)\n    kl = free_bits_kl(kl, free_bits_coeff)  # shape: (n_layers,)\n\n    # In 3D case, rescale by Z dim\n    # TODO If we have downsampling in Z dimension, then this needs to change.\n    if len(img_shape) == 3:\n        kl = kl / img_shape[0]\n\n    # Rescaling\n    if rescaling == \"latent_dim\":\n        for i in range(len(kl)):\n            latent_dim = topdown_data[\"z\"][i].shape[1:]\n            norm_factor = np.prod(latent_dim)\n            kl[i] = kl[i] / norm_factor\n    elif rescaling == \"image_dim\":\n        kl = kl / np.prod(img_shape[-2:])\n\n    # Aggregation\n    if aggregation == \"mean\":\n        kl = kl.mean()  # shape: (1,)\n    elif aggregation == \"sum\":\n        kl = kl.sum()  # shape: (1,)\n\n    return kl\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.get_reconstruction_loss","title":"<code>get_reconstruction_loss(reconstruction, target, likelihood_obj)</code>","text":"<p>Compute the reconstruction loss (negative log-likelihood).</p> <p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Tensor</code> <p>The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), where C is the number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>target</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, C, [Z], Y, X), where C is the number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).</p> required <code>likelihood_obj</code> <code>Likelihood</code> <p>The likelihood object used to compute the reconstruction loss.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The recontruction loss (negative log-likelihood).</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def get_reconstruction_loss(\n    reconstruction: torch.Tensor,\n    target: torch.Tensor,\n    likelihood_obj: Likelihood,\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Compute the reconstruction loss (negative log-likelihood).\n\n    Parameters\n    ----------\n    reconstruction: torch.Tensor\n        The output of the LVAE decoder. Shape is (B, C, [Z], Y, X), where C is the\n        number of output channels (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).\n    target: torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, C, [Z], Y, X), where C is the number of output channels\n        (e.g., 1 in HDN, &gt;1 in muSplit/denoiSplit).\n    likelihood_obj: Likelihood\n        The likelihood object used to compute the reconstruction loss.\n\n    Returns\n    -------\n    torch.Tensor\n        The recontruction loss (negative log-likelihood).\n    \"\"\"\n    # Compute Log likelihood\n    ll, _ = likelihood_obj(reconstruction, target)  # shape: (B, C, [Z], Y, X)\n    return -1 * ll.mean()\n</code></pre>"},{"location":"reference/careamics/losses/lvae/losses/#careamics.losses.lvae.losses.musplit_loss","title":"<code>musplit_loss(model_outputs, targets, config, gaussian_likelihood, noise_model_likelihood=None)</code>","text":"<p>Loss function for muSplit.</p> <p>Parameters:</p> Name Type Description Default <code>model_outputs</code> <code>tuple[Tensor, dict[str, Any]]</code> <p>Tuple containing the model predictions (shape is (B, <code>target_ch</code>, [Z], Y, X)) and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).</p> required <code>targets</code> <code>Tensor</code> <p>The target image used to compute the reconstruction loss. Shape is (B, <code>target_ch</code>, [Z], Y, X).</p> required <code>config</code> <code>LVAELossConfig</code> <p>The config for loss function (e.g., KL hyperparameters, likelihood module, noise model, etc.).</p> required <code>gaussian_likelihood</code> <code>GaussianLikelihood</code> <p>The Gaussian likelihood object.</p> required <code>noise_model_likelihood</code> <code>Optional[NoiseModelLikelihood]</code> <p>The noise model likelihood object. Not used here.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>output</code> <code>Optional[dict[str, Tensor]]</code> <p>A dictionary containing the overall loss <code>[\"loss\"]</code>, the reconstruction loss <code>[\"reconstruction_loss\"]</code>, and the KL divergence loss <code>[\"kl_loss\"]</code>.</p> Source code in <code>src/careamics/losses/lvae/losses.py</code> <pre><code>def musplit_loss(\n    model_outputs: tuple[torch.Tensor, dict[str, Any]],\n    targets: torch.Tensor,\n    config: LVAELossConfig,\n    gaussian_likelihood: Optional[GaussianLikelihood],\n    noise_model_likelihood: Optional[NoiseModelLikelihood] = None,  # TODO: ugly\n) -&gt; Optional[dict[str, torch.Tensor]]:\n    \"\"\"Loss function for muSplit.\n\n    Parameters\n    ----------\n    model_outputs : tuple[torch.Tensor, dict[str, Any]]\n        Tuple containing the model predictions (shape is (B, `target_ch`, [Z], Y, X))\n        and the top-down layer data (e.g., sampled latents, KL-loss values, etc.).\n    targets : torch.Tensor\n        The target image used to compute the reconstruction loss. Shape is\n        (B, `target_ch`, [Z], Y, X).\n    config : LVAELossConfig\n        The config for loss function (e.g., KL hyperparameters, likelihood module,\n        noise model, etc.).\n    gaussian_likelihood : GaussianLikelihood\n        The Gaussian likelihood object.\n    noise_model_likelihood : Optional[NoiseModelLikelihood]\n        The noise model likelihood object. Not used here.\n\n    Returns\n    -------\n    output : Optional[dict[str, torch.Tensor]]\n        A dictionary containing the overall loss `[\"loss\"]`, the reconstruction loss\n        `[\"reconstruction_loss\"]`, and the KL divergence loss `[\"kl_loss\"]`.\n    \"\"\"\n    assert gaussian_likelihood is not None\n\n    predictions, td_data = model_outputs\n\n    # Reconstruction loss computation\n    recons_loss = config.reconstruction_weight * get_reconstruction_loss(\n        reconstruction=predictions,\n        target=targets,\n        likelihood_obj=gaussian_likelihood,\n    )\n    if torch.isnan(recons_loss).any():\n        recons_loss = 0.0\n\n    # KL loss computation\n    kl_weight = get_kl_weight(\n        config.kl_params.annealing,\n        config.kl_params.start,\n        config.kl_params.annealtime,\n        config.kl_weight,\n        config.kl_params.current_epoch,\n    )\n    kl_loss = (\n        _get_kl_divergence_loss_musplit(\n            topdown_data=td_data,\n            img_shape=targets.shape[2:],\n            kl_type=config.kl_params.loss_type,\n        )\n        * kl_weight\n    )\n\n    net_loss = recons_loss + kl_loss\n    output = {\n        \"loss\": net_loss,\n        \"reconstruction_loss\": (\n            recons_loss.detach()\n            if isinstance(recons_loss, torch.Tensor)\n            else recons_loss\n        ),\n        \"kl_loss\": kl_loss.detach(),\n    }\n    # https://github.com/openai/vdvae/blob/main/train.py#L26\n    if torch.isnan(net_loss).any():\n        return None\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/","title":"calibration","text":""},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration","title":"<code>Calibration</code>","text":"<p>Calibrate the uncertainty computed over samples from LVAE model.</p> <p>Calibration is done by learning a scalar that maps the pixel-wise standard deviation of the the predicted samples into the actual prediction error.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>class Calibration:\n    \"\"\"Calibrate the uncertainty computed over samples from LVAE model.\n\n    Calibration is done by learning a scalar that maps the pixel-wise standard\n    deviation of the the predicted samples into the actual prediction error.\n    \"\"\"\n\n    def __init__(self, num_bins: int = 15):\n        self._bins = num_bins\n        self._bin_boundaries = None\n\n    def compute_bin_boundaries(self, predict_std: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Compute the bin boundaries for `num_bins` bins and predicted std values.\"\"\"\n        min_std = np.min(predict_std)\n        max_std = np.max(predict_std)\n        return np.linspace(min_std, max_std, self._bins + 1)\n\n    def compute_stats(\n        self, pred: np.ndarray, pred_std: np.ndarray, target: np.ndarray\n    ) -&gt; dict[int, dict[str, Union[np.ndarray, list]]]:\n        \"\"\"\n        It computes the bin-wise RMSE and RMV for each channel of the predicted image.\n\n        Recall that:\n            - RMSE = np.sqrt((pred - target)**2 / num_pixels)\n            - RMV = np.sqrt(np.mean(pred_std**2))\n\n        ALGORITHM\n        - For each channel:\n            - Given the bin boundaries, assign pixels of `std_ch` array to a specific bin index.\n            - For each bin index:\n                - Compute the RMSE, RMV, and number of pixels for that bin.\n\n        NOTE: each channel of the predicted image/logvar has its own stats.\n\n        Parameters\n        ----------\n        pred: np.ndarray\n            Predicted patches, shape (n, h, w, c).\n        pred_std: np.ndarray\n            Std computed over the predicted patches, shape (n, h, w, c).\n        target: np.ndarray\n            Target GT image, shape (n, h, w, c).\n        \"\"\"\n        self._bin_boundaries = {}\n        stats_dict = {}\n        for ch_idx in range(pred.shape[-1]):\n            stats_dict[ch_idx] = {\n                \"bin_count\": [],\n                \"rmv\": [],\n                \"rmse\": [],\n                \"bin_boundaries\": None,\n                \"bin_matrix\": [],\n                \"rmse_err\": [],\n            }\n            pred_ch = pred[..., ch_idx]\n            std_ch = pred_std[..., ch_idx]\n            target_ch = target[..., ch_idx]\n            boundaries = self.compute_bin_boundaries(std_ch)\n            stats_dict[ch_idx][\"bin_boundaries\"] = boundaries\n            bin_matrix = np.digitize(std_ch.reshape(-1), boundaries)\n            bin_matrix = bin_matrix.reshape(std_ch.shape)\n            stats_dict[ch_idx][\"bin_matrix\"] = bin_matrix\n            error = (pred_ch - target_ch) ** 2\n            for bin_idx in range(1, 1 + self._bins):\n                bin_mask = bin_matrix == bin_idx\n                bin_error = error[bin_mask]\n                bin_size = np.sum(bin_mask)\n                bin_error = (\n                    np.sqrt(np.sum(bin_error) / bin_size) if bin_size &gt; 0 else None\n                )\n                stderr = (\n                    np.std(error[bin_mask]) / np.sqrt(bin_size)\n                    if bin_size &gt; 0\n                    else None\n                )\n                rmse_stderr = np.sqrt(stderr) if stderr is not None else None\n\n                bin_var = np.mean(std_ch[bin_mask] ** 2)\n                stats_dict[ch_idx][\"rmse\"].append(bin_error)\n                stats_dict[ch_idx][\"rmse_err\"].append(rmse_stderr)\n                stats_dict[ch_idx][\"rmv\"].append(np.sqrt(bin_var))\n                stats_dict[ch_idx][\"bin_count\"].append(bin_size)\n        self.stats_dict = stats_dict\n        return stats_dict\n\n    def get_calibrated_factor_for_stdev(\n        self,\n        pred: Optional[np.ndarray] = None,\n        pred_std: Optional[np.ndarray] = None,\n        target: Optional[np.ndarray] = None,\n        q_s: float = 0.00001,\n        q_e: float = 0.99999,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calibrate the uncertainty by multiplying the predicted std with a scalar.\n\n        Parameters\n        ----------\n        stats_dict : dict[int, dict[str, Union[np.ndarray, list]]]\n            Dictionary containing the stats for each channel.\n        q_s : float, optional\n            Start quantile, by default 0.00001.\n        q_e : float, optional\n            End quantile, by default 0.99999.\n\n        Returns\n        -------\n        dict[str, float]\n            Calibrated factor for each channel (slope + intercept).\n        \"\"\"\n        if not hasattr(self, \"stats_dict\"):\n            print(\"No stats found. Computing stats...\")\n            if any(v is None for v in [pred, pred_std, target]):\n                raise ValueError(\"pred, pred_std, and target must be provided.\")\n            self.stats_dict = self.compute_stats(\n                pred=pred, pred_std=pred_std, target=target\n            )\n        outputs = {}\n        for ch_idx in self.stats_dict.keys():\n            y = self.stats_dict[ch_idx][\"rmse\"]\n            x = self.stats_dict[ch_idx][\"rmv\"]\n            count = self.stats_dict[ch_idx][\"bin_count\"]\n\n            first_idx = get_first_index(count, q_s)\n            last_idx = get_last_index(count, q_e)\n            x = x[first_idx:-last_idx]\n            y = y[first_idx:-last_idx]\n            slope, intercept, *_ = stats.linregress(x, y)\n            output = {\"scalar\": slope, \"offset\": intercept}\n            outputs[ch_idx] = output\n        factors = self.get_factors_array(factors_dict=outputs)\n        return outputs, factors\n\n    def get_factors_array(self, factors_dict: list[dict]):\n        \"\"\"Get the calibration factors as a numpy array.\"\"\"\n        calib_scalar = [factors_dict[i][\"scalar\"] for i in range(len(factors_dict))]\n        calib_scalar = np.array(calib_scalar).reshape(1, 1, 1, -1)\n        calib_offset = [\n            factors_dict[i].get(\"offset\", 0.0) for i in range(len(factors_dict))\n        ]\n        calib_offset = np.array(calib_offset).reshape(1, 1, 1, -1)\n        return {\"scalar\": calib_scalar, \"offset\": calib_offset}\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.compute_bin_boundaries","title":"<code>compute_bin_boundaries(predict_std)</code>","text":"<p>Compute the bin boundaries for <code>num_bins</code> bins and predicted std values.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def compute_bin_boundaries(self, predict_std: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute the bin boundaries for `num_bins` bins and predicted std values.\"\"\"\n    min_std = np.min(predict_std)\n    max_std = np.max(predict_std)\n    return np.linspace(min_std, max_std, self._bins + 1)\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.compute_stats","title":"<code>compute_stats(pred, pred_std, target)</code>","text":"<p>It computes the bin-wise RMSE and RMV for each channel of the predicted image.</p> <p>Recall that:     - RMSE = np.sqrt((pred - target)2 / num_pixels)     - RMV = np.sqrt(np.mean(pred_std2))</p> <p>ALGORITHM - For each channel:     - Given the bin boundaries, assign pixels of <code>std_ch</code> array to a specific bin index.     - For each bin index:         - Compute the RMSE, RMV, and number of pixels for that bin.</p> <p>NOTE: each channel of the predicted image/logvar has its own stats.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>ndarray</code> <p>Predicted patches, shape (n, h, w, c).</p> required <code>pred_std</code> <code>ndarray</code> <p>Std computed over the predicted patches, shape (n, h, w, c).</p> required <code>target</code> <code>ndarray</code> <p>Target GT image, shape (n, h, w, c).</p> required Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def compute_stats(\n    self, pred: np.ndarray, pred_std: np.ndarray, target: np.ndarray\n) -&gt; dict[int, dict[str, Union[np.ndarray, list]]]:\n    \"\"\"\n    It computes the bin-wise RMSE and RMV for each channel of the predicted image.\n\n    Recall that:\n        - RMSE = np.sqrt((pred - target)**2 / num_pixels)\n        - RMV = np.sqrt(np.mean(pred_std**2))\n\n    ALGORITHM\n    - For each channel:\n        - Given the bin boundaries, assign pixels of `std_ch` array to a specific bin index.\n        - For each bin index:\n            - Compute the RMSE, RMV, and number of pixels for that bin.\n\n    NOTE: each channel of the predicted image/logvar has its own stats.\n\n    Parameters\n    ----------\n    pred: np.ndarray\n        Predicted patches, shape (n, h, w, c).\n    pred_std: np.ndarray\n        Std computed over the predicted patches, shape (n, h, w, c).\n    target: np.ndarray\n        Target GT image, shape (n, h, w, c).\n    \"\"\"\n    self._bin_boundaries = {}\n    stats_dict = {}\n    for ch_idx in range(pred.shape[-1]):\n        stats_dict[ch_idx] = {\n            \"bin_count\": [],\n            \"rmv\": [],\n            \"rmse\": [],\n            \"bin_boundaries\": None,\n            \"bin_matrix\": [],\n            \"rmse_err\": [],\n        }\n        pred_ch = pred[..., ch_idx]\n        std_ch = pred_std[..., ch_idx]\n        target_ch = target[..., ch_idx]\n        boundaries = self.compute_bin_boundaries(std_ch)\n        stats_dict[ch_idx][\"bin_boundaries\"] = boundaries\n        bin_matrix = np.digitize(std_ch.reshape(-1), boundaries)\n        bin_matrix = bin_matrix.reshape(std_ch.shape)\n        stats_dict[ch_idx][\"bin_matrix\"] = bin_matrix\n        error = (pred_ch - target_ch) ** 2\n        for bin_idx in range(1, 1 + self._bins):\n            bin_mask = bin_matrix == bin_idx\n            bin_error = error[bin_mask]\n            bin_size = np.sum(bin_mask)\n            bin_error = (\n                np.sqrt(np.sum(bin_error) / bin_size) if bin_size &gt; 0 else None\n            )\n            stderr = (\n                np.std(error[bin_mask]) / np.sqrt(bin_size)\n                if bin_size &gt; 0\n                else None\n            )\n            rmse_stderr = np.sqrt(stderr) if stderr is not None else None\n\n            bin_var = np.mean(std_ch[bin_mask] ** 2)\n            stats_dict[ch_idx][\"rmse\"].append(bin_error)\n            stats_dict[ch_idx][\"rmse_err\"].append(rmse_stderr)\n            stats_dict[ch_idx][\"rmv\"].append(np.sqrt(bin_var))\n            stats_dict[ch_idx][\"bin_count\"].append(bin_size)\n    self.stats_dict = stats_dict\n    return stats_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.get_calibrated_factor_for_stdev","title":"<code>get_calibrated_factor_for_stdev(pred=None, pred_std=None, target=None, q_s=1e-05, q_e=0.99999)</code>","text":"<p>Calibrate the uncertainty by multiplying the predicted std with a scalar.</p> <p>Parameters:</p> Name Type Description Default <code>stats_dict</code> <code>dict[int, dict[str, Union[ndarray, list]]]</code> <p>Dictionary containing the stats for each channel.</p> required <code>q_s</code> <code>float</code> <p>Start quantile, by default 0.00001.</p> <code>1e-05</code> <code>q_e</code> <code>float</code> <p>End quantile, by default 0.99999.</p> <code>0.99999</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Calibrated factor for each channel (slope + intercept).</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def get_calibrated_factor_for_stdev(\n    self,\n    pred: Optional[np.ndarray] = None,\n    pred_std: Optional[np.ndarray] = None,\n    target: Optional[np.ndarray] = None,\n    q_s: float = 0.00001,\n    q_e: float = 0.99999,\n) -&gt; dict[str, float]:\n    \"\"\"Calibrate the uncertainty by multiplying the predicted std with a scalar.\n\n    Parameters\n    ----------\n    stats_dict : dict[int, dict[str, Union[np.ndarray, list]]]\n        Dictionary containing the stats for each channel.\n    q_s : float, optional\n        Start quantile, by default 0.00001.\n    q_e : float, optional\n        End quantile, by default 0.99999.\n\n    Returns\n    -------\n    dict[str, float]\n        Calibrated factor for each channel (slope + intercept).\n    \"\"\"\n    if not hasattr(self, \"stats_dict\"):\n        print(\"No stats found. Computing stats...\")\n        if any(v is None for v in [pred, pred_std, target]):\n            raise ValueError(\"pred, pred_std, and target must be provided.\")\n        self.stats_dict = self.compute_stats(\n            pred=pred, pred_std=pred_std, target=target\n        )\n    outputs = {}\n    for ch_idx in self.stats_dict.keys():\n        y = self.stats_dict[ch_idx][\"rmse\"]\n        x = self.stats_dict[ch_idx][\"rmv\"]\n        count = self.stats_dict[ch_idx][\"bin_count\"]\n\n        first_idx = get_first_index(count, q_s)\n        last_idx = get_last_index(count, q_e)\n        x = x[first_idx:-last_idx]\n        y = y[first_idx:-last_idx]\n        slope, intercept, *_ = stats.linregress(x, y)\n        output = {\"scalar\": slope, \"offset\": intercept}\n        outputs[ch_idx] = output\n    factors = self.get_factors_array(factors_dict=outputs)\n    return outputs, factors\n</code></pre>"},{"location":"reference/careamics/lvae_training/calibration/#careamics.lvae_training.calibration.Calibration.get_factors_array","title":"<code>get_factors_array(factors_dict)</code>","text":"<p>Get the calibration factors as a numpy array.</p> Source code in <code>src/careamics/lvae_training/calibration.py</code> <pre><code>def get_factors_array(self, factors_dict: list[dict]):\n    \"\"\"Get the calibration factors as a numpy array.\"\"\"\n    calib_scalar = [factors_dict[i][\"scalar\"] for i in range(len(factors_dict))]\n    calib_scalar = np.array(calib_scalar).reshape(1, 1, 1, -1)\n    calib_offset = [\n        factors_dict[i].get(\"offset\", 0.0) for i in range(len(factors_dict))\n    ]\n    calib_offset = np.array(calib_offset).reshape(1, 1, 1, -1)\n    return {\"scalar\": calib_scalar, \"offset\": calib_offset}\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/","title":"eval_utils","text":"<p>This script provides methods to evaluate the performance of the LVAE model. It includes functions to:     - make predictions,     - quantify the performance of the model     - create plots to visualize the results.</p>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.PatchLocation","title":"<code>PatchLocation</code>","text":"<p>Encapsulates t_idx and spatial location.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>class PatchLocation:\n    \"\"\"\n    Encapsulates t_idx and spatial location.\n    \"\"\"\n\n    def __init__(self, h_idx_range, w_idx_range, t_idx):\n        self.t = t_idx\n        self.h_start, self.h_end = h_idx_range\n        self.w_start, self.w_end = w_idx_range\n\n    def __str__(self):\n        msg = f\"T:{self.t} [{self.h_start}-{self.h_end}) [{self.w_start}-{self.w_end}) \"\n        return msg\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.TilingMode","title":"<code>TilingMode</code>","text":"<p>Enum for the tiling mode.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>class TilingMode:\n    \"\"\"\n    Enum for the tiling mode.\n    \"\"\"\n\n    TrimBoundary = 0\n    PadBoundary = 1\n    ShiftBoundary = 2\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.add_psnr_str","title":"<code>add_psnr_str(ax_, psnr)</code>","text":"<p>Add psnr string to the axes</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def add_psnr_str(ax_, psnr):\n    \"\"\"\n    Add psnr string to the axes\n    \"\"\"\n    textstr = f\"PSNR\\n{psnr}\"\n    props = dict(boxstyle=\"round\", facecolor=\"gray\", alpha=0.5)\n    # place a text box in upper left in axes coords\n    ax_.text(\n        0.05,\n        0.95,\n        textstr,\n        transform=ax_.transAxes,\n        fontsize=11,\n        verticalalignment=\"top\",\n        bbox=props,\n        color=\"white\",\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.clean_ax","title":"<code>clean_ax(ax)</code>","text":"<p>Helper function to remove ticks from axes in plots.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def clean_ax(ax):\n    \"\"\"\n    Helper function to remove ticks from axes in plots.\n    \"\"\"\n    # 2D or 1D axes are of type np.ndarray\n    if isinstance(ax, np.ndarray):\n        for one_ax in ax:\n            clean_ax(one_ax)\n        return\n\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n    ax.tick_params(left=False, right=False, top=False, bottom=False)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_eval_output_dir","title":"<code>get_eval_output_dir(saveplotsdir, patch_size, mmse_count=50)</code>","text":"<p>Given the path to a root directory to save plots, patch size, and mmse count, it returns the specific directory to save the plots.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_eval_output_dir(\n    saveplotsdir: str, patch_size: int, mmse_count: int = 50\n) -&gt; str:\n    \"\"\"\n    Given the path to a root directory to save plots, patch size, and mmse count,\n    it returns the specific directory to save the plots.\n    \"\"\"\n    eval_out_dir = os.path.join(\n        saveplotsdir, f\"eval_outputs/patch_{patch_size}_mmse_{mmse_count}\"\n    )\n    os.makedirs(eval_out_dir, exist_ok=True)\n    print(eval_out_dir)\n    return eval_out_dir\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_fractional_change","title":"<code>get_fractional_change(target, prediction, max_val=None)</code>","text":"<p>Get relative difference between target and prediction.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_fractional_change(target, prediction, max_val=None):\n    \"\"\"\n    Get relative difference between target and prediction.\n    \"\"\"\n    if max_val is None:\n        max_val = target.max()\n    return (target - prediction) / max_val\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_location_from_idx","title":"<code>get_location_from_idx(dset, dset_input_idx, pred_h, pred_w)</code>","text":"<p>For a given idx of the dataset, it returns where exactly in the dataset, does this prediction lies. Note that this prediction also has padded pixels and so a subset of it will be used in the final prediction. Which time frame, which spatial location (h_start, h_end, w_start,w_end) Args:     dset:     dset_input_idx:     pred_h:     pred_w:</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_location_from_idx(dset, dset_input_idx, pred_h, pred_w):\n    \"\"\"\n    For a given idx of the dataset, it returns where exactly in the dataset, does this prediction lies.\n    Note that this prediction also has padded pixels and so a subset of it will be used in the final prediction.\n    Which time frame, which spatial location (h_start, h_end, w_start,w_end)\n    Args:\n        dset:\n        dset_input_idx:\n        pred_h:\n        pred_w:\n\n    Returns\n    -------\n    \"\"\"\n    extra_padding = dset.per_side_overlap_pixelcount()\n    htw = dset.get_idx_manager().hwt_from_idx(\n        dset_input_idx, grid_size=dset.get_grid_size()\n    )\n    return _get_location(extra_padding, htw, pred_h, pred_w)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_predictions","title":"<code>get_predictions(model, dset, batch_size, tile_size=None, grid_size=None, mmse_count=1, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for the entire dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>VAEModule</code> <p>Lightning model used for prediction.</p> required <code>dset</code> <code>Dataset</code> <p>Dataset to predict on.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use for prediction.</p> required <code>loss_type</code> <p>Type of reconstruction loss used by the model, by default <code>None</code>.</p> required <code>mmse_count</code> <code>int</code> <p>Number of samples to generate for each input and then to average over for MMSE estimation, by default 1.</p> <code>1</code> <code>num_workers</code> <code>int</code> <p>Number of workers to use for DataLoader, by default 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray, ndarray, List[float]]</code> <p>Tuple containing:     - predictions: Predicted images for the dataset.     - predictions_std: Standard deviation of the predicted images.     - logvar_arr: Log variance of the predicted images.     - losses: Reconstruction losses for the predictions.     - psnr: PSNR values for the predictions.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_predictions(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    mmse_count: int = 1,\n    num_workers: int = 4,\n) -&gt; tuple[dict, dict, dict]:\n    \"\"\"Get patch-wise predictions from a model for the entire dataset.\n\n    Parameters\n    ----------\n    model : VAEModule\n        Lightning model used for prediction.\n    dset : Dataset\n        Dataset to predict on.\n    batch_size : int\n        Batch size to use for prediction.\n    loss_type :\n        Type of reconstruction loss used by the model, by default `None`.\n    mmse_count : int, optional\n        Number of samples to generate for each input and then to average over for\n        MMSE estimation, by default 1.\n    num_workers : int, optional\n        Number of workers to use for DataLoader, by default 4.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[float]]\n        Tuple containing:\n            - predictions: Predicted images for the dataset.\n            - predictions_std: Standard deviation of the predicted images.\n            - logvar_arr: Log variance of the predicted images.\n            - losses: Reconstruction losses for the predictions.\n            - psnr: PSNR values for the predictions.\n    \"\"\"\n    if hasattr(dset, \"dsets\"):\n        multifile_stitched_predictions = {}\n        multifile_stitched_stds = {}\n        for d in dset.dsets:\n            stitched_predictions, stitched_stds = get_single_file_mmse(\n                model=model,\n                dset=d,\n                batch_size=batch_size,\n                tile_size=tile_size,\n                grid_size=grid_size,\n                mmse_count=mmse_count,\n                num_workers=num_workers,\n            )\n            # get filename without extension and path\n            filename = d._fpath.name\n            multifile_stitched_predictions[filename] = stitched_predictions\n            multifile_stitched_stds[filename] = stitched_stds\n        return (\n            multifile_stitched_predictions,\n            multifile_stitched_stds,\n        )\n    else:\n        stitched_predictions, stitched_stds = get_single_file_mmse(\n            model=model,\n            dset=dset,\n            batch_size=batch_size,\n            tile_size=tile_size,\n            grid_size=grid_size,\n            mmse_count=mmse_count,\n            num_workers=num_workers,\n        )\n        # get filename without extension and path\n        filename = dset._fpath.name\n        return (\n            {filename: stitched_predictions},\n            {filename: stitched_stds},\n        )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_psnr_str","title":"<code>get_psnr_str(tar_hsnr, pred, col_idx)</code>","text":"<p>Compute PSNR between the ground truth (<code>tar_hsnr</code>) and the predicted image (<code>pred</code>).</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_psnr_str(tar_hsnr, pred, col_idx):\n    \"\"\"\n    Compute PSNR between the ground truth (`tar_hsnr`) and the predicted image (`pred`).\n    \"\"\"\n    return f\"{scale_invariant_psnr(tar_hsnr[col_idx][None], pred[col_idx][None]).item():.1f}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_single_file_mmse","title":"<code>get_single_file_mmse(model, dset, batch_size, tile_size=None, grid_size=None, mmse_count=1, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for a single file dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_single_file_mmse(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    mmse_count: int = 1,\n    num_workers: int = 4,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get patch-wise predictions from a model for a single file dataset.\"\"\"\n    device = get_device()\n\n    dloader = DataLoader(\n        dset,\n        pin_memory=False,\n        num_workers=num_workers,\n        shuffle=False,\n        batch_size=batch_size,\n    )\n    if tile_size and grid_size:\n        dset.set_img_sz(tile_size, grid_size)\n\n    model.eval()\n    model.to(device)\n    tile_mmse = []\n    tile_stds = []\n    logvar_arr = []\n    with torch.no_grad():\n        for batch in tqdm(dloader, desc=\"Predicting tiles\"):\n            inp, tar = batch\n            inp = inp.to(device)\n            tar = tar.to(device)\n\n            rec_img_list = []\n            for _ in range(mmse_count):\n\n                # get model output\n                rec, _ = model(inp)\n\n                # get reconstructed img\n                if model.model.predict_logvar is None:\n                    rec_img = rec\n                    logvar = torch.tensor([-1])\n                else:\n                    rec_img, logvar = torch.chunk(rec, chunks=2, dim=1)\n                rec_img_list.append(rec_img.cpu().unsqueeze(0))  # add MMSE dim\n                logvar_arr.append(logvar.cpu().numpy())  # Why do we need this ?\n\n            # aggregate results\n            samples = torch.cat(rec_img_list, dim=0)\n            mmse_imgs = torch.mean(samples, dim=0)  # avg over MMSE dim\n            std_imgs = torch.std(samples, dim=0)  # std over MMSE dim\n\n            tile_mmse.append(mmse_imgs.cpu().numpy())\n            tile_stds.append(std_imgs.cpu().numpy())\n\n    tiles_arr = np.concatenate(tile_mmse, axis=0)\n    tile_stds = np.concatenate(tile_stds, axis=0)\n    stitched_predictions = stitch_predictions_new(tiles_arr, dset)\n    stitched_stds = stitch_predictions_new(tile_stds, dset)\n    return stitched_predictions, stitched_stds\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_single_file_predictions","title":"<code>get_single_file_predictions(model, dset, batch_size, tile_size=None, grid_size=None, num_workers=4)</code>","text":"<p>Get patch-wise predictions from a model for a single file dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_single_file_predictions(\n    model: VAEModule,\n    dset: Dataset,\n    batch_size: int,\n    tile_size: Optional[tuple[int, int]] = None,\n    grid_size: Optional[int] = None,\n    num_workers: int = 4,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get patch-wise predictions from a model for a single file dataset.\"\"\"\n    if tile_size and grid_size:\n        dset.set_img_sz(tile_size, grid_size)\n\n    device = get_device()\n\n    dloader = DataLoader(\n        dset,\n        pin_memory=False,\n        num_workers=num_workers,\n        shuffle=False,\n        batch_size=batch_size,\n    )\n    model.eval()\n    model.to(device)\n    tiles = []\n    logvar_arr = []\n    with torch.no_grad():\n        for batch in tqdm(dloader, desc=\"Predicting tiles\"):\n            inp, tar = batch\n            inp = inp.to(device)\n            tar = tar.to(device)\n\n            # get model output\n            rec, _ = model(inp)\n\n            # get reconstructed img\n            if model.model.predict_logvar is None:\n                rec_img = rec\n                logvar = torch.tensor([-1])\n            else:\n                rec_img, logvar = torch.chunk(rec, chunks=2, dim=1)\n            logvar_arr.append(logvar.cpu().numpy())  # Why do we need this ?\n\n            tiles.append(rec_img.cpu().numpy())\n\n    tile_samples = np.concatenate(tiles, axis=0)\n    return stitch_predictions_new(tile_samples, dset)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.get_zero_centered_midval","title":"<code>get_zero_centered_midval(error)</code>","text":"<p>When done this way, the midval ensures that the colorbar is centered at 0. (Don't know how, but it works ;))</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def get_zero_centered_midval(error):\n    \"\"\"\n    When done this way, the midval ensures that the colorbar is centered at 0. (Don't know how, but it works ;))\n    \"\"\"\n    vmax = error.max()\n    vmin = error.min()\n    midval = 1 - vmax / (vmax + abs(vmin))\n    return midval\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.plot_calibration","title":"<code>plot_calibration(ax, calibration_stats)</code>","text":"<p>To plot calibration statistics (RMV vs RMSE).</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def plot_calibration(ax, calibration_stats):\n    \"\"\"\n    To plot calibration statistics (RMV vs RMSE).\n    \"\"\"\n    first_idx = get_first_index(calibration_stats[0][\"bin_count\"], 0.001)\n    last_idx = get_last_index(calibration_stats[0][\"bin_count\"], 0.999)\n    ax.plot(\n        calibration_stats[0][\"rmv\"][first_idx:-last_idx],\n        calibration_stats[0][\"rmse\"][first_idx:-last_idx],\n        \"o\",\n        label=r\"$\\hat{C}_0$\",\n    )\n\n    first_idx = get_first_index(calibration_stats[1][\"bin_count\"], 0.001)\n    last_idx = get_last_index(calibration_stats[1][\"bin_count\"], 0.999)\n    ax.plot(\n        calibration_stats[1][\"rmv\"][first_idx:-last_idx],\n        calibration_stats[1][\"rmse\"][first_idx:-last_idx],\n        \"o\",\n        label=r\"$\\hat{C}_1$\",\n    )\n\n    ax.set_xlabel(\"RMV\")\n    ax.set_ylabel(\"RMSE\")\n    ax.legend()\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.plot_error","title":"<code>plot_error(target, prediction, cmap=matplotlib.cm.coolwarm, ax=None, max_val=None)</code>","text":"<p>Plot the relative difference between target and prediction. NOTE: The plot is overlapped to the prediction image (in gray scale). NOTE: The colorbar is centered at 0.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def plot_error(target, prediction, cmap=matplotlib.cm.coolwarm, ax=None, max_val=None):\n    \"\"\"\n    Plot the relative difference between target and prediction.\n    NOTE: The plot is overlapped to the prediction image (in gray scale).\n    NOTE: The colorbar is centered at 0.\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(figsize=(6, 6))\n\n    # Relative difference between target and prediction\n    rel_diff = get_fractional_change(target, prediction, max_val=max_val)\n    midval = get_zero_centered_midval(rel_diff)\n    shifted_cmap = shiftedColorMap(\n        cmap, start=0, midpoint=midval, stop=1.0, name=\"shiftedcmap\"\n    )\n    ax.imshow(prediction, cmap=\"gray\")\n    img_err = ax.imshow(rel_diff, cmap=shifted_cmap, alpha=1)\n    plt.colorbar(img_err, ax=ax)\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.shiftedColorMap","title":"<code>shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap')</code>","text":"<p>Adapted from https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in-matplotlib</p> <p>Function to offset the \"center\" of a colormap. Useful for data with a negative min and positive max and you want the middle of the colormap's dynamic range to be at zero.</p> Input <p>cmap : The matplotlib colormap to be altered   start : Offset from lowest point in the colormap's range.       Defaults to 0.0 (no lower offset). Should be between       0.0 and <code>midpoint</code>.   midpoint : The new center of the colormap. Defaults to       0.5 (no shift). Should be between 0.0 and 1.0. In       general, this should be  1 - vmax / (vmax + abs(vmin))       For example if your data range from -15.0 to +5.0 and       you want the center of the colormap at 0.0, <code>midpoint</code>       should be set to  1 - 5/(5 + 15)) or 0.75   stop : Offset from highest point in the colormap's range.       Defaults to 1.0 (no upper offset). Should be between       <code>midpoint</code> and 1.0.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name=\"shiftedcmap\"):\n    \"\"\"\n    Adapted from https://stackoverflow.com/questions/7404116/defining-the-midpoint-of-a-colormap-in-matplotlib\n\n    Function to offset the \"center\" of a colormap. Useful for\n    data with a negative min and positive max and you want the\n    middle of the colormap's dynamic range to be at zero.\n\n    Input\n    -----\n      cmap : The matplotlib colormap to be altered\n      start : Offset from lowest point in the colormap's range.\n          Defaults to 0.0 (no lower offset). Should be between\n          0.0 and `midpoint`.\n      midpoint : The new center of the colormap. Defaults to\n          0.5 (no shift). Should be between 0.0 and 1.0. In\n          general, this should be  1 - vmax / (vmax + abs(vmin))\n          For example if your data range from -15.0 to +5.0 and\n          you want the center of the colormap at 0.0, `midpoint`\n          should be set to  1 - 5/(5 + 15)) or 0.75\n      stop : Offset from highest point in the colormap's range.\n          Defaults to 1.0 (no upper offset). Should be between\n          `midpoint` and 1.0.\n    \"\"\"\n    cdict = {\"red\": [], \"green\": [], \"blue\": [], \"alpha\": []}\n\n    # regular index to compute the colors\n    reg_index = np.linspace(start, stop, 257)\n    mid_idx = len(reg_index) // 2\n    # shifted index to match the data\n    shift_index = np.hstack(\n        [\n            np.linspace(0.0, midpoint, 128, endpoint=False),\n            np.linspace(midpoint, 1.0, 129, endpoint=True),\n        ]\n    )\n\n    for ri, si in zip(reg_index, shift_index):\n        r, g, b, a = cmap(ri)\n        a = np.abs(ri - reg_index[mid_idx]) / reg_index[mid_idx]\n        # print(a)\n        cdict[\"red\"].append((si, r, r))\n        cdict[\"green\"].append((si, g, g))\n        cdict[\"blue\"].append((si, b, b))\n        cdict[\"alpha\"].append((si, a, a))\n\n    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n    matplotlib.colormaps.register(cmap=newcmap, force=True)\n\n    return newcmap\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.show_for_one","title":"<code>show_for_one(idx, val_dset, highsnr_val_dset, model, calibration_stats, mmse_count=5, patch_size=256, num_samples=2, baseline_preds=None)</code>","text":"<p>Given an index, it plots the input, target, reconstructed images and the difference image. Note the the difference image is computed with respect to a ground truth image, obtained from the high SNR dataset.</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def show_for_one(\n    idx,\n    val_dset,\n    highsnr_val_dset,\n    model,\n    calibration_stats,\n    mmse_count=5,\n    patch_size=256,\n    num_samples=2,\n    baseline_preds=None,\n):\n    \"\"\"\n    Given an index, it plots the input, target, reconstructed images and the difference image.\n    Note the the difference image is computed with respect to a ground truth image, obtained from the high SNR dataset.\n    \"\"\"\n    highsnr_val_dset.set_img_sz(patch_size, 64)\n    highsnr_val_dset.disable_noise()\n    _, tar_hsnr = highsnr_val_dset[idx]\n    inp, tar, recon_img_list = get_predictions(\n        idx, val_dset, model, mmse_count=mmse_count, patch_size=patch_size\n    )\n    plot_crops(\n        inp,\n        tar,\n        tar_hsnr,\n        recon_img_list,\n        calibration_stats,\n        num_samples=num_samples,\n        baseline_preds=baseline_preds,\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.stitch_predictions","title":"<code>stitch_predictions(predictions, dset, smoothening_pixelcount=0)</code>","text":"<p>Args:     smoothening_pixelcount: number of pixels which can be interpolated</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def stitch_predictions(predictions, dset, smoothening_pixelcount=0):\n    \"\"\"\n    Args:\n        smoothening_pixelcount: number of pixels which can be interpolated\n    \"\"\"\n    assert smoothening_pixelcount &gt;= 0 and isinstance(smoothening_pixelcount, int)\n    extra_padding = dset.per_side_overlap_pixelcount()\n    # if there are more channels, use all of them.\n    shape = list(dset.get_data_shape())\n    shape[-1] = max(shape[-1], predictions.shape[1])\n\n    output = np.zeros(shape, dtype=predictions.dtype)\n    frame_shape = dset.get_data_shape()[1:3]\n    for dset_input_idx in range(predictions.shape[0]):\n        loc = get_location_from_idx(\n            dset, dset_input_idx, predictions.shape[-2], predictions.shape[-1]\n        )\n\n        mask = None\n        cropped_pred_list = []\n        for ch_idx in range(predictions.shape[1]):\n            # class i\n            cropped_pred_i = remove_pad(\n                predictions[dset_input_idx, ch_idx],\n                loc,\n                extra_padding,\n                smoothening_pixelcount,\n                frame_shape,\n            )\n\n            if mask is None:\n                # NOTE: don't need to compute it for every patch.\n                assert (\n                    smoothening_pixelcount == 0\n                ), \"For smoothing,enable the get_smoothing_mask. It is disabled since I don't use it and it needs modification to work with non-square images\"\n                mask = 1\n                # mask = _get_smoothing_mask(cropped_pred_i.shape, smoothening_pixelcount, loc, frame_size)\n\n            cropped_pred_list.append(cropped_pred_i)\n\n        loc = update_loc_for_final_insertion(loc, extra_padding, smoothening_pixelcount)\n        for ch_idx in range(predictions.shape[1]):\n            output[loc.t, loc.h_start : loc.h_end, loc.w_start : loc.w_end, ch_idx] += (\n                cropped_pred_list[ch_idx] * mask\n            )\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/eval_utils/#careamics.lvae_training.eval_utils.stitch_predictions_new","title":"<code>stitch_predictions_new(predictions, dset)</code>","text":"<p>Args:     smoothening_pixelcount: number of pixels which can be interpolated</p> Source code in <code>src/careamics/lvae_training/eval_utils.py</code> <pre><code>def stitch_predictions_new(predictions, dset):\n    \"\"\"\n    Args:\n        smoothening_pixelcount: number of pixels which can be interpolated\n    \"\"\"\n    # Commented out since it is not used as of now\n    # if isinstance(dset, MultiFileDset):\n    #     cum_count = 0\n    #     output = []\n    #     for dset in dset.dsets:\n    #         cnt = dset.idx_manager.total_grid_count()\n    #         output.append(\n    #             stitch_predictions(predictions[cum_count:cum_count + cnt], dset))\n    #         cum_count += cnt\n    #     return output\n\n    # else:\n    mng = dset.idx_manager\n\n    # if there are more channels, use all of them.\n    shape = list(dset.get_data_shape())\n    shape[-1] = max(shape[-1], predictions.shape[1])\n\n    output = np.zeros(shape, dtype=predictions.dtype)\n    # frame_shape = dset.get_data_shape()[:-1]\n    for dset_idx in range(predictions.shape[0]):\n        # loc = get_location_from_idx(dset, dset_idx, predictions.shape[-2], predictions.shape[-1])\n        # grid start, grid end\n        gs = np.array(mng.get_location_from_dataset_idx(dset_idx), dtype=int)\n        ge = gs + mng.grid_shape\n\n        # patch start, patch end\n        ps = gs - mng.patch_offset()\n        pe = ps + mng.patch_shape\n        # print('PS')\n        # print(ps)\n        # print(pe)\n\n        # valid grid start, valid grid end\n        vgs = np.array([max(0, x) for x in gs], dtype=int)\n        vge = np.array([min(x, y) for x, y in zip(ge, mng.data_shape)], dtype=int)\n        # assert np.all(vgs == gs)\n        # assert np.all(vge == ge) # TODO comented out this shit cuz I have no interest to dig why it's failing at this point !\n        # print('VGS')\n        # print(gs)\n        # print(ge)\n\n        if mng.tiling_mode == TilingMode.ShiftBoundary:\n            for dim in range(len(vgs)):\n                if ps[dim] == 0:\n                    vgs[dim] = 0\n                if pe[dim] == mng.data_shape[dim]:\n                    vge[dim] = mng.data_shape[dim]\n\n        # relative start, relative end. This will be used on pred_tiled\n        rs = vgs - ps\n        re = rs + (vge - vgs)\n        # print('RS')\n        # print(rs)\n        # print(re)\n\n        # print(output.shape)\n        # print(predictions.shape)\n        for ch_idx in range(predictions.shape[1]):\n            if len(output.shape) == 4:\n                # channel dimension is the last one.\n                output[vgs[0] : vge[0], vgs[1] : vge[1], vgs[2] : vge[2], ch_idx] = (\n                    predictions[dset_idx][ch_idx, rs[1] : re[1], rs[2] : re[2]]\n                )\n            elif len(output.shape) == 5:\n                # channel dimension is the last one.\n                assert vge[0] - vgs[0] == 1, \"Only one frame is supported\"\n                output[\n                    vgs[0], vgs[1] : vge[1], vgs[2] : vge[2], vgs[3] : vge[3], ch_idx\n                ] = predictions[dset_idx][\n                    ch_idx, rs[1] : re[1], rs[2] : re[2], rs[3] : re[3]\n                ]\n            else:\n                raise ValueError(f\"Unsupported shape {output.shape}\")\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/get_config/","title":"get_config","text":"<p>Here there are functions to define a config file.</p>"},{"location":"reference/careamics/lvae_training/get_config/#careamics.lvae_training.get_config._init_config","title":"<code>_init_config()</code>","text":"<p>Create a default config object with all the required fields.</p> Source code in <code>src/careamics/lvae_training/get_config.py</code> <pre><code>def _init_config():\n    \"\"\"\n    Create a default config object with all the required fields.\n    \"\"\"\n    config = ml_collections.ConfigDict()\n\n    config.data = ml_collections.ConfigDict()\n\n    config.model = ml_collections.ConfigDict()\n\n    config.loss = ml_collections.ConfigDict()\n\n    config.training = ml_collections.ConfigDict()\n\n    config.workdir = os.getcwd()\n    config.datadir = \"\"\n    return config\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/","title":"lightning_module","text":"<p>Lightning Module for LadderVAE.</p>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight","title":"<code>LadderVAELight</code>","text":"<p>               Bases: <code>LightningModule</code></p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>class LadderVAELight(L.LightningModule):\n\n    def __init__(\n        self,\n        config: ml_collections.ConfigDict,\n        data_mean: Dict[str, torch.Tensor],\n        data_std: Dict[str, torch.Tensor],\n        target_ch: int,\n    ):\n        \"\"\"\n        Here we will do the following:\n            - initialize the model (from LadderVAE class)\n            - initialize the parameters related to the training and loss.\n\n        NOTE:\n        Some of the model attributes are defined in the model object itself, while some others will be defined here.\n        Note that all the attributes related to the training and loss that were already defined in the model object\n        are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr).\n        The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).\n\n        NOTE: HC stands for Hard Coded attribute.\n        \"\"\"\n        super().__init__()\n\n        self.data_mean = data_mean\n        self.data_std = data_std\n        self.target_ch = target_ch\n\n        # Initialize LVAE model\n        self.model = LadderVAE(\n            data_mean=data_mean, data_std=data_std, config=config, target_ch=target_ch\n        )\n\n        ##### Define attributes from config #####\n        self.workdir = config.workdir\n        self._input_is_sum = False\n        self.kl_loss_formulation = config.loss.kl_loss_formulation\n        assert self.kl_loss_formulation in [\n            None,\n            \"\",\n            \"usplit\",\n            \"denoisplit\",\n            \"denoisplit_usplit\",\n        ], f\"\"\"\n            Invalid kl_loss_formulation. {self.kl_loss_formulation}\"\"\"\n\n        ##### Define loss attributes #####\n        # Parameters already defined in the model object\n        self.loss_type = self.model.loss_type\n        self._denoisplit_w = self._usplit_w = None\n        if self.loss_type == LossType.DenoiSplitMuSplit:\n            self._usplit_w = 0\n            self._denoisplit_w = 1 - self._usplit_w\n            assert self._denoisplit_w + self._usplit_w == 1\n        self._restricted_kl = self.model._restricted_kl\n\n        # General loss parameters\n        self.channel_1_w = 1\n        self.channel_2_w = 1\n\n        # About Reconsruction Loss\n        self.reconstruction_mode = False\n        self.skip_nboundary_pixels_from_loss = None\n        self.reconstruction_weight = 1.0\n        self._exclusion_loss_weight = 0\n        self.ch1_recons_w = 1\n        self.ch2_recons_w = 1\n        self.enable_mixed_rec = False\n        self.mixed_rec_w_step = 0\n\n        # About KL Loss\n        self.kl_weight = 1.0  # HC\n        self.usplit_kl_weight = None  # HC\n        self.free_bits = 1.0  # HC\n        self.kl_annealing = False  # HC\n        self.kl_annealtime = self.kl_start = None\n        if self.kl_annealing:\n            self.kl_annealtime = 10  # HC\n            self.kl_start = -1  # HC\n\n        ##### Define training attributes #####\n        self.lr = config.training.lr\n        self.lr_scheduler_patience = config.training.lr_scheduler_patience\n        self.lr_scheduler_monitor = config.model.get(\"monitor\", \"val_loss\")\n        self.lr_scheduler_mode = MetricMonitor(self.lr_scheduler_monitor).mode()\n\n        # Initialize object for keeping track of PSNR for each output channel\n        self.channels_psnr = [RunningPSNR() for _ in range(self.model.target_ch)]\n\n    def forward(self, x: Any) -&gt; Any:\n        return self.model(x)\n\n    def training_step(\n        self, batch: torch.Tensor, batch_idx: int, enable_logging: bool = True\n    ) -&gt; Dict[str, torch.Tensor]:\n\n        if self.current_epoch == 0 and batch_idx == 0:\n            self.log(\"val_psnr\", 1.0, on_epoch=True)\n\n        # Pre-processing of inputs\n        x, target = batch[:2]\n        self.set_params_to_same_device_as(x)\n        x_normalized = self.normalize_input(x)\n        if self.reconstruction_mode:  # just for experimental purpose\n            target_normalized = x_normalized[:, :1].repeat(1, 2, 1, 1)\n            target = None\n            mask = None\n        else:\n            target_normalized = self.normalize_target(target)\n            mask = ~((target == 0).reshape(len(target), -1).all(dim=1))\n\n        # Forward pass\n        out, td_data = self.forward(x_normalized)\n\n        if (\n            self.model.encoder_no_padding_mode\n            and out.shape[-2:] != target_normalized.shape[-2:]\n        ):\n            target_normalized = F.center_crop(target_normalized, out.shape[-2:])\n\n        # Loss Computations\n        # mask = torch.isnan(target.reshape(len(x), -1)).all(dim=1)\n        recons_loss_dict, imgs = self.get_reconstruction_loss(\n            reconstruction=out,\n            target=target_normalized,\n            input=x_normalized,\n            splitting_mask=mask,\n            return_predicted_img=True,\n        )\n\n        # This `if` is not used by default config\n        if self.skip_nboundary_pixels_from_loss:\n            pad = self.skip_nboundary_pixels_from_loss\n            target_normalized = target_normalized[:, :, pad:-pad, pad:-pad]\n\n        recons_loss = recons_loss_dict[\"loss\"] * self.reconstruction_weight\n\n        if torch.isnan(recons_loss).any():\n            recons_loss = 0.0\n\n        if self.model.non_stochastic_version:\n            kl_loss = torch.Tensor([0.0]).cuda()\n            net_loss = recons_loss\n        else:\n            if self.loss_type == LossType.DenoiSplitMuSplit:\n                msg = f\"For the loss type {LossType.name(self.loss_type)}, kl_loss_formulation must be denoisplit_usplit\"\n                assert self.kl_loss_formulation == \"denoisplit_usplit\", msg\n                assert self._denoisplit_w is not None and self._usplit_w is not None\n\n                kl_key_denoisplit = \"kl_restricted\" if self._restricted_kl else \"kl\"\n                # NOTE: 'kl' key stands for the 'kl_samplewise' key in the TopDownLayer class.\n                # The different naming comes from `top_down_pass()` method in the LadderVAE class.\n                denoisplit_kl = self.get_kl_divergence_loss(\n                    topdown_layer_data_dict=td_data, kl_key=kl_key_denoisplit\n                )\n                usplit_kl = self.get_kl_divergence_loss_usplit(\n                    topdown_layer_data_dict=td_data\n                )\n                kl_loss = (\n                    self._denoisplit_w * denoisplit_kl + self._usplit_w * usplit_kl\n                )\n                kl_loss = self.kl_weight * kl_loss\n\n                recons_loss = self.reconstruction_loss_musplit_denoisplit(\n                    out, target_normalized\n                )\n                # recons_loss = self._denoisplit_w * recons_loss_nm + self._usplit_w * recons_loss_gm\n\n            elif self.kl_loss_formulation == \"usplit\":\n                kl_loss = self.get_kl_weight() * self.get_kl_divergence_loss_usplit(\n                    td_data\n                )\n            elif self.kl_loss_formulation in [\"\", \"denoisplit\"]:\n                kl_loss = self.get_kl_weight() * self.get_kl_divergence_loss(td_data)\n            net_loss = recons_loss + kl_loss\n\n        # Logging\n        if enable_logging:\n            for i, x in enumerate(td_data[\"debug_qvar_max\"]):\n                self.log(f\"qvar_max:{i}\", x.item(), on_epoch=True)\n\n            self.log(\"reconstruction_loss\", recons_loss_dict[\"loss\"], on_epoch=True)\n            self.log(\"kl_loss\", kl_loss, on_epoch=True)\n            self.log(\"training_loss\", net_loss, on_epoch=True)\n            self.log(\"lr\", self.lr, on_epoch=True)\n            if self.model._tethered_ch2_scalar is not None:\n                self.log(\n                    \"tethered_ch2_scalar\",\n                    self.model._tethered_ch2_scalar,\n                    on_epoch=True,\n                )\n                self.log(\n                    \"tethered_ch1_scalar\",\n                    self.model._tethered_ch1_scalar,\n                    on_epoch=True,\n                )\n\n            # self.log('grad_norm_bottom_up', self.grad_norm_bottom_up, on_epoch=True)\n            # self.log('grad_norm_top_down', self.grad_norm_top_down, on_epoch=True)\n\n        output = {\n            \"loss\": net_loss,\n            \"reconstruction_loss\": (\n                recons_loss.detach()\n                if isinstance(recons_loss, torch.Tensor)\n                else recons_loss\n            ),\n            \"kl_loss\": kl_loss.detach(),\n        }\n        # https://github.com/openai/vdvae/blob/main/train.py#L26\n        if torch.isnan(net_loss).any():\n            return None\n\n        return output\n\n    def validation_step(self, batch: torch.Tensor, batch_idx: int):\n        # Pre-processing of inputs\n        x, target = batch[:2]\n        self.set_params_to_same_device_as(x)\n        x_normalized = self.normalize_input(x)\n        if self.reconstruction_mode:  # only for experimental purpose\n            target_normalized = x_normalized[:, :1].repeat(1, 2, 1, 1)\n            target = None\n            mask = None\n        else:\n            target_normalized = self.normalize_target(target)\n            mask = ~((target == 0).reshape(len(target), -1).all(dim=1))\n\n        # Forward pass\n        out, _ = self.forward(x_normalized)\n\n        if self.model.predict_logvar is not None:\n            out_mean, _ = out.chunk(2, dim=1)\n        else:\n            out_mean = out\n\n        if (\n            self.model.encoder_no_padding_mode\n            and out.shape[-2:] != target_normalized.shape[-2:]\n        ):\n            target_normalized = F.center_crop(target_normalized, out.shape[-2:])\n\n        if self.loss_type == LossType.DenoiSplitMuSplit:\n            recons_loss = self.reconstruction_loss_musplit_denoisplit(\n                out, target_normalized\n            )\n            recons_loss_dict = {\"loss\": recons_loss}\n            recons_img = out_mean\n        else:\n            # Metrics computation\n            recons_loss_dict, recons_img = self.get_reconstruction_loss(\n                reconstruction=out_mean,\n                target=target_normalized,\n                input=x_normalized,\n                splitting_mask=mask,\n                return_predicted_img=True,\n            )\n\n        # This `if` is not used by default config\n        if self.skip_nboundary_pixels_from_loss:\n            pad = self.skip_nboundary_pixels_from_loss\n            target_normalized = target_normalized[:, :, pad:-pad, pad:-pad]\n\n        channels_rinvpsnr = []\n        for i in range(target_normalized.shape[1]):\n            self.channels_psnr[i].update(recons_img[:, i], target_normalized[:, i])\n            psnr = RangeInvariantPsnr(\n                target_normalized[:, i].clone(), recons_img[:, i].clone()\n            )\n            channels_rinvpsnr.append(psnr)\n            psnr = torch_nanmean(psnr).item()\n            self.log(f\"val_psnr_l{i+1}\", psnr, on_epoch=True)\n\n        recons_loss = recons_loss_dict[\"loss\"]\n        if torch.isnan(recons_loss).any():\n            return\n\n        self.log(\"val_loss\", recons_loss, on_epoch=True)\n        # self.log('val_psnr', (val_psnr_l1 + val_psnr_l2) / 2, on_epoch=True)\n\n        # if batch_idx == 0 and self.power_of_2(self.current_epoch):\n        #     all_samples = []\n        #     for i in range(20):\n        #         sample, _ = self(x_normalized[0:1, ...])\n        #         sample = self.likelihood.get_mean_lv(sample)[0]\n        #         all_samples.append(sample[None])\n\n        #     all_samples = torch.cat(all_samples, dim=0)\n        #     all_samples = all_samples * self.data_std + self.data_mean\n        #     all_samples = all_samples.cpu()\n        #     img_mmse = torch.mean(all_samples, dim=0)[0]\n        #     self.log_images_for_tensorboard(all_samples[:, 0, 0, ...], target[0, 0, ...], img_mmse[0], 'label1')\n        #     self.log_images_for_tensorboard(all_samples[:, 0, 1, ...], target[0, 1, ...], img_mmse[1], 'label2')\n\n        # return net_loss\n\n    def on_validation_epoch_end(self):\n        psnr_arr = []\n        for i in range(len(self.channels_psnr)):\n            psnr = self.channels_psnr[i].get()\n            if psnr is None:\n                psnr_arr = None\n                break\n            psnr_arr.append(psnr.cpu().numpy())\n            self.channels_psnr[i].reset()\n\n        if psnr_arr is not None:\n            psnr = np.mean(psnr_arr)\n            self.log(\"val_psnr\", psnr, on_epoch=True)\n        else:\n            self.log(\"val_psnr\", 0.0, on_epoch=True)\n\n        if self.mixed_rec_w_step:\n            self.mixed_rec_w = max(self.mixed_rec_w - self.mixed_rec_w_step, 0.0)\n            self.log(\"mixed_rec_w\", self.mixed_rec_w, on_epoch=True)\n\n    def predict_step(self, batch: torch.Tensor, batch_idx: Any) -&gt; Any:\n        raise NotImplementedError(\"predict_step is not implemented\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adamax(self.parameters(), lr=self.lr, weight_decay=0)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            self.lr_scheduler_mode,\n            patience=self.lr_scheduler_patience,\n            factor=0.5,\n            min_lr=1e-12,\n            verbose=True,\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": scheduler,\n            \"monitor\": self.lr_scheduler_monitor,\n        }\n\n    ##### REQUIRED Methods for Loss Computation #####\n    def get_reconstruction_loss(\n        self,\n        reconstruction: torch.Tensor,\n        target: torch.Tensor,\n        input: torch.Tensor,\n        splitting_mask: torch.Tensor = None,\n        return_predicted_img: bool = False,\n        likelihood_obj: LikelihoodModule = None,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Parameters\n        ----------\n        reconstruction: torch.Tensor,\n        target: torch.Tensor\n        input: torch.Tensor\n        splitting_mask: torch.Tensor = None\n            A boolean tensor that indicates which items to keep for reconstruction loss computation.\n            If `None`, all the elements of the items are considered (i.e., the mask is all `True`).\n        return_predicted_img: bool = False\n        likelihood_obj: LikelihoodModule = None\n        \"\"\"\n        output = self._get_reconstruction_loss_vector(\n            reconstruction=reconstruction,\n            target=target,\n            input=input,\n            return_predicted_img=return_predicted_img,\n            likelihood_obj=likelihood_obj,\n        )\n        loss_dict = output[0] if return_predicted_img else output\n\n        if splitting_mask is None:\n            splitting_mask = torch.ones_like(loss_dict[\"loss\"]).bool()\n\n        # print(len(target) - (torch.isnan(loss_dict['loss'])).sum())\n\n        loss_dict[\"loss\"] = loss_dict[\"loss\"][splitting_mask].sum() / len(\n            reconstruction\n        )\n        for i in range(1, 1 + target.shape[1]):\n            key = f\"ch{i}_loss\"\n            loss_dict[key] = loss_dict[key][splitting_mask].sum() / len(reconstruction)\n\n        if \"mixed_loss\" in loss_dict:\n            loss_dict[\"mixed_loss\"] = torch.mean(loss_dict[\"mixed_loss\"])\n        if return_predicted_img:\n            assert len(output) == 2\n            return loss_dict, output[1]\n        else:\n            return loss_dict\n\n    def _get_reconstruction_loss_vector(\n        self,\n        reconstruction: torch.Tensor,\n        target: torch.Tensor,\n        input: torch.Tensor,\n        return_predicted_img: bool = False,\n        likelihood_obj: LikelihoodModule = None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        return_predicted_img: bool\n            If set to `True`, the besides the loss, the reconstructed image is also returned.\n            Default is `False`.\n        \"\"\"\n        output = {\n            \"loss\": None,\n            \"mixed_loss\": None,\n        }\n\n        for i in range(1, 1 + target.shape[1]):\n            output[f\"ch{i}_loss\"] = None\n\n        if likelihood_obj is None:\n            likelihood_obj = self.model.likelihood\n\n        # Log likelihood\n        ll, like_dict = likelihood_obj(reconstruction, target)\n        ll = self._get_weighted_likelihood(ll)\n        if (\n            self.skip_nboundary_pixels_from_loss is not None\n            and self.skip_nboundary_pixels_from_loss &gt; 0\n        ):\n            pad = self.skip_nboundary_pixels_from_loss\n            ll = ll[:, :, pad:-pad, pad:-pad]\n            like_dict[\"params\"][\"mean\"] = like_dict[\"params\"][\"mean\"][\n                :, :, pad:-pad, pad:-pad\n            ]\n\n        # assert ll.shape[1] == 2, f\"Change the code below to handle &gt;2 channels first. ll.shape {ll.shape}\"\n        output = {\"loss\": compute_batch_mean(-1 * ll)}\n        if ll.shape[1] &gt; 1:\n            for i in range(1, 1 + target.shape[1]):\n                output[f\"ch{i}_loss\"] = compute_batch_mean(-ll[:, i - 1])\n        else:\n            assert ll.shape[1] == 1\n            output[\"ch1_loss\"] = output[\"loss\"]\n            output[\"ch2_loss\"] = output[\"loss\"]\n\n        if (\n            self.channel_1_w is not None\n            and self.channel_2_w is not None\n            and (self.channel_1_w != 1 or self.channel_2_w != 1)\n        ):\n            assert ll.shape[1] == 2, \"Only 2 channels are supported for now.\"\n            output[\"loss\"] = (\n                self.channel_1_w * output[\"ch1_loss\"]\n                + self.channel_2_w * output[\"ch2_loss\"]\n            ) / (self.channel_1_w + self.channel_2_w)\n\n        # This `if` is not used by default config\n        if self.enable_mixed_rec:\n            mixed_pred, mixed_logvar = self.get_mixed_prediction(\n                like_dict[\"params\"][\"mean\"],\n                like_dict[\"params\"][\"logvar\"],\n                self.data_mean,\n                self.data_std,\n            )\n            if (\n                self.model._multiscale_count is not None\n                and self.model._multiscale_count &gt; 1\n            ):\n                assert input.shape[1] == self.model._multiscale_count\n                input = input[:, :1]\n\n            assert (\n                input.shape == mixed_pred.shape\n            ), \"No fucking room for vectorization induced bugs.\"\n            mixed_recons_ll = self.model.likelihood.log_likelihood(\n                input, {\"mean\": mixed_pred, \"logvar\": mixed_logvar}\n            )\n            output[\"mixed_loss\"] = compute_batch_mean(-1 * mixed_recons_ll)\n\n        # This `if` is not used by default config\n        if self._exclusion_loss_weight:\n            raise NotImplementedError(\n                \"Exclusion loss is not well defined here, so it should not be used.\"\n            )\n            imgs = like_dict[\"params\"][\"mean\"]\n            exclusion_loss = compute_exclusion_loss(imgs[:, :1], imgs[:, 1:])\n            output[\"exclusion_loss\"] = exclusion_loss\n\n        if return_predicted_img:\n            return output, like_dict[\"params\"][\"mean\"]\n\n        return output\n\n    def reconstruction_loss_musplit_denoisplit(self, out, target_normalized):\n        if self.model.predict_logvar is not None:\n            out_mean, _ = out.chunk(2, dim=1)\n        else:\n            out_mean = out\n\n        recons_loss_nm = (\n            -1 * self.model.likelihood_NM(out_mean, target_normalized)[0].mean()\n        )\n        recons_loss_gm = -1 * self.model.likelihood_gm(out, target_normalized)[0].mean()\n        recons_loss = (\n            self._denoisplit_w * recons_loss_nm + self._usplit_w * recons_loss_gm\n        )\n        return recons_loss\n\n    def _get_weighted_likelihood(self, ll):\n        \"\"\"\n        Each of the channels gets multiplied with a different weight.\n        \"\"\"\n        if self.ch1_recons_w == 1 and self.ch2_recons_w == 1:\n            return ll\n\n        assert ll.shape[1] == 2, \"This function is only for 2 channel images\"\n\n        mask1 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n        mask1[:, 0] = 1\n        mask2 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n        mask2[:, 1] = 1\n\n        return ll * mask1 * self.ch1_recons_w + ll * mask2 * self.ch2_recons_w\n\n    def get_kl_weight(self):\n        \"\"\"\n        KL loss can be weighted depending whether any annealing procedure is used.\n        This function computes the weight of the KL loss in case of annealing.\n        \"\"\"\n        if self.kl_annealing == True:\n            # calculate relative weight\n            kl_weight = (self.current_epoch - self.kl_start) * (\n                1.0 / self.kl_annealtime\n            )\n            # clamp to [0,1]\n            kl_weight = min(max(0.0, kl_weight), 1.0)\n\n            # if the final weight is given, then apply that weight on top of it\n            if self.kl_weight is not None:\n                kl_weight = kl_weight * self.kl_weight\n        elif self.kl_weight is not None:\n            return self.kl_weight\n        else:\n            kl_weight = 1.0\n        return kl_weight\n\n    def get_kl_divergence_loss_usplit(\n        self, topdown_layer_data_dict: Dict[str, torch.Tensor]\n    ) -&gt; torch.Tensor:\n        \"\"\" \"\"\"\n        kl = torch.cat(\n            [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[\"kl\"]], dim=1\n        )\n        # NOTE: kl.shape = (16,4) 16 is batch size. 4 is number of layers.\n        # Values are sum() and so are of the order 30000\n        # Example values: 30626.6758, 31028.8145, 29509.8809, 29945.4922, 28919.1875, 29075.2988\n\n        nlayers = kl.shape[1]\n        for i in range(nlayers):\n            # topdown_layer_data_dict['z'][2].shape[-3:] = 128 * 32 * 32\n            norm_factor = np.prod(topdown_layer_data_dict[\"z\"][i].shape[-3:])\n            # if self._restricted_kl:\n            #     pow = np.power(2,min(i + 1, self._multiscale_count-1))\n            #     norm_factor /= pow * pow\n\n            kl[:, i] = kl[:, i] / norm_factor\n\n        kl_loss = free_bits_kl(kl, 0.0).mean()\n        return kl_loss\n\n    def get_kl_divergence_loss(self, topdown_layer_data_dict, kl_key=\"kl\"):\n        \"\"\"\n        kl[i] for each i has length batch_size\n        resulting kl shape: (batch_size, layers)\n        \"\"\"\n        kl = torch.cat(\n            [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[kl_key]],\n            dim=1,\n        )\n\n        # As compared to uSplit kl divergence,\n        # more by a factor of 4 just because we do sum and not mean.\n        kl_loss = free_bits_kl(kl, self.free_bits).sum()\n        # NOTE: at each hierarchy, it is more by a factor of 128/i**2).\n        # 128/(2*2) = 32 (bottommost layer)\n        # 128/(4*4) = 8\n        # 128/(8*8) = 2\n        # 128/(16*16) = 0.5 (topmost layer)\n\n        # Normalize the KL-loss w.r.t. the  latent space\n        kl_loss = kl_loss / np.prod(self.model.img_shape)\n        return kl_loss\n\n    ##### UTILS Methods #####\n    def normalize_input(self, x):\n        if self.model.normalized_input:\n            return x\n        return (x - self.data_mean[\"input\"].mean()) / self.data_std[\"input\"].mean()\n\n    def normalize_target(self, target, batch=None):\n        return (target - self.data_mean[\"target\"]) / self.data_std[\"target\"]\n\n    def unnormalize_target(self, target_normalized):\n        return target_normalized * self.data_std[\"target\"] + self.data_mean[\"target\"]\n\n    ##### ADDITIONAL Methods #####\n    # def log_images_for_tensorboard(self, pred, target, img_mmse, label):\n    #     clamped_pred = torch.clamp((pred - pred.min()) / (pred.max() - pred.min()), 0, 1)\n    #     clamped_mmse = torch.clamp((img_mmse - img_mmse.min()) / (img_mmse.max() - img_mmse.min()), 0, 1)\n    #     if target is not None:\n    #         clamped_input = torch.clamp((target - target.min()) / (target.max() - target.min()), 0, 1)\n    #         img = wandb.Image(clamped_input[None].cpu().numpy())\n    #         self.logger.experiment.log({f'target_for{label}': img})\n    #         # self.trainer.logger.experiment.add_image(f'target_for{label}', clamped_input[None], self.current_epoch)\n    #     for i in range(3):\n    #         # self.trainer.logger.experiment.add_image(f'{label}/sample_{i}', clamped_pred[i:i + 1], self.current_epoch)\n    #         img = wandb.Image(clamped_pred[i:i + 1].cpu().numpy())\n    #         self.logger.experiment.log({f'{label}/sample_{i}': img})\n\n    #     img = wandb.Image(clamped_mmse[None].cpu().numpy())\n    #     self.trainer.logger.experiment.log({f'{label}/mmse (100 samples)': img})\n\n    @property\n    def global_step(self) -&gt; int:\n        \"\"\"Global step.\"\"\"\n        return self._global_step\n\n    def increment_global_step(self):\n        \"\"\"Increments global step by 1.\"\"\"\n        self._global_step += 1\n\n    def set_params_to_same_device_as(self, correct_device_tensor: torch.Tensor):\n\n        self.model.likelihood.set_params_to_same_device_as(correct_device_tensor)\n        if isinstance(self.data_mean, torch.Tensor):\n            if self.data_mean.device != correct_device_tensor.device:\n                self.data_mean = self.data_mean.to(correct_device_tensor.device)\n                self.data_std = self.data_std.to(correct_device_tensor.device)\n        elif isinstance(self.data_mean, dict):\n            for k, v in self.data_mean.items():\n                if v.device != correct_device_tensor.device:\n                    self.data_mean[k] = v.to(correct_device_tensor.device)\n                    self.data_std[k] = self.data_std[k].to(correct_device_tensor.device)\n\n    def get_mixed_prediction(\n        self, prediction, prediction_logvar, data_mean, data_std, channel_weights=None\n    ):\n        pred_unorm = prediction * data_std[\"target\"] + data_mean[\"target\"]\n        if channel_weights is None:\n            channel_weights = 1\n\n        if self._input_is_sum:\n            mixed_prediction = torch.sum(\n                pred_unorm * channel_weights, dim=1, keepdim=True\n            )\n        else:\n            mixed_prediction = torch.mean(\n                pred_unorm * channel_weights, dim=1, keepdim=True\n            )\n\n        mixed_prediction = (mixed_prediction - data_mean[\"input\"].mean()) / data_std[\n            \"input\"\n        ].mean()\n\n        if prediction_logvar is not None:\n            if data_std[\"target\"].shape == data_std[\"input\"].shape and torch.all(\n                data_std[\"target\"] == data_std[\"input\"]\n            ):\n                assert channel_weights == 1\n                logvar = prediction_logvar\n            else:\n                var = torch.exp(prediction_logvar)\n                var = var * (data_std[\"target\"] / data_std[\"input\"]) ** 2\n                if channel_weights != 1:\n                    var = var * torch.square(channel_weights)\n\n                # sum of variance.\n                mixed_var = 0\n                for i in range(var.shape[1]):\n                    mixed_var += var[:, i : i + 1]\n\n                logvar = torch.log(mixed_var)\n        else:\n            logvar = None\n        return mixed_prediction, logvar\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.global_step","title":"<code>global_step</code>  <code>property</code>","text":"<p>Global step.</p>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.__init__","title":"<code>__init__(config, data_mean, data_std, target_ch)</code>","text":"<p>Here we will do the following:     - initialize the model (from LadderVAE class)     - initialize the parameters related to the training and loss.</p> <p>NOTE: Some of the model attributes are defined in the model object itself, while some others will be defined here. Note that all the attributes related to the training and loss that were already defined in the model object are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr). The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).</p> <p>NOTE: HC stands for Hard Coded attribute.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def __init__(\n    self,\n    config: ml_collections.ConfigDict,\n    data_mean: Dict[str, torch.Tensor],\n    data_std: Dict[str, torch.Tensor],\n    target_ch: int,\n):\n    \"\"\"\n    Here we will do the following:\n        - initialize the model (from LadderVAE class)\n        - initialize the parameters related to the training and loss.\n\n    NOTE:\n    Some of the model attributes are defined in the model object itself, while some others will be defined here.\n    Note that all the attributes related to the training and loss that were already defined in the model object\n    are redefined here as Lightning module attributes (e.g., self.some_attr = model.some_attr).\n    The attributes related to the model itself are treated as model attributes (e.g., self.model.some_attr).\n\n    NOTE: HC stands for Hard Coded attribute.\n    \"\"\"\n    super().__init__()\n\n    self.data_mean = data_mean\n    self.data_std = data_std\n    self.target_ch = target_ch\n\n    # Initialize LVAE model\n    self.model = LadderVAE(\n        data_mean=data_mean, data_std=data_std, config=config, target_ch=target_ch\n    )\n\n    ##### Define attributes from config #####\n    self.workdir = config.workdir\n    self._input_is_sum = False\n    self.kl_loss_formulation = config.loss.kl_loss_formulation\n    assert self.kl_loss_formulation in [\n        None,\n        \"\",\n        \"usplit\",\n        \"denoisplit\",\n        \"denoisplit_usplit\",\n    ], f\"\"\"\n        Invalid kl_loss_formulation. {self.kl_loss_formulation}\"\"\"\n\n    ##### Define loss attributes #####\n    # Parameters already defined in the model object\n    self.loss_type = self.model.loss_type\n    self._denoisplit_w = self._usplit_w = None\n    if self.loss_type == LossType.DenoiSplitMuSplit:\n        self._usplit_w = 0\n        self._denoisplit_w = 1 - self._usplit_w\n        assert self._denoisplit_w + self._usplit_w == 1\n    self._restricted_kl = self.model._restricted_kl\n\n    # General loss parameters\n    self.channel_1_w = 1\n    self.channel_2_w = 1\n\n    # About Reconsruction Loss\n    self.reconstruction_mode = False\n    self.skip_nboundary_pixels_from_loss = None\n    self.reconstruction_weight = 1.0\n    self._exclusion_loss_weight = 0\n    self.ch1_recons_w = 1\n    self.ch2_recons_w = 1\n    self.enable_mixed_rec = False\n    self.mixed_rec_w_step = 0\n\n    # About KL Loss\n    self.kl_weight = 1.0  # HC\n    self.usplit_kl_weight = None  # HC\n    self.free_bits = 1.0  # HC\n    self.kl_annealing = False  # HC\n    self.kl_annealtime = self.kl_start = None\n    if self.kl_annealing:\n        self.kl_annealtime = 10  # HC\n        self.kl_start = -1  # HC\n\n    ##### Define training attributes #####\n    self.lr = config.training.lr\n    self.lr_scheduler_patience = config.training.lr_scheduler_patience\n    self.lr_scheduler_monitor = config.model.get(\"monitor\", \"val_loss\")\n    self.lr_scheduler_mode = MetricMonitor(self.lr_scheduler_monitor).mode()\n\n    # Initialize object for keeping track of PSNR for each output channel\n    self.channels_psnr = [RunningPSNR() for _ in range(self.model.target_ch)]\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight._get_reconstruction_loss_vector","title":"<code>_get_reconstruction_loss_vector(reconstruction, target, input, return_predicted_img=False, likelihood_obj=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>return_predicted_img</code> <code>bool</code> <p>If set to <code>True</code>, the besides the loss, the reconstructed image is also returned. Default is <code>False</code>.</p> <code>False</code> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def _get_reconstruction_loss_vector(\n    self,\n    reconstruction: torch.Tensor,\n    target: torch.Tensor,\n    input: torch.Tensor,\n    return_predicted_img: bool = False,\n    likelihood_obj: LikelihoodModule = None,\n):\n    \"\"\"\n    Parameters\n    ----------\n    return_predicted_img: bool\n        If set to `True`, the besides the loss, the reconstructed image is also returned.\n        Default is `False`.\n    \"\"\"\n    output = {\n        \"loss\": None,\n        \"mixed_loss\": None,\n    }\n\n    for i in range(1, 1 + target.shape[1]):\n        output[f\"ch{i}_loss\"] = None\n\n    if likelihood_obj is None:\n        likelihood_obj = self.model.likelihood\n\n    # Log likelihood\n    ll, like_dict = likelihood_obj(reconstruction, target)\n    ll = self._get_weighted_likelihood(ll)\n    if (\n        self.skip_nboundary_pixels_from_loss is not None\n        and self.skip_nboundary_pixels_from_loss &gt; 0\n    ):\n        pad = self.skip_nboundary_pixels_from_loss\n        ll = ll[:, :, pad:-pad, pad:-pad]\n        like_dict[\"params\"][\"mean\"] = like_dict[\"params\"][\"mean\"][\n            :, :, pad:-pad, pad:-pad\n        ]\n\n    # assert ll.shape[1] == 2, f\"Change the code below to handle &gt;2 channels first. ll.shape {ll.shape}\"\n    output = {\"loss\": compute_batch_mean(-1 * ll)}\n    if ll.shape[1] &gt; 1:\n        for i in range(1, 1 + target.shape[1]):\n            output[f\"ch{i}_loss\"] = compute_batch_mean(-ll[:, i - 1])\n    else:\n        assert ll.shape[1] == 1\n        output[\"ch1_loss\"] = output[\"loss\"]\n        output[\"ch2_loss\"] = output[\"loss\"]\n\n    if (\n        self.channel_1_w is not None\n        and self.channel_2_w is not None\n        and (self.channel_1_w != 1 or self.channel_2_w != 1)\n    ):\n        assert ll.shape[1] == 2, \"Only 2 channels are supported for now.\"\n        output[\"loss\"] = (\n            self.channel_1_w * output[\"ch1_loss\"]\n            + self.channel_2_w * output[\"ch2_loss\"]\n        ) / (self.channel_1_w + self.channel_2_w)\n\n    # This `if` is not used by default config\n    if self.enable_mixed_rec:\n        mixed_pred, mixed_logvar = self.get_mixed_prediction(\n            like_dict[\"params\"][\"mean\"],\n            like_dict[\"params\"][\"logvar\"],\n            self.data_mean,\n            self.data_std,\n        )\n        if (\n            self.model._multiscale_count is not None\n            and self.model._multiscale_count &gt; 1\n        ):\n            assert input.shape[1] == self.model._multiscale_count\n            input = input[:, :1]\n\n        assert (\n            input.shape == mixed_pred.shape\n        ), \"No fucking room for vectorization induced bugs.\"\n        mixed_recons_ll = self.model.likelihood.log_likelihood(\n            input, {\"mean\": mixed_pred, \"logvar\": mixed_logvar}\n        )\n        output[\"mixed_loss\"] = compute_batch_mean(-1 * mixed_recons_ll)\n\n    # This `if` is not used by default config\n    if self._exclusion_loss_weight:\n        raise NotImplementedError(\n            \"Exclusion loss is not well defined here, so it should not be used.\"\n        )\n        imgs = like_dict[\"params\"][\"mean\"]\n        exclusion_loss = compute_exclusion_loss(imgs[:, :1], imgs[:, 1:])\n        output[\"exclusion_loss\"] = exclusion_loss\n\n    if return_predicted_img:\n        return output, like_dict[\"params\"][\"mean\"]\n\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight._get_weighted_likelihood","title":"<code>_get_weighted_likelihood(ll)</code>","text":"<p>Each of the channels gets multiplied with a different weight.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def _get_weighted_likelihood(self, ll):\n    \"\"\"\n    Each of the channels gets multiplied with a different weight.\n    \"\"\"\n    if self.ch1_recons_w == 1 and self.ch2_recons_w == 1:\n        return ll\n\n    assert ll.shape[1] == 2, \"This function is only for 2 channel images\"\n\n    mask1 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n    mask1[:, 0] = 1\n    mask2 = torch.zeros((len(ll), ll.shape[1], 1, 1), device=ll.device)\n    mask2[:, 1] = 1\n\n    return ll * mask1 * self.ch1_recons_w + ll * mask2 * self.ch2_recons_w\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_divergence_loss","title":"<code>get_kl_divergence_loss(topdown_layer_data_dict, kl_key='kl')</code>","text":"<p>kl[i] for each i has length batch_size resulting kl shape: (batch_size, layers)</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_divergence_loss(self, topdown_layer_data_dict, kl_key=\"kl\"):\n    \"\"\"\n    kl[i] for each i has length batch_size\n    resulting kl shape: (batch_size, layers)\n    \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[kl_key]],\n        dim=1,\n    )\n\n    # As compared to uSplit kl divergence,\n    # more by a factor of 4 just because we do sum and not mean.\n    kl_loss = free_bits_kl(kl, self.free_bits).sum()\n    # NOTE: at each hierarchy, it is more by a factor of 128/i**2).\n    # 128/(2*2) = 32 (bottommost layer)\n    # 128/(4*4) = 8\n    # 128/(8*8) = 2\n    # 128/(16*16) = 0.5 (topmost layer)\n\n    # Normalize the KL-loss w.r.t. the  latent space\n    kl_loss = kl_loss / np.prod(self.model.img_shape)\n    return kl_loss\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_divergence_loss_usplit","title":"<code>get_kl_divergence_loss_usplit(topdown_layer_data_dict)</code>","text":"Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_divergence_loss_usplit(\n    self, topdown_layer_data_dict: Dict[str, torch.Tensor]\n) -&gt; torch.Tensor:\n    \"\"\" \"\"\"\n    kl = torch.cat(\n        [kl_layer.unsqueeze(1) for kl_layer in topdown_layer_data_dict[\"kl\"]], dim=1\n    )\n    # NOTE: kl.shape = (16,4) 16 is batch size. 4 is number of layers.\n    # Values are sum() and so are of the order 30000\n    # Example values: 30626.6758, 31028.8145, 29509.8809, 29945.4922, 28919.1875, 29075.2988\n\n    nlayers = kl.shape[1]\n    for i in range(nlayers):\n        # topdown_layer_data_dict['z'][2].shape[-3:] = 128 * 32 * 32\n        norm_factor = np.prod(topdown_layer_data_dict[\"z\"][i].shape[-3:])\n        # if self._restricted_kl:\n        #     pow = np.power(2,min(i + 1, self._multiscale_count-1))\n        #     norm_factor /= pow * pow\n\n        kl[:, i] = kl[:, i] / norm_factor\n\n    kl_loss = free_bits_kl(kl, 0.0).mean()\n    return kl_loss\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_kl_weight","title":"<code>get_kl_weight()</code>","text":"<p>KL loss can be weighted depending whether any annealing procedure is used. This function computes the weight of the KL loss in case of annealing.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_kl_weight(self):\n    \"\"\"\n    KL loss can be weighted depending whether any annealing procedure is used.\n    This function computes the weight of the KL loss in case of annealing.\n    \"\"\"\n    if self.kl_annealing == True:\n        # calculate relative weight\n        kl_weight = (self.current_epoch - self.kl_start) * (\n            1.0 / self.kl_annealtime\n        )\n        # clamp to [0,1]\n        kl_weight = min(max(0.0, kl_weight), 1.0)\n\n        # if the final weight is given, then apply that weight on top of it\n        if self.kl_weight is not None:\n            kl_weight = kl_weight * self.kl_weight\n    elif self.kl_weight is not None:\n        return self.kl_weight\n    else:\n        kl_weight = 1.0\n    return kl_weight\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.get_reconstruction_loss","title":"<code>get_reconstruction_loss(reconstruction, target, input, splitting_mask=None, return_predicted_img=False, likelihood_obj=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Tensor</code> required <code>target</code> <code>Tensor</code> required <code>input</code> <code>Tensor</code> required <code>splitting_mask</code> <code>Tensor</code> <p>A boolean tensor that indicates which items to keep for reconstruction loss computation. If <code>None</code>, all the elements of the items are considered (i.e., the mask is all <code>True</code>).</p> <code>None</code> <code>return_predicted_img</code> <code>bool</code> <code>False</code> <code>likelihood_obj</code> <code>LikelihoodModule</code> <code>None</code> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def get_reconstruction_loss(\n    self,\n    reconstruction: torch.Tensor,\n    target: torch.Tensor,\n    input: torch.Tensor,\n    splitting_mask: torch.Tensor = None,\n    return_predicted_img: bool = False,\n    likelihood_obj: LikelihoodModule = None,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Parameters\n    ----------\n    reconstruction: torch.Tensor,\n    target: torch.Tensor\n    input: torch.Tensor\n    splitting_mask: torch.Tensor = None\n        A boolean tensor that indicates which items to keep for reconstruction loss computation.\n        If `None`, all the elements of the items are considered (i.e., the mask is all `True`).\n    return_predicted_img: bool = False\n    likelihood_obj: LikelihoodModule = None\n    \"\"\"\n    output = self._get_reconstruction_loss_vector(\n        reconstruction=reconstruction,\n        target=target,\n        input=input,\n        return_predicted_img=return_predicted_img,\n        likelihood_obj=likelihood_obj,\n    )\n    loss_dict = output[0] if return_predicted_img else output\n\n    if splitting_mask is None:\n        splitting_mask = torch.ones_like(loss_dict[\"loss\"]).bool()\n\n    # print(len(target) - (torch.isnan(loss_dict['loss'])).sum())\n\n    loss_dict[\"loss\"] = loss_dict[\"loss\"][splitting_mask].sum() / len(\n        reconstruction\n    )\n    for i in range(1, 1 + target.shape[1]):\n        key = f\"ch{i}_loss\"\n        loss_dict[key] = loss_dict[key][splitting_mask].sum() / len(reconstruction)\n\n    if \"mixed_loss\" in loss_dict:\n        loss_dict[\"mixed_loss\"] = torch.mean(loss_dict[\"mixed_loss\"])\n    if return_predicted_img:\n        assert len(output) == 2\n        return loss_dict, output[1]\n    else:\n        return loss_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/lightning_module/#careamics.lvae_training.lightning_module.LadderVAELight.increment_global_step","title":"<code>increment_global_step()</code>","text":"<p>Increments global step by 1.</p> Source code in <code>src/careamics/lvae_training/lightning_module.py</code> <pre><code>def increment_global_step(self):\n    \"\"\"Increments global step by 1.\"\"\"\n    self._global_step += 1\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/","title":"metrics","text":"<p>This script contains the functions/classes to compute loss and metrics used to train and evaluate the performance of the model.</p>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR","title":"<code>RunningPSNR</code>","text":"<p>This class allows to compute the running PSNR during validation step in training. In this way it is possible to compute the PSNR on the entire validation set one batch at the time.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>class RunningPSNR:\n    \"\"\"\n    This class allows to compute the running PSNR during validation step in training.\n    In this way it is possible to compute the PSNR on the entire validation set one batch at the time.\n    \"\"\"\n\n    def __init__(self):\n        # number of elements seen so far during the epoch\n        self.N = None\n        # running sum of the MSE over the self.N elements seen so far\n        self.mse_sum = None\n        # running max and min values of the self.N target images seen so far\n        self.max = self.min = None\n        self.reset()\n\n    def reset(self):\n        \"\"\"\n        Used to reset the running PSNR (usually called at the end of each epoch).\n        \"\"\"\n        self.mse_sum = 0\n        self.N = 0\n        self.max = self.min = None\n\n    def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n        \"\"\"\n        Given a batch of reconstructed and target images, it updates the MSE and.\n\n        Parameters\n        ----------\n        rec: torch.Tensor\n            Batch of reconstructed images (B, H, W).\n        tar: torch.Tensor\n            Batch of target images (B, H, W).\n        \"\"\"\n        ins_max = torch.max(tar).item()\n        ins_min = torch.min(tar).item()\n        if self.max is None:\n            assert self.min is None\n            self.max = ins_max\n            self.min = ins_min\n        else:\n            self.max = max(self.max, ins_max)\n            self.min = min(self.min, ins_min)\n\n        mse = (rec - tar) ** 2\n        elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n        self.mse_sum += torch.nansum(elementwise_mse)\n        self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n\n    def get(self):\n        \"\"\"\n        The get the actual PSNR value given the running statistics.\n        \"\"\"\n        if self.N == 0 or self.N is None:\n            return None\n        rmse = torch.sqrt(self.mse_sum / self.N)\n        return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.get","title":"<code>get()</code>","text":"<p>The get the actual PSNR value given the running statistics.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def get(self):\n    \"\"\"\n    The get the actual PSNR value given the running statistics.\n    \"\"\"\n    if self.N == 0 or self.N is None:\n        return None\n    rmse = torch.sqrt(self.mse_sum / self.N)\n    return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.reset","title":"<code>reset()</code>","text":"<p>Used to reset the running PSNR (usually called at the end of each epoch).</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Used to reset the running PSNR (usually called at the end of each epoch).\n    \"\"\"\n    self.mse_sum = 0\n    self.N = 0\n    self.max = self.min = None\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RunningPSNR.update","title":"<code>update(rec, tar)</code>","text":"<p>Given a batch of reconstructed and target images, it updates the MSE and.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>Tensor</code> <p>Batch of reconstructed images (B, H, W).</p> required <code>tar</code> <code>Tensor</code> <p>Batch of target images (B, H, W).</p> required Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n    \"\"\"\n    Given a batch of reconstructed and target images, it updates the MSE and.\n\n    Parameters\n    ----------\n    rec: torch.Tensor\n        Batch of reconstructed images (B, H, W).\n    tar: torch.Tensor\n        Batch of target images (B, H, W).\n    \"\"\"\n    ins_max = torch.max(tar).item()\n    ins_min = torch.min(tar).item()\n    if self.max is None:\n        assert self.min is None\n        self.max = ins_max\n        self.min = ins_min\n    else:\n        self.max = max(self.max, ins_max)\n        self.min = min(self.min, ins_min)\n\n    mse = (rec - tar) ** 2\n    elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n    self.mse_sum += torch.nansum(elementwise_mse)\n    self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.PSNR","title":"<code>PSNR(gt, pred, range_=None)</code>","text":"<p>Compute PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <p>Ground truth image.</p> required <code>pred</code> <p>Predicted image.</p> required Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef PSNR(gt, pred, range_=None):\n    \"\"\"\n    Compute PSNR.\n\n    Parameters\n    ----------\n    gt: array\n        Ground truth image.\n    pred: array\n        Predicted image.\n    \"\"\"\n    assert len(gt.shape) == 3, \"Images must be in shape: (batch,H,W)\"\n\n    gt = gt.view(len(gt), -1)\n    pred = pred.view(len(gt), -1)\n    return _PSNR_internal(gt, pred, range_=range_)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.RangeInvariantPsnr","title":"<code>RangeInvariantPsnr(gt, pred)</code>","text":"<p>NOTE: Works only for grayscale images. Adapted from https://github.com/juglab/ScaleInvPSNR/blob/master/psnr.py It rescales the prediction to ensure that the prediction has the same range as the ground truth.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef RangeInvariantPsnr(gt: torch.Tensor, pred: torch.Tensor):\n    \"\"\"\n    NOTE: Works only for grayscale images.\n    Adapted from https://github.com/juglab/ScaleInvPSNR/blob/master/psnr.py\n    It rescales the prediction to ensure that the prediction has the same range as the ground truth.\n    \"\"\"\n    assert len(gt.shape) == 3, \"Images must be in shape: (batch,H,W)\"\n    gt = gt.view(len(gt), -1)\n    pred = pred.view(len(gt), -1)\n    ra = (torch.max(gt, dim=1).values - torch.min(gt, dim=1).values) / torch.std(\n        gt, dim=1\n    )\n    gt_ = zero_mean(gt) / torch.std(gt, dim=1, keepdim=True)\n    return _PSNR_internal(zero_mean(gt_), fix(gt_, pred), ra)\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.compute_multiscale_ssim","title":"<code>compute_multiscale_ssim(gt_, pred_, range_invariant=True)</code>","text":"<p>Computes multiscale ssim for each channel. Args: gt_: ground truth image with shape (N, H, W, C) pred_: predicted image with shape (N, H, W, C) range_invariant: whether to use range invariant multiscale ssim</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>def compute_multiscale_ssim(gt_, pred_, range_invariant=True):\n    \"\"\"\n    Computes multiscale ssim for each channel.\n    Args:\n    gt_: ground truth image with shape (N, H, W, C)\n    pred_: predicted image with shape (N, H, W, C)\n    range_invariant: whether to use range invariant multiscale ssim\n    \"\"\"\n    ms_ssim_values = {i: None for i in range(gt_.shape[-1])}\n    for ch_idx in range(gt_.shape[-1]):\n        tar_tmp = gt_[..., ch_idx]\n        pred_tmp = pred_[..., ch_idx]\n        if range_invariant:\n            ms_ssim_values[ch_idx] = range_invariant_multiscale_ssim(tar_tmp, pred_tmp)\n        else:\n            ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n                data_range=tar_tmp.max() - tar_tmp.min()\n            )\n            ms_ssim_values[ch_idx] = ms_ssim(\n                torch.Tensor(pred_tmp[:, None]), torch.Tensor(tar_tmp[:, None])\n            ).item()\n\n    output = [ms_ssim_values[i] for i in range(gt_.shape[-1])]\n    return output\n</code></pre>"},{"location":"reference/careamics/lvae_training/metrics/#careamics.lvae_training.metrics.range_invariant_multiscale_ssim","title":"<code>range_invariant_multiscale_ssim(gt_, pred_)</code>","text":"<p>Computes range invariant multiscale ssim for one channel. This has the benefit that it is invariant to scalar multiplications in the prediction.</p> Source code in <code>src/careamics/lvae_training/metrics.py</code> <pre><code>@allow_numpy\ndef range_invariant_multiscale_ssim(gt_, pred_):\n    \"\"\"\n    Computes range invariant multiscale ssim for one channel.\n    This has the benefit that it is invariant to scalar multiplications in the prediction.\n    \"\"\"\n    shape = gt_.shape\n    gt_ = torch.Tensor(gt_.reshape((shape[0], -1)))\n    pred_ = torch.Tensor(pred_.reshape((shape[0], -1)))\n    gt_ = zero_mean(gt_)\n    pred_ = zero_mean(pred_)\n    pred_ = fix(gt_, pred_)\n    pred_ = pred_.reshape(shape)\n    gt_ = gt_.reshape(shape)\n\n    ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n        data_range=gt_.max() - gt_.min()\n    )\n    return ms_ssim(torch.Tensor(pred_[:, None]), torch.Tensor(gt_[:, None])).item()\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_lvae/","title":"train_lvae","text":"<p>This script is meant to load data, initialize the model, and provide the logic for training it.</p>"},{"location":"reference/careamics/lvae_training/train_lvae/#careamics.lvae_training.train_lvae.get_mean_std_dict_for_model","title":"<code>get_mean_std_dict_for_model(config, train_dset)</code>","text":"<p>Computes the mean and std for the model. This will be subsequently passed to the model.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_mean_std_dict_for_model(config, train_dset):\n    \"\"\"\n    Computes the mean and std for the model. This will be subsequently passed to the model.\n    \"\"\"\n    mean_dict, std_dict = train_dset.get_mean_std()\n\n    return deepcopy(mean_dict), deepcopy(std_dict)\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_lvae/#careamics.lvae_training.train_lvae.get_new_model_version","title":"<code>get_new_model_version(model_dir)</code>","text":"<p>A model will have multiple runs. Each run will have a different version.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_new_model_version(model_dir: str) -&gt; str:\n    \"\"\"\n    A model will have multiple runs. Each run will have a different version.\n    \"\"\"\n    versions = []\n    for version_dir in os.listdir(model_dir):\n        try:\n            versions.append(int(version_dir))\n        except:\n            print(\n                f\"Invalid subdirectory:{model_dir}/{version_dir}. Only integer versions are allowed\"\n            )\n            exit()\n    if len(versions) == 0:\n        return \"0\"\n    return f\"{max(versions) + 1}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_utils/","title":"train_utils","text":"<p>This script contains the utility functions for training the LVAE model. These functions are mainly used in <code>train.py</code> script.</p>"},{"location":"reference/careamics/lvae_training/train_utils/#careamics.lvae_training.train_utils.get_mean_std_dict_for_model","title":"<code>get_mean_std_dict_for_model(config, train_dset)</code>","text":"<p>Computes the mean and std for the model. This will be subsequently passed to the model.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_mean_std_dict_for_model(config, train_dset):\n    \"\"\"\n    Computes the mean and std for the model. This will be subsequently passed to the model.\n    \"\"\"\n    mean_dict, std_dict = train_dset.get_mean_std()\n\n    return deepcopy(mean_dict), deepcopy(std_dict)\n</code></pre>"},{"location":"reference/careamics/lvae_training/train_utils/#careamics.lvae_training.train_utils.get_new_model_version","title":"<code>get_new_model_version(model_dir)</code>","text":"<p>A model will have multiple runs. Each run will have a different version.</p> Source code in <code>src/careamics/lvae_training/train_utils.py</code> <pre><code>def get_new_model_version(model_dir: str) -&gt; str:\n    \"\"\"\n    A model will have multiple runs. Each run will have a different version.\n    \"\"\"\n    versions = []\n    for version_dir in os.listdir(model_dir):\n        try:\n            versions.append(int(version_dir))\n        except:\n            print(\n                f\"Invalid subdirectory:{model_dir}/{version_dir}. Only integer versions are allowed\"\n            )\n            exit()\n    if len(versions) == 0:\n        return \"0\"\n    return f\"{max(versions) + 1}\"\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/config/","title":"config","text":""},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig","title":"<code>DatasetConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/careamics/lvae_training/dataset/config.py</code> <pre><code>class DatasetConfig(BaseModel):\n    model_config = ConfigDict(validate_assignment=True, extra=\"forbid\")\n\n    data_type: Optional[DataType]\n    \"\"\"Type of the dataset, should be one of DataType\"\"\"\n\n    depth3D: Optional[int] = 1\n    \"\"\"Number of slices in 3D. If data is 2D depth3D is equal to 1\"\"\"\n\n    datasplit_type: Optional[DataSplitType] = None\n    \"\"\"Whether to return training, validation or test split, should be one of \n    DataSplitType\"\"\"\n\n    num_channels: Optional[int] = 2\n    \"\"\"Number of channels in the input\"\"\"\n\n    # TODO: remove ch*_fname parameters, should be parsed automatically from a name list\n    ch1_fname: Optional[str] = None\n    ch2_fname: Optional[str] = None\n    ch_input_fname: Optional[str] = None\n\n    input_is_sum: Optional[bool] = False\n    \"\"\"Whether the input is the sum or average of channels\"\"\"\n\n    input_idx: Optional[int] = None\n    \"\"\"Index of the channel where the input is stored in the data\"\"\"\n\n    target_idx_list: Optional[list[int]] = None\n    \"\"\"Indices of the channels where the targets are stored in the data\"\"\"\n\n    # TODO: where are there used?\n    start_alpha: Optional[Any] = None\n    end_alpha: Optional[Any] = None\n\n    image_size: tuple  # TODO: revisit, new model_config uses tuple\n    \"\"\"Size of one patch of data\"\"\"\n\n    grid_size: Optional[int] = None\n    \"\"\"Frame is divided into square grids of this size. A patch centered on a grid \n    having size `image_size` is returned. Grid size not used in training,\n    used only during val / test, grid size controls the overlap of the patches\"\"\"\n\n    empty_patch_replacement_enabled: Optional[bool] = False\n    \"\"\"Whether to replace the content of one of the channels\n    with background with given probability\"\"\"\n    empty_patch_replacement_channel_idx: Optional[Any] = None\n    empty_patch_replacement_probab: Optional[Any] = None\n    empty_patch_max_val_threshold: Optional[Any] = None\n\n    uncorrelated_channels: Optional[bool] = False\n    \"\"\"Replace the content in one of the channels with given probability to make \n    channel content 'uncorrelated'\"\"\"\n    uncorrelated_channel_probab: Optional[float] = 0.5\n\n    poisson_noise_factor: Optional[float] = -1\n    \"\"\"The added poisson noise factor\"\"\"\n\n    synthetic_gaussian_scale: Optional[float] = 0.1\n\n    # TODO: set to True in training code, recheck\n    input_has_dependant_noise: Optional[bool] = False\n\n    # TODO: sometimes max_val differs between runs with fixed seeds with noise enabled\n    enable_gaussian_noise: Optional[bool] = False\n    \"\"\"Whether to enable gaussian noise\"\"\"\n\n    # TODO: is this parameter used?\n    allow_generation: bool = False\n\n    # TODO: both used in IndexSwitcher, insure correct passing\n    training_validtarget_fraction: Any = None\n    deterministic_grid: Any = None\n\n    # TODO: why is this not used?\n    enable_rotation_aug: Optional[bool] = False\n\n    max_val: Optional[float] = None\n    \"\"\"Maximum data in the dataset. Is calculated for train split, and should be \n    externally set for val and test splits.\"\"\"\n\n    overlapping_padding_kwargs: Any = None\n    \"\"\"Parameters for np.pad method\"\"\"\n\n    # TODO: remove this parameter, controls debug print\n    print_vars: Optional[bool] = False\n\n    # Hard-coded parameters (used to be in the config file)\n    normalized_input: bool = True\n    \"\"\"If this is set to true, then one mean and stdev is used\n                for both channels. Otherwise, two different mean and stdev are used.\"\"\"\n    use_one_mu_std: Optional[bool] = True\n\n    # TODO: is this parameter used?\n    train_aug_rotate: Optional[bool] = False\n    enable_random_cropping: Optional[bool] = True\n\n    multiscale_lowres_count: Optional[int] = None\n    \"\"\"Number of LC scales\"\"\"\n\n    tiling_mode: Optional[TilingMode] = TilingMode.ShiftBoundary\n\n    target_separate_normalization: Optional[bool] = True\n\n    mode_3D: Optional[bool] = False\n    \"\"\"If training in 3D mode or not\"\"\"\n\n    trainig_datausage_fraction: Optional[float] = 1.0\n\n    validtarget_random_fraction: Optional[float] = None\n\n    validation_datausage_fraction: Optional[float] = 1.0\n\n    random_flip_z_3D: Optional[bool] = False\n\n    padding_kwargs: Optional[dict] = None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.data_type","title":"<code>data_type</code>  <code>instance-attribute</code>","text":"<p>Type of the dataset, should be one of DataType</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.datasplit_type","title":"<code>datasplit_type = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to return training, validation or test split, should be one of  DataSplitType</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.depth3D","title":"<code>depth3D = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of slices in 3D. If data is 2D depth3D is equal to 1</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.empty_patch_replacement_enabled","title":"<code>empty_patch_replacement_enabled = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to replace the content of one of the channels with background with given probability</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.enable_gaussian_noise","title":"<code>enable_gaussian_noise = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to enable gaussian noise</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.grid_size","title":"<code>grid_size = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frame is divided into square grids of this size. A patch centered on a grid  having size <code>image_size</code> is returned. Grid size not used in training, used only during val / test, grid size controls the overlap of the patches</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.image_size","title":"<code>image_size</code>  <code>instance-attribute</code>","text":"<p>Size of one patch of data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.input_idx","title":"<code>input_idx = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the channel where the input is stored in the data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.input_is_sum","title":"<code>input_is_sum = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the input is the sum or average of channels</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.max_val","title":"<code>max_val = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum data in the dataset. Is calculated for train split, and should be  externally set for val and test splits.</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.mode_3D","title":"<code>mode_3D = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If training in 3D mode or not</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.multiscale_lowres_count","title":"<code>multiscale_lowres_count = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of LC scales</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.normalized_input","title":"<code>normalized_input = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If this is set to true, then one mean and stdev is used for both channels. Otherwise, two different mean and stdev are used.</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.num_channels","title":"<code>num_channels = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels in the input</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.overlapping_padding_kwargs","title":"<code>overlapping_padding_kwargs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parameters for np.pad method</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.poisson_noise_factor","title":"<code>poisson_noise_factor = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The added poisson noise factor</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.target_idx_list","title":"<code>target_idx_list = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indices of the channels where the targets are stored in the data</p>"},{"location":"reference/careamics/lvae_training/dataset/config/#careamics.lvae_training.dataset.config.DatasetConfig.uncorrelated_channels","title":"<code>uncorrelated_channels = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Replace the content in one of the channels with given probability to make  channel content 'uncorrelated'</p>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/","title":"lc_dataset","text":"<p>A place for Datasets and Dataloaders.</p>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/#careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader","title":"<code>LCMultiChDloader</code>","text":"<p>               Bases: <code>MultiChDloader</code></p> Source code in <code>src/careamics/lvae_training/dataset/lc_dataset.py</code> <pre><code>class LCMultiChDloader(MultiChDloader):\n    def __init__(\n        self,\n        data_config: DatasetConfig,\n        fpath: str,\n        load_data_fn: Callable,\n        val_fraction=None,\n        test_fraction=None,\n    ):\n        self._padding_kwargs = (\n            data_config.padding_kwargs  # mode=padding_mode, constant_values=constant_value\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n\n        super().__init__(\n            data_config,\n            fpath,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n        )\n\n        if data_config.overlapping_padding_kwargs is not None:\n            assert (\n                self._padding_kwargs == data_config.overlapping_padding_kwargs\n            ), \"During evaluation, overlapping_padding_kwargs should be same as padding_args. \\\n                It should be so since we just use overlapping_padding_kwargs when it is not None\"\n\n        else:\n            self._overlapping_padding_kwargs = data_config.padding_kwargs\n\n        self.multiscale_lowres_count = data_config.multiscale_lowres_count\n        assert self.multiscale_lowres_count is not None\n        self._scaled_data = [self._data]\n        self._scaled_noise_data = [self._noise_data]\n\n        assert (\n            isinstance(self.multiscale_lowres_count, int)\n            and self.multiscale_lowres_count &gt;= 1\n        )\n        assert isinstance(self._padding_kwargs, dict)\n        assert \"mode\" in self._padding_kwargs\n\n        for _ in range(1, self.multiscale_lowres_count):\n            shape = self._scaled_data[-1].shape\n            assert len(shape) == 4\n            new_shape = (shape[0], shape[1] // 2, shape[2] // 2, shape[3])\n            ds_data = resize(\n                self._scaled_data[-1].astype(np.float32), new_shape\n            ).astype(self._scaled_data[-1].dtype)\n            # NOTE: These asserts are important. the resize method expects np.float32. otherwise, one gets weird results.\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &lt; 5\n            ), \"Downsampled image should not have very different values\"\n            assert (\n                ds_data.max() / self._scaled_data[-1].max() &gt; 0.2\n            ), \"Downsampled image should not have very different values\"\n\n            self._scaled_data.append(ds_data)\n            # do the same for noise\n            if self._noise_data is not None:\n                noise_data = resize(self._scaled_noise_data[-1], new_shape)\n                self._scaled_noise_data.append(noise_data)\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert t_list is not None\n        assert h_start is None\n        assert h_end is None\n        assert w_start is None\n        assert w_end is None\n\n        self._data = self._data[t_list].copy()\n        self._scaled_data = [\n            self._scaled_data[i][t_list].copy() for i in range(len(self._scaled_data))\n        ]\n\n        if self._noise_data is not None:\n            self._noise_data = self._noise_data[t_list].copy()\n            self._scaled_noise_data = [\n                self._scaled_noise_data[i][t_list].copy()\n                for i in range(len(self._scaled_noise_data))\n            ]\n\n        self.N = len(t_list)\n        # TODO where tf is self._img_sz defined?\n        self.set_img_sz([self._img_sz, self._img_sz], self._grid_sz)\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data shape: {self._data.shape}\"\n        )\n\n    def _init_msg(self):\n        msg = super()._init_msg()\n        msg += f\" Pad:{self._padding_kwargs}\"\n        if self._uncorrelated_channels:\n            msg += f\" UncorrChProbab:{self._uncorrelated_channel_probab}\"\n        return msg\n\n    def _load_scaled_img(\n        self, scaled_index, index: Union[int, Tuple[int, int]]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(index, int):\n            idx = index\n        else:\n            idx, _ = index\n\n        # tidx = self.idx_manager.get_t(idx)\n        patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n        nidx = patch_loc_list[0]\n\n        imgs = self._scaled_data[scaled_index][nidx]\n        imgs = tuple([imgs[None, ..., i] for i in range(imgs.shape[-1])])\n        if self._noise_data is not None:\n            noisedata = self._scaled_noise_data[scaled_index][nidx]\n            noise = tuple([noisedata[None, ..., i] for i in range(noisedata.shape[-1])])\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            imgs = tuple([img + noise[0] * factor for img in imgs])\n        return imgs\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: Tuple):\n        \"\"\"\n        Here, h_start, w_start could be negative. That simply means we need to pick the content from 0. So,\n        the cropped image will be smaller than self._img_sz * self._img_sz\n        \"\"\"\n        max_len_vals = list(self.idx_manager.data_shape[1:-1])\n        max_len_vals[-2:] = img.shape[-2:]\n        return self._crop_img_with_padding(\n            img, patch_start_loc, max_len_vals=max_len_vals\n        )\n\n    def _get_img(self, index: int):\n        \"\"\"\n        Returns the primary patch along with low resolution patches centered on the primary patch.\n        \"\"\"\n        # Noise_tuples is populated when there is synthetic noise in training\n        # Should have similar type of noise with the noise model\n        # Starting with microsplit, dump the noise, use it instead as an augmentation if nessesary\n        img_tuples, noise_tuples = self._load_img(index)\n        assert self._img_sz is not None\n        h, w = img_tuples[0].shape[-2:]\n        if self._enable_random_cropping:\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._5Ddata:\n                patch_start_loc = (\n                    np.random.choice(img_tuples[0].shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            patch_start_loc = self._get_deterministic_loc(index)\n\n        # LC logic is located here, the function crops the image of the highest resolution\n        cropped_img_tuples = [\n            self._crop_flip_img(img, patch_start_loc, False, False)\n            for img in img_tuples\n        ]\n        cropped_noise_tuples = [\n            self._crop_flip_img(noise, patch_start_loc, False, False)\n            for noise in noise_tuples\n        ]\n        patch_start_loc = list(patch_start_loc)\n        h_start, w_start = patch_start_loc[-2], patch_start_loc[-1]\n        h_center = h_start + self._img_sz // 2\n        w_center = w_start + self._img_sz // 2\n        allres_versions = {\n            i: [cropped_img_tuples[i]] for i in range(len(cropped_img_tuples))\n        }\n        for scale_idx in range(1, self.multiscale_lowres_count):\n            # Returning the image of the lower resolution\n            scaled_img_tuples = self._load_scaled_img(scale_idx, index)\n\n            h_center = h_center // 2\n            w_center = w_center // 2\n\n            h_start = h_center - self._img_sz // 2\n            w_start = w_center - self._img_sz // 2\n            patch_start_loc[-2:] = [h_start, w_start]\n            scaled_cropped_img_tuples = [\n                self._crop_flip_img(img, patch_start_loc, False, False)\n                for img in scaled_img_tuples\n            ]\n            for ch_idx in range(len(img_tuples)):\n                allres_versions[ch_idx].append(scaled_cropped_img_tuples[ch_idx])\n\n        output_img_tuples = tuple(\n            [\n                np.concatenate(allres_versions[ch_idx])\n                for ch_idx in range(len(img_tuples))\n            ]\n        )\n        return output_img_tuples, cropped_noise_tuples\n\n    def __getitem__(self, index: Union[int, Tuple[int, int]]):\n        img_tuples, noise_tuples = self._get_img(index)\n        if self._uncorrelated_channels:\n            assert (\n                self._input_idx is None\n            ), \"Uncorrelated channels is not implemented when there is a separate input channel.\"\n            if np.random.rand() &lt; self._uncorrelated_channel_probab:\n                img_tuples_new = [None] * len(img_tuples)\n                img_tuples_new[0] = img_tuples[0]\n                for i in range(1, len(img_tuples)):\n                    new_index = np.random.randint(len(self))\n                    img_tuples_tmp, _ = self._get_img(new_index)\n                    img_tuples_new[i] = img_tuples_tmp[i]\n                img_tuples = img_tuples_new\n\n        if self._is_train:\n            if self._empty_patch_replacement_enabled:\n                if np.random.rand() &lt; self._empty_patch_replacement_probab:\n                    img_tuples = self.replace_with_empty_patch(img_tuples)\n\n        if self._enable_rotation:\n            img_tuples, noise_tuples = self._rotate(img_tuples, noise_tuples)\n\n        # add noise to input, if noise is present combine it with the image\n        # factor is for the compute input not to have too much noise because the average of two gaussians\n        if len(noise_tuples) &gt; 0:\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            input_tuples = []\n            for x in img_tuples:\n                x = (\n                    x.copy()\n                )  # to avoid changing the original image since it is later used for target\n                # NOTE: other LC levels already have noise added. So, we just need to add noise to the highest resolution.\n                x[0] = x[0] + noise_tuples[0] * factor\n                input_tuples.append(x)\n        else:\n            input_tuples = img_tuples\n\n        # Compute the input by sum / average the channels\n        # Alpha is an amount of weight which is applied to the channels when combining them\n        # How to sample alpha is still under research\n        inp, alpha = self._compute_input(input_tuples)\n        target_tuples = [img[:1] for img in img_tuples]\n        # add noise to target.\n        if len(noise_tuples) &gt;= 1:\n            target_tuples = [\n                x + noise for x, noise in zip(target_tuples, noise_tuples[1:])\n            ]\n\n        target = self._compute_target(target_tuples, alpha)\n\n        norm_target = self.normalize_target(target)\n\n        output = [inp, norm_target]\n\n        if self._return_alpha:\n            output.append(alpha)\n\n        if isinstance(index, int):\n            return tuple(output)\n\n        _, grid_size = index\n        output.append(grid_size)\n        return tuple(output)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/#careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader._crop_img","title":"<code>_crop_img(img, patch_start_loc)</code>","text":"<p>Here, h_start, w_start could be negative. That simply means we need to pick the content from 0. So, the cropped image will be smaller than self._img_sz * self._img_sz</p> Source code in <code>src/careamics/lvae_training/dataset/lc_dataset.py</code> <pre><code>def _crop_img(self, img: np.ndarray, patch_start_loc: Tuple):\n    \"\"\"\n    Here, h_start, w_start could be negative. That simply means we need to pick the content from 0. So,\n    the cropped image will be smaller than self._img_sz * self._img_sz\n    \"\"\"\n    max_len_vals = list(self.idx_manager.data_shape[1:-1])\n    max_len_vals[-2:] = img.shape[-2:]\n    return self._crop_img_with_padding(\n        img, patch_start_loc, max_len_vals=max_len_vals\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/lc_dataset/#careamics.lvae_training.dataset.lc_dataset.LCMultiChDloader._get_img","title":"<code>_get_img(index)</code>","text":"<p>Returns the primary patch along with low resolution patches centered on the primary patch.</p> Source code in <code>src/careamics/lvae_training/dataset/lc_dataset.py</code> <pre><code>def _get_img(self, index: int):\n    \"\"\"\n    Returns the primary patch along with low resolution patches centered on the primary patch.\n    \"\"\"\n    # Noise_tuples is populated when there is synthetic noise in training\n    # Should have similar type of noise with the noise model\n    # Starting with microsplit, dump the noise, use it instead as an augmentation if nessesary\n    img_tuples, noise_tuples = self._load_img(index)\n    assert self._img_sz is not None\n    h, w = img_tuples[0].shape[-2:]\n    if self._enable_random_cropping:\n        patch_start_loc = self._get_random_hw(h, w)\n        if self._5Ddata:\n            patch_start_loc = (\n                np.random.choice(img_tuples[0].shape[-3] - self._depth3D),\n            ) + patch_start_loc\n    else:\n        patch_start_loc = self._get_deterministic_loc(index)\n\n    # LC logic is located here, the function crops the image of the highest resolution\n    cropped_img_tuples = [\n        self._crop_flip_img(img, patch_start_loc, False, False)\n        for img in img_tuples\n    ]\n    cropped_noise_tuples = [\n        self._crop_flip_img(noise, patch_start_loc, False, False)\n        for noise in noise_tuples\n    ]\n    patch_start_loc = list(patch_start_loc)\n    h_start, w_start = patch_start_loc[-2], patch_start_loc[-1]\n    h_center = h_start + self._img_sz // 2\n    w_center = w_start + self._img_sz // 2\n    allres_versions = {\n        i: [cropped_img_tuples[i]] for i in range(len(cropped_img_tuples))\n    }\n    for scale_idx in range(1, self.multiscale_lowres_count):\n        # Returning the image of the lower resolution\n        scaled_img_tuples = self._load_scaled_img(scale_idx, index)\n\n        h_center = h_center // 2\n        w_center = w_center // 2\n\n        h_start = h_center - self._img_sz // 2\n        w_start = w_center - self._img_sz // 2\n        patch_start_loc[-2:] = [h_start, w_start]\n        scaled_cropped_img_tuples = [\n            self._crop_flip_img(img, patch_start_loc, False, False)\n            for img in scaled_img_tuples\n        ]\n        for ch_idx in range(len(img_tuples)):\n            allres_versions[ch_idx].append(scaled_cropped_img_tuples[ch_idx])\n\n    output_img_tuples = tuple(\n        [\n            np.concatenate(allres_versions[ch_idx])\n            for ch_idx in range(len(img_tuples))\n        ]\n    )\n    return output_img_tuples, cropped_noise_tuples\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/","title":"multich_dataset","text":"<p>A place for Datasets and Dataloaders.</p>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader","title":"<code>MultiChDloader</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>class MultiChDloader:\n    def __init__(\n        self,\n        data_config: DatasetConfig,\n        fpath: str,\n        load_data_fn: Callable,\n        val_fraction: float = None,\n        test_fraction: float = None,\n    ):\n        \"\"\" \"\"\"\n        self._data_type = data_config.data_type\n        self._fpath = fpath\n        self._data = self._noise_data = None\n        self.Z = 1\n        self._5Ddata = False\n        self._tiling_mode = data_config.tiling_mode\n        # by default, if the noise is present, add it to the input and target.\n        self._disable_noise = False  # to add synthetic noise\n        self._poisson_noise_factor = None\n        self._train_index_switcher = None\n        self._depth3D = data_config.depth3D\n        self._mode_3D = data_config.mode_3D\n        # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n        self._input_is_sum = data_config.input_is_sum\n        self._num_channels = data_config.num_channels\n        self._input_idx = data_config.input_idx\n        self._tar_idx_list = data_config.target_idx_list\n\n        if data_config.datasplit_type == DataSplitType.Train:\n            self._datausage_fraction = data_config.trainig_datausage_fraction\n            # assert self._datausage_fraction == 1.0, 'Not supported. Use validtarget_random_fraction and training_validtarget_fraction to get the same effect'\n            self._validtarget_rand_fract = data_config.validtarget_random_fraction\n            # self._validtarget_random_fraction_final = data_config.get('validtarget_random_fraction_final', None)\n            # self._validtarget_random_fraction_stepepoch = data_config.get('validtarget_random_fraction_stepepoch', None)\n            # self._idx_count = 0\n        elif data_config.datasplit_type == DataSplitType.Val:\n            self._datausage_fraction = data_config.validation_datausage_fraction\n        else:\n            self._datausage_fraction = 1.0\n\n        self.load_data(\n            data_config,\n            data_config.datasplit_type,\n            load_data_fn=load_data_fn,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=data_config.allow_generation,\n        )\n        self._normalized_input = data_config.normalized_input\n        self._quantile = 1.0\n        self._channelwise_quantile = False\n        self._background_quantile = 0.0\n        self._clip_background_noise_to_zero = False\n        self._skip_normalization_using_mean = False\n        self._empty_patch_replacement_enabled = False\n\n        self._background_values = None\n\n        self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            if (\n                self._overlapping_padding_kwargs is None\n                or data_config.multiscale_lowres_count is not None\n            ):\n                # raise warning\n                print(\"Padding is not used with this alignement style\")\n        else:\n            assert (\n                self._overlapping_padding_kwargs is not None\n            ), \"When not trimming boudnary, padding is needed.\"\n\n        self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n        # input = alpha * ch1 + (1-alpha)*ch2.\n        # alpha is sampled randomly between these two extremes\n        self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n        self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n        # changed set_img_sz because \"grid_size\" in data_config returns false\n        try:\n            grid_size = data_config.grid_size\n        except AttributeError:\n            grid_size = data_config.image_size\n\n        if self._is_train:\n            self._start_alpha_arr = data_config.start_alpha\n            self._end_alpha_arr = data_config.end_alpha\n\n            self.set_img_sz(data_config.image_size, grid_size)\n\n            if self._validtarget_rand_fract is not None:\n                self._train_index_switcher = IndexSwitcher(\n                    self.idx_manager, data_config, self._img_sz\n                )\n\n        else:\n            self.set_img_sz(data_config.image_size, grid_size)\n\n        self._return_alpha = False\n        self._return_index = False\n\n        self._empty_patch_replacement_enabled = (\n            data_config.empty_patch_replacement_enabled and self._is_train\n        )\n        if self._empty_patch_replacement_enabled:\n            self._empty_patch_replacement_channel_idx = (\n                data_config.empty_patch_replacement_channel_idx\n            )\n            self._empty_patch_replacement_probab = (\n                data_config.empty_patch_replacement_probab\n            )\n            data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n            # NOTE: This is on the raw data. So, it must be called before removing the background.\n            self._empty_patch_fetcher = EmptyPatchFetcher(\n                self.idx_manager,\n                self._img_sz,\n                data_frames,\n                max_val_threshold=data_config.empty_patch_max_val_threshold,\n            )\n\n        self.rm_bkground_set_max_val_and_upperclip_data(\n            data_config.max_val, data_config.datasplit_type\n        )\n\n        # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n        self._mean = None\n        self._std = None\n        self._use_one_mu_std = data_config.use_one_mu_std\n\n        self._target_separate_normalization = data_config.target_separate_normalization\n\n        self._enable_rotation = data_config.enable_rotation_aug\n        flipz_3D = data_config.random_flip_z_3D\n        self._flipz_3D = flipz_3D and self._enable_rotation\n\n        self._enable_random_cropping = data_config.enable_random_cropping\n        self._uncorrelated_channels = (\n            data_config.uncorrelated_channels and self._is_train\n        )\n        self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n        assert self._is_train or self._uncorrelated_channels is False\n        assert (\n            self._enable_random_cropping is True or self._uncorrelated_channels is False\n        )\n        # Randomly rotate [-90,90]\n\n        self._rotation_transform = None\n        if self._enable_rotation:\n            # TODO: fix this import\n            import albumentations as A\n\n            self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n\n        # TODO: remove print log messages\n        # if print_vars:\n        #     msg = self._init_msg()\n        #     print(msg)\n\n    def disable_noise(self):\n        assert (\n            self._poisson_noise_factor is None\n        ), \"This is not supported. Poisson noise is added to the data itself and so the noise cannot be disabled.\"\n        self._disable_noise = True\n\n    def enable_noise(self):\n        self._disable_noise = False\n\n    def get_data_shape(self):\n        return self._data.shape\n\n    def load_data(\n        self,\n        data_config,\n        datasplit_type,\n        load_data_fn: Callable,\n        val_fraction=None,\n        test_fraction=None,\n        allow_generation=None,\n    ):\n        self._data = load_data_fn(\n            data_config,\n            self._fpath,\n            datasplit_type,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n            allow_generation=allow_generation,\n        )\n        self._loaded_data_preprocessing(data_config)\n\n    def _loaded_data_preprocessing(self, data_config):\n        old_shape = self._data.shape\n        if self._datausage_fraction &lt; 1.0:\n            framepixelcount = np.prod(self._data.shape[1:3])\n            pixelcount = int(\n                len(self._data) * framepixelcount * self._datausage_fraction\n            )\n            frame_count = int(np.ceil(pixelcount / framepixelcount))\n            last_frame_reduced_size, _ = IndexSwitcher.get_reduced_frame_size(\n                self._data.shape[:3], self._datausage_fraction\n            )\n            self._data = self._data[:frame_count].copy()\n            if frame_count == 1:\n                self._data = self._data[\n                    :, :last_frame_reduced_size, :last_frame_reduced_size\n                ].copy()\n            print(\n                f\"[{self.__class__.__name__}] New data shape: {self._data.shape} Old: {old_shape}\"\n            )\n\n        msg = \"\"\n        if data_config.poisson_noise_factor &gt; 0:\n            self._poisson_noise_factor = data_config.poisson_noise_factor\n            msg += f\"Adding Poisson noise with factor {self._poisson_noise_factor}.\\t\"\n            self._data = np.random.poisson(self._data / self._poisson_noise_factor)\n\n        if data_config.enable_gaussian_noise:\n            synthetic_scale = data_config.synthetic_gaussian_scale\n            msg += f\"Adding Gaussian noise with scale {synthetic_scale}\"\n            # 0 =&gt; noise for input. 1: =&gt; noise for all targets.\n            shape = self._data.shape\n            self._noise_data = np.random.normal(\n                0, synthetic_scale, (*shape[:-1], shape[-1] + 1)\n            )\n            if data_config.input_has_dependant_noise:\n                msg += \". Moreover, input has dependent noise\"\n                self._noise_data[..., 0] = np.mean(self._noise_data[..., 1:], axis=-1)\n        print(msg)\n\n        if len(self._data.shape) == 5:\n            if self._mode_3D:\n                self._5Ddata = True\n            else:\n                assert self._depth3D == 1, \"Depth3D must be 1 for 2D training\"\n                self._data = self._data.reshape(-1, *self._data.shape[2:])\n\n        if self._5Ddata:\n            self.Z = self._data.shape[1]\n\n        if self._depth3D &gt; 1:\n            assert self._5Ddata, \"Data must be 5D:NxZxHxWxC for 3D data\"\n\n        assert (\n            self._data.shape[-1] == self._num_channels\n        ), \"Number of channels in data and config do not match.\"\n\n    def save_background(self, channel_idx, frame_idx, background_value):\n        self._background_values[frame_idx, channel_idx] = background_value\n\n    def get_background(self, channel_idx, frame_idx):\n        return self._background_values[frame_idx, channel_idx]\n\n    def remove_background(self):\n\n        self._background_values = np.zeros((self._data.shape[0], self._data.shape[-1]))\n\n        if self._background_quantile == 0.0:\n            assert (\n                self._clip_background_noise_to_zero is False\n            ), \"This operation currently happens later in this function.\"\n            return\n\n        if self._data.dtype in [np.uint16]:\n            # unsigned integer creates havoc\n            self._data = self._data.astype(np.int32)\n\n        for ch in range(self._data.shape[-1]):\n            for idx in range(self._data.shape[0]):\n                qval = np.quantile(self._data[idx, ..., ch], self._background_quantile)\n                assert (\n                    np.abs(qval) &gt; 20\n                ), \"We are truncating the qval to an integer which will only make sense if it is large enough\"\n                # NOTE: Here, there can be an issue if you work with normalized data\n                qval = int(qval)\n                self.save_background(ch, idx, qval)\n                self._data[idx, ..., ch] -= qval\n\n        if self._clip_background_noise_to_zero:\n            self._data[self._data &lt; 0] = 0\n\n    def rm_bkground_set_max_val_and_upperclip_data(self, max_val, datasplit_type):\n        self.remove_background()\n        self.set_max_val(max_val, datasplit_type)\n        self.upperclip_data()\n\n    def upperclip_data(self):\n        if isinstance(self.max_val, list):\n            chN = self._data.shape[-1]\n            assert chN == len(self.max_val)\n            for ch in range(chN):\n                ch_data = self._data[..., ch]\n                ch_q = self.max_val[ch]\n                ch_data[ch_data &gt; ch_q] = ch_q\n                self._data[..., ch] = ch_data\n        else:\n            self._data[self._data &gt; self.max_val] = self.max_val\n\n    def compute_max_val(self):\n        if self._channelwise_quantile:\n            max_val_arr = [\n                np.quantile(self._data[..., i], self._quantile)\n                for i in range(self._data.shape[-1])\n            ]\n            return max_val_arr\n        else:\n            return np.quantile(self._data, self._quantile)\n\n    def set_max_val(self, max_val, datasplit_type):\n\n        if max_val is None:\n            assert datasplit_type == DataSplitType.Train\n            self.max_val = self.compute_max_val()\n        else:\n            assert max_val is not None\n            self.max_val = max_val\n\n    def get_max_val(self):\n        return self.max_val\n\n    def get_img_sz(self):\n        return self._img_sz\n\n    def get_num_frames(self):\n        return self._data.shape[0]\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert not self._5Ddata, \"This function is not supported for 3D data.\"\n        if t_list is None:\n            t_list = list(range(self._data.shape[0]))\n        if h_start is None:\n            h_start = 0\n        if h_end is None:\n            h_end = self._data.shape[1]\n        if w_start is None:\n            w_start = 0\n        if w_end is None:\n            w_end = self._data.shape[2]\n\n        self._data = self._data[t_list, h_start:h_end, w_start:w_end, :].copy()\n        if self._noise_data is not None:\n            self._noise_data = self._noise_data[\n                t_list, h_start:h_end, w_start:w_end, :\n            ].copy()\n        # TODO where tf is self._img_sz defined?\n        self.set_img_sz([self._img_sz, self._img_sz], self._grid_sz)\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data shape: {self._data.shape}\"\n        )\n\n    def get_idx_manager_shapes(\n        self, patch_size: int, grid_size: Union[int, Tuple[int, int, int]]\n    ):\n        numC = self._data.shape[-1]\n        if self._5Ddata:\n            patch_shape = (1, self._depth3D, patch_size, patch_size, numC)\n            if isinstance(grid_size, int):\n                grid_shape = (1, 1, grid_size, grid_size, numC)\n            else:\n                assert len(grid_size) == 3\n                assert all(\n                    [g &lt;= p for g, p in zip(grid_size, patch_shape[1:-1])]\n                ), f\"Grid size {grid_size} must be less than patch size {patch_shape[1:-1]}\"\n                grid_shape = (1, grid_size[0], grid_size[1], grid_size[2], numC)\n        else:\n            assert isinstance(grid_size, int)\n            grid_shape = (1, grid_size, grid_size, numC)\n            patch_shape = (1, patch_size, patch_size, numC)\n\n        return patch_shape, grid_shape\n\n    def set_img_sz(self, image_size, grid_size: Union[int, Tuple[int, int, int]]):\n        \"\"\"\n        If one wants to change the image size on the go, then this can be used.\n        Args:\n            image_size: size of one patch\n            grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n        \"\"\"\n        # hacky way to deal with image shape from new conf\n        self._img_sz = image_size[-1]  # TODO revisit!\n        self._grid_sz = grid_size\n        shape = self._data.shape\n\n        patch_shape, grid_shape = self.get_idx_manager_shapes(\n            self._img_sz, self._grid_sz\n        )\n        self.idx_manager = GridIndexManager(\n            shape, grid_shape, patch_shape, self._tiling_mode\n        )\n        # self.set_repeat_factor()\n\n    def __len__(self):\n        # Vera: N is the number of frames in Z stack\n        # Repeat factor is n_rows * n_cols\n        return self.idx_manager.total_grid_count()\n\n    def set_repeat_factor(self):\n        if self._grid_sz &gt; 1:\n            self._repeat_factor = self.idx_manager.grid_rows(\n                self._grid_sz\n            ) * self.idx_manager.grid_cols(self._grid_sz)\n        else:\n            self._repeat_factor = self.idx_manager.grid_rows(\n                self._img_sz\n            ) * self.idx_manager.grid_cols(self._img_sz)\n\n    def _init_msg(\n        self,\n    ):\n        msg = (\n            f\"[{self.__class__.__name__}] Train:{int(self._is_train)} Sz:{self._img_sz}\"\n        )\n        dim_sizes = [\n            self.idx_manager.get_individual_dim_grid_count(dim)\n            for dim in range(len(self._data.shape))\n        ]\n        dim_sizes = \",\".join([str(x) for x in dim_sizes])\n        msg += f\" N:{self.N} NumPatchPerN:{self._repeat_factor}\"\n        msg += f\"{self.idx_manager.total_grid_count()} DimSz:({dim_sizes})\"\n        msg += f\" TrimB:{self._tiling_mode}\"\n        # msg += f' NormInp:{self._normalized_input}'\n        # msg += f' SingleNorm:{self._use_one_mu_std}'\n        msg += f\" Rot:{self._enable_rotation}\"\n        if self._flipz_3D:\n            msg += f\" FlipZ:{self._flipz_3D}\"\n\n        msg += f\" RandCrop:{self._enable_random_cropping}\"\n        msg += f\" Channel:{self._num_channels}\"\n        # msg += f' Q:{self._quantile}'\n        if self._input_is_sum:\n            msg += f\" SummedInput:{self._input_is_sum}\"\n\n        if self._empty_patch_replacement_enabled:\n            msg += f\" ReplaceWithRandSample:{self._empty_patch_replacement_enabled}\"\n        if self._uncorrelated_channels:\n            msg += f\" Uncorr:{self._uncorrelated_channels}\"\n        if self._empty_patch_replacement_enabled:\n            msg += f\"-{self._empty_patch_replacement_channel_idx}-{self._empty_patch_replacement_probab}\"\n        if self._background_quantile &gt; 0.0:\n            msg += f\" BckQ:{self._background_quantile}\"\n\n        if self._start_alpha_arr is not None:\n            msg += f\" Alpha:[{self._start_alpha_arr},{self._end_alpha_arr}]\"\n        return msg\n\n    def _crop_imgs(self, index, *img_tuples: np.ndarray):\n        h, w = img_tuples[0].shape[-2:]\n        if self._img_sz is None:\n            return (\n                *img_tuples,\n                {\"h\": [0, h], \"w\": [0, w], \"hflip\": False, \"wflip\": False},\n            )\n\n        if self._enable_random_cropping:\n            patch_start_loc = self._get_random_hw(h, w)\n            if self._5Ddata:\n                patch_start_loc = (\n                    np.random.choice(1 + img_tuples[0].shape[-3] - self._depth3D),\n                ) + patch_start_loc\n        else:\n            patch_start_loc = self._get_deterministic_loc(index)\n\n        cropped_imgs = []\n        for img in img_tuples:\n            img = self._crop_flip_img(img, patch_start_loc, False, False)\n            cropped_imgs.append(img)\n\n        return (\n            *tuple(cropped_imgs),\n            {\n                \"hflip\": False,\n                \"wflip\": False,\n            },\n        )\n\n    def _crop_img(self, img: np.ndarray, patch_start_loc: Tuple):\n        if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n            # In training, this is used.\n            # NOTE: It is my opinion that if I just use self._crop_img_with_padding, it will work perfectly fine.\n            # The only benefit this if else loop provides is that it makes it easier to see what happens during training.\n            patch_end_loc = (\n                np.array(patch_start_loc, dtype=np.int32)\n                + self.idx_manager.patch_shape[1:-1]\n            )\n            if self._5Ddata:\n                z_start, h_start, w_start = patch_start_loc\n                z_end, h_end, w_end = patch_end_loc\n                new_img = img[..., z_start:z_end, h_start:h_end, w_start:w_end]\n            else:\n                h_start, w_start = patch_start_loc\n                h_end, w_end = patch_end_loc\n                new_img = img[..., h_start:h_end, w_start:w_end]\n\n            return new_img\n        else:\n            # During evaluation, this is used. In this situation, we can have negative h_start, w_start. Or h_start +self._img_sz can be larger than frame\n            # In these situations, we need some sort of padding. This is not needed  in the LeftTop alignement.\n            return self._crop_img_with_padding(img, patch_start_loc)\n\n    def get_begin_end_padding(self, start_pos, end_pos, max_len):\n        \"\"\"\n        The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n        padding on all four sides so that the final patch size is self._img_sz.\n        \"\"\"\n        pad_start = 0\n        pad_end = 0\n        if start_pos &lt; 0:\n            pad_start = -1 * start_pos\n\n        pad_end = max(0, end_pos - max_len)\n\n        return pad_start, pad_end\n\n    def _crop_img_with_padding(\n        self, img: np.ndarray, patch_start_loc, max_len_vals=None\n    ):\n        if max_len_vals is None:\n            max_len_vals = self.idx_manager.data_shape[1:-1]\n        patch_end_loc = np.array(patch_start_loc, dtype=int) + np.array(\n            self.idx_manager.patch_shape[1:-1], dtype=int\n        )\n        boundary_crossed = []\n        valid_slice = []\n        padding = [[0, 0]]\n        for start_idx, end_idx, max_len in zip(\n            patch_start_loc, patch_end_loc, max_len_vals\n        ):\n            boundary_crossed.append(end_idx &gt; max_len or start_idx &lt; 0)\n            valid_slice.append((max(0, start_idx), min(max_len, end_idx)))\n            pad = [0, 0]\n            if boundary_crossed[-1]:\n                pad = self.get_begin_end_padding(start_idx, end_idx, max_len)\n            padding.append(pad)\n        # max() is needed since h_start could be negative.\n        if self._5Ddata:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n                valid_slice[2][0] : valid_slice[2][1],\n            ]\n        else:\n            new_img = img[\n                ...,\n                valid_slice[0][0] : valid_slice[0][1],\n                valid_slice[1][0] : valid_slice[1][1],\n            ]\n\n        # print(np.array(padding).shape, img.shape, new_img.shape)\n        # print(padding)\n        if not np.all(padding == 0):\n            new_img = np.pad(new_img, padding, **self._overlapping_padding_kwargs)\n\n        return new_img\n\n    def _crop_flip_img(\n        self, img: np.ndarray, patch_start_loc: Tuple, h_flip: bool, w_flip: bool\n    ):\n        new_img = self._crop_img(img, patch_start_loc)\n        if h_flip:\n            new_img = new_img[..., ::-1, :]\n        if w_flip:\n            new_img = new_img[..., :, ::-1]\n\n        return new_img.astype(np.float32)\n\n    def _load_img(\n        self, index: Union[int, Tuple[int, int]]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the channels and also the respective noise channels.\n        \"\"\"\n        if isinstance(index, int) or isinstance(index, np.int64):\n            idx = index\n        else:\n            idx = index[0]\n\n        patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n        imgs = self._data[patch_loc_list[0]]\n        # if self._5Ddata:\n        #     assert self._noise_data is None, 'Noise is not supported for 5D data'\n        #     n_loc, z_loc = patch_loc_list[:2]\n        #     z_loc_interval = range(z_loc, z_loc + self._depth3D)\n        #     imgs = self._data[n_loc, z_loc_interval]\n        # else:\n        #     imgs = self._data[patch_loc_list[0]]\n\n        loaded_imgs = [imgs[None, ..., i] for i in range(imgs.shape[-1])]\n        noise = []\n        if self._noise_data is not None and not self._disable_noise:\n            noise = [\n                self._noise_data[patch_loc_list[0]][None, ..., i]\n                for i in range(self._noise_data.shape[-1])\n            ]\n        return tuple(loaded_imgs), tuple(noise)\n\n    def get_mean_std(self):\n        return self._mean, self._std\n\n    def set_mean_std(self, mean_val, std_val):\n        self._mean = mean_val\n        self._std = std_val\n\n    def normalize_img(self, *img_tuples):\n        mean, std = self.get_mean_std()\n        mean = mean[\"target\"]\n        std = std[\"target\"]\n        mean = mean.squeeze()\n        std = std.squeeze()\n        normalized_imgs = []\n        for i, img in enumerate(img_tuples):\n            img = (img - mean[i]) / std[i]\n            normalized_imgs.append(img)\n        return tuple(normalized_imgs)\n\n    def normalize_input(self, x):\n        mean_dict, std_dict = self.get_mean_std()\n        mean_ = mean_dict[\"input\"].mean()\n        std_ = std_dict[\"input\"].mean()\n        return (x - mean_) / std_\n\n    def normalize_target(self, target):\n        mean_dict, std_dict = self.get_mean_std()\n        mean_ = mean_dict[\"target\"].squeeze(0)\n        std_ = std_dict[\"target\"].squeeze(0)\n        return (target - mean_) / std_\n\n    def get_grid_size(self):\n        return self._grid_sz\n\n    def get_idx_manager(self):\n        return self.idx_manager\n\n    def per_side_overlap_pixelcount(self):\n        return (self._img_sz - self._grid_sz) // 2\n\n    # def on_boundary(self, cur_loc, frame_size):\n    #     return cur_loc + self._img_sz &gt; frame_size or cur_loc &lt; 0\n\n    def _get_deterministic_loc(self, index: int):\n        \"\"\"\n        It returns the top-left corner of the patch corresponding to index.\n        \"\"\"\n        loc_list = self.idx_manager.get_patch_location_from_dataset_idx(index)\n        # last dim is channel. we need to take the third and the second last element.\n        return loc_list[1:-1]\n\n    def compute_individual_mean_std(self):\n        # numpy 1.19.2 has issues in computing for large arrays. https://github.com/numpy/numpy/issues/8869\n        # mean = np.mean(self._data, axis=(0, 1, 2))\n        # std = np.std(self._data, axis=(0, 1, 2))\n        mean_arr = []\n        std_arr = []\n        for ch_idx in range(self._data.shape[-1]):\n            mean_ = (\n                0.0\n                if self._skip_normalization_using_mean\n                else self._data[..., ch_idx].mean()\n            )\n            if self._noise_data is not None:\n                std_ = (\n                    self._data[..., ch_idx] + self._noise_data[..., ch_idx + 1]\n                ).std()\n            else:\n                std_ = self._data[..., ch_idx].std()\n\n            mean_arr.append(mean_)\n            std_arr.append(std_)\n\n        mean = np.array(mean_arr)\n        std = np.array(std_arr)\n        if (\n            self._5Ddata\n        ):  # NOTE: IDEALLY this should be only when the model expects 3D data.\n            return mean[None, :, None, None, None], std[None, :, None, None, None]\n\n        return mean[None, :, None, None], std[None, :, None, None]\n\n    def compute_mean_std(self, allow_for_validation_data=False):\n        \"\"\"\n        Note that we must compute this only for training data.\n        \"\"\"\n        assert (\n            self._is_train is True or allow_for_validation_data\n        ), \"This is just allowed for training data\"\n        assert self._use_one_mu_std is True, \"This is the only supported case\"\n\n        if self._input_idx is not None:\n            assert (\n                self._tar_idx_list is not None\n            ), \"tar_idx_list must be set if input_idx is set.\"\n            assert self._noise_data is None, \"This is not supported with noise\"\n            assert (\n                self._target_separate_normalization is True\n            ), \"This is not supported with target_separate_normalization=False\"\n\n            mean, std = self.compute_individual_mean_std()\n            mean_dict = {\n                \"input\": mean[:, self._input_idx : self._input_idx + 1],\n                \"target\": mean[:, self._tar_idx_list],\n            }\n            std_dict = {\n                \"input\": std[:, self._input_idx : self._input_idx + 1],\n                \"target\": std[:, self._tar_idx_list],\n            }\n            return mean_dict, std_dict\n\n        if self._input_is_sum:\n            assert self._noise_data is None, \"This is not supported with noise\"\n            mean = [\n                np.mean(self._data[..., k : k + 1], keepdims=True)\n                for k in range(self._num_channels)\n            ]\n            mean = np.sum(mean, keepdims=True)[0]\n            std = np.linalg.norm(\n                [\n                    np.std(self._data[..., k : k + 1], keepdims=True)\n                    for k in range(self._num_channels)\n                ],\n                keepdims=True,\n            )[0]\n        else:\n            mean = np.mean(self._data, keepdims=True).reshape(1, 1, 1, 1)\n            if self._noise_data is not None:\n                std = np.std(\n                    self._data + self._noise_data[..., 1:], keepdims=True\n                ).reshape(1, 1, 1, 1)\n            else:\n                std = np.std(self._data, keepdims=True).reshape(1, 1, 1, 1)\n\n        mean = np.repeat(mean, self._num_channels, axis=1)\n        std = np.repeat(std, self._num_channels, axis=1)\n\n        if self._skip_normalization_using_mean:\n            mean = np.zeros_like(mean)\n\n        if self._5Ddata:\n            mean = mean[:, :, None]\n            std = std[:, :, None]\n\n        mean_dict = {\"input\": mean}  # , 'target':mean}\n        std_dict = {\"input\": std}  # , 'target':std}\n\n        if self._target_separate_normalization:\n            mean, std = self.compute_individual_mean_std()\n\n        mean_dict[\"target\"] = mean\n        std_dict[\"target\"] = std\n        return mean_dict, std_dict\n\n    def _get_random_hw(self, h: int, w: int):\n        \"\"\"\n        Random starting position for the crop for the img with index `index`.\n        \"\"\"\n        if h != self._img_sz:\n            h_start = np.random.choice(h - self._img_sz)\n            w_start = np.random.choice(w - self._img_sz)\n        else:\n            h_start = 0\n            w_start = 0\n        return h_start, w_start\n\n    def _get_img(self, index: Union[int, Tuple[int, int]]):\n        \"\"\"\n        Loads an image.\n        Crops the image such that cropped image has content.\n        \"\"\"\n        img_tuples, noise_tuples = self._load_img(index)\n        cropped_img_tuples = self._crop_imgs(index, *img_tuples, *noise_tuples)[:-1]\n        cropped_noise_tuples = cropped_img_tuples[len(img_tuples) :]\n        cropped_img_tuples = cropped_img_tuples[: len(img_tuples)]\n        return cropped_img_tuples, cropped_noise_tuples\n\n    def replace_with_empty_patch(self, img_tuples):\n        \"\"\"\n        Replaces the content of one of the channels with background\n        \"\"\"\n        empty_index = self._empty_patch_fetcher.sample()\n        empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n        assert (\n            len(empty_img_noise_tuples) == 0\n        ), \"Noise is not supported with empty patch replacement\"\n        final_img_tuples = []\n        for tuple_idx in range(len(img_tuples)):\n            if tuple_idx == self._empty_patch_replacement_channel_idx:\n                final_img_tuples.append(empty_img_tuples[tuple_idx])\n            else:\n                final_img_tuples.append(img_tuples[tuple_idx])\n        return tuple(final_img_tuples)\n\n    def get_mean_std_for_input(self):\n        mean, std = self.get_mean_std()\n        return mean[\"input\"], std[\"input\"]\n\n    def _compute_target(self, img_tuples, alpha):\n        if self._tar_idx_list is not None and isinstance(self._tar_idx_list, int):\n            target = img_tuples[self._tar_idx_list]\n        else:\n            if self._tar_idx_list is not None:\n                assert isinstance(self._tar_idx_list, list) or isinstance(\n                    self._tar_idx_list, tuple\n                )\n                img_tuples = [img_tuples[i] for i in self._tar_idx_list]\n\n            target = np.concatenate(img_tuples, axis=0)\n        return target\n\n    def _compute_input_with_alpha(self, img_tuples, alpha_list):\n        # assert self._normalized_input is True, \"normalization should happen here\"\n        if self._input_idx is not None:\n            inp = img_tuples[self._input_idx]\n        else:\n            inp = 0\n            for alpha, img in zip(alpha_list, img_tuples):\n                inp += img * alpha\n\n            if self._normalized_input is False:\n                return inp.astype(np.float32)\n\n        mean, std = self.get_mean_std_for_input()\n        mean = mean.squeeze()\n        std = std.squeeze()\n        if mean.size == 1:\n            mean = mean.reshape(\n                1,\n            )\n            std = std.reshape(\n                1,\n            )\n\n        for i in range(len(mean)):\n            assert mean[0] == mean[i]\n            assert std[0] == std[i]\n\n        inp = (inp - mean[0]) / std[0]\n        return inp.astype(np.float32)\n\n    def _sample_alpha(self):\n        alpha_arr = []\n        for i in range(self._num_channels):\n            alpha_pos = np.random.rand()\n            alpha = self._start_alpha_arr[i] + alpha_pos * (\n                self._end_alpha_arr[i] - self._start_alpha_arr[i]\n            )\n            alpha_arr.append(alpha)\n        return alpha_arr\n\n    def _compute_input(self, img_tuples):\n        alpha = [1 / len(img_tuples) for _ in range(len(img_tuples))]\n        if self._start_alpha_arr is not None:\n            alpha = self._sample_alpha()\n\n        inp = self._compute_input_with_alpha(img_tuples, alpha)\n        if self._input_is_sum:\n            inp = len(img_tuples) * inp\n        return inp, alpha\n\n    def _get_index_from_valid_target_logic(self, index):\n        if self._validtarget_rand_fract is not None:\n            if np.random.rand() &lt; self._validtarget_rand_fract:\n                index = self._train_index_switcher.get_valid_target_index()\n            else:\n                index = self._train_index_switcher.get_invalid_target_index()\n        return index\n\n    def _rotate2D(self, img_tuples, noise_tuples):\n        img_kwargs = {}\n        for i, img in enumerate(img_tuples):\n            for k in range(len(img)):\n                img_kwargs[f\"img{i}_{k}\"] = img[k]\n\n        noise_kwargs = {}\n        for i, nimg in enumerate(noise_tuples):\n            for k in range(len(nimg)):\n                noise_kwargs[f\"noise{i}_{k}\"] = nimg[k]\n\n        keys = list(img_kwargs.keys()) + list(noise_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(\n            image=img_tuples[0][0], **img_kwargs, **noise_kwargs\n        )\n\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(rot_dic[f\"img{i}_0\"][None])\n            else:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [rot_dic[f\"img{i}_{k}\"][None] for k in range(len(img))], axis=0\n                    )\n                )\n\n        rotated_noise_tuples = []\n        for i, nimg in enumerate(noise_tuples):\n            if len(nimg) == 1:\n                rotated_noise_tuples.append(rot_dic[f\"noise{i}_0\"][None])\n            else:\n                rotated_noise_tuples.append(\n                    np.concatenate(\n                        [rot_dic[f\"noise{i}_{k}\"][None] for k in range(len(nimg))],\n                        axis=0,\n                    )\n                )\n\n        return rotated_img_tuples, rotated_noise_tuples\n\n    def _rotate(self, img_tuples, noise_tuples):\n\n        if self._5Ddata:\n            return self._rotate3D(img_tuples, noise_tuples)\n        else:\n            return self._rotate2D(img_tuples, noise_tuples)\n\n    def _rotate3D(self, img_tuples, noise_tuples):\n        img_kwargs = {}\n        # random flip in z direction\n        flip_z = self._flipz_3D and np.random.rand() &lt; 0.5\n        for i, img in enumerate(img_tuples):\n            for j in range(self._depth3D):\n                for k in range(len(img)):\n                    if flip_z:\n                        z_idx = self._depth3D - 1 - j\n                    else:\n                        z_idx = j\n                    img_kwargs[f\"img{i}_{z_idx}_{k}\"] = img[k, j]\n\n        noise_kwargs = {}\n        for i, nimg in enumerate(noise_tuples):\n            for j in range(self._depth3D):\n                for k in range(len(nimg)):\n                    if flip_z:\n                        z_idx = self._depth3D - 1 - j\n                    else:\n                        z_idx = j\n                    noise_kwargs[f\"noise{i}_{z_idx}_{k}\"] = nimg[k, j]\n\n        keys = list(img_kwargs.keys()) + list(noise_kwargs.keys())\n        self._rotation_transform.add_targets({k: \"image\" for k in keys})\n        rot_dic = self._rotation_transform(\n            image=img_tuples[0][0][0], **img_kwargs, **noise_kwargs\n        )\n        rotated_img_tuples = []\n        for i, img in enumerate(img_tuples):\n            if len(img) == 1:\n                rotated_img_tuples.append(\n                    np.concatenate(\n                        [\n                            rot_dic[f\"img{i}_{j}_0\"][None, None]\n                            for j in range(self._depth3D)\n                        ],\n                        axis=1,\n                    )\n                )\n            else:\n                temp_arr = []\n                for k in range(len(img)):\n                    temp_arr.append(\n                        np.concatenate(\n                            [\n                                rot_dic[f\"img{i}_{j}_{k}\"][None, None]\n                                for j in range(self._depth3D)\n                            ],\n                            axis=1,\n                        )\n                    )\n                rotated_img_tuples.append(np.concatenate(temp_arr, axis=0))\n\n        rotated_noise_tuples = []\n        for i, nimg in enumerate(noise_tuples):\n            if len(nimg) == 1:\n                rotated_noise_tuples.append(\n                    np.concatenate(\n                        [\n                            rot_dic[f\"noise{i}_{j}_0\"][None, None]\n                            for j in range(self._depth3D)\n                        ],\n                        axis=1,\n                    )\n                )\n            else:\n                temp_arr = []\n                for k in range(len(nimg)):\n                    temp_arr.append(\n                        np.concatenate(\n                            [\n                                rot_dic[f\"noise{i}_{j}_{k}\"][None, None]\n                                for j in range(self._depth3D)\n                            ],\n                            axis=1,\n                        )\n                    )\n                rotated_noise_tuples.append(np.concatenate(temp_arr, axis=0))\n\n        return rotated_img_tuples, rotated_noise_tuples\n\n    def get_uncorrelated_img_tuples(self, index):\n        \"\"\"\n        Content of channels like actin and nuclei is \"correlated\" in its\n        respective location, this function allows to pick channels' content\n        from different patches of the image to make it \"uncorrelated\".\n        \"\"\"\n        img_tuples, noise_tuples = self._get_img(index)\n        assert len(noise_tuples) == 0\n        img_tuples = [img_tuples[0]]\n        for ch_idx in range(1, len(img_tuples)):\n            new_index = np.random.randint(len(self))\n            other_img_tuples, _ = self._get_img(new_index)\n            img_tuples.append(other_img_tuples[ch_idx])\n        return img_tuples, noise_tuples\n\n    def __getitem__(\n        self, index: Union[int, Tuple[int, int]]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        # Vera: input can be both real microscopic image and two separate channels that are summed in the code\n\n        if self._train_index_switcher is not None:\n            index = self._get_index_from_valid_target_logic(index)\n\n        if (\n            self._uncorrelated_channels\n            and np.random.rand() &lt; self._uncorrelated_channel_probab\n        ):\n            img_tuples, noise_tuples = self.get_uncorrelated_img_tuples(index)\n        else:\n            img_tuples, noise_tuples = self._get_img(index)\n\n        assert (\n            self._empty_patch_replacement_enabled != True\n        ), \"This is not supported with noise\"\n\n        # Replace the content of one of the channels\n        # with background with given probability\n        if self._empty_patch_replacement_enabled:\n            if np.random.rand() &lt; self._empty_patch_replacement_probab:\n                img_tuples = self.replace_with_empty_patch(img_tuples)\n\n        # Noise tuples are not needed for the paper\n        # the image tuples are noisy by default\n        # TODO: remove noise tuples completely?\n        if self._enable_rotation:\n            img_tuples, noise_tuples = self._rotate(img_tuples, noise_tuples)\n\n        # Add noise tuples with image tuples to create the input\n        if len(noise_tuples) &gt; 0:\n            factor = np.sqrt(2) if self._input_is_sum else 1.0\n            input_tuples = [x + noise_tuples[0] * factor for x in img_tuples]\n        else:\n            input_tuples = img_tuples\n\n        # Weight the individual channels, typically alpha is fixed\n        inp, alpha = self._compute_input(input_tuples)\n\n        # Add noise tuples to the image tuples to create the target\n        if len(noise_tuples) &gt;= 1:\n            img_tuples = [x + noise for x, noise in zip(img_tuples, noise_tuples[1:])]\n\n        target = self._compute_target(img_tuples, alpha)\n        norm_target = self.normalize_target(target)\n\n        output = [inp, norm_target]\n\n        if self._return_alpha:\n            output.append(alpha)\n\n        if self._return_index:\n            output.append(index)\n\n        return tuple(output)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.__init__","title":"<code>__init__(data_config, fpath, load_data_fn, val_fraction=None, test_fraction=None)</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_config: DatasetConfig,\n    fpath: str,\n    load_data_fn: Callable,\n    val_fraction: float = None,\n    test_fraction: float = None,\n):\n    \"\"\" \"\"\"\n    self._data_type = data_config.data_type\n    self._fpath = fpath\n    self._data = self._noise_data = None\n    self.Z = 1\n    self._5Ddata = False\n    self._tiling_mode = data_config.tiling_mode\n    # by default, if the noise is present, add it to the input and target.\n    self._disable_noise = False  # to add synthetic noise\n    self._poisson_noise_factor = None\n    self._train_index_switcher = None\n    self._depth3D = data_config.depth3D\n    self._mode_3D = data_config.mode_3D\n    # NOTE: Input is the sum of the different channels. It is not the average of the different channels.\n    self._input_is_sum = data_config.input_is_sum\n    self._num_channels = data_config.num_channels\n    self._input_idx = data_config.input_idx\n    self._tar_idx_list = data_config.target_idx_list\n\n    if data_config.datasplit_type == DataSplitType.Train:\n        self._datausage_fraction = data_config.trainig_datausage_fraction\n        # assert self._datausage_fraction == 1.0, 'Not supported. Use validtarget_random_fraction and training_validtarget_fraction to get the same effect'\n        self._validtarget_rand_fract = data_config.validtarget_random_fraction\n        # self._validtarget_random_fraction_final = data_config.get('validtarget_random_fraction_final', None)\n        # self._validtarget_random_fraction_stepepoch = data_config.get('validtarget_random_fraction_stepepoch', None)\n        # self._idx_count = 0\n    elif data_config.datasplit_type == DataSplitType.Val:\n        self._datausage_fraction = data_config.validation_datausage_fraction\n    else:\n        self._datausage_fraction = 1.0\n\n    self.load_data(\n        data_config,\n        data_config.datasplit_type,\n        load_data_fn=load_data_fn,\n        val_fraction=val_fraction,\n        test_fraction=test_fraction,\n        allow_generation=data_config.allow_generation,\n    )\n    self._normalized_input = data_config.normalized_input\n    self._quantile = 1.0\n    self._channelwise_quantile = False\n    self._background_quantile = 0.0\n    self._clip_background_noise_to_zero = False\n    self._skip_normalization_using_mean = False\n    self._empty_patch_replacement_enabled = False\n\n    self._background_values = None\n\n    self._overlapping_padding_kwargs = data_config.overlapping_padding_kwargs\n    if self._tiling_mode in [TilingMode.TrimBoundary, TilingMode.ShiftBoundary]:\n        if (\n            self._overlapping_padding_kwargs is None\n            or data_config.multiscale_lowres_count is not None\n        ):\n            # raise warning\n            print(\"Padding is not used with this alignement style\")\n    else:\n        assert (\n            self._overlapping_padding_kwargs is not None\n        ), \"When not trimming boudnary, padding is needed.\"\n\n    self._is_train = data_config.datasplit_type == DataSplitType.Train\n\n    # input = alpha * ch1 + (1-alpha)*ch2.\n    # alpha is sampled randomly between these two extremes\n    self._start_alpha_arr = self._end_alpha_arr = self._return_alpha = None\n\n    self._img_sz = self._grid_sz = self._repeat_factor = self.idx_manager = None\n\n    # changed set_img_sz because \"grid_size\" in data_config returns false\n    try:\n        grid_size = data_config.grid_size\n    except AttributeError:\n        grid_size = data_config.image_size\n\n    if self._is_train:\n        self._start_alpha_arr = data_config.start_alpha\n        self._end_alpha_arr = data_config.end_alpha\n\n        self.set_img_sz(data_config.image_size, grid_size)\n\n        if self._validtarget_rand_fract is not None:\n            self._train_index_switcher = IndexSwitcher(\n                self.idx_manager, data_config, self._img_sz\n            )\n\n    else:\n        self.set_img_sz(data_config.image_size, grid_size)\n\n    self._return_alpha = False\n    self._return_index = False\n\n    self._empty_patch_replacement_enabled = (\n        data_config.empty_patch_replacement_enabled and self._is_train\n    )\n    if self._empty_patch_replacement_enabled:\n        self._empty_patch_replacement_channel_idx = (\n            data_config.empty_patch_replacement_channel_idx\n        )\n        self._empty_patch_replacement_probab = (\n            data_config.empty_patch_replacement_probab\n        )\n        data_frames = self._data[..., self._empty_patch_replacement_channel_idx]\n        # NOTE: This is on the raw data. So, it must be called before removing the background.\n        self._empty_patch_fetcher = EmptyPatchFetcher(\n            self.idx_manager,\n            self._img_sz,\n            data_frames,\n            max_val_threshold=data_config.empty_patch_max_val_threshold,\n        )\n\n    self.rm_bkground_set_max_val_and_upperclip_data(\n        data_config.max_val, data_config.datasplit_type\n    )\n\n    # For overlapping dloader, image_size and repeat_factors are not related. hence a different function.\n\n    self._mean = None\n    self._std = None\n    self._use_one_mu_std = data_config.use_one_mu_std\n\n    self._target_separate_normalization = data_config.target_separate_normalization\n\n    self._enable_rotation = data_config.enable_rotation_aug\n    flipz_3D = data_config.random_flip_z_3D\n    self._flipz_3D = flipz_3D and self._enable_rotation\n\n    self._enable_random_cropping = data_config.enable_random_cropping\n    self._uncorrelated_channels = (\n        data_config.uncorrelated_channels and self._is_train\n    )\n    self._uncorrelated_channel_probab = data_config.uncorrelated_channel_probab\n    assert self._is_train or self._uncorrelated_channels is False\n    assert (\n        self._enable_random_cropping is True or self._uncorrelated_channels is False\n    )\n    # Randomly rotate [-90,90]\n\n    self._rotation_transform = None\n    if self._enable_rotation:\n        # TODO: fix this import\n        import albumentations as A\n\n        self._rotation_transform = A.Compose([A.Flip(), A.RandomRotate90()])\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader._get_deterministic_loc","title":"<code>_get_deterministic_loc(index)</code>","text":"<p>It returns the top-left corner of the patch corresponding to index.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def _get_deterministic_loc(self, index: int):\n    \"\"\"\n    It returns the top-left corner of the patch corresponding to index.\n    \"\"\"\n    loc_list = self.idx_manager.get_patch_location_from_dataset_idx(index)\n    # last dim is channel. we need to take the third and the second last element.\n    return loc_list[1:-1]\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader._get_img","title":"<code>_get_img(index)</code>","text":"<p>Loads an image. Crops the image such that cropped image has content.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def _get_img(self, index: Union[int, Tuple[int, int]]):\n    \"\"\"\n    Loads an image.\n    Crops the image such that cropped image has content.\n    \"\"\"\n    img_tuples, noise_tuples = self._load_img(index)\n    cropped_img_tuples = self._crop_imgs(index, *img_tuples, *noise_tuples)[:-1]\n    cropped_noise_tuples = cropped_img_tuples[len(img_tuples) :]\n    cropped_img_tuples = cropped_img_tuples[: len(img_tuples)]\n    return cropped_img_tuples, cropped_noise_tuples\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader._get_random_hw","title":"<code>_get_random_hw(h, w)</code>","text":"<p>Random starting position for the crop for the img with index <code>index</code>.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def _get_random_hw(self, h: int, w: int):\n    \"\"\"\n    Random starting position for the crop for the img with index `index`.\n    \"\"\"\n    if h != self._img_sz:\n        h_start = np.random.choice(h - self._img_sz)\n        w_start = np.random.choice(w - self._img_sz)\n    else:\n        h_start = 0\n        w_start = 0\n    return h_start, w_start\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader._load_img","title":"<code>_load_img(index)</code>","text":"<p>Returns the channels and also the respective noise channels.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def _load_img(\n    self, index: Union[int, Tuple[int, int]]\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the channels and also the respective noise channels.\n    \"\"\"\n    if isinstance(index, int) or isinstance(index, np.int64):\n        idx = index\n    else:\n        idx = index[0]\n\n    patch_loc_list = self.idx_manager.get_patch_location_from_dataset_idx(idx)\n    imgs = self._data[patch_loc_list[0]]\n    # if self._5Ddata:\n    #     assert self._noise_data is None, 'Noise is not supported for 5D data'\n    #     n_loc, z_loc = patch_loc_list[:2]\n    #     z_loc_interval = range(z_loc, z_loc + self._depth3D)\n    #     imgs = self._data[n_loc, z_loc_interval]\n    # else:\n    #     imgs = self._data[patch_loc_list[0]]\n\n    loaded_imgs = [imgs[None, ..., i] for i in range(imgs.shape[-1])]\n    noise = []\n    if self._noise_data is not None and not self._disable_noise:\n        noise = [\n            self._noise_data[patch_loc_list[0]][None, ..., i]\n            for i in range(self._noise_data.shape[-1])\n        ]\n    return tuple(loaded_imgs), tuple(noise)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.compute_mean_std","title":"<code>compute_mean_std(allow_for_validation_data=False)</code>","text":"<p>Note that we must compute this only for training data.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def compute_mean_std(self, allow_for_validation_data=False):\n    \"\"\"\n    Note that we must compute this only for training data.\n    \"\"\"\n    assert (\n        self._is_train is True or allow_for_validation_data\n    ), \"This is just allowed for training data\"\n    assert self._use_one_mu_std is True, \"This is the only supported case\"\n\n    if self._input_idx is not None:\n        assert (\n            self._tar_idx_list is not None\n        ), \"tar_idx_list must be set if input_idx is set.\"\n        assert self._noise_data is None, \"This is not supported with noise\"\n        assert (\n            self._target_separate_normalization is True\n        ), \"This is not supported with target_separate_normalization=False\"\n\n        mean, std = self.compute_individual_mean_std()\n        mean_dict = {\n            \"input\": mean[:, self._input_idx : self._input_idx + 1],\n            \"target\": mean[:, self._tar_idx_list],\n        }\n        std_dict = {\n            \"input\": std[:, self._input_idx : self._input_idx + 1],\n            \"target\": std[:, self._tar_idx_list],\n        }\n        return mean_dict, std_dict\n\n    if self._input_is_sum:\n        assert self._noise_data is None, \"This is not supported with noise\"\n        mean = [\n            np.mean(self._data[..., k : k + 1], keepdims=True)\n            for k in range(self._num_channels)\n        ]\n        mean = np.sum(mean, keepdims=True)[0]\n        std = np.linalg.norm(\n            [\n                np.std(self._data[..., k : k + 1], keepdims=True)\n                for k in range(self._num_channels)\n            ],\n            keepdims=True,\n        )[0]\n    else:\n        mean = np.mean(self._data, keepdims=True).reshape(1, 1, 1, 1)\n        if self._noise_data is not None:\n            std = np.std(\n                self._data + self._noise_data[..., 1:], keepdims=True\n            ).reshape(1, 1, 1, 1)\n        else:\n            std = np.std(self._data, keepdims=True).reshape(1, 1, 1, 1)\n\n    mean = np.repeat(mean, self._num_channels, axis=1)\n    std = np.repeat(std, self._num_channels, axis=1)\n\n    if self._skip_normalization_using_mean:\n        mean = np.zeros_like(mean)\n\n    if self._5Ddata:\n        mean = mean[:, :, None]\n        std = std[:, :, None]\n\n    mean_dict = {\"input\": mean}  # , 'target':mean}\n    std_dict = {\"input\": std}  # , 'target':std}\n\n    if self._target_separate_normalization:\n        mean, std = self.compute_individual_mean_std()\n\n    mean_dict[\"target\"] = mean\n    std_dict[\"target\"] = std\n    return mean_dict, std_dict\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.get_begin_end_padding","title":"<code>get_begin_end_padding(start_pos, end_pos, max_len)</code>","text":"<p>The effect is that the image with size self._grid_sz is in the center of the patch with sufficient padding on all four sides so that the final patch size is self._img_sz.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def get_begin_end_padding(self, start_pos, end_pos, max_len):\n    \"\"\"\n    The effect is that the image with size self._grid_sz is in the center of the patch with sufficient\n    padding on all four sides so that the final patch size is self._img_sz.\n    \"\"\"\n    pad_start = 0\n    pad_end = 0\n    if start_pos &lt; 0:\n        pad_start = -1 * start_pos\n\n    pad_end = max(0, end_pos - max_len)\n\n    return pad_start, pad_end\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.get_uncorrelated_img_tuples","title":"<code>get_uncorrelated_img_tuples(index)</code>","text":"<p>Content of channels like actin and nuclei is \"correlated\" in its respective location, this function allows to pick channels' content from different patches of the image to make it \"uncorrelated\".</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def get_uncorrelated_img_tuples(self, index):\n    \"\"\"\n    Content of channels like actin and nuclei is \"correlated\" in its\n    respective location, this function allows to pick channels' content\n    from different patches of the image to make it \"uncorrelated\".\n    \"\"\"\n    img_tuples, noise_tuples = self._get_img(index)\n    assert len(noise_tuples) == 0\n    img_tuples = [img_tuples[0]]\n    for ch_idx in range(1, len(img_tuples)):\n        new_index = np.random.randint(len(self))\n        other_img_tuples, _ = self._get_img(new_index)\n        img_tuples.append(other_img_tuples[ch_idx])\n    return img_tuples, noise_tuples\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.replace_with_empty_patch","title":"<code>replace_with_empty_patch(img_tuples)</code>","text":"<p>Replaces the content of one of the channels with background</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def replace_with_empty_patch(self, img_tuples):\n    \"\"\"\n    Replaces the content of one of the channels with background\n    \"\"\"\n    empty_index = self._empty_patch_fetcher.sample()\n    empty_img_tuples, empty_img_noise_tuples = self._get_img(empty_index)\n    assert (\n        len(empty_img_noise_tuples) == 0\n    ), \"Noise is not supported with empty patch replacement\"\n    final_img_tuples = []\n    for tuple_idx in range(len(img_tuples)):\n        if tuple_idx == self._empty_patch_replacement_channel_idx:\n            final_img_tuples.append(empty_img_tuples[tuple_idx])\n        else:\n            final_img_tuples.append(img_tuples[tuple_idx])\n    return tuple(final_img_tuples)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multich_dataset/#careamics.lvae_training.dataset.multich_dataset.MultiChDloader.set_img_sz","title":"<code>set_img_sz(image_size, grid_size)</code>","text":"<p>If one wants to change the image size on the go, then this can be used. Args:     image_size: size of one patch     grid_size: frame is divided into square grids of this size. A patch centered on a grid having size <code>image_size</code> is returned.</p> Source code in <code>src/careamics/lvae_training/dataset/multich_dataset.py</code> <pre><code>def set_img_sz(self, image_size, grid_size: Union[int, Tuple[int, int, int]]):\n    \"\"\"\n    If one wants to change the image size on the go, then this can be used.\n    Args:\n        image_size: size of one patch\n        grid_size: frame is divided into square grids of this size. A patch centered on a grid having size `image_size` is returned.\n    \"\"\"\n    # hacky way to deal with image shape from new conf\n    self._img_sz = image_size[-1]  # TODO revisit!\n    self._grid_sz = grid_size\n    shape = self._data.shape\n\n    patch_shape, grid_shape = self.get_idx_manager_shapes(\n        self._img_sz, self._grid_sz\n    )\n    self.idx_manager = GridIndexManager(\n        shape, grid_shape, patch_shape, self._tiling_mode\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/","title":"multifile_dataset","text":""},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.MultiChannelData","title":"<code>MultiChannelData</code>","text":"<p>               Bases: <code>Sequence</code></p> <p>each element in data_arr should be a NHW array</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class MultiChannelData(Sequence):\n    \"\"\"\n    each element in data_arr should be a N*H*W array\n    \"\"\"\n\n    def __init__(self, data_arr, paths=None):\n        self.paths = paths\n\n        self._data = data_arr\n\n    def __len__(self):\n        n = 0\n        for x in self._data:\n            n += x.shape[0]\n        return n\n\n    def __getitem__(self, idx):\n        n = 0\n        for dataidx, x in enumerate(self._data):\n            if idx &lt; n + x.shape[0]:\n                if self.paths is None:\n                    return x[idx - n], None\n                else:\n                    return x[idx - n], (self.paths[dataidx])\n            n += x.shape[0]\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.MultiFileDset","title":"<code>MultiFileDset</code>","text":"<p>Here, we handle dataset having multiple files. Each file can have a different spatial dimension and number of frames (Z stack).</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class MultiFileDset:\n    \"\"\"\n    Here, we handle dataset having multiple files. Each file can have a different spatial dimension and number of frames (Z stack).\n    \"\"\"\n\n    def __init__(\n        self,\n        data_config: DatasetConfig,\n        fpath: str,\n        load_data_fn: Callable[..., Union[TwoChannelData, MultiChannelData]],\n        val_fraction=None,\n        test_fraction=None,\n    ):\n        self._fpath = fpath\n        data: Union[TwoChannelData, MultiChannelData] = load_data_fn(\n            data_config,\n            self._fpath,\n            data_config.datasplit_type,\n            val_fraction=val_fraction,\n            test_fraction=test_fraction,\n        )\n        self.dsets = []\n\n        for i in range(len(data)):\n            prefetched_data, fpath_tuple = data[i]\n            if (\n                data_config.multiscale_lowres_count is not None\n                and data_config.multiscale_lowres_count &gt; 1\n            ):\n\n                self.dsets.append(\n                    SingleFileLCDset(\n                        prefetched_data[None],\n                        data_config,\n                        fpath_tuple,\n                        load_data_fn,\n                        val_fraction=val_fraction,\n                        test_fraction=test_fraction,\n                    )\n                )\n\n            else:\n                self.dsets.append(\n                    SingleFileDset(\n                        prefetched_data[None],\n                        data_config,\n                        fpath_tuple,\n                        load_data_fn,\n                        val_fraction=val_fraction,\n                        test_fraction=test_fraction,\n                    )\n                )\n\n        self.rm_bkground_set_max_val_and_upperclip_data(\n            data_config.max_val, data_config.datasplit_type\n        )\n        count = 0\n        avg_height = 0\n        avg_width = 0\n        for dset in self.dsets:\n            shape = dset.get_data_shape()\n            avg_height += shape[1]\n            avg_width += shape[2]\n            count += shape[0]\n\n        avg_height = int(avg_height / len(self.dsets))\n        avg_width = int(avg_width / len(self.dsets))\n        print(\n            f\"{self.__class__.__name__} avg height: {avg_height}, avg width: {avg_width}, count: {count}\"\n        )\n\n    def rm_bkground_set_max_val_and_upperclip_data(self, max_val, datasplit_type):\n        self.set_max_val(max_val, datasplit_type)\n        self.upperclip_data()\n\n    def set_mean_std(self, mean_val, std_val):\n        for dset in self.dsets:\n            dset.set_mean_std(mean_val, std_val)\n\n    def get_mean_std(self):\n        return self.dsets[0].get_mean_std()\n\n    def compute_max_val(self):\n        max_val_arr = []\n        for dset in self.dsets:\n            max_val_arr.append(dset.compute_max_val())\n        return np.max(max_val_arr)\n\n    def set_max_val(self, max_val, datasplit_type):\n        if datasplit_type == DataSplitType.Train:\n            assert max_val is None\n            max_val = self.compute_max_val()\n        for dset in self.dsets:\n            dset.set_max_val(max_val, datasplit_type)\n\n    def upperclip_data(self):\n        for dset in self.dsets:\n            dset.upperclip_data()\n\n    def get_max_val(self):\n        return self.dsets[0].get_max_val()\n\n    def get_img_sz(self):\n        return self.dsets[0].get_img_sz()\n\n    def set_img_sz(self, image_size, grid_size):\n        for dset in self.dsets:\n            dset.set_img_sz(image_size, grid_size)\n\n    def compute_mean_std(self):\n        cur_mean = {\"target\": 0, \"input\": 0}\n        cur_std = {\"target\": 0, \"input\": 0}\n        for dset in self.dsets:\n            mean, std = dset.compute_mean_std()\n            cur_mean[\"target\"] += mean[\"target\"]\n            cur_mean[\"input\"] += mean[\"input\"]\n\n            cur_std[\"target\"] += std[\"target\"]\n            cur_std[\"input\"] += std[\"input\"]\n\n        cur_mean[\"target\"] /= len(self.dsets)\n        cur_mean[\"input\"] /= len(self.dsets)\n        cur_std[\"target\"] /= len(self.dsets)\n        cur_std[\"input\"] /= len(self.dsets)\n        return cur_mean, cur_std\n\n    def compute_individual_mean_std(self):\n        cum_mean = 0\n        cum_std = 0\n        for dset in self.dsets:\n            mean, std = dset.compute_individual_mean_std()\n            cum_mean += mean\n            cum_std += std\n        return cum_mean / len(self.dsets), cum_std / len(self.dsets)\n\n    def get_num_frames(self):\n        return len(self.dsets)\n\n    def reduce_data(\n        self, t_list=None, h_start=None, h_end=None, w_start=None, w_end=None\n    ):\n        assert h_start is None\n        assert h_end is None\n        assert w_start is None\n        assert w_end is None\n        self.dsets = [self.dsets[t] for t in t_list]\n        print(\n            f\"[{self.__class__.__name__}] Data reduced. New data count: {len(self.dsets)}\"\n        )\n\n    def __len__(self):\n        out = 0\n        for dset in self.dsets:\n            out += len(dset)\n        return out\n\n    def __getitem__(self, idx):\n        cum_len = 0\n        for dset in self.dsets:\n            cum_len += len(dset)\n            if idx &lt; cum_len:\n                rel_idx = idx - (cum_len - len(dset))\n                return dset[rel_idx]\n\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/multifile_dataset/#careamics.lvae_training.dataset.multifile_dataset.TwoChannelData","title":"<code>TwoChannelData</code>","text":"<p>               Bases: <code>Sequence</code></p> <p>each element in data_arr should be a NHW array</p> Source code in <code>src/careamics/lvae_training/dataset/multifile_dataset.py</code> <pre><code>class TwoChannelData(Sequence):\n    \"\"\"\n    each element in data_arr should be a N*H*W array\n    \"\"\"\n\n    def __init__(self, data_arr1, data_arr2, paths_data1=None, paths_data2=None):\n        assert len(data_arr1) == len(data_arr2)\n        self.paths1 = paths_data1\n        self.paths2 = paths_data2\n\n        self._data = []\n        for i in range(len(data_arr1)):\n            assert data_arr1[i].shape == data_arr2[i].shape\n            assert (\n                len(data_arr1[i].shape) == 3\n            ), f\"Each element in data arrays should be a N*H*W, but {data_arr1[i].shape}\"\n            self._data.append(\n                np.concatenate(\n                    [data_arr1[i][..., None], data_arr2[i][..., None]], axis=-1\n                )\n            )\n\n    def __len__(self):\n        n = 0\n        for x in self._data:\n            n += x.shape[0]\n        return n\n\n    def __getitem__(self, idx):\n        n = 0\n        for dataidx, x in enumerate(self._data):\n            if idx &lt; n + x.shape[0]:\n                if self.paths1 is None:\n                    return x[idx - n], None\n                else:\n                    return x[idx - n], (self.paths1[dataidx], self.paths2[dataidx])\n            n += x.shape[0]\n        raise IndexError(\"Index out of range\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/types/","title":"types","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/","title":"data_utils","text":"<p>Utility functions needed by dataloader &amp; co.</p>"},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/#careamics.lvae_training.dataset.utils.data_utils.adjust_for_imbalance_in_fraction_value","title":"<code>adjust_for_imbalance_in_fraction_value(val, test, val_fraction, test_fraction, total_size)</code>","text":"<p>here, val and test are divided almost equally. Here, we need to take into account their respective fractions and pick elements rendomly from one array and put in the other array.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/data_utils.py</code> <pre><code>def adjust_for_imbalance_in_fraction_value(\n    val: List[int],\n    test: List[int],\n    val_fraction: float,\n    test_fraction: float,\n    total_size: int,\n):\n    \"\"\"\n    here, val and test are divided almost equally. Here, we need to take into account their respective fractions\n    and pick elements rendomly from one array and put in the other array.\n    \"\"\"\n    if val_fraction == 0:\n        test += val\n        val = []\n    elif test_fraction == 0:\n        val += test\n        test = []\n    else:\n        diff_fraction = test_fraction - val_fraction\n        if diff_fraction &gt; 0:\n            imb_count = int(diff_fraction * total_size / 2)\n            val = list(np.random.RandomState(seed=955).permutation(val))\n            test += val[:imb_count]\n            val = val[imb_count:]\n        elif diff_fraction &lt; 0:\n            imb_count = int(-1 * diff_fraction * total_size / 2)\n            test = list(np.random.RandomState(seed=955).permutation(test))\n            val += test[:imb_count]\n            test = test[imb_count:]\n    return val, test\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/data_utils/#careamics.lvae_training.dataset.utils.data_utils.load_tiff","title":"<code>load_tiff(path)</code>","text":"<p>Returns a 4d numpy array: num_imgshw*num_channels</p> Source code in <code>src/careamics/lvae_training/dataset/utils/data_utils.py</code> <pre><code>def load_tiff(path):\n    \"\"\"\n    Returns a 4d numpy array: num_imgs*h*w*num_channels\n    \"\"\"\n    data = imread(path, plugin=\"tifffile\")\n    return data\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/","title":"empty_patch_fetcher","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/#careamics.lvae_training.dataset.utils.empty_patch_fetcher.EmptyPatchFetcher","title":"<code>EmptyPatchFetcher</code>","text":"<p>The idea is to fetch empty patches so that real content can be replaced with this.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/empty_patch_fetcher.py</code> <pre><code>class EmptyPatchFetcher:\n    \"\"\"\n    The idea is to fetch empty patches so that real content can be replaced with this.\n    \"\"\"\n\n    def __init__(self, idx_manager, patch_size, data_frames, max_val_threshold=None):\n        self._frames = data_frames\n        self._idx_manager = idx_manager\n        self._max_val_threshold = max_val_threshold\n        self._idx_list = []\n        self._patch_size = patch_size\n        self._grid_size = 1\n        self.set_empty_idx()\n\n        print(f\"[{self.__class__.__name__}] MaxVal:{self._max_val_threshold}\")\n\n    def compute_max(self, window):\n        \"\"\"\n        Rolling compute.\n        \"\"\"\n        N, H, W = self._frames.shape\n        randnum = -954321\n        assert self._grid_size == 1\n        max_data = np.zeros((N, H - window, W - window)) * randnum\n\n        for h in tqdm(range(H - window)):\n            for w in range(W - window):\n                max_data[:, h, w] = self._frames[:, h : h + window, w : w + window].max(\n                    axis=(1, 2)\n                )\n\n        assert (max_data != 954321).any()\n        return max_data\n\n    def set_empty_idx(self):\n        max_data = self.compute_max(self._patch_size)\n        empty_loc = np.where(\n            np.logical_and(max_data &gt;= 0, max_data &lt; self._max_val_threshold)\n        )\n        # print(max_data.shape, len(empty_loc))\n        self._idx_list = []\n        for idx in range(len(empty_loc[0])):\n            n_idx = empty_loc[0][idx]\n            h_start = empty_loc[1][idx]\n            w_start = empty_loc[2][idx]\n            # print(n_idx,h_start,w_start)\n            # channel_idx = self._idx_manager.get_location_from_dataset_idx(0)[-1]\n            loc = (n_idx, h_start, w_start, 0)\n            idx = self._idx_manager.get_dataset_idx_from_location(loc)\n            t, h, w, _ = self._idx_manager.get_location_from_dataset_idx(idx)\n            assert h == h_start, f\"{h} != {h_start}\"\n            assert w == w_start, f\"{w} != {w_start}\"\n            assert t == n_idx, f\"{t} != {n_idx}\"\n            self._idx_list.append(idx)\n\n        self._idx_list = np.array(self._idx_list)\n\n        assert len(self._idx_list) &gt; 0\n\n    def sample(self):\n        return (np.random.choice(self._idx_list), self._grid_size)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/empty_patch_fetcher/#careamics.lvae_training.dataset.utils.empty_patch_fetcher.EmptyPatchFetcher.compute_max","title":"<code>compute_max(window)</code>","text":"<p>Rolling compute.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/empty_patch_fetcher.py</code> <pre><code>def compute_max(self, window):\n    \"\"\"\n    Rolling compute.\n    \"\"\"\n    N, H, W = self._frames.shape\n    randnum = -954321\n    assert self._grid_size == 1\n    max_data = np.zeros((N, H - window, W - window)) * randnum\n\n    for h in tqdm(range(H - window)):\n        for w in range(W - window):\n            max_data[:, h, w] = self._frames[:, h : h + window, w : w + window].max(\n                axis=(1, 2)\n            )\n\n    assert (max_data != 954321).any()\n    return max_data\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/","title":"index_manager","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager","title":"<code>GridIndexManager</code>  <code>dataclass</code>","text":"Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>@dataclass\nclass GridIndexManager:\n    data_shape: tuple\n    grid_shape: tuple\n    patch_shape: tuple\n    tiling_mode: TilingMode\n\n    # Patch is centered on index in the grid, grid size not used in training,\n    # used only during val / test, grid size controls the overlap of the patches\n    # in training you only get random patches every time\n    # For borders - just cropped the data, so it perfectly divisible\n\n    def __post_init__(self):\n        assert len(self.data_shape) == len(\n            self.grid_shape\n        ), f\"Data shape:{self.data_shape} and grid size:{self.grid_shape} must have the same dimension\"\n        assert len(self.data_shape) == len(\n            self.patch_shape\n        ), f\"Data shape:{self.data_shape} and patch shape:{self.patch_shape} must have the same dimension\"\n        innerpad = np.array(self.patch_shape) - np.array(self.grid_shape)\n        for dim, pad in enumerate(innerpad):\n            if pad &lt; 0:\n                raise ValueError(\n                    f\"Patch shape:{self.patch_shape} must be greater than or equal to grid shape:{self.grid_shape} in dimension {dim}\"\n                )\n            if pad % 2 != 0:\n                raise ValueError(\n                    f\"Patch shape:{self.patch_shape} must have even padding in dimension {dim}\"\n                )\n\n    def patch_offset(self):\n        return (np.array(self.patch_shape) - np.array(self.grid_shape)) // 2\n\n    def get_individual_dim_grid_count(self, dim: int):\n        \"\"\"\n        Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return self.data_shape[dim]\n        elif self.tiling_mode == TilingMode.PadBoundary:\n            return int(np.ceil(self.data_shape[dim] / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(\n                np.ceil((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n            )\n        else:\n            excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n            return int(\n                np.floor((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n            )\n\n    def total_grid_count(self):\n        \"\"\"\n        Returns the total number of grids in the dataset.\n        \"\"\"\n        return self.grid_count(0) * self.get_individual_dim_grid_count(0)\n\n    def grid_count(self, dim: int):\n        \"\"\"\n        Returns the total number of grids for one value in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        if dim == len(self.data_shape) - 1:\n            return 1\n\n        return self.get_individual_dim_grid_count(dim + 1) * self.grid_count(dim + 1)\n\n    def get_grid_index(self, dim: int, coordinate: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        assert (\n            coordinate &lt; self.data_shape[dim]\n        ), f\"Coordinate {coordinate} is out of bounds for data shape {self.data_shape}\"\n\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return coordinate\n        elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n            return np.floor(coordinate / self.grid_shape[dim])\n        elif self.tiling_mode == TilingMode.TrimBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            if coordinate + self.grid_shape[dim] + excess_size == self.data_shape[dim]:\n                return self.get_individual_dim_grid_count(dim) - 1\n            else:\n                # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n                return max(\n                    0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n                )\n\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def dataset_idx_from_grid_idx(self, grid_idx: tuple):\n        \"\"\"\n        Returns the index of the grid in the dataset.\n        \"\"\"\n        assert len(grid_idx) == len(\n            self.data_shape\n        ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {self.data_shape}\"\n        index = 0\n        for dim in range(len(grid_idx)):\n            index += grid_idx[dim] * self.grid_count(dim)\n        return index\n\n    def get_patch_location_from_dataset_idx(self, dataset_idx: int):\n        \"\"\"\n        Returns the patch location of the grid in the dataset.\n        \"\"\"\n        grid_location = self.get_location_from_dataset_idx(dataset_idx)\n        offset = self.patch_offset()\n        return tuple(np.array(grid_location) - np.array(offset))\n\n    def get_dataset_idx_from_grid_location(self, location: tuple):\n        assert len(location) == len(\n            self.data_shape\n        ), f\"Location {location} must have the same dimension as data shape {self.data_shape}\"\n        grid_idx = [\n            self.get_grid_index(dim, location[dim]) for dim in range(len(location))\n        ]\n        return self.dataset_idx_from_grid_idx(tuple(grid_idx))\n\n    def get_gridstart_location_from_dim_index(self, dim: int, dim_index: int):\n        \"\"\"\n        Returns the grid-start coordinate of the grid in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        # assert dim_index &lt; self.get_individual_dim_grid_count(\n        #     dim\n        # ), f\"Dimension index {dim_index} is out of bounds for data shape {self.data_shape}\"\n        # TODO comented out this shit cuz I have no interest to dig why it's failing at this point !\n        if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n            return dim_index\n        elif self.tiling_mode == TilingMode.PadBoundary:\n            return dim_index * self.grid_shape[dim]\n        elif self.tiling_mode == TilingMode.TrimBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            return dim_index * self.grid_shape[dim] + excess_size\n        elif self.tiling_mode == TilingMode.ShiftBoundary:\n            excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n            if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n                return dim_index * self.grid_shape[dim] + excess_size\n            else:\n                # on boundary. grid should be placed such that the patch covers the entire data.\n                return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n        else:\n            raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n\n    def get_location_from_dataset_idx(self, dataset_idx: int):\n        \"\"\"\n        Returns the start location of the grid in the dataset.\n        \"\"\"\n        grid_idx = []\n        for dim in range(len(self.data_shape)):\n            grid_idx.append(dataset_idx // self.grid_count(dim))\n            dataset_idx = dataset_idx % self.grid_count(dim)\n        location = [\n            self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n            for dim in range(len(self.data_shape))\n        ]\n        return tuple(location)\n\n    def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n        \"\"\"\n        Returns True if the grid is on the boundary in the specified dimension.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n        if dim &gt; 0:\n            dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n        dim_index = dataset_idx // self.grid_count(dim)\n        if only_end:\n            return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n        return (\n            dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n        )\n\n    def next_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx + self.grid_count(dim)\n        if new_idx &gt;= self.total_grid_count():\n            return None\n        return new_idx\n\n    def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n        \"\"\"\n        Returns the index of the grid in the specified dimension in the specified direction.\n        \"\"\"\n        assert dim &lt; len(\n            self.data_shape\n        ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n        assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n        new_idx = dataset_idx - self.grid_count(dim)\n        if new_idx &lt; 0:\n            return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.dataset_idx_from_grid_idx","title":"<code>dataset_idx_from_grid_idx(grid_idx)</code>","text":"<p>Returns the index of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def dataset_idx_from_grid_idx(self, grid_idx: tuple):\n    \"\"\"\n    Returns the index of the grid in the dataset.\n    \"\"\"\n    assert len(grid_idx) == len(\n        self.data_shape\n    ), f\"Dimension indices {grid_idx} must have the same dimension as data shape {self.data_shape}\"\n    index = 0\n    for dim in range(len(grid_idx)):\n        index += grid_idx[dim] * self.grid_count(dim)\n    return index\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_grid_index","title":"<code>get_grid_index(dim, coordinate)</code>","text":"<p>Returns the index of the grid in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_grid_index(self, dim: int, coordinate: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    assert (\n        coordinate &lt; self.data_shape[dim]\n    ), f\"Coordinate {coordinate} is out of bounds for data shape {self.data_shape}\"\n\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return coordinate\n    elif self.tiling_mode == TilingMode.PadBoundary:  # self.trim_boundary is False:\n        return np.floor(coordinate / self.grid_shape[dim])\n    elif self.tiling_mode == TilingMode.TrimBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n        return max(0, np.floor((coordinate - excess_size) / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        if coordinate + self.grid_shape[dim] + excess_size == self.data_shape[dim]:\n            return self.get_individual_dim_grid_count(dim) - 1\n        else:\n            # can be &lt;0 if coordinate is in [0,grid_shape[dim]]\n            return max(\n                0, np.floor((coordinate - excess_size) / self.grid_shape[dim])\n            )\n\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_gridstart_location_from_dim_index","title":"<code>get_gridstart_location_from_dim_index(dim, dim_index)</code>","text":"<p>Returns the grid-start coordinate of the grid in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_gridstart_location_from_dim_index(self, dim: int, dim_index: int):\n    \"\"\"\n    Returns the grid-start coordinate of the grid in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    # assert dim_index &lt; self.get_individual_dim_grid_count(\n    #     dim\n    # ), f\"Dimension index {dim_index} is out of bounds for data shape {self.data_shape}\"\n    # TODO comented out this shit cuz I have no interest to dig why it's failing at this point !\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return dim_index\n    elif self.tiling_mode == TilingMode.PadBoundary:\n        return dim_index * self.grid_shape[dim]\n    elif self.tiling_mode == TilingMode.TrimBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        return dim_index * self.grid_shape[dim] + excess_size\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = (self.patch_shape[dim] - self.grid_shape[dim]) // 2\n        if dim_index &lt; self.get_individual_dim_grid_count(dim) - 1:\n            return dim_index * self.grid_shape[dim] + excess_size\n        else:\n            # on boundary. grid should be placed such that the patch covers the entire data.\n            return self.data_shape[dim] - self.grid_shape[dim] - excess_size\n    else:\n        raise ValueError(f\"Unsupported tiling mode {self.tiling_mode}\")\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_individual_dim_grid_count","title":"<code>get_individual_dim_grid_count(dim)</code>","text":"<p>Returns the number of the grid in the specified dimension, ignoring all other dimensions.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_individual_dim_grid_count(self, dim: int):\n    \"\"\"\n    Returns the number of the grid in the specified dimension, ignoring all other dimensions.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n    if self.grid_shape[dim] == 1 and self.patch_shape[dim] == 1:\n        return self.data_shape[dim]\n    elif self.tiling_mode == TilingMode.PadBoundary:\n        return int(np.ceil(self.data_shape[dim] / self.grid_shape[dim]))\n    elif self.tiling_mode == TilingMode.ShiftBoundary:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(\n            np.ceil((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n        )\n    else:\n        excess_size = self.patch_shape[dim] - self.grid_shape[dim]\n        return int(\n            np.floor((self.data_shape[dim] - excess_size) / self.grid_shape[dim])\n        )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_location_from_dataset_idx","title":"<code>get_location_from_dataset_idx(dataset_idx)</code>","text":"<p>Returns the start location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_location_from_dataset_idx(self, dataset_idx: int):\n    \"\"\"\n    Returns the start location of the grid in the dataset.\n    \"\"\"\n    grid_idx = []\n    for dim in range(len(self.data_shape)):\n        grid_idx.append(dataset_idx // self.grid_count(dim))\n        dataset_idx = dataset_idx % self.grid_count(dim)\n    location = [\n        self.get_gridstart_location_from_dim_index(dim, grid_idx[dim])\n        for dim in range(len(self.data_shape))\n    ]\n    return tuple(location)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.get_patch_location_from_dataset_idx","title":"<code>get_patch_location_from_dataset_idx(dataset_idx)</code>","text":"<p>Returns the patch location of the grid in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def get_patch_location_from_dataset_idx(self, dataset_idx: int):\n    \"\"\"\n    Returns the patch location of the grid in the dataset.\n    \"\"\"\n    grid_location = self.get_location_from_dataset_idx(dataset_idx)\n    offset = self.patch_offset()\n    return tuple(np.array(grid_location) - np.array(offset))\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.grid_count","title":"<code>grid_count(dim)</code>","text":"<p>Returns the total number of grids for one value in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def grid_count(self, dim: int):\n    \"\"\"\n    Returns the total number of grids for one value in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    if dim == len(self.data_shape) - 1:\n        return 1\n\n    return self.get_individual_dim_grid_count(dim + 1) * self.grid_count(dim + 1)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.next_grid_along_dim","title":"<code>next_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def next_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx + self.grid_count(dim)\n    if new_idx &gt;= self.total_grid_count():\n        return None\n    return new_idx\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.on_boundary","title":"<code>on_boundary(dataset_idx, dim, only_end=False)</code>","text":"<p>Returns True if the grid is on the boundary in the specified dimension.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def on_boundary(self, dataset_idx: int, dim: int, only_end: bool = False):\n    \"\"\"\n    Returns True if the grid is on the boundary in the specified dimension.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n\n    if dim &gt; 0:\n        dataset_idx = dataset_idx % self.grid_count(dim - 1)\n\n    dim_index = dataset_idx // self.grid_count(dim)\n    if only_end:\n        return dim_index == self.get_individual_dim_grid_count(dim) - 1\n\n    return (\n        dim_index == 0 or dim_index == self.get_individual_dim_grid_count(dim) - 1\n    )\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.prev_grid_along_dim","title":"<code>prev_grid_along_dim(dataset_idx, dim)</code>","text":"<p>Returns the index of the grid in the specified dimension in the specified direction.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def prev_grid_along_dim(self, dataset_idx: int, dim: int):\n    \"\"\"\n    Returns the index of the grid in the specified dimension in the specified direction.\n    \"\"\"\n    assert dim &lt; len(\n        self.data_shape\n    ), f\"Dimension {dim} is out of bounds for data shape {self.data_shape}\"\n    assert dim &gt;= 0, \"Dimension must be greater than or equal to 0\"\n    new_idx = dataset_idx - self.grid_count(dim)\n    if new_idx &lt; 0:\n        return None\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_manager/#careamics.lvae_training.dataset.utils.index_manager.GridIndexManager.total_grid_count","title":"<code>total_grid_count()</code>","text":"<p>Returns the total number of grids in the dataset.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_manager.py</code> <pre><code>def total_grid_count(self):\n    \"\"\"\n    Returns the total number of grids in the dataset.\n    \"\"\"\n    return self.grid_count(0) * self.get_individual_dim_grid_count(0)\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/","title":"index_switcher","text":""},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher","title":"<code>IndexSwitcher</code>","text":"<p>The idea is to switch from valid indices for target to invalid indices for target. If index in invalid for the target, then we return all zero vector as target. This combines both logic: 1. Using less amount of total data. 2. Using less amount of target data but using full data.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>class IndexSwitcher:\n    \"\"\"\n    The idea is to switch from valid indices for target to invalid indices for target.\n    If index in invalid for the target, then we return all zero vector as target.\n    This combines both logic:\n    1. Using less amount of total data.\n    2. Using less amount of target data but using full data.\n    \"\"\"\n\n    def __init__(self, idx_manager, data_config, patch_size) -&gt; None:\n        self.idx_manager = idx_manager\n        self._data_shape = self.idx_manager.get_data_shape()\n        self._training_validtarget_fraction = data_config.get(\n            \"training_validtarget_fraction\", 1.0\n        )\n        self._validtarget_ceilT = int(\n            np.ceil(self._data_shape[0] * self._training_validtarget_fraction)\n        )\n        self._patch_size = patch_size\n        assert (\n            data_config.deterministic_grid is True\n        ), \"This only works when the dataset has deterministic grid. Needed randomness comes from this class.\"\n        assert (\n            \"grid_size\" in data_config and data_config.grid_size == 1\n        ), \"We need a one to one mapping between index and h, w, t\"\n\n        self._h_validmax, self._w_validmax = self.get_reduced_frame_size(\n            self._data_shape[:3], self._training_validtarget_fraction\n        )\n        if self._h_validmax &lt; self._patch_size or self._w_validmax &lt; self._patch_size:\n            print(\n                \"WARNING: The valid target size is smaller than the patch size. This will result in all zero target. so, we are ignoring this frame for target.\"\n            )\n            self._h_validmax = 0\n            self._w_validmax = 0\n\n        print(\n            f\"[{self.__class__.__name__}] Target Indices: [0,{self._validtarget_ceilT - 1}]. Index={self._validtarget_ceilT - 1} has shape [:{self._h_validmax},:{self._w_validmax}].  Available data: {self._data_shape[0]}\"\n        )\n\n    def get_valid_target_index(self):\n        \"\"\"\n        Returns an index which corresponds to a frame which is expected to have a target.\n        \"\"\"\n        _, h, w, _ = self._data_shape\n        framepixelcount = h * w\n        targetpixels = np.array(\n            [framepixelcount] * (self._validtarget_ceilT - 1)\n            + [self._h_validmax * self._w_validmax]\n        )\n        targetpixels = targetpixels / np.sum(targetpixels)\n        t = np.random.choice(self._validtarget_ceilT, p=targetpixels)\n        # t = np.random.randint(0, self._validtarget_ceilT) if self._validtarget_ceilT &gt;= 1 else 0\n        h, w = self.get_valid_target_hw(t)\n        index = self.idx_manager.idx_from_hwt(h, w, t)\n        # print('Valid', index, h,w,t)\n        return index\n\n    def get_invalid_target_index(self):\n        # if self._validtarget_ceilT == 0:\n        # TODO: There may not be enough data for this to work. The better way is to skip using 0 for invalid target.\n        # t = np.random.randint(1, self._data_shape[0])\n        # elif self._validtarget_ceilT &lt; self._data_shape[0]:\n        #     t = np.random.randint(self._validtarget_ceilT, self._data_shape[0])\n        # else:\n        #     t = self._validtarget_ceilT - 1\n        # 5\n        # 1.2 =&gt; 2\n        total_t, h, w, _ = self._data_shape\n        framepixelcount = h * w\n        available_h = h - self._h_validmax\n        if available_h &lt; self._patch_size:\n            available_h = 0\n        available_w = w - self._w_validmax\n        if available_w &lt; self._patch_size:\n            available_w = 0\n\n        targetpixels = np.array(\n            [available_h * available_w]\n            + [framepixelcount] * (total_t - self._validtarget_ceilT)\n        )\n        t_probab = targetpixels / np.sum(targetpixels)\n        t = np.random.choice(\n            np.arange(self._validtarget_ceilT - 1, total_t), p=t_probab\n        )\n\n        h, w = self.get_invalid_target_hw(t)\n        index = self.idx_manager.idx_from_hwt(h, w, t)\n        # print('Invalid', index, h,w,t)\n        return index\n\n    def get_valid_target_hw(self, t):\n        \"\"\"\n        This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target.\n        This is only valid for single frame setup.\n        \"\"\"\n        if t == self._validtarget_ceilT - 1:\n            h = np.random.randint(0, self._h_validmax - self._patch_size)\n            w = np.random.randint(0, self._w_validmax - self._patch_size)\n        else:\n            h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n            w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n        return h, w\n\n    def get_invalid_target_hw(self, t):\n        \"\"\"\n        This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target.\n        This is only valid for single frame setup.\n        \"\"\"\n        if t == self._validtarget_ceilT - 1:\n            h = np.random.randint(\n                self._h_validmax, self._data_shape[1] - self._patch_size\n            )\n            w = np.random.randint(\n                self._w_validmax, self._data_shape[2] - self._patch_size\n            )\n        else:\n            h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n            w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n        return h, w\n\n    def _get_tidx(self, index):\n        if isinstance(index, int) or isinstance(index, np.int64):\n            idx = index\n        else:\n            idx = index[0]\n        return self.idx_manager.get_t(idx)\n\n    def index_should_have_target(self, index):\n        tidx = self._get_tidx(index)\n        if tidx &lt; self._validtarget_ceilT - 1:\n            return True\n        elif tidx &gt; self._validtarget_ceilT - 1:\n            return False\n        else:\n            h, w, _ = self.idx_manager.hwt_from_idx(index)\n            return (\n                h + self._patch_size &lt; self._h_validmax\n                and w + self._patch_size &lt; self._w_validmax\n            )\n\n    @staticmethod\n    def get_reduced_frame_size(data_shape_nhw, fraction):\n        n, h, w = data_shape_nhw\n\n        framepixelcount = h * w\n        targetpixelcount = int(n * framepixelcount * fraction)\n\n        # We are currently supporting this only when there is just one frame.\n        # if np.ceil(pixelcount / framepixelcount) &gt; 1:\n        #     return None, None\n\n        lastframepixelcount = targetpixelcount % framepixelcount\n        assert data_shape_nhw[1] == data_shape_nhw[2]\n        if lastframepixelcount &gt; 0:\n            new_size = int(np.sqrt(lastframepixelcount))\n            return new_size, new_size\n        else:\n            assert (\n                targetpixelcount / framepixelcount &gt;= 1\n            ), \"This is not possible in euclidean space :D (so this is a bug)\"\n            return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_invalid_target_hw","title":"<code>get_invalid_target_hw(t)</code>","text":"<p>This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target. This is only valid for single frame setup.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_invalid_target_hw(self, t):\n    \"\"\"\n    This is the opposite of get_valid_target_hw. It returns a h,w which is not valid for target.\n    This is only valid for single frame setup.\n    \"\"\"\n    if t == self._validtarget_ceilT - 1:\n        h = np.random.randint(\n            self._h_validmax, self._data_shape[1] - self._patch_size\n        )\n        w = np.random.randint(\n            self._w_validmax, self._data_shape[2] - self._patch_size\n        )\n    else:\n        h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n        w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n    return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_valid_target_hw","title":"<code>get_valid_target_hw(t)</code>","text":"<p>This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target. This is only valid for single frame setup.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_valid_target_hw(self, t):\n    \"\"\"\n    This is the opposite of get_invalid_target_hw. It returns a h,w which is valid for target.\n    This is only valid for single frame setup.\n    \"\"\"\n    if t == self._validtarget_ceilT - 1:\n        h = np.random.randint(0, self._h_validmax - self._patch_size)\n        w = np.random.randint(0, self._w_validmax - self._patch_size)\n    else:\n        h = np.random.randint(0, self._data_shape[1] - self._patch_size)\n        w = np.random.randint(0, self._data_shape[2] - self._patch_size)\n    return h, w\n</code></pre>"},{"location":"reference/careamics/lvae_training/dataset/utils/index_switcher/#careamics.lvae_training.dataset.utils.index_switcher.IndexSwitcher.get_valid_target_index","title":"<code>get_valid_target_index()</code>","text":"<p>Returns an index which corresponds to a frame which is expected to have a target.</p> Source code in <code>src/careamics/lvae_training/dataset/utils/index_switcher.py</code> <pre><code>def get_valid_target_index(self):\n    \"\"\"\n    Returns an index which corresponds to a frame which is expected to have a target.\n    \"\"\"\n    _, h, w, _ = self._data_shape\n    framepixelcount = h * w\n    targetpixels = np.array(\n        [framepixelcount] * (self._validtarget_ceilT - 1)\n        + [self._h_validmax * self._w_validmax]\n    )\n    targetpixels = targetpixels / np.sum(targetpixels)\n    t = np.random.choice(self._validtarget_ceilT, p=targetpixels)\n    # t = np.random.randint(0, self._validtarget_ceilT) if self._validtarget_ceilT &gt;= 1 else 0\n    h, w = self.get_valid_target_hw(t)\n    index = self.idx_manager.idx_from_hwt(h, w, t)\n    # print('Valid', index, h,w,t)\n    return index\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/","title":"bmz_io","text":"<p>Function to export to the BioImage Model Zoo format.</p>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io._export_state_dict","title":"<code>_export_state_dict(model, path)</code>","text":"<p>Export the model state dictionary to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CAREamicsKiln</code> <p>CAREamics model to export.</p> required <code>path</code> <code>Union[Path, str]</code> <p>Path to the file where to save the model state dictionary.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved model state dictionary.</p> Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def _export_state_dict(\n    model: Union[FCNModule, VAEModule], path: Union[Path, str]\n) -&gt; Path:\n    \"\"\"\n    Export the model state dictionary to a file.\n\n    Parameters\n    ----------\n    model : CAREamicsKiln\n        CAREamics model to export.\n    path : Union[Path, str]\n        Path to the file where to save the model state dictionary.\n\n    Returns\n    -------\n    Path\n        Path to the saved model state dictionary.\n    \"\"\"\n    path = Path(path)\n\n    # make sure it has the correct suffix\n    if path.suffix not in \".pth\":\n        path = path.with_suffix(\".pth\")\n\n    # save model state dictionary\n    # we save through the torch model itself to avoid the initial \"model.\" in the\n    # layers naming, which is incompatible with the way the BMZ load torch state dicts\n    save(model.model.state_dict(), path)\n\n    return path\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io._load_state_dict","title":"<code>_load_state_dict(model, path)</code>","text":"<p>Load a model from a state dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CAREamicsKiln</code> <p>CAREamics model to be updated with the weights.</p> required <code>path</code> <code>Union[Path, str]</code> <p>Path to the model state dictionary.</p> required Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def _load_state_dict(\n    model: Union[FCNModule, VAEModule], path: Union[Path, str]\n) -&gt; None:\n    \"\"\"\n    Load a model from a state dictionary.\n\n    Parameters\n    ----------\n    model : CAREamicsKiln\n        CAREamics model to be updated with the weights.\n    path : Union[Path, str]\n        Path to the model state dictionary.\n    \"\"\"\n    path = Path(path)\n\n    # load model state dictionary\n    # same as in _export_state_dict, we load through the torch model to be compatible\n    # witht bioimageio.core expectations for a torch state dict\n    state_dict = load(path)\n    model.model.load_state_dict(state_dict)\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io.export_to_bmz","title":"<code>export_to_bmz(model, config, path_to_archive, model_name, general_description, data_description, authors, input_array, output_array, covers=None, channel_names=None, model_version='0.1.0')</code>","text":"<p>Export the model to BioImage Model Zoo format.</p> <p>Arrays are expected to be SC(Z)YX with singleton dimensions allowed for S and C.</p> <p><code>model_name</code> should consist of letters, numbers, dashes, underscores and parentheses only.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CAREamicsModule</code> <p>CAREamics model to export.</p> required <code>config</code> <code>Configuration</code> <p>Model configuration.</p> required <code>path_to_archive</code> <code>Union[Path, str]</code> <p>Path to the output file.</p> required <code>model_name</code> <code>str</code> <p>Model name.</p> required <code>general_description</code> <code>str</code> <p>General description of the model.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>authors</code> <code>list[dict]</code> <p>Authors of the model.</p> required <code>input_array</code> <code>ndarray</code> <p>Input array, should not have been normalized.</p> required <code>output_array</code> <code>ndarray</code> <p>Output array, should have been denormalized.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to the cover images.</p> <code>None</code> <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Model version.</p> <code>\"0.1.0\"</code> Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def export_to_bmz(\n    model: Union[FCNModule, VAEModule],\n    config: Configuration,\n    path_to_archive: Union[Path, str],\n    model_name: str,\n    general_description: str,\n    data_description: str,\n    authors: list[dict],\n    input_array: np.ndarray,\n    output_array: np.ndarray,\n    covers: Optional[list[Union[Path, str]]] = None,\n    channel_names: Optional[list[str]] = None,\n    model_version: str = \"0.1.0\",\n) -&gt; None:\n    \"\"\"Export the model to BioImage Model Zoo format.\n\n    Arrays are expected to be SC(Z)YX with singleton dimensions allowed for S and C.\n\n    `model_name` should consist of letters, numbers, dashes, underscores and parentheses\n    only.\n\n    Parameters\n    ----------\n    model : CAREamicsModule\n        CAREamics model to export.\n    config : Configuration\n        Model configuration.\n    path_to_archive : Union[Path, str]\n        Path to the output file.\n    model_name : str\n        Model name.\n    general_description : str\n        General description of the model.\n    data_description : str\n        Description of the data the model was trained on.\n    authors : list[dict]\n        Authors of the model.\n    input_array : np.ndarray\n        Input array, should not have been normalized.\n    output_array : np.ndarray\n        Output array, should have been denormalized.\n    covers : list of pathlib.Path or str, default=None\n        Paths to the cover images.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n    model_version : str, default=\"0.1.0\"\n        Model version.\n    \"\"\"\n    path_to_archive = Path(path_to_archive)\n\n    if path_to_archive.suffix != \".zip\":\n        raise ValueError(\n            f\"Path to archive must point to a zip file, got {path_to_archive}.\"\n        )\n\n    if not path_to_archive.parent.exists():\n        path_to_archive.parent.mkdir(parents=True, exist_ok=True)\n\n    # versions\n    careamics_version = get_careamics_version()\n\n    # save files in temporary folder\n    with tempfile.TemporaryDirectory() as tmpdirname:\n        temp_path = Path(tmpdirname)\n\n        # create environment file\n        # TODO move in bioimage module\n        env_path = temp_path / \"environment.yml\"\n        env_path.write_text(create_env_text(PYTORCH_VERSION, TORCHVISION_VERSION))\n\n        # export input and ouputs\n        inputs = temp_path / \"inputs.npy\"\n        np.save(inputs, input_array)\n        outputs = temp_path / \"outputs.npy\"\n        np.save(outputs, output_array)\n\n        # export configuration\n        config_path = save_configuration(config, temp_path / \"careamics.yaml\")\n\n        # export model state dictionary\n        weight_path = _export_state_dict(model, temp_path / \"weights.pth\")\n\n        # export cover if necesary\n        if covers is None:\n            covers = [create_cover(temp_path, input_array, output_array)]\n\n        # create model description\n        model_description = create_model_description(\n            config=config,\n            name=model_name,\n            general_description=general_description,\n            data_description=data_description,\n            authors=authors,\n            inputs=inputs,\n            outputs=outputs,\n            weights_path=weight_path,\n            torch_version=PYTORCH_VERSION,\n            careamics_version=careamics_version,\n            config_path=config_path,\n            env_path=env_path,\n            covers=covers,\n            channel_names=channel_names,\n            model_version=model_version,\n        )\n\n        # test model description\n        test_kwargs = (\n            model_description.config.get(\"bioimageio\", {})\n            .get(\"test_kwargs\", {})\n            .get(\"pytorch_state_dict\", {})\n        )\n        summary: ValidationSummary = test_model(model_description, **test_kwargs)\n        if summary.status == \"failed\":\n            raise ValueError(f\"Model description test failed: {summary}\")\n\n        # save bmz model\n        save_bioimageio_package(model_description, output_path=path_to_archive)\n</code></pre>"},{"location":"reference/careamics/model_io/bmz_io/#careamics.model_io.bmz_io.load_from_bmz","title":"<code>load_from_bmz(path)</code>","text":"<p>Load a model from a BioImage Model Zoo archive.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>(Path, str or HttpUrl)</code> <p>Path to the BioImage Model Zoo archive. A Http URL must point to a downloadable location.</p> required <p>Returns:</p> Type Description <code>FCNModel or VAEModel</code> <p>The loaded CAREamics model.</p> <code>Configuration</code> <p>The loaded CAREamics configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path is not a zip file.</p> Source code in <code>src/careamics/model_io/bmz_io.py</code> <pre><code>def load_from_bmz(\n    path: Union[Path, str, HttpUrl]\n) -&gt; tuple[Union[FCNModule, VAEModule], Configuration]:\n    \"\"\"Load a model from a BioImage Model Zoo archive.\n\n    Parameters\n    ----------\n    path : Path, str or HttpUrl\n        Path to the BioImage Model Zoo archive. A Http URL must point to a downloadable\n        location.\n\n    Returns\n    -------\n    FCNModel or VAEModel\n        The loaded CAREamics model.\n    Configuration\n        The loaded CAREamics configuration.\n\n    Raises\n    ------\n    ValueError\n        If the path is not a zip file.\n    \"\"\"\n    # load description, this creates an unzipped folder next to the archive\n    model_desc = load_model_description(path)\n\n    # extract paths\n    weights_path, config_path = extract_model_path(model_desc)\n\n    # load configuration\n    config = load_configuration(config_path)\n\n    # create careamics lightning module\n    if config.algorithm_config.model.architecture == SupportedArchitecture.UNET:\n        model = FCNModule(algorithm_config=config.algorithm_config)\n    elif config.algorithm_config.model.architecture == SupportedArchitecture.LVAE:\n        model = VAEModule(algorithm_config=config.algorithm_config)\n    else:\n        raise ValueError(\n            f\"Unsupported architecture {config.algorithm_config.model.architecture}\"\n        )  # TODO ugly ?\n\n    # load model state dictionary\n    _load_state_dict(model, weights_path)\n\n    return model, config\n</code></pre>"},{"location":"reference/careamics/model_io/model_io_utils/","title":"model_io_utils","text":"<p>Utility functions to load pretrained models.</p>"},{"location":"reference/careamics/model_io/model_io_utils/#careamics.model_io.model_io_utils._load_checkpoint","title":"<code>_load_checkpoint(path)</code>","text":"<p>Load a model from a checkpoint and return both model and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path to the checkpoint.</p> required <p>Returns:</p> Type Description <code>tuple[CAREamicsKiln, Configuration]</code> <p>tuple of CAREamics model and its configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the checkpoint file does not contain hyper parameters (configuration).</p> Source code in <code>src/careamics/model_io/model_io_utils.py</code> <pre><code>def _load_checkpoint(\n    path: Union[Path, str]\n) -&gt; tuple[Union[FCNModule, VAEModule], Configuration]:\n    \"\"\"\n    Load a model from a checkpoint and return both model and configuration.\n\n    Parameters\n    ----------\n    path : Union[Path, str]\n        Path to the checkpoint.\n\n    Returns\n    -------\n    tuple[CAREamicsKiln, Configuration]\n        tuple of CAREamics model and its configuration.\n\n    Raises\n    ------\n    ValueError\n        If the checkpoint file does not contain hyper parameters (configuration).\n    \"\"\"\n    # load checkpoint\n    # here we might run into issues between devices\n    # see https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint: dict = torch.load(path, map_location=device)\n\n    # attempt to load configuration\n    try:\n        cfg_dict = checkpoint[\"hyper_parameters\"]\n    except KeyError as e:\n        raise ValueError(\n            f\"Invalid checkpoint file. No `hyper_parameters` found in the \"\n            f\"checkpoint: {checkpoint.keys()}\"\n        ) from e\n\n    if cfg_dict[\"algorithm_config\"][\"model\"][\"architecture\"] == \"UNet\":\n        model = FCNModule.load_from_checkpoint(path)\n    elif cfg_dict[\"algorithm_config\"][\"model\"][\"architecture\"] == \"LVAE\":\n        model = VAEModule.load_from_checkpoint(path)\n    else:\n        raise ValueError(\n            \"Invalid model architecture: \"\n            f\"{cfg_dict['algorithm_config']['model']['architecture']}\"\n        )\n\n    return model, Configuration(**cfg_dict)\n</code></pre>"},{"location":"reference/careamics/model_io/model_io_utils/#careamics.model_io.model_io_utils.load_pretrained","title":"<code>load_pretrained(path)</code>","text":"<p>Load a pretrained model from a checkpoint or a BioImage Model Zoo model.</p> <p>Expected formats are .ckpt or .zip files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path to the pretrained model.</p> required <p>Returns:</p> Type Description <code>tuple[CAREamicsKiln, Configuration]</code> <p>tuple of CAREamics model and its configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model format is not supported.</p> Source code in <code>src/careamics/model_io/model_io_utils.py</code> <pre><code>def load_pretrained(\n    path: Union[Path, str]\n) -&gt; tuple[Union[FCNModule, VAEModule], Configuration]:\n    \"\"\"\n    Load a pretrained model from a checkpoint or a BioImage Model Zoo model.\n\n    Expected formats are .ckpt or .zip files.\n\n    Parameters\n    ----------\n    path : Union[Path, str]\n        Path to the pretrained model.\n\n    Returns\n    -------\n    tuple[CAREamicsKiln, Configuration]\n        tuple of CAREamics model and its configuration.\n\n    Raises\n    ------\n    ValueError\n        If the model format is not supported.\n    \"\"\"\n    path = check_path_exists(path)\n\n    if path.suffix == \".ckpt\":\n        return _load_checkpoint(path)\n    elif path.suffix == \".zip\":\n        return load_from_bmz(path)\n    else:\n        raise ValueError(\n            f\"Invalid model format. Expected .ckpt or .zip, got {path.suffix}.\"\n        )\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/_readme_factory/","title":"_readme_factory","text":"<p>Functions used to create a README.md file for BMZ export.</p>"},{"location":"reference/careamics/model_io/bioimage/_readme_factory/#careamics.model_io.bioimage._readme_factory._yaml_block","title":"<code>_yaml_block(yaml_str)</code>","text":"<p>Return a markdown code block with a yaml string.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_str</code> <code>str</code> <p>YAML string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Markdown code block with the YAML string.</p> Source code in <code>src/careamics/model_io/bioimage/_readme_factory.py</code> <pre><code>def _yaml_block(yaml_str: str) -&gt; str:\n    \"\"\"Return a markdown code block with a yaml string.\n\n    Parameters\n    ----------\n    yaml_str : str\n        YAML string.\n\n    Returns\n    -------\n    str\n        Markdown code block with the YAML string.\n    \"\"\"\n    return f\"```yaml\\n{yaml_str}\\n```\"\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/_readme_factory/#careamics.model_io.bioimage._readme_factory.readme_factory","title":"<code>readme_factory(config, careamics_version, data_description)</code>","text":"<p>Create a README file for the model.</p> <p><code>data_description</code> can be used to add more information about the content of the data the model was trained on.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration.</p> required <code>careamics_version</code> <code>str</code> <p>CAREamics version.</p> required <code>data_description</code> <code>str</code> <p>Description of the data.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the README file.</p> Source code in <code>src/careamics/model_io/bioimage/_readme_factory.py</code> <pre><code>def readme_factory(\n    config: Configuration,\n    careamics_version: str,\n    data_description: str,\n) -&gt; Path:\n    \"\"\"Create a README file for the model.\n\n    `data_description` can be used to add more information about the content of the\n    data the model was trained on.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration.\n    careamics_version : str\n        CAREamics version.\n    data_description : str\n        Description of the data.\n\n    Returns\n    -------\n    Path\n        Path to the README file.\n    \"\"\"\n    # create file\n    # TODO use tempfile as in the bmz_io module\n    with cwd(get_careamics_home()):\n        readme = Path(\"README.md\")\n        readme.touch()\n\n        # algorithm pretty name\n        algorithm_flavour = config.get_algorithm_friendly_name()\n        algorithm_pretty_name = algorithm_flavour + \" - CAREamics\"\n\n        description = [f\"# {algorithm_pretty_name}\\n\\n\"]\n\n        # data description\n        description.append(\"## Data description\\n\\n\")\n        description.append(data_description)\n        description.append(\"\\n\\n\")\n\n        # algorithm description\n        description.append(\"## Algorithm description:\\n\\n\")\n        description.append(config.get_algorithm_description())\n        description.append(\"\\n\\n\")\n\n        # configuration description\n        description.append(\"## Configuration\\n\\n\")\n\n        description.append(\n            f\"{algorithm_flavour} was trained using CAREamics (version \"\n            f\"{careamics_version}) using the following configuration:\\n\\n\"\n        )\n\n        description.append(_yaml_block(yaml.dump(config.model_dump(exclude_none=True))))\n        description.append(\"\\n\\n\")\n\n        # validation\n        description.append(\"# Validation\\n\\n\")\n\n        description.append(\n            \"In order to validate the model, we encourage users to acquire a \"\n            \"test dataset with ground-truth data. Comparing the ground-truth data \"\n            \"with the prediction allows unbiased evaluation of the model performances. \"\n            \"This can be done for instance by using metrics such as PSNR, SSIM, or\"\n            \"MicroSSIM. In the absence of ground-truth, inspecting the residual image \"\n            \"(difference between input and predicted image) can be helpful to identify \"\n            \"whether real signal is removed from the input image.\\n\\n\"\n        )\n\n        # references\n        reference = config.get_algorithm_references()\n        if reference != \"\":\n            description.append(\"## References\\n\\n\")\n            description.append(reference)\n            description.append(\"\\n\\n\")\n\n        # links\n        description.append(\n            \"# Links\\n\\n\"\n            \"- [CAREamics repository](https://github.com/CAREamics/careamics)\\n\"\n            \"- [CAREamics documentation](https://careamics.github.io/)\\n\"\n        )\n\n        readme.write_text(\"\".join(description))\n\n        return readme.absolute()\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/","title":"bioimage_utils","text":"<p>Bioimage.io utils.</p>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/#careamics.model_io.bioimage.bioimage_utils.create_env_text","title":"<code>create_env_text(pytorch_version, torchvision_version)</code>","text":"<p>Create environment yaml content for the bioimage model.</p> <p>This installs an environment with the specified pytorch version and the latest changes to careamics.</p> <p>Parameters:</p> Name Type Description Default <code>pytorch_version</code> <code>str</code> <p>Pytorch version.</p> required <code>torchvision_version</code> <code>str</code> <p>Torchvision version.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Environment text.</p> Source code in <code>src/careamics/model_io/bioimage/bioimage_utils.py</code> <pre><code>def create_env_text(pytorch_version: str, torchvision_version: str) -&gt; str:\n    \"\"\"Create environment yaml content for the bioimage model.\n\n    This installs an environment with the specified pytorch version and the latest\n    changes to careamics.\n\n    Parameters\n    ----------\n    pytorch_version : str\n        Pytorch version.\n    torchvision_version : str\n        Torchvision version.\n\n    Returns\n    -------\n    str\n        Environment text.\n    \"\"\"\n    env = (\n        f\"name: careamics\\n\"\n        f\"dependencies:\\n\"\n        f\"  - python=3.10\\n\"\n        f\"  - pip\\n\"\n        f\"  - pip:\\n\"\n        f\"    - torch=={pytorch_version}\\n\"\n        f\"    - torchvision=={torchvision_version}\\n\"\n        f\"    - careamics=={get_careamics_version()}\"\n    )\n\n    return env\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/bioimage_utils/#careamics.model_io.bioimage.bioimage_utils.get_unzip_path","title":"<code>get_unzip_path(zip_path)</code>","text":"<p>Generate unzipped folder path from the bioimage.io model path.</p> <p>Parameters:</p> Name Type Description Default <code>zip_path</code> <code>Path</code> <p>Path to the bioimage.io model.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the unzipped folder.</p> Source code in <code>src/careamics/model_io/bioimage/bioimage_utils.py</code> <pre><code>def get_unzip_path(zip_path: Union[Path, str]) -&gt; Path:\n    \"\"\"Generate unzipped folder path from the bioimage.io model path.\n\n    Parameters\n    ----------\n    zip_path : Path\n        Path to the bioimage.io model.\n\n    Returns\n    -------\n    Path\n        Path to the unzipped folder.\n    \"\"\"\n    zip_path = Path(zip_path)\n\n    return zip_path.parent / (str(zip_path.name) + \".unzip\")\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/","title":"cover_factory","text":"<p>Convenience function to create covers for the BMZ.</p>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/#careamics.model_io.bioimage.cover_factory._convert_to_image","title":"<code>_convert_to_image(original_shape, array)</code>","text":"<p>Convert to Image.</p> <p>Parameters:</p> Name Type Description Default <code>original_shape</code> <code>tuple</code> <p>Original shape of the array.</p> required <code>array</code> <code>NDArray</code> <p>Normalized array to convert.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Converted array.</p> Source code in <code>src/careamics/model_io/bioimage/cover_factory.py</code> <pre><code>def _convert_to_image(original_shape: tuple[int, ...], array: NDArray) -&gt; Image:\n    \"\"\"Convert to Image.\n\n    Parameters\n    ----------\n    original_shape : tuple\n        Original shape of the array.\n    array : NDArray\n        Normalized array to convert.\n\n    Returns\n    -------\n    Image\n        Converted array.\n    \"\"\"\n    n_channels = original_shape[1]\n\n    if n_channels &gt; 1:\n        if n_channels == 3:\n            return Image.fromarray(array).convert(\"RGB\")\n        elif n_channels == 2:\n            # add an empty channel to the numpy array\n            array = np.concatenate([np.zeros_like(array[..., 0:1]), array], axis=-1)\n\n            return Image.fromarray(array).convert(\"RGB\")\n        else:  # more than 4\n            return _four_channel_image(array[..., :4])\n    else:\n        return Image.fromarray(array).convert(\"L\").convert(\"RGB\")\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/#careamics.model_io.bioimage.cover_factory._four_channel_image","title":"<code>_four_channel_image(array)</code>","text":"<p>Convert 4-channel array to Image.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>Normalized array to convert.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Converted array.</p> Source code in <code>src/careamics/model_io/bioimage/cover_factory.py</code> <pre><code>def _four_channel_image(array: NDArray) -&gt; Image:\n    \"\"\"Convert 4-channel array to Image.\n\n    Parameters\n    ----------\n    array : NDArray\n        Normalized array to convert.\n\n    Returns\n    -------\n    Image\n        Converted array.\n    \"\"\"\n    colors = color_palette[np.newaxis, np.newaxis, :, :]\n    four_c_array = np.sum(array[..., :4, np.newaxis] * colors, axis=-2).astype(np.uint8)\n\n    return Image.fromarray(four_c_array).convert(\"RGB\")\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/#careamics.model_io.bioimage.cover_factory._get_norm_slice","title":"<code>_get_norm_slice(array)</code>","text":"<p>Get the normalized middle slice of a 4D or 5D array (SC(Z)YX).</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>Array from which to get the middle slice.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Normalized middle slice of the input array.</p> Source code in <code>src/careamics/model_io/bioimage/cover_factory.py</code> <pre><code>def _get_norm_slice(array: NDArray) -&gt; NDArray:\n    \"\"\"Get the normalized middle slice of a 4D or 5D array (SC(Z)YX).\n\n    Parameters\n    ----------\n    array : NDArray\n        Array from which to get the middle slice.\n\n    Returns\n    -------\n    NDArray\n        Normalized middle slice of the input array.\n    \"\"\"\n    if array.ndim not in (4, 5):\n        raise ValueError(\"Array must be 4D or 5D.\")\n\n    channels = array.shape[1] &gt; 1\n    z_stack = array.ndim == 5\n\n    # get slice\n    if z_stack:\n        array_slice = array[0, :, array.shape[2] // 2, ...]\n    else:\n        array_slice = array[0, ...]\n\n    # channels\n    if channels:\n        array_slice = np.moveaxis(array_slice, 0, -1)\n    else:\n        array_slice = array_slice[0, ...]\n\n    # normalize\n    array_slice = (\n        255\n        * (array_slice - array_slice.min())\n        / (array_slice.max() - array_slice.min())\n    )\n\n    return array_slice.astype(np.uint8)\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/cover_factory/#careamics.model_io.bioimage.cover_factory.create_cover","title":"<code>create_cover(directory, array_in, array_out)</code>","text":"<p>Create a cover image from input and output arrays.</p> <p>Input and output arrays are expected to be SC(Z)YX. For images with a Z dimension, the middle slice is taken.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>Directory in which to save the cover.</p> required <code>array_in</code> <code>ndarray</code> <p>Array from which to create the cover image.</p> required <code>array_out</code> <code>ndarray</code> <p>Array from which to create the cover image.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved cover image.</p> Source code in <code>src/careamics/model_io/bioimage/cover_factory.py</code> <pre><code>def create_cover(directory: Path, array_in: NDArray, array_out: NDArray) -&gt; Path:\n    \"\"\"Create a cover image from input and output arrays.\n\n    Input and output arrays are expected to be SC(Z)YX. For images with a Z\n    dimension, the middle slice is taken.\n\n    Parameters\n    ----------\n    directory : Path\n        Directory in which to save the cover.\n    array_in : numpy.ndarray\n        Array from which to create the cover image.\n    array_out : numpy.ndarray\n        Array from which to create the cover image.\n\n    Returns\n    -------\n    Path\n        Path to the saved cover image.\n    \"\"\"\n    # extract slice and normalize arrays\n    slice_in = _get_norm_slice(array_in)\n    slice_out = _get_norm_slice(array_out)\n\n    horizontal_split = slice_in.shape[-1] == slice_out.shape[-1]\n    if not horizontal_split:\n        if slice_in.shape[-2] != slice_out.shape[-2]:\n            raise ValueError(\"Input and output arrays have different shapes.\")\n\n    # convert to Image\n    image_in = _convert_to_image(array_in.shape, slice_in)\n    image_out = _convert_to_image(array_out.shape, slice_out)\n\n    # split horizontally or vertically\n    if horizontal_split:\n        width = image_in.width // 2\n\n        cover = Image.new(\"RGB\", (image_in.width, image_in.height))\n        cover.paste(image_in.crop((0, 0, width, image_in.height)), (0, 0))\n        cover.paste(\n            image_out.crop(\n                (image_in.width - width, 0, image_in.width, image_in.height)\n            ),\n            (width, 0),\n        )\n    else:\n        height = image_in.height // 2\n\n        cover = Image.new(\"RGB\", (image_in.width, image_in.height))\n        cover.paste(image_in.crop((0, 0, image_in.width, height)), (0, 0))\n        cover.paste(\n            image_out.crop(\n                (0, image_in.height - height, image_in.width, image_in.height)\n            ),\n            (0, height),\n        )\n\n    # save\n    cover_path = directory / \"cover.png\"\n    cover.save(cover_path)\n\n    return cover_path\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/","title":"model_description","text":"<p>Module use to build BMZ model description.</p>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description._create_axes","title":"<code>_create_axes(array, data_config, channel_names=None, is_input=True)</code>","text":"<p>Create axes description.</p> <p>Array shape is expected to be SC(Z)YX.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array.</p> required <code>data_config</code> <code>DataModel</code> <p>CAREamics data configuration.</p> required <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <code>is_input</code> <code>bool</code> <p>Whether the axes are input axes, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[AxisBase]</code> <p>list of axes description.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If channel names are not provided when channel axis is present.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def _create_axes(\n    array: np.ndarray,\n    data_config: DataConfig,\n    channel_names: Optional[list[str]] = None,\n    is_input: bool = True,\n) -&gt; list[AxisBase]:\n    \"\"\"Create axes description.\n\n    Array shape is expected to be SC(Z)YX.\n\n    Parameters\n    ----------\n    array : np.ndarray\n        Array.\n    data_config : DataModel\n        CAREamics data configuration.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n    is_input : bool, optional\n        Whether the axes are input axes, by default True.\n\n    Returns\n    -------\n    list[AxisBase]\n        list of axes description.\n\n    Raises\n    ------\n    ValueError\n        If channel names are not provided when channel axis is present.\n    \"\"\"\n    # axes have to be SC(Z)YX\n    spatial_axes = data_config.axes.replace(\"S\", \"\").replace(\"C\", \"\")\n\n    # batch is always present\n    axes_model = [BatchAxis()]\n\n    if \"C\" in data_config.axes:\n        if channel_names is not None:\n            axes_model.append(\n                ChannelAxis(channel_names=[Identifier(name) for name in channel_names])\n            )\n        else:\n            raise ValueError(\n                f\"Channel names must be provided if channel axis is present, axes: \"\n                f\"{data_config.axes}.\"\n            )\n    else:\n        # singleton channel\n        axes_model.append(ChannelAxis(channel_names=[Identifier(\"channel\")]))\n\n    # spatial axes\n    for ind, axes in enumerate(spatial_axes):\n        if axes in [\"X\", \"Y\", \"Z\"]:\n            if is_input:\n                axes_model.append(\n                    SpaceInputAxis(id=AxisId(axes.lower()), size=array.shape[2 + ind])\n                )\n            else:\n                axes_model.append(\n                    SpaceOutputAxis(id=AxisId(axes.lower()), size=array.shape[2 + ind])\n                )\n\n    return axes_model\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description._create_inputs_ouputs","title":"<code>_create_inputs_ouputs(input_array, output_array, data_config, input_path, output_path, channel_names=None)</code>","text":"<p>Create input and output tensor description.</p> <p>Input and output paths must point to a <code>.npy</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>input_array</code> <code>ndarray</code> <p>Input array.</p> required <code>output_array</code> <code>ndarray</code> <p>Output array.</p> required <code>data_config</code> <code>DataModel</code> <p>CAREamics data configuration.</p> required <code>input_path</code> <code>Union[Path, str]</code> <p>Path to input .npy file.</p> required <code>output_path</code> <code>Union[Path, str]</code> <p>Path to output .npy file.</p> required <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[InputTensorDescr, OutputTensorDescr]</code> <p>Input and output tensor descriptions.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def _create_inputs_ouputs(\n    input_array: np.ndarray,\n    output_array: np.ndarray,\n    data_config: DataConfig,\n    input_path: Union[Path, str],\n    output_path: Union[Path, str],\n    channel_names: Optional[list[str]] = None,\n) -&gt; tuple[InputTensorDescr, OutputTensorDescr]:\n    \"\"\"Create input and output tensor description.\n\n    Input and output paths must point to a `.npy` file.\n\n    Parameters\n    ----------\n    input_array : np.ndarray\n        Input array.\n    output_array : np.ndarray\n        Output array.\n    data_config : DataModel\n        CAREamics data configuration.\n    input_path : Union[Path, str]\n        Path to input .npy file.\n    output_path : Union[Path, str]\n        Path to output .npy file.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n\n    Returns\n    -------\n    tuple[InputTensorDescr, OutputTensorDescr]\n        Input and output tensor descriptions.\n    \"\"\"\n    input_axes = _create_axes(input_array, data_config, channel_names)\n    output_axes = _create_axes(output_array, data_config, channel_names, False)\n\n    # mean and std\n    assert data_config.image_means is not None, \"Mean cannot be None.\"\n    assert data_config.image_means is not None, \"Std cannot be None.\"\n    means = data_config.image_means\n    stds = data_config.image_stds\n\n    # and the mean and std required to invert the normalization\n    # CAREamics denormalization: x = y * (std + eps) + mean\n    # BMZ normalization : x = (y - mean') / (std' + eps)\n    # to apply the BMZ normalization as a denormalization step, we need:\n    eps = 1e-6\n    inv_means = []\n    inv_stds = []\n    if means and stds:\n        for mean, std in zip(means, stds):\n            inv_means.append(-mean / (std + eps))\n            inv_stds.append(1 / (std + eps) - eps)\n\n        # create input/output descriptions\n        input_descr = InputTensorDescr(\n            id=TensorId(\"input\"),\n            axes=input_axes,\n            test_tensor=FileDescr(source=input_path),\n            preprocessing=[\n                FixedZeroMeanUnitVarianceDescr(\n                    kwargs=FixedZeroMeanUnitVarianceAlongAxisKwargs(\n                        mean=means, std=stds, axis=\"channel\"\n                    )\n                )\n            ],\n        )\n        output_descr = OutputTensorDescr(\n            id=TensorId(\"prediction\"),\n            axes=output_axes,\n            test_tensor=FileDescr(source=output_path),\n            postprocessing=[\n                FixedZeroMeanUnitVarianceDescr(\n                    kwargs=FixedZeroMeanUnitVarianceAlongAxisKwargs(  # invert norm\n                        mean=inv_means, std=inv_stds, axis=\"channel\"\n                    )\n                )\n            ],\n        )\n\n        return input_descr, output_descr\n    else:\n        raise ValueError(\"Mean and std cannot be None.\")\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description.create_model_description","title":"<code>create_model_description(config, name, general_description, data_description, authors, inputs, outputs, weights_path, torch_version, careamics_version, config_path, env_path, covers, channel_names=None, model_version='0.1.0')</code>","text":"<p>Create model description.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Configuration</code> <p>CAREamics configuration.</p> required <code>name</code> <code>str</code> <p>Name of the model.</p> required <code>general_description</code> <code>str</code> <p>General description of the model.</p> required <code>data_description</code> <code>str</code> <p>Description of the data the model was trained on.</p> required <code>authors</code> <code>list[Author]</code> <p>Authors of the model.</p> required <code>inputs</code> <code>Union[Path, str]</code> <p>Path to input .npy file.</p> required <code>outputs</code> <code>Union[Path, str]</code> <p>Path to output .npy file.</p> required <code>weights_path</code> <code>Union[Path, str]</code> <p>Path to model weights.</p> required <code>torch_version</code> <code>str</code> <p>Pytorch version.</p> required <code>careamics_version</code> <code>str</code> <p>CAREamics version.</p> required <code>config_path</code> <code>Union[Path, str]</code> <p>Path to model configuration.</p> required <code>env_path</code> <code>Union[Path, str]</code> <p>Path to environment file.</p> required <code>covers</code> <code>list of pathlib.Path or str</code> <p>Paths to cover images.</p> required <code>channel_names</code> <code>Optional[list[str]]</code> <p>Channel names, by default None.</p> <code>None</code> <code>model_version</code> <code>str</code> <p>Model version.</p> <code>\"0.1.0\"</code> <p>Returns:</p> Type Description <code>ModelDescr</code> <p>Model description.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def create_model_description(\n    config: Configuration,\n    name: str,\n    general_description: str,\n    data_description: str,\n    authors: list[Author],\n    inputs: Union[Path, str],\n    outputs: Union[Path, str],\n    weights_path: Union[Path, str],\n    torch_version: str,\n    careamics_version: str,\n    config_path: Union[Path, str],\n    env_path: Union[Path, str],\n    covers: list[Union[Path, str]],\n    channel_names: Optional[list[str]] = None,\n    model_version: str = \"0.1.0\",\n) -&gt; ModelDescr:\n    \"\"\"Create model description.\n\n    Parameters\n    ----------\n    config : Configuration\n        CAREamics configuration.\n    name : str\n        Name of the model.\n    general_description : str\n        General description of the model.\n    data_description : str\n        Description of the data the model was trained on.\n    authors : list[Author]\n        Authors of the model.\n    inputs : Union[Path, str]\n        Path to input .npy file.\n    outputs : Union[Path, str]\n        Path to output .npy file.\n    weights_path : Union[Path, str]\n        Path to model weights.\n    torch_version : str\n        Pytorch version.\n    careamics_version : str\n        CAREamics version.\n    config_path : Union[Path, str]\n        Path to model configuration.\n    env_path : Union[Path, str]\n        Path to environment file.\n    covers : list of pathlib.Path or str\n        Paths to cover images.\n    channel_names : Optional[list[str]], optional\n        Channel names, by default None.\n    model_version : str, default \"0.1.0\"\n        Model version.\n\n    Returns\n    -------\n    ModelDescr\n        Model description.\n    \"\"\"\n    # documentation\n    doc = readme_factory(\n        config,\n        careamics_version=careamics_version,\n        data_description=data_description,\n    )\n\n    # inputs, outputs\n    input_descr, output_descr = _create_inputs_ouputs(\n        input_array=np.load(inputs),\n        output_array=np.load(outputs),\n        data_config=config.data_config,\n        input_path=inputs,\n        output_path=outputs,\n        channel_names=channel_names,\n    )\n\n    # weights description\n    architecture_descr = ArchitectureFromLibraryDescr(\n        import_from=\"careamics.models.unet\",\n        callable=f\"{config.algorithm_config.model.architecture}\",\n        kwargs=config.algorithm_config.model.model_dump(),\n    )\n\n    weights_descr = WeightsDescr(\n        pytorch_state_dict=PytorchStateDictWeightsDescr(\n            source=weights_path,\n            architecture=architecture_descr,\n            pytorch_version=Version(torch_version),\n            dependencies=EnvironmentFileDescr(source=env_path),\n        ),\n    )\n\n    # overall model description\n    model = ModelDescr(\n        name=name,\n        authors=authors,\n        description=general_description,\n        documentation=doc,\n        inputs=[input_descr],\n        outputs=[output_descr],\n        tags=config.get_algorithm_keywords(),\n        links=[\n            \"https://github.com/CAREamics/careamics\",\n            \"https://careamics.github.io/latest/\",\n        ],\n        license=\"BSD-3-Clause\",\n        config={\n            \"bioimageio\": {\n                \"test_kwargs\": {\n                    \"pytorch_state_dict\": {\n                        \"absolute_tolerance\": 1e-2,\n                        \"relative_tolerance\": 1e-2,\n                    }\n                }\n            }\n        },\n        version=model_version,\n        weights=weights_descr,\n        attachments=[FileDescr(source=config_path)],\n        cite=config.get_algorithm_citations(),\n        covers=covers,\n    )\n\n    return model\n</code></pre>"},{"location":"reference/careamics/model_io/bioimage/model_description/#careamics.model_io.bioimage.model_description.extract_model_path","title":"<code>extract_model_path(model_desc)</code>","text":"<p>Return the relative path to the weights and configuration files.</p> <p>Parameters:</p> Name Type Description Default <code>model_desc</code> <code>ModelDescr</code> <p>Model description.</p> required <p>Returns:</p> Type Description <code>tuple of (path, path)</code> <p>Weights and configuration paths.</p> Source code in <code>src/careamics/model_io/bioimage/model_description.py</code> <pre><code>def extract_model_path(model_desc: ModelDescr) -&gt; tuple[Path, Path]:\n    \"\"\"Return the relative path to the weights and configuration files.\n\n    Parameters\n    ----------\n    model_desc : ModelDescr\n        Model description.\n\n    Returns\n    -------\n    tuple of (path, path)\n        Weights and configuration paths.\n    \"\"\"\n    if model_desc.weights.pytorch_state_dict is None:\n        raise ValueError(\"No model weights found in model description.\")\n    weights_path = resolve_and_extract(\n        model_desc.weights.pytorch_state_dict.source\n    ).path\n\n    for file in model_desc.attachments:\n        file_path = file.source if isinstance(file.source, Path) else file.source.path\n        if file_path is None:\n            continue\n        file_path = Path(file_path)\n        if file_path.name == \"careamics.yaml\":\n            config_path = resolve_and_extract(file.source).path\n            break\n    else:\n        raise ValueError(\"Configuration file not found.\")\n\n    return weights_path, config_path\n</code></pre>"},{"location":"reference/careamics/models/activation/","title":"activation","text":"<p>Activations for CAREamics models.</p>"},{"location":"reference/careamics/models/activation/#careamics.models.activation.get_activation","title":"<code>get_activation(activation)</code>","text":"<p>Get activation function.</p> <p>Parameters:</p> Name Type Description Default <code>activation</code> <code>str</code> <p>Activation function name.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Activation function.</p> Source code in <code>src/careamics/models/activation.py</code> <pre><code>def get_activation(activation: Union[SupportedActivation, str]) -&gt; Callable:\n    \"\"\"\n    Get activation function.\n\n    Parameters\n    ----------\n    activation : str\n        Activation function name.\n\n    Returns\n    -------\n    Callable\n        Activation function.\n    \"\"\"\n    if activation == SupportedActivation.RELU:\n        return nn.ReLU()\n    elif activation == SupportedActivation.ELU:\n        return nn.ELU()\n    elif activation == SupportedActivation.LEAKYRELU:\n        return nn.LeakyReLU()\n    elif activation == SupportedActivation.TANH:\n        return nn.Tanh()\n    elif activation == SupportedActivation.SIGMOID:\n        return nn.Sigmoid()\n    elif activation == SupportedActivation.SOFTMAX:\n        return nn.Softmax(dim=1)\n    elif activation == SupportedActivation.NONE:\n        return nn.Identity()\n    else:\n        raise ValueError(f\"Activation {activation} not supported.\")\n</code></pre>"},{"location":"reference/careamics/models/layers/","title":"layers","text":"<p>Layer module.</p> <p>This submodule contains layers used in the CAREamics models.</p>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block","title":"<code>Conv_Block</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolution block used in UNets.</p> <p>Convolution block consist of two convolution layers with optional batch norm, dropout and with a final activation function.</p> <p>The parameters are directly mapped to PyTorch Conv2D and Conv3d parameters, see PyTorch torch.nn.Conv2d and torch.nn.Conv3d for more information.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolutions, 2 or 3.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>intermediate_channel_multiplier</code> <code>int</code> <p>Multiplied for the number of output channels, by default 1.</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride of the convolutions, by default 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding of the convolutions, by default 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of the convolutions, by default True.</p> <code>True</code> <code>groups</code> <code>int</code> <p>Controls the connections between inputs and outputs, by default 1.</p> <code>1</code> <code>activation</code> <code>str</code> <p>Activation function, by default \"ReLU\".</p> <code>'ReLU'</code> <code>dropout_perc</code> <code>float</code> <p>Dropout percentage, by default 0.</p> <code>0</code> <code>use_batch_norm</code> <code>bool</code> <p>Use batch norm, by default False.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>class Conv_Block(nn.Module):\n    \"\"\"\n    Convolution block used in UNets.\n\n    Convolution block consist of two convolution layers with optional batch norm,\n    dropout and with a final activation function.\n\n    The parameters are directly mapped to PyTorch Conv2D and Conv3d parameters, see\n    PyTorch torch.nn.Conv2d and torch.nn.Conv3d for more information.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolutions, 2 or 3.\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    intermediate_channel_multiplier : int, optional\n        Multiplied for the number of output channels, by default 1.\n    stride : int, optional\n        Stride of the convolutions, by default 1.\n    padding : int, optional\n        Padding of the convolutions, by default 1.\n    bias : bool, optional\n        Bias of the convolutions, by default True.\n    groups : int, optional\n        Controls the connections between inputs and outputs, by default 1.\n    activation : str, optional\n        Activation function, by default \"ReLU\".\n    dropout_perc : float, optional\n        Dropout percentage, by default 0.\n    use_batch_norm : bool, optional\n        Use batch norm, by default False.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        in_channels: int,\n        out_channels: int,\n        intermediate_channel_multiplier: int = 1,\n        stride: int = 1,\n        padding: int = 1,\n        bias: bool = True,\n        groups: int = 1,\n        activation: str = \"ReLU\",\n        dropout_perc: float = 0,\n        use_batch_norm: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolutions, 2 or 3.\n        in_channels : int\n            Number of input channels.\n        out_channels : int\n            Number of output channels.\n        intermediate_channel_multiplier : int, optional\n            Multiplied for the number of output channels, by default 1.\n        stride : int, optional\n            Stride of the convolutions, by default 1.\n        padding : int, optional\n            Padding of the convolutions, by default 1.\n        bias : bool, optional\n            Bias of the convolutions, by default True.\n        groups : int, optional\n            Controls the connections between inputs and outputs, by default 1.\n        activation : str, optional\n            Activation function, by default \"ReLU\".\n        dropout_perc : float, optional\n            Dropout percentage, by default 0.\n        use_batch_norm : bool, optional\n            Use batch norm, by default False.\n        \"\"\"\n        super().__init__()\n        self.use_batch_norm = use_batch_norm\n        self.conv1 = getattr(nn, f\"Conv{conv_dim}d\")(\n            in_channels,\n            out_channels * intermediate_channel_multiplier,\n            kernel_size=3,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n\n        self.conv2 = getattr(nn, f\"Conv{conv_dim}d\")(\n            out_channels * intermediate_channel_multiplier,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=padding,\n            bias=bias,\n            groups=groups,\n        )\n\n        self.batch_norm1 = getattr(nn, f\"BatchNorm{conv_dim}d\")(\n            out_channels * intermediate_channel_multiplier\n        )\n        self.batch_norm2 = getattr(nn, f\"BatchNorm{conv_dim}d\")(out_channels)\n\n        self.dropout = (\n            getattr(nn, f\"Dropout{conv_dim}d\")(dropout_perc)\n            if dropout_perc &gt; 0\n            else None\n        )\n        self.activation = (\n            getattr(nn, f\"{activation}\")() if activation is not None else nn.Identity()\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor.\n        \"\"\"\n        if self.use_batch_norm:\n            x = self.conv1(x)\n            x = self.batch_norm1(x)\n            x = self.activation(x)\n            x = self.conv2(x)\n            x = self.batch_norm2(x)\n            x = self.activation(x)\n        else:\n            x = self.conv1(x)\n            x = self.activation(x)\n            x = self.conv2(x)\n            x = self.activation(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block.__init__","title":"<code>__init__(conv_dim, in_channels, out_channels, intermediate_channel_multiplier=1, stride=1, padding=1, bias=True, groups=1, activation='ReLU', dropout_perc=0, use_batch_norm=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolutions, 2 or 3.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>intermediate_channel_multiplier</code> <code>int</code> <p>Multiplied for the number of output channels, by default 1.</p> <code>1</code> <code>stride</code> <code>int</code> <p>Stride of the convolutions, by default 1.</p> <code>1</code> <code>padding</code> <code>int</code> <p>Padding of the convolutions, by default 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of the convolutions, by default True.</p> <code>True</code> <code>groups</code> <code>int</code> <p>Controls the connections between inputs and outputs, by default 1.</p> <code>1</code> <code>activation</code> <code>str</code> <p>Activation function, by default \"ReLU\".</p> <code>'ReLU'</code> <code>dropout_perc</code> <code>float</code> <p>Dropout percentage, by default 0.</p> <code>0</code> <code>use_batch_norm</code> <code>bool</code> <p>Use batch norm, by default False.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    in_channels: int,\n    out_channels: int,\n    intermediate_channel_multiplier: int = 1,\n    stride: int = 1,\n    padding: int = 1,\n    bias: bool = True,\n    groups: int = 1,\n    activation: str = \"ReLU\",\n    dropout_perc: float = 0,\n    use_batch_norm: bool = False,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolutions, 2 or 3.\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    intermediate_channel_multiplier : int, optional\n        Multiplied for the number of output channels, by default 1.\n    stride : int, optional\n        Stride of the convolutions, by default 1.\n    padding : int, optional\n        Padding of the convolutions, by default 1.\n    bias : bool, optional\n        Bias of the convolutions, by default True.\n    groups : int, optional\n        Controls the connections between inputs and outputs, by default 1.\n    activation : str, optional\n        Activation function, by default \"ReLU\".\n    dropout_perc : float, optional\n        Dropout percentage, by default 0.\n    use_batch_norm : bool, optional\n        Use batch norm, by default False.\n    \"\"\"\n    super().__init__()\n    self.use_batch_norm = use_batch_norm\n    self.conv1 = getattr(nn, f\"Conv{conv_dim}d\")(\n        in_channels,\n        out_channels * intermediate_channel_multiplier,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n\n    self.conv2 = getattr(nn, f\"Conv{conv_dim}d\")(\n        out_channels * intermediate_channel_multiplier,\n        out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=padding,\n        bias=bias,\n        groups=groups,\n    )\n\n    self.batch_norm1 = getattr(nn, f\"BatchNorm{conv_dim}d\")(\n        out_channels * intermediate_channel_multiplier\n    )\n    self.batch_norm2 = getattr(nn, f\"BatchNorm{conv_dim}d\")(out_channels)\n\n    self.dropout = (\n        getattr(nn, f\"Dropout{conv_dim}d\")(dropout_perc)\n        if dropout_perc &gt; 0\n        else None\n    )\n    self.activation = (\n        getattr(nn, f\"{activation}\")() if activation is not None else nn.Identity()\n    )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.Conv_Block.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    if self.use_batch_norm:\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        x = self.activation(x)\n    else:\n        x = self.conv1(x)\n        x = self.activation(x)\n        x = self.conv2(x)\n        x = self.activation(x)\n    if self.dropout is not None:\n        x = self.dropout(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool","title":"<code>MaxBlurPool</code>","text":"<p>               Bases: <code>Module</code></p> <p>Compute pools and blurs and downsample a given feature map.</p> <p>Inspired by Kornia MaxBlurPool implementation. Equivalent to <code>nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))</code></p>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool--parameters","title":"Parameters","text":"<p>dim : int     Toggles between 2D and 3D. kernel_size : Union[tuple[int, int], int]     Kernel size for max pooling. stride : int     Stride for pooling. max_pool_size : int     Max kernel size for max pooling. ceil_mode : bool     Ceil mode, by default False. Set to True to match output size of conv2d.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>class MaxBlurPool(nn.Module):\n    \"\"\"Compute pools and blurs and downsample a given feature map.\n\n    Inspired by Kornia MaxBlurPool implementation. Equivalent to\n    ```nn.Sequential(nn.MaxPool2d(...), BlurPool2D(...))```\n\n    Parameters\n    ----------\n    dim : int\n        Toggles between 2D and 3D.\n    kernel_size : Union[tuple[int, int], int]\n        Kernel size for max pooling.\n    stride : int\n        Stride for pooling.\n    max_pool_size : int\n        Max kernel size for max pooling.\n    ceil_mode : bool\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        kernel_size: Union[tuple[int, int], int],\n        stride: int = 2,\n        max_pool_size: int = 2,\n        ceil_mode: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        dim : int\n            Dimension of the convolution.\n        kernel_size : Union[tuple[int, int], int]\n            Kernel size for max pooling.\n        stride : int, optional\n            Stride, by default 2.\n        max_pool_size : int, optional\n            Maximum pool size, by default 2.\n        ceil_mode : bool, optional\n            Ceil mode, by default False. Set to True to match output size of conv2d.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.max_pool_size = max_pool_size\n        self.ceil_mode = ceil_mode\n        kernel = _get_pascal_kernel_nd(kernel_size, norm=True, dim=self.dim)\n        self.register_buffer(\"kernel\", kernel, persistent=False)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass of the function.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor.\n        \"\"\"\n        kernel = self.kernel.to(dtype=x.dtype)\n        num_channels = int(x.size(1))\n        if self.dim == 2:\n            return _max_blur_pool_by_kernel2d(\n                x,\n                kernel.repeat((num_channels, 1, 1, 1)),\n                self.stride,\n                self.max_pool_size,\n                self.ceil_mode,\n            )\n        else:\n            return _max_blur_pool_by_kernel3d(\n                x,\n                kernel.repeat((num_channels, 1, 1, 1, 1)),\n                self.stride,\n                self.max_pool_size,\n                self.ceil_mode,\n            )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool.__init__","title":"<code>__init__(dim, kernel_size, stride=2, max_pool_size=2, ceil_mode=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimension of the convolution.</p> required <code>kernel_size</code> <code>Union[tuple[int, int], int]</code> <p>Kernel size for max pooling.</p> required <code>stride</code> <code>int</code> <p>Stride, by default 2.</p> <code>2</code> <code>max_pool_size</code> <code>int</code> <p>Maximum pool size, by default 2.</p> <code>2</code> <code>ceil_mode</code> <code>bool</code> <p>Ceil mode, by default False. Set to True to match output size of conv2d.</p> <code>False</code> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    kernel_size: Union[tuple[int, int], int],\n    stride: int = 2,\n    max_pool_size: int = 2,\n    ceil_mode: bool = False,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    dim : int\n        Dimension of the convolution.\n    kernel_size : Union[tuple[int, int], int]\n        Kernel size for max pooling.\n    stride : int, optional\n        Stride, by default 2.\n    max_pool_size : int, optional\n        Maximum pool size, by default 2.\n    ceil_mode : bool, optional\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n    \"\"\"\n    super().__init__()\n    self.dim = dim\n    self.kernel_size = kernel_size\n    self.stride = stride\n    self.max_pool_size = max_pool_size\n    self.ceil_mode = ceil_mode\n    kernel = _get_pascal_kernel_nd(kernel_size, norm=True, dim=self.dim)\n    self.register_buffer(\"kernel\", kernel, persistent=False)\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.MaxBlurPool.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass of the function.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    kernel = self.kernel.to(dtype=x.dtype)\n    num_channels = int(x.size(1))\n    if self.dim == 2:\n        return _max_blur_pool_by_kernel2d(\n            x,\n            kernel.repeat((num_channels, 1, 1, 1)),\n            self.stride,\n            self.max_pool_size,\n            self.ceil_mode,\n        )\n    else:\n        return _max_blur_pool_by_kernel3d(\n            x,\n            kernel.repeat((num_channels, 1, 1, 1, 1)),\n            self.stride,\n            self.max_pool_size,\n            self.ceil_mode,\n        )\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers._compute_zero_padding","title":"<code>_compute_zero_padding(kernel_size, dim)</code>","text":"<p>Utility function that computes zero padding tuple.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>Union[tuple[int, ...], int]</code> <p>Kernel size.</p> required <code>dim</code> <code>int</code> <p>Number of dimensions.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Zero padding tuple.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def _compute_zero_padding(\n    kernel_size: Union[tuple[int, ...], int], dim: int\n) -&gt; tuple[int, ...]:\n    \"\"\"Utility function that computes zero padding tuple.\n\n    Parameters\n    ----------\n    kernel_size : Union[tuple[int, ...], int]\n        Kernel size.\n    dim : int\n        Number of dimensions.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Zero padding tuple.\n    \"\"\"\n    kernel_dims = _unpack_kernel_size(kernel_size, dim)\n    return tuple([(kd - 1) // 2 for kd in kernel_dims])\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers._get_pascal_kernel_nd","title":"<code>_get_pascal_kernel_nd(kernel_size, norm=True, dim=2, *, device=None, dtype=None)</code>","text":"<p>Generate pascal filter kernel by kernel size.</p> <p>If kernel_size is an integer the kernel will be shaped as (kernel_size, kernel_size) otherwise the kernel will be shaped as kernel_size</p> <p>Inspired by Kornia implementation.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>Union[tuple[int, int], int]</code> <p>Kernel size for the pascal kernel.</p> required <code>norm</code> <code>bool</code> <p>Normalize the kernel, by default True.</p> <code>True</code> <code>dim</code> <code>int</code> <p>Number of dimensions, by default 2.</p> <code>2</code> <code>device</code> <code>Optional[device]</code> <p>Device of the tensor, by default None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>Data type of the tensor, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Pascal kernel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _get_pascal_kernel_nd(1)\ntensor([[1.]])\n&gt;&gt;&gt; _get_pascal_kernel_nd(4)\ntensor([[0.0156, 0.0469, 0.0469, 0.0156],\n        [0.0469, 0.1406, 0.1406, 0.0469],\n        [0.0469, 0.1406, 0.1406, 0.0469],\n        [0.0156, 0.0469, 0.0469, 0.0156]])\n&gt;&gt;&gt; _get_pascal_kernel_nd(4, norm=False)\ntensor([[1., 3., 3., 1.],\n        [3., 9., 9., 3.],\n        [3., 9., 9., 3.],\n        [1., 3., 3., 1.]])\n</code></pre> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def _get_pascal_kernel_nd(\n    kernel_size: Union[tuple[int, int], int],\n    norm: bool = True,\n    dim: int = 2,\n    *,\n    device: Optional[torch.device] = None,\n    dtype: Optional[torch.dtype] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Generate pascal filter kernel by kernel size.\n\n    If kernel_size is an integer the kernel will be shaped as (kernel_size, kernel_size)\n    otherwise the kernel will be shaped as kernel_size\n\n    Inspired by Kornia implementation.\n\n    Parameters\n    ----------\n    kernel_size : Union[tuple[int, int], int]\n        Kernel size for the pascal kernel.\n    norm : bool\n        Normalize the kernel, by default True.\n    dim : int\n        Number of dimensions, by default 2.\n    device : Optional[torch.device]\n        Device of the tensor, by default None.\n    dtype : Optional[torch.dtype]\n        Data type of the tensor, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        Pascal kernel.\n\n    Examples\n    --------\n    &gt;&gt;&gt; _get_pascal_kernel_nd(1)\n    tensor([[1.]])\n    &gt;&gt;&gt; _get_pascal_kernel_nd(4)\n    tensor([[0.0156, 0.0469, 0.0469, 0.0156],\n            [0.0469, 0.1406, 0.1406, 0.0469],\n            [0.0469, 0.1406, 0.1406, 0.0469],\n            [0.0156, 0.0469, 0.0469, 0.0156]])\n    &gt;&gt;&gt; _get_pascal_kernel_nd(4, norm=False)\n    tensor([[1., 3., 3., 1.],\n            [3., 9., 9., 3.],\n            [3., 9., 9., 3.],\n            [1., 3., 3., 1.]])\n    \"\"\"\n    kernel_dims = _unpack_kernel_size(kernel_size, dim)\n\n    kernel = [\n        get_pascal_kernel_1d(kd, device=device, dtype=dtype) for kd in kernel_dims\n    ]\n\n    if dim == 2:\n        kernel = kernel[0][:, None] * kernel[1][None, :]\n    elif dim == 3:\n        kernel = (\n            kernel[0][:, None, None]\n            * kernel[1][None, :, None]\n            * kernel[2][None, None, :]\n        )\n    if norm:\n        kernel = kernel / torch.sum(kernel)\n    return kernel\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers._max_blur_pool_by_kernel2d","title":"<code>_max_blur_pool_by_kernel2d(x, kernel, stride, max_pool_size, ceil_mode)</code>","text":"<p>Compute max_blur_pool by a given :math:<code>CxC_(out, None)xNxN</code> kernel.</p> <p>Inspired by Kornia implementation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>kernel</code> <code>Tensor</code> <p>Kernel tensor.</p> required <code>stride</code> <code>int</code> <p>Stride.</p> required <code>max_pool_size</code> <code>int</code> <p>Maximum pool size.</p> required <code>ceil_mode</code> <code>bool</code> <p>Ceil mode, by default False. Set to True to match output size of conv2d.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def _max_blur_pool_by_kernel2d(\n    x: torch.Tensor,\n    kernel: torch.Tensor,\n    stride: int,\n    max_pool_size: int,\n    ceil_mode: bool,\n) -&gt; torch.Tensor:\n    \"\"\"Compute max_blur_pool by a given :math:`CxC_(out, None)xNxN` kernel.\n\n    Inspired by Kornia implementation.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    kernel : torch.Tensor\n        Kernel tensor.\n    stride : int\n        Stride.\n    max_pool_size : int\n        Maximum pool size.\n    ceil_mode : bool\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    # compute local maxima\n    x = F.max_pool2d(\n        x, kernel_size=max_pool_size, padding=0, stride=1, ceil_mode=ceil_mode\n    )\n    # blur and downsample\n    padding = _compute_zero_padding((kernel.shape[-2], kernel.shape[-1]), dim=2)\n    return F.conv2d(x, kernel, padding=padding, stride=stride, groups=x.size(1))\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers._max_blur_pool_by_kernel3d","title":"<code>_max_blur_pool_by_kernel3d(x, kernel, stride, max_pool_size, ceil_mode)</code>","text":"<p>Compute max_blur_pool by a given :math:<code>CxC_(out, None)xNxNxN</code> kernel.</p> <p>Inspired by Kornia implementation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>kernel</code> <code>Tensor</code> <p>Kernel tensor.</p> required <code>stride</code> <code>int</code> <p>Stride.</p> required <code>max_pool_size</code> <code>int</code> <p>Maximum pool size.</p> required <code>ceil_mode</code> <code>bool</code> <p>Ceil mode, by default False. Set to True to match output size of conv2d.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def _max_blur_pool_by_kernel3d(\n    x: torch.Tensor,\n    kernel: torch.Tensor,\n    stride: int,\n    max_pool_size: int,\n    ceil_mode: bool,\n) -&gt; torch.Tensor:\n    \"\"\"Compute max_blur_pool by a given :math:`CxC_(out, None)xNxNxN` kernel.\n\n    Inspired by Kornia implementation.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n    kernel : torch.Tensor\n        Kernel tensor.\n    stride : int\n        Stride.\n    max_pool_size : int\n        Maximum pool size.\n    ceil_mode : bool\n        Ceil mode, by default False. Set to True to match output size of conv2d.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    # compute local maxima\n    x = F.max_pool3d(\n        x, kernel_size=max_pool_size, padding=0, stride=1, ceil_mode=ceil_mode\n    )\n    # blur and downsample\n    padding = _compute_zero_padding(\n        (kernel.shape[-3], kernel.shape[-2], kernel.shape[-1]), dim=3\n    )\n    return F.conv3d(x, kernel, padding=padding, stride=stride, groups=x.size(1))\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers._unpack_kernel_size","title":"<code>_unpack_kernel_size(kernel_size, dim)</code>","text":"<p>Unpack kernel_size to a tuple of ints.</p> <p>Inspired by Kornia implementation. TODO: link</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>Union[tuple[int, ...], int]</code> <p>Kernel size.</p> required <code>dim</code> <code>int</code> <p>Number of dimensions.</p> required <p>Returns:</p> Type Description <code>tuple[int, ...]</code> <p>Kernel size tuple.</p> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def _unpack_kernel_size(\n    kernel_size: Union[tuple[int, ...], int], dim: int\n) -&gt; tuple[int, ...]:\n    \"\"\"Unpack kernel_size to a tuple of ints.\n\n    Inspired by Kornia implementation. TODO: link\n\n    Parameters\n    ----------\n    kernel_size : Union[tuple[int, ...], int]\n        Kernel size.\n    dim : int\n        Number of dimensions.\n\n    Returns\n    -------\n    tuple[int, ...]\n        Kernel size tuple.\n    \"\"\"\n    if isinstance(kernel_size, int):\n        kernel_dims = tuple([kernel_size for _ in range(dim)])\n    else:\n        kernel_dims = kernel_size\n    return kernel_dims\n</code></pre>"},{"location":"reference/careamics/models/layers/#careamics.models.layers.get_pascal_kernel_1d","title":"<code>get_pascal_kernel_1d(kernel_size, norm=False, *, device=None, dtype=None)</code>","text":"<p>Generate Yang Hui triangle (Pascal's triangle) for a given number.</p> <p>Inspired by Kornia implementation. TODO link</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int</code> <p>Kernel size.</p> required <code>norm</code> <code>bool</code> <p>Normalize the kernel, by default False.</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>Device of the tensor, by default None.</p> <code>None</code> <code>dtype</code> <code>Optional[dtype]</code> <p>Data type of the tensor, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Pascal kernel.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_pascal_kernel_1d(1)\ntensor([1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(2)\ntensor([1., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(3)\ntensor([1., 2., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(4)\ntensor([1., 3., 3., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(5)\ntensor([1., 4., 6., 4., 1.])\n&gt;&gt;&gt; get_pascal_kernel_1d(6)\ntensor([ 1.,  5., 10., 10.,  5.,  1.])\n</code></pre> Source code in <code>src/careamics/models/layers.py</code> <pre><code>def get_pascal_kernel_1d(\n    kernel_size: int,\n    norm: bool = False,\n    *,\n    device: Optional[torch.device] = None,\n    dtype: Optional[torch.dtype] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Generate Yang Hui triangle (Pascal's triangle) for a given number.\n\n    Inspired by Kornia implementation. TODO link\n\n    Parameters\n    ----------\n    kernel_size : int\n        Kernel size.\n    norm : bool\n        Normalize the kernel, by default False.\n    device : Optional[torch.device]\n        Device of the tensor, by default None.\n    dtype : Optional[torch.dtype]\n        Data type of the tensor, by default None.\n\n    Returns\n    -------\n    torch.Tensor\n        Pascal kernel.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_pascal_kernel_1d(1)\n    tensor([1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(2)\n    tensor([1., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(3)\n    tensor([1., 2., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(4)\n    tensor([1., 3., 3., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(5)\n    tensor([1., 4., 6., 4., 1.])\n    &gt;&gt;&gt; get_pascal_kernel_1d(6)\n    tensor([ 1.,  5., 10., 10.,  5.,  1.])\n    \"\"\"\n    pre: list[float] = []\n    cur: list[float] = []\n    for i in range(kernel_size):\n        cur = [1.0] * (i + 1)\n\n        for j in range(1, i // 2 + 1):\n            value = pre[j - 1] + pre[j]\n            cur[j] = value\n            if i != 2 * j:\n                cur[-j - 1] = value\n        pre = cur\n\n    out = torch.tensor(cur, device=device, dtype=dtype)\n\n    if norm:\n        out = out / out.sum()\n\n    return out\n</code></pre>"},{"location":"reference/careamics/models/model_factory/","title":"model_factory","text":"<p>Model creation factory functions.</p>"},{"location":"reference/careamics/models/model_factory/#careamics.models.model_factory.model_factory","title":"<code>model_factory(model_configuration)</code>","text":"<p>Deep learning model factory.</p> <p>Supported models are defined in careamics.config.SupportedArchitecture.</p> <p>Parameters:</p> Name Type Description Default <code>model_configuration</code> <code>Union[UNetModel, VAEModel]</code> <p>Model configuration.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>Model class.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the requested architecture is not implemented.</p> Source code in <code>src/careamics/models/model_factory.py</code> <pre><code>def model_factory(\n    model_configuration: Union[UNetModel, LVAEModel],\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Deep learning model factory.\n\n    Supported models are defined in careamics.config.SupportedArchitecture.\n\n    Parameters\n    ----------\n    model_configuration : Union[UNetModel, VAEModel]\n        Model configuration.\n\n    Returns\n    -------\n    torch.nn.Module\n        Model class.\n\n    Raises\n    ------\n    NotImplementedError\n        If the requested architecture is not implemented.\n    \"\"\"\n    if model_configuration.architecture == SupportedArchitecture.UNET:\n        return UNet(**model_configuration.model_dump())\n    elif model_configuration.architecture == SupportedArchitecture.LVAE:\n        return LVAE(**model_configuration.model_dump())\n    else:\n        raise NotImplementedError(\n            f\"Model {model_configuration.architecture} is not implemented or unknown.\"\n        )\n</code></pre>"},{"location":"reference/careamics/models/unet/","title":"unet","text":"<p>UNet model.</p> <p>A UNet encoder, decoder and complete model.</p>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNet model.</p> <p>Adapted for PyTorch from: https://github.com/juglab/n2v/blob/main/n2v/nets/unet_blocks.py.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dims</code> <code>int</code> <p>Number of dimensions of the convolution layers (2 or 3).</p> required <code>num_classes</code> <code>int</code> <p>Number of classes to predict, by default 1.</p> <code>1</code> <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of downsamplings, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of filters in the first convolution layer, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size of the pooling layers, by default 2.</p> <code>2</code> <code>final_activation</code> <code>Optional[Callable]</code> <p>Activation function to use for the last layer, by default None.</p> <code>NONE</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train the channels independently, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UNet(nn.Module):\n    \"\"\"\n    UNet model.\n\n    Adapted for PyTorch from:\n    https://github.com/juglab/n2v/blob/main/n2v/nets/unet_blocks.py.\n\n    Parameters\n    ----------\n    conv_dims : int\n        Number of dimensions of the convolution layers (2 or 3).\n    num_classes : int, optional\n        Number of classes to predict, by default 1.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of downsamplings, by default 3.\n    num_channels_init : int, optional\n        Number of filters in the first convolution layer, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size of the pooling layers, by default 2.\n    final_activation : Optional[Callable], optional\n        Activation function to use for the last layer, by default None.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    independent_channels : bool\n        Whether to train the channels independently, by default True.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dims: int,\n        num_classes: int = 1,\n        in_channels: int = 1,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        pool_kernel: int = 2,\n        final_activation: Union[SupportedActivation, str] = SupportedActivation.NONE,\n        n2v2: bool = False,\n        independent_channels: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dims : int\n            Number of dimensions of the convolution layers (2 or 3).\n        num_classes : int, optional\n            Number of classes to predict, by default 1.\n        in_channels : int, optional\n            Number of input channels, by default 1.\n        depth : int, optional\n            Number of downsamplings, by default 3.\n        num_channels_init : int, optional\n            Number of filters in the first convolution layer, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        pool_kernel : int, optional\n            Kernel size of the pooling layers, by default 2.\n        final_activation : Optional[Callable], optional\n            Activation function to use for the last layer, by default None.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        independent_channels : bool\n            Whether to train parallel independent networks for each channel, by\n            default True.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n        \"\"\"\n        super().__init__()\n\n        groups = in_channels if independent_channels else 1\n\n        self.encoder = UnetEncoder(\n            conv_dims,\n            in_channels=in_channels,\n            depth=depth,\n            num_channels_init=num_channels_init,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            pool_kernel=pool_kernel,\n            n2v2=n2v2,\n            groups=groups,\n        )\n\n        self.decoder = UnetDecoder(\n            conv_dims,\n            depth=depth,\n            num_channels_init=num_channels_init,\n            use_batch_norm=use_batch_norm,\n            dropout=dropout,\n            n2v2=n2v2,\n            groups=groups,\n        )\n        self.final_conv = getattr(nn, f\"Conv{conv_dims}d\")(\n            in_channels=num_channels_init * groups,\n            out_channels=num_classes,\n            kernel_size=1,\n            groups=groups,\n        )\n        self.final_activation = get_activation(final_activation)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x :  torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output of the model.\n        \"\"\"\n        encoder_features = self.encoder(x)\n        x = self.decoder(*encoder_features)\n        x = self.final_conv(x)\n        x = self.final_activation(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet.__init__","title":"<code>__init__(conv_dims, num_classes=1, in_channels=1, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, pool_kernel=2, final_activation=SupportedActivation.NONE, n2v2=False, independent_channels=True, **kwargs)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dims</code> <code>int</code> <p>Number of dimensions of the convolution layers (2 or 3).</p> required <code>num_classes</code> <code>int</code> <p>Number of classes to predict, by default 1.</p> <code>1</code> <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of downsamplings, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of filters in the first convolution layer, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size of the pooling layers, by default 2.</p> <code>2</code> <code>final_activation</code> <code>Optional[Callable]</code> <p>Activation function to use for the last layer, by default None.</p> <code>NONE</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>independent_channels</code> <code>bool</code> <p>Whether to train parallel independent networks for each channel, by default True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dims: int,\n    num_classes: int = 1,\n    in_channels: int = 1,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    pool_kernel: int = 2,\n    final_activation: Union[SupportedActivation, str] = SupportedActivation.NONE,\n    n2v2: bool = False,\n    independent_channels: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dims : int\n        Number of dimensions of the convolution layers (2 or 3).\n    num_classes : int, optional\n        Number of classes to predict, by default 1.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of downsamplings, by default 3.\n    num_channels_init : int, optional\n        Number of filters in the first convolution layer, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size of the pooling layers, by default 2.\n    final_activation : Optional[Callable], optional\n        Activation function to use for the last layer, by default None.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    independent_channels : bool\n        Whether to train parallel independent networks for each channel, by\n        default True.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n    \"\"\"\n    super().__init__()\n\n    groups = in_channels if independent_channels else 1\n\n    self.encoder = UnetEncoder(\n        conv_dims,\n        in_channels=in_channels,\n        depth=depth,\n        num_channels_init=num_channels_init,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        pool_kernel=pool_kernel,\n        n2v2=n2v2,\n        groups=groups,\n    )\n\n    self.decoder = UnetDecoder(\n        conv_dims,\n        depth=depth,\n        num_channels_init=num_channels_init,\n        use_batch_norm=use_batch_norm,\n        dropout=dropout,\n        n2v2=n2v2,\n        groups=groups,\n    )\n    self.final_conv = getattr(nn, f\"Conv{conv_dims}d\")(\n        in_channels=num_channels_init * groups,\n        out_channels=num_classes,\n        kernel_size=1,\n        groups=groups,\n    )\n    self.final_activation = get_activation(final_activation)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UNet.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code> torch.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the model.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x :  torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output of the model.\n    \"\"\"\n    encoder_features = self.encoder(x)\n    x = self.decoder(*encoder_features)\n    x = self.final_conv(x)\n    x = self.final_activation(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder","title":"<code>UnetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unet decoder pathway.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>depth</code> <code>int</code> <p>Number of decoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UnetDecoder(nn.Module):\n    \"\"\"\n    Unet decoder pathway.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    depth : int, optional\n        Number of decoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        n2v2: bool = False,\n        groups: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n        depth : int, optional\n            Number of decoder blocks, by default 3.\n        num_channels_init : int, optional\n            Number of channels in the first encoder block, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        groups : int, optional\n            Number of blocked connections from input channels to output\n            channels, by default 1.\n        \"\"\"\n        super().__init__()\n\n        upsampling = nn.Upsample(\n            scale_factor=2, mode=\"bilinear\" if conv_dim == 2 else \"trilinear\"\n        )\n        in_channels = out_channels = num_channels_init * groups * (2 ** (depth - 1))\n\n        self.n2v2 = n2v2\n        self.groups = groups\n\n        self.bottleneck = Conv_Block(\n            conv_dim,\n            in_channels=in_channels,\n            out_channels=out_channels,\n            intermediate_channel_multiplier=2,\n            use_batch_norm=use_batch_norm,\n            dropout_perc=dropout,\n            groups=self.groups,\n        )\n\n        decoder_blocks: list[nn.Module] = []\n        for n in range(depth):\n            decoder_blocks.append(upsampling)\n            in_channels = (num_channels_init * 2 ** (depth - n)) * groups\n            out_channels = in_channels // 2\n            decoder_blocks.append(\n                Conv_Block(\n                    conv_dim,\n                    in_channels=(\n                        in_channels + in_channels // 2 if n &gt; 0 else in_channels\n                    ),\n                    out_channels=out_channels,\n                    intermediate_channel_multiplier=2,\n                    dropout_perc=dropout,\n                    activation=\"ReLU\",\n                    use_batch_norm=use_batch_norm,\n                    groups=groups,\n                )\n            )\n\n        self.decoder_blocks = nn.ModuleList(decoder_blocks)\n\n    def forward(self, *features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        *features :  list[torch.Tensor]\n            List containing the output of each encoder block(skip connections) and final\n            output of the encoder.\n\n        Returns\n        -------\n        torch.Tensor\n            Output of the decoder.\n        \"\"\"\n        x: torch.Tensor = features[0]\n        skip_connections: tuple[torch.Tensor, ...] = features[-1:0:-1]\n\n        x = self.bottleneck(x)\n\n        for i, module in enumerate(self.decoder_blocks):\n            x = module(x)\n            if isinstance(module, nn.Upsample):\n                # divide index by 2 because of upsampling layers\n                skip_connection: torch.Tensor = skip_connections[i // 2]\n                if self.n2v2:\n                    if x.shape != skip_connections[-1].shape:\n                        x = self._interleave(x, skip_connection, self.groups)\n                else:\n                    x = self._interleave(x, skip_connection, self.groups)\n        return x\n\n    @staticmethod\n    def _interleave(A: torch.Tensor, B: torch.Tensor, groups: int) -&gt; torch.Tensor:\n        \"\"\"Interleave two tensors.\n\n        Splits the tensors `A` and `B` into equally sized groups along the channel\n        axis (axis=1); then concatenates the groups in alternating order along the\n        channel axis, starting with the first group from tensor A.\n\n        Parameters\n        ----------\n        A : torch.Tensor\n            First tensor.\n        B : torch.Tensor\n            Second tensor.\n        groups : int\n            The number of groups.\n\n        Returns\n        -------\n        torch.Tensor\n            Interleaved tensor.\n\n        Raises\n        ------\n        ValueError:\n            If either of `A` or `B`'s channel axis is not divisible by `groups`.\n        \"\"\"\n        if (A.shape[1] % groups != 0) or (B.shape[1] % groups != 0):\n            raise ValueError(f\"Number of channels not divisible by {groups} groups.\")\n\n        m = A.shape[1] // groups\n        n = B.shape[1] // groups\n\n        A_groups: list[torch.Tensor] = [\n            A[:, i * m : (i + 1) * m] for i in range(groups)\n        ]\n        B_groups: list[torch.Tensor] = [\n            B[:, i * n : (i + 1) * n] for i in range(groups)\n        ]\n\n        interleaved = torch.cat(\n            [\n                tensor_list[i]\n                for i in range(groups)\n                for tensor_list in [A_groups, B_groups]\n            ],\n            dim=1,\n        )\n\n        return interleaved\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder.__init__","title":"<code>__init__(conv_dim, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, n2v2=False, groups=1)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>depth</code> <code>int</code> <p>Number of decoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    n2v2: bool = False,\n    groups: int = 1,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    depth : int, optional\n        Number of decoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n    super().__init__()\n\n    upsampling = nn.Upsample(\n        scale_factor=2, mode=\"bilinear\" if conv_dim == 2 else \"trilinear\"\n    )\n    in_channels = out_channels = num_channels_init * groups * (2 ** (depth - 1))\n\n    self.n2v2 = n2v2\n    self.groups = groups\n\n    self.bottleneck = Conv_Block(\n        conv_dim,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        intermediate_channel_multiplier=2,\n        use_batch_norm=use_batch_norm,\n        dropout_perc=dropout,\n        groups=self.groups,\n    )\n\n    decoder_blocks: list[nn.Module] = []\n    for n in range(depth):\n        decoder_blocks.append(upsampling)\n        in_channels = (num_channels_init * 2 ** (depth - n)) * groups\n        out_channels = in_channels // 2\n        decoder_blocks.append(\n            Conv_Block(\n                conv_dim,\n                in_channels=(\n                    in_channels + in_channels // 2 if n &gt; 0 else in_channels\n                ),\n                out_channels=out_channels,\n                intermediate_channel_multiplier=2,\n                dropout_perc=dropout,\n                activation=\"ReLU\",\n                use_batch_norm=use_batch_norm,\n                groups=groups,\n            )\n        )\n\n    self.decoder_blocks = nn.ModuleList(decoder_blocks)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder._interleave","title":"<code>_interleave(A, B, groups)</code>  <code>staticmethod</code>","text":"<p>Interleave two tensors.</p> <p>Splits the tensors <code>A</code> and <code>B</code> into equally sized groups along the channel axis (axis=1); then concatenates the groups in alternating order along the channel axis, starting with the first group from tensor A.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>Tensor</code> <p>First tensor.</p> required <code>B</code> <code>Tensor</code> <p>Second tensor.</p> required <code>groups</code> <code>int</code> <p>The number of groups.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Interleaved tensor.</p> <p>Raises:</p> Type Description <code>ValueError:</code> <p>If either of <code>A</code> or <code>B</code>'s channel axis is not divisible by <code>groups</code>.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>@staticmethod\ndef _interleave(A: torch.Tensor, B: torch.Tensor, groups: int) -&gt; torch.Tensor:\n    \"\"\"Interleave two tensors.\n\n    Splits the tensors `A` and `B` into equally sized groups along the channel\n    axis (axis=1); then concatenates the groups in alternating order along the\n    channel axis, starting with the first group from tensor A.\n\n    Parameters\n    ----------\n    A : torch.Tensor\n        First tensor.\n    B : torch.Tensor\n        Second tensor.\n    groups : int\n        The number of groups.\n\n    Returns\n    -------\n    torch.Tensor\n        Interleaved tensor.\n\n    Raises\n    ------\n    ValueError:\n        If either of `A` or `B`'s channel axis is not divisible by `groups`.\n    \"\"\"\n    if (A.shape[1] % groups != 0) or (B.shape[1] % groups != 0):\n        raise ValueError(f\"Number of channels not divisible by {groups} groups.\")\n\n    m = A.shape[1] // groups\n    n = B.shape[1] // groups\n\n    A_groups: list[torch.Tensor] = [\n        A[:, i * m : (i + 1) * m] for i in range(groups)\n    ]\n    B_groups: list[torch.Tensor] = [\n        B[:, i * n : (i + 1) * n] for i in range(groups)\n    ]\n\n    interleaved = torch.cat(\n        [\n            tensor_list[i]\n            for i in range(groups)\n            for tensor_list in [A_groups, B_groups]\n        ],\n        dim=1,\n    )\n\n    return interleaved\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetDecoder.forward","title":"<code>forward(*features)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>*features</code> <code> list[torch.Tensor]</code> <p>List containing the output of each encoder block(skip connections) and final output of the encoder.</p> <code>()</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the decoder.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, *features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    *features :  list[torch.Tensor]\n        List containing the output of each encoder block(skip connections) and final\n        output of the encoder.\n\n    Returns\n    -------\n    torch.Tensor\n        Output of the decoder.\n    \"\"\"\n    x: torch.Tensor = features[0]\n    skip_connections: tuple[torch.Tensor, ...] = features[-1:0:-1]\n\n    x = self.bottleneck(x)\n\n    for i, module in enumerate(self.decoder_blocks):\n        x = module(x)\n        if isinstance(module, nn.Upsample):\n            # divide index by 2 because of upsampling layers\n            skip_connection: torch.Tensor = skip_connections[i // 2]\n            if self.n2v2:\n                if x.shape != skip_connections[-1].shape:\n                    x = self._interleave(x, skip_connection, self.groups)\n            else:\n                x = self._interleave(x, skip_connection, self.groups)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder","title":"<code>UnetEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Unet encoder pathway.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of encoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size for the max pooling layers, by default 2.</p> <code>2</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>class UnetEncoder(nn.Module):\n    \"\"\"\n    Unet encoder pathway.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of encoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size for the max pooling layers, by default 2.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        conv_dim: int,\n        in_channels: int = 1,\n        depth: int = 3,\n        num_channels_init: int = 64,\n        use_batch_norm: bool = True,\n        dropout: float = 0.0,\n        pool_kernel: int = 2,\n        n2v2: bool = False,\n        groups: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        conv_dim : int\n            Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n        in_channels : int, optional\n            Number of input channels, by default 1.\n        depth : int, optional\n            Number of encoder blocks, by default 3.\n        num_channels_init : int, optional\n            Number of channels in the first encoder block, by default 64.\n        use_batch_norm : bool, optional\n            Whether to use batch normalization, by default True.\n        dropout : float, optional\n            Dropout probability, by default 0.0.\n        pool_kernel : int, optional\n            Kernel size for the max pooling layers, by default 2.\n        n2v2 : bool, optional\n            Whether to use N2V2 architecture, by default False.\n        groups : int, optional\n            Number of blocked connections from input channels to output\n            channels, by default 1.\n        \"\"\"\n        super().__init__()\n\n        self.pooling = (\n            getattr(nn, f\"MaxPool{conv_dim}d\")(kernel_size=pool_kernel)\n            if not n2v2\n            else MaxBlurPool(dim=conv_dim, kernel_size=3, max_pool_size=pool_kernel)\n        )\n\n        encoder_blocks = []\n\n        for n in range(depth):\n            out_channels = num_channels_init * (2**n) * groups\n            in_channels = in_channels if n == 0 else out_channels // 2\n            encoder_blocks.append(\n                Conv_Block(\n                    conv_dim,\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout_perc=dropout,\n                    use_batch_norm=use_batch_norm,\n                    groups=groups,\n                )\n            )\n            encoder_blocks.append(self.pooling)\n        self.encoder_blocks = nn.ModuleList(encoder_blocks)\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        list[torch.Tensor]\n            Output of each encoder block (skip connections) and final output of the\n            encoder.\n        \"\"\"\n        encoder_features = []\n        for module in self.encoder_blocks:\n            x = module(x)\n            if isinstance(module, Conv_Block):\n                encoder_features.append(x)\n        features = [x, *encoder_features]\n        return features\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder.__init__","title":"<code>__init__(conv_dim, in_channels=1, depth=3, num_channels_init=64, use_batch_norm=True, dropout=0.0, pool_kernel=2, n2v2=False, groups=1)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conv_dim</code> <code>int</code> <p>Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels, by default 1.</p> <code>1</code> <code>depth</code> <code>int</code> <p>Number of encoder blocks, by default 3.</p> <code>3</code> <code>num_channels_init</code> <code>int</code> <p>Number of channels in the first encoder block, by default 64.</p> <code>64</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization, by default True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout probability, by default 0.0.</p> <code>0.0</code> <code>pool_kernel</code> <code>int</code> <p>Kernel size for the max pooling layers, by default 2.</p> <code>2</code> <code>n2v2</code> <code>bool</code> <p>Whether to use N2V2 architecture, by default False.</p> <code>False</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels, by default 1.</p> <code>1</code> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def __init__(\n    self,\n    conv_dim: int,\n    in_channels: int = 1,\n    depth: int = 3,\n    num_channels_init: int = 64,\n    use_batch_norm: bool = True,\n    dropout: float = 0.0,\n    pool_kernel: int = 2,\n    n2v2: bool = False,\n    groups: int = 1,\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    conv_dim : int\n        Number of dimension of the convolution layers, 2 for 2D or 3 for 3D.\n    in_channels : int, optional\n        Number of input channels, by default 1.\n    depth : int, optional\n        Number of encoder blocks, by default 3.\n    num_channels_init : int, optional\n        Number of channels in the first encoder block, by default 64.\n    use_batch_norm : bool, optional\n        Whether to use batch normalization, by default True.\n    dropout : float, optional\n        Dropout probability, by default 0.0.\n    pool_kernel : int, optional\n        Kernel size for the max pooling layers, by default 2.\n    n2v2 : bool, optional\n        Whether to use N2V2 architecture, by default False.\n    groups : int, optional\n        Number of blocked connections from input channels to output\n        channels, by default 1.\n    \"\"\"\n    super().__init__()\n\n    self.pooling = (\n        getattr(nn, f\"MaxPool{conv_dim}d\")(kernel_size=pool_kernel)\n        if not n2v2\n        else MaxBlurPool(dim=conv_dim, kernel_size=3, max_pool_size=pool_kernel)\n    )\n\n    encoder_blocks = []\n\n    for n in range(depth):\n        out_channels = num_channels_init * (2**n) * groups\n        in_channels = in_channels if n == 0 else out_channels // 2\n        encoder_blocks.append(\n            Conv_Block(\n                conv_dim,\n                in_channels=in_channels,\n                out_channels=out_channels,\n                dropout_perc=dropout,\n                use_batch_norm=use_batch_norm,\n                groups=groups,\n            )\n        )\n        encoder_blocks.append(self.pooling)\n    self.encoder_blocks = nn.ModuleList(encoder_blocks)\n</code></pre>"},{"location":"reference/careamics/models/unet/#careamics.models.unet.UnetEncoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>Output of each encoder block (skip connections) and final output of the encoder.</p> Source code in <code>src/careamics/models/unet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    list[torch.Tensor]\n        Output of each encoder block (skip connections) and final output of the\n        encoder.\n    \"\"\"\n    encoder_features = []\n    for module in self.encoder_blocks:\n        x = module(x)\n        if isinstance(module, Conv_Block):\n            encoder_features.append(x)\n    features = [x, *encoder_features]\n    return features\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/","title":"layers","text":"<p>Script containing the common basic blocks (nn.Module) reused by the LadderVAE.</p>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpDeterministicResBlock","title":"<code>BottomUpDeterministicResBlock</code>","text":"<p>               Bases: <code>ResBlockWithResampling</code></p> <p>Resnet block for bottom-up deterministic layers.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class BottomUpDeterministicResBlock(ResBlockWithResampling):\n    \"\"\"Resnet block for bottom-up deterministic layers.\"\"\"\n\n    def __init__(self, *args, downsample: bool = False, **kwargs):\n        kwargs[\"resample\"] = downsample\n        super().__init__(\"bottom-up\", *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer","title":"<code>BottomUpLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Bottom-up deterministic layer.</p> <p>It consists of one or a stack of <code>BottomUpDeterministicResBlock</code>'s. The outputs are the so-called <code>bu_values</code> that are later used in the Decoder to update the generative distributions.</p> <p>NOTE: When Lateral Contextualization is Enabled (i.e., <code>enable_multiscale=True</code>), the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB) (without downsampling), and then merged to the latent tensor produced by the primary flow of the <code>BottomUpLayer</code> through the <code>MergeLowRes</code> layer. It is meaningful to remark that the BUDB that takes care of encoding the low-res input can be either shared with the primary flow (and in that case it is the \"same_size\" BUDB (or stack of BUDBs) -&gt; see <code>self.net</code>), or can be a deep-copy of the primary flow's BUDB. This behaviour is controlled by <code>lowres_separate_branch</code> parameter.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class BottomUpLayer(nn.Module):\n    \"\"\"\n    Bottom-up deterministic layer.\n\n    It consists of one or a stack of `BottomUpDeterministicResBlock`'s.\n    The outputs are the so-called `bu_values` that are later used in the Decoder to update the\n    generative distributions.\n\n    NOTE: When Lateral Contextualization is Enabled (i.e., `enable_multiscale=True`),\n    the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB)\n    (without downsampling), and then merged to the latent tensor produced by the primary flow\n    of the `BottomUpLayer` through the `MergeLowRes` layer. It is meaningful to remark that\n    the BUDB that takes care of encoding the low-res input can be either shared with the\n    primary flow (and in that case it is the \"same_size\" BUDB (or stack of BUDBs) -&gt; see `self.net`),\n    or can be a deep-copy of the primary flow's BUDB.\n    This behaviour is controlled by `lowres_separate_branch` parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_res_blocks: int,\n        n_filters: int,\n        conv_strides: tuple[int] = (2, 2),\n        downsampling_steps: int = 0,\n        nonlin: Optional[Callable] = None,\n        batchnorm: bool = True,\n        dropout: Optional[float] = None,\n        res_block_type: Optional[str] = None,\n        res_block_kernel: Optional[int] = None,\n        gated: Optional[bool] = None,\n        enable_multiscale: bool = False,\n        multiscale_lowres_size_factor: Optional[int] = None,\n        lowres_separate_branch: bool = False,\n        multiscale_retain_spatial_dims: bool = False,\n        decoder_retain_spatial_dims: bool = False,\n        output_expected_shape: Optional[Iterable[int]] = None,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        n_res_blocks: int\n            Number of `BottomUpDeterministicResBlock` modules stacked in this layer.\n        n_filters: int\n            Number of channels present through out the layers of this block.\n        downsampling_steps: int, optional\n            Number of downsampling steps that has to be done in this layer (typically 1).\n            Default is 0.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `None`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        enable_multiscale: bool, optional\n            Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.\n        multiscale_lowres_size_factor: int, optional\n            A factor the expresses the relative size of the primary flow tensor with respect to the\n            lower-resolution lateral input tensor. Default in `None`.\n        lowres_separate_branch: bool, optional\n            Whether the residual block(s) encoding the low-res input should be shared (`False`) or\n            not (`True`) with the primary flow \"same-size\" residual block(s). Default is `False`.\n        multiscale_retain_spatial_dims: bool, optional\n            Whether to pad the latent tensor resulting from the bottom-up layer's primary flow\n            to match the size of the low-res input. Default is `False`.\n        decoder_retain_spatial_dims: bool, optional\n            Whether in the corresponding top-down layer the shape of tensor is retained between\n            input and output. Default is `False`.\n        output_expected_shape: Iterable[int], optional\n            The expected shape of the layer output (only used if `enable_multiscale == True`).\n            Default is `None`.\n        \"\"\"\n        super().__init__()\n\n        # Define attributes for Lateral Contextualization\n        self.enable_multiscale = enable_multiscale\n        self.lowres_separate_branch = lowres_separate_branch\n        self.multiscale_retain_spatial_dims = multiscale_retain_spatial_dims\n        self.multiscale_lowres_size_factor = multiscale_lowres_size_factor\n        self.decoder_retain_spatial_dims = decoder_retain_spatial_dims\n        self.output_expected_shape = output_expected_shape\n        assert self.output_expected_shape is None or self.enable_multiscale is True\n\n        bu_blocks_downsized = []\n        bu_blocks_samesize = []\n        for _ in range(n_res_blocks):\n            do_resample = False\n            if downsampling_steps &gt; 0:\n                do_resample = True\n                downsampling_steps -= 1\n            block = BottomUpDeterministicResBlock(\n                conv_strides=conv_strides,\n                c_in=n_filters,\n                c_out=n_filters,\n                nonlin=nonlin,\n                downsample=do_resample,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                gated=gated,\n            )\n            if do_resample:\n                bu_blocks_downsized.append(block)\n            else:\n                bu_blocks_samesize.append(block)\n\n        self.net_downsized = nn.Sequential(*bu_blocks_downsized)\n        self.net = nn.Sequential(*bu_blocks_samesize)\n\n        # Using the same net for the low resolution (and larger sized image)\n        self.lowres_net = self.lowres_merge = None\n        if self.enable_multiscale:\n            self._init_multiscale(\n                n_filters=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n            )\n\n        # msg = f'[{self.__class__.__name__}] McEnabled:{int(enable_multiscale)} '\n        # if enable_multiscale:\n        #     msg += f'McParallelBeam:{int(multiscale_retain_spatial_dims)} McFactor{multiscale_lowres_size_factor}'\n        # print(msg)\n\n    def _init_multiscale(\n        self,\n        nonlin: Callable = None,\n        n_filters: int = None,\n        conv_strides: tuple[int] = (2, 2),\n        batchnorm: bool = None,\n        dropout: float = None,\n        res_block_type: str = None,\n    ) -&gt; None:\n        \"\"\"\n        Bottom-up layer's method that initializes the LC modules.\n\n        Defines the modules responsible of merging compressed lateral inputs to the\n        outputs of the primary flow at different hierarchical levels in the\n        multiresolution approach (LC). Specifically, the method initializes `lowres_net`\n        , which is a stack of `BottomUpDeterministicBlock`'s (w/out downsampling) that\n        takes care of additionally processing the low-res input, and `lowres_merge`,\n        which is the module responsible of merging the compressed lateral input to the\n        main flow.\n\n        NOTE: The merge modality is set by default to \"residual\", meaning that the\n        merge layer performs concatenation on dim=1, followed by 1x1 convolution and\n        a Residual Gated block.\n\n        Parameters\n        ----------\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `None`.\n        n_filters: int\n            Number of channels present through out the layers of this block.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        \"\"\"\n        self.lowres_net = self.net\n        if self.lowres_separate_branch:\n            self.lowres_net = deepcopy(self.net)\n\n        self.lowres_merge = MergeLowRes(\n            channels=n_filters,\n            conv_strides=conv_strides,\n            merge_type=\"residual\",\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n            multiscale_lowres_size_factor=self.multiscale_lowres_size_factor,\n        )\n\n    def forward(\n        self, x: torch.Tensor, lowres_x: Union[torch.Tensor, None] = None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input of the `BottomUpLayer`, i.e., the input image or the output of the\n            previous layer.\n        lowres_x: torch.Tensor, optional\n            The low-res input used for Lateral Contextualization (LC). Default is `None`.\n\n        NOTE: first returned tensor is used as input for the next BU layer, while the second\n        tensor is the bu_value passed to the top-down layer.\n        \"\"\"\n        # The input is fed through the residual downsampling block(s)\n        primary_flow = self.net_downsized(x)\n        # The downsampling output is fed through additional residual block(s)\n        primary_flow = self.net(primary_flow)\n\n        # If LC is not used, simply return output of primary-flow\n        if self.enable_multiscale is False:\n            assert lowres_x is None\n            return primary_flow, primary_flow\n\n        if lowres_x is not None:\n            # First encode the low-res lateral input\n            lowres_flow = self.lowres_net(lowres_x)\n            # Then pass the result through the MergeLowRes layer\n            merged = self.lowres_merge(primary_flow, lowres_flow)\n        else:\n            merged = primary_flow\n\n        # NOTE: Explanation of possible cases for the conditionals:\n        # - if both are `True` -&gt; `merged` has the same spatial dims as the input (`x`) since\n        #   spatial dims are retained by padding `primary_flow` in `MergeLowRes`. This is\n        #   OK for the corresp TopDown layer, as it also retains spatial dims.\n        # - if both are `False` -&gt; `merged`'s spatial dims are equal to `self.net_downsized(x)`,\n        #   since no padding is done in `MergeLowRes` and, instead, the lowres input is cropped.\n        #   This is OK for the corresp TopDown layer, as it also halves the spatial dims.\n        # - if 1st is `False` and 2nd is `True` -&gt; not a concern, it cannot happen\n        #   (see lvae.py, line 111, intialization of `multiscale_decoder_retain_spatial_dims`).\n        if (\n            self.multiscale_retain_spatial_dims is False\n            or self.decoder_retain_spatial_dims is True\n        ):\n            return merged, merged\n\n        # NOTE: if we reach here, it means that `multiscale_retain_spatial_dims` is `True`,\n        # but `decoder_retain_spatial_dims` is `False`, meaning that merging LC preserves\n        # the spatial dimensions, but at the same time we don't want to retain the spatial\n        # dims in the corresponding top-down layer. Therefore, we need to crop the tensor.\n        if self.output_expected_shape is not None:\n            expected_shape = self.output_expected_shape\n        else:\n            fac = self.multiscale_lowres_size_factor\n            expected_shape = (merged.shape[-2] // fac, merged.shape[-1] // fac)\n            assert merged.shape[-2:] != expected_shape\n\n        # Crop the resulting tensor so that it matches with the Decoder\n        value_to_use_in_topdown = crop_img_tensor(merged, expected_shape)\n        return merged, value_to_use_in_topdown\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer.__init__","title":"<code>__init__(n_res_blocks, n_filters, conv_strides=(2, 2), downsampling_steps=0, nonlin=None, batchnorm=True, dropout=None, res_block_type=None, res_block_kernel=None, gated=None, enable_multiscale=False, multiscale_lowres_size_factor=None, lowres_separate_branch=False, multiscale_retain_spatial_dims=False, decoder_retain_spatial_dims=False, output_expected_shape=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n_res_blocks</code> <code>int</code> <p>Number of <code>BottomUpDeterministicResBlock</code> modules stacked in this layer.</p> required <code>n_filters</code> <code>int</code> <p>Number of channels present through out the layers of this block.</p> required <code>downsampling_steps</code> <code>int</code> <p>Number of downsampling steps that has to be done in this layer (typically 1). Default is 0.</p> <code>0</code> <code>nonlin</code> <code>Optional[Callable]</code> <p>The non-linearity function used in the block. Default is <code>None</code>.</p> <code>None</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Optional[float]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>res_block_type</code> <code>Optional[str]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Optional[int]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>Optional[bool]</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>enable_multiscale</code> <code>bool</code> <p>Whether to enable multiscale (Lateral Contextualization) or not. Default is <code>False</code>.</p> <code>False</code> <code>multiscale_lowres_size_factor</code> <code>Optional[int]</code> <p>A factor the expresses the relative size of the primary flow tensor with respect to the lower-resolution lateral input tensor. Default in <code>None</code>.</p> <code>None</code> <code>lowres_separate_branch</code> <code>bool</code> <p>Whether the residual block(s) encoding the low-res input should be shared (<code>False</code>) or not (<code>True</code>) with the primary flow \"same-size\" residual block(s). Default is <code>False</code>.</p> <code>False</code> <code>multiscale_retain_spatial_dims</code> <code>bool</code> <p>Whether to pad the latent tensor resulting from the bottom-up layer's primary flow to match the size of the low-res input. Default is <code>False</code>.</p> <code>False</code> <code>decoder_retain_spatial_dims</code> <code>bool</code> <p>Whether in the corresponding top-down layer the shape of tensor is retained between input and output. Default is <code>False</code>.</p> <code>False</code> <code>output_expected_shape</code> <code>Optional[Iterable[int]]</code> <p>The expected shape of the layer output (only used if <code>enable_multiscale == True</code>). Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    n_res_blocks: int,\n    n_filters: int,\n    conv_strides: tuple[int] = (2, 2),\n    downsampling_steps: int = 0,\n    nonlin: Optional[Callable] = None,\n    batchnorm: bool = True,\n    dropout: Optional[float] = None,\n    res_block_type: Optional[str] = None,\n    res_block_kernel: Optional[int] = None,\n    gated: Optional[bool] = None,\n    enable_multiscale: bool = False,\n    multiscale_lowres_size_factor: Optional[int] = None,\n    lowres_separate_branch: bool = False,\n    multiscale_retain_spatial_dims: bool = False,\n    decoder_retain_spatial_dims: bool = False,\n    output_expected_shape: Optional[Iterable[int]] = None,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    n_res_blocks: int\n        Number of `BottomUpDeterministicResBlock` modules stacked in this layer.\n    n_filters: int\n        Number of channels present through out the layers of this block.\n    downsampling_steps: int, optional\n        Number of downsampling steps that has to be done in this layer (typically 1).\n        Default is 0.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `None`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    enable_multiscale: bool, optional\n        Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.\n    multiscale_lowres_size_factor: int, optional\n        A factor the expresses the relative size of the primary flow tensor with respect to the\n        lower-resolution lateral input tensor. Default in `None`.\n    lowres_separate_branch: bool, optional\n        Whether the residual block(s) encoding the low-res input should be shared (`False`) or\n        not (`True`) with the primary flow \"same-size\" residual block(s). Default is `False`.\n    multiscale_retain_spatial_dims: bool, optional\n        Whether to pad the latent tensor resulting from the bottom-up layer's primary flow\n        to match the size of the low-res input. Default is `False`.\n    decoder_retain_spatial_dims: bool, optional\n        Whether in the corresponding top-down layer the shape of tensor is retained between\n        input and output. Default is `False`.\n    output_expected_shape: Iterable[int], optional\n        The expected shape of the layer output (only used if `enable_multiscale == True`).\n        Default is `None`.\n    \"\"\"\n    super().__init__()\n\n    # Define attributes for Lateral Contextualization\n    self.enable_multiscale = enable_multiscale\n    self.lowres_separate_branch = lowres_separate_branch\n    self.multiscale_retain_spatial_dims = multiscale_retain_spatial_dims\n    self.multiscale_lowres_size_factor = multiscale_lowres_size_factor\n    self.decoder_retain_spatial_dims = decoder_retain_spatial_dims\n    self.output_expected_shape = output_expected_shape\n    assert self.output_expected_shape is None or self.enable_multiscale is True\n\n    bu_blocks_downsized = []\n    bu_blocks_samesize = []\n    for _ in range(n_res_blocks):\n        do_resample = False\n        if downsampling_steps &gt; 0:\n            do_resample = True\n            downsampling_steps -= 1\n        block = BottomUpDeterministicResBlock(\n            conv_strides=conv_strides,\n            c_in=n_filters,\n            c_out=n_filters,\n            nonlin=nonlin,\n            downsample=do_resample,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            gated=gated,\n        )\n        if do_resample:\n            bu_blocks_downsized.append(block)\n        else:\n            bu_blocks_samesize.append(block)\n\n    self.net_downsized = nn.Sequential(*bu_blocks_downsized)\n    self.net = nn.Sequential(*bu_blocks_samesize)\n\n    # Using the same net for the low resolution (and larger sized image)\n    self.lowres_net = self.lowres_merge = None\n    if self.enable_multiscale:\n        self._init_multiscale(\n            n_filters=n_filters,\n            conv_strides=conv_strides,\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer._init_multiscale","title":"<code>_init_multiscale(nonlin=None, n_filters=None, conv_strides=(2, 2), batchnorm=None, dropout=None, res_block_type=None)</code>","text":"<p>Bottom-up layer's method that initializes the LC modules.</p> <p>Defines the modules responsible of merging compressed lateral inputs to the outputs of the primary flow at different hierarchical levels in the multiresolution approach (LC). Specifically, the method initializes <code>lowres_net</code> , which is a stack of <code>BottomUpDeterministicBlock</code>'s (w/out downsampling) that takes care of additionally processing the low-res input, and <code>lowres_merge</code>, which is the module responsible of merging the compressed lateral input to the main flow.</p> <p>NOTE: The merge modality is set by default to \"residual\", meaning that the merge layer performs concatenation on dim=1, followed by 1x1 convolution and a Residual Gated block.</p> <p>Parameters:</p> Name Type Description Default <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block. Default is <code>None</code>.</p> <code>None</code> <code>n_filters</code> <code>int</code> <p>Number of channels present through out the layers of this block.</p> <code>None</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>res_block_type</code> <code>str</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def _init_multiscale(\n    self,\n    nonlin: Callable = None,\n    n_filters: int = None,\n    conv_strides: tuple[int] = (2, 2),\n    batchnorm: bool = None,\n    dropout: float = None,\n    res_block_type: str = None,\n) -&gt; None:\n    \"\"\"\n    Bottom-up layer's method that initializes the LC modules.\n\n    Defines the modules responsible of merging compressed lateral inputs to the\n    outputs of the primary flow at different hierarchical levels in the\n    multiresolution approach (LC). Specifically, the method initializes `lowres_net`\n    , which is a stack of `BottomUpDeterministicBlock`'s (w/out downsampling) that\n    takes care of additionally processing the low-res input, and `lowres_merge`,\n    which is the module responsible of merging the compressed lateral input to the\n    main flow.\n\n    NOTE: The merge modality is set by default to \"residual\", meaning that the\n    merge layer performs concatenation on dim=1, followed by 1x1 convolution and\n    a Residual Gated block.\n\n    Parameters\n    ----------\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `None`.\n    n_filters: int\n        Number of channels present through out the layers of this block.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    \"\"\"\n    self.lowres_net = self.net\n    if self.lowres_separate_branch:\n        self.lowres_net = deepcopy(self.net)\n\n    self.lowres_merge = MergeLowRes(\n        channels=n_filters,\n        conv_strides=conv_strides,\n        merge_type=\"residual\",\n        nonlin=nonlin,\n        batchnorm=batchnorm,\n        dropout=dropout,\n        res_block_type=res_block_type,\n        multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n        multiscale_lowres_size_factor=self.multiscale_lowres_size_factor,\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.BottomUpLayer.forward","title":"<code>forward(x, lowres_x=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input of the <code>BottomUpLayer</code>, i.e., the input image or the output of the previous layer.</p> required <code>lowres_x</code> <code>Union[Tensor, None]</code> <p>The low-res input used for Lateral Contextualization (LC). Default is <code>None</code>.</p> <code>None</code> <code>NOTE</code> required <code>tensor</code> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, lowres_x: Union[torch.Tensor, None] = None\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input of the `BottomUpLayer`, i.e., the input image or the output of the\n        previous layer.\n    lowres_x: torch.Tensor, optional\n        The low-res input used for Lateral Contextualization (LC). Default is `None`.\n\n    NOTE: first returned tensor is used as input for the next BU layer, while the second\n    tensor is the bu_value passed to the top-down layer.\n    \"\"\"\n    # The input is fed through the residual downsampling block(s)\n    primary_flow = self.net_downsized(x)\n    # The downsampling output is fed through additional residual block(s)\n    primary_flow = self.net(primary_flow)\n\n    # If LC is not used, simply return output of primary-flow\n    if self.enable_multiscale is False:\n        assert lowres_x is None\n        return primary_flow, primary_flow\n\n    if lowres_x is not None:\n        # First encode the low-res lateral input\n        lowres_flow = self.lowres_net(lowres_x)\n        # Then pass the result through the MergeLowRes layer\n        merged = self.lowres_merge(primary_flow, lowres_flow)\n    else:\n        merged = primary_flow\n\n    # NOTE: Explanation of possible cases for the conditionals:\n    # - if both are `True` -&gt; `merged` has the same spatial dims as the input (`x`) since\n    #   spatial dims are retained by padding `primary_flow` in `MergeLowRes`. This is\n    #   OK for the corresp TopDown layer, as it also retains spatial dims.\n    # - if both are `False` -&gt; `merged`'s spatial dims are equal to `self.net_downsized(x)`,\n    #   since no padding is done in `MergeLowRes` and, instead, the lowres input is cropped.\n    #   This is OK for the corresp TopDown layer, as it also halves the spatial dims.\n    # - if 1st is `False` and 2nd is `True` -&gt; not a concern, it cannot happen\n    #   (see lvae.py, line 111, intialization of `multiscale_decoder_retain_spatial_dims`).\n    if (\n        self.multiscale_retain_spatial_dims is False\n        or self.decoder_retain_spatial_dims is True\n    ):\n        return merged, merged\n\n    # NOTE: if we reach here, it means that `multiscale_retain_spatial_dims` is `True`,\n    # but `decoder_retain_spatial_dims` is `False`, meaning that merging LC preserves\n    # the spatial dimensions, but at the same time we don't want to retain the spatial\n    # dims in the corresponding top-down layer. Therefore, we need to crop the tensor.\n    if self.output_expected_shape is not None:\n        expected_shape = self.output_expected_shape\n    else:\n        fac = self.multiscale_lowres_size_factor\n        expected_shape = (merged.shape[-2] // fac, merged.shape[-1] // fac)\n        assert merged.shape[-2:] != expected_shape\n\n    # Crop the resulting tensor so that it matches with the Decoder\n    value_to_use_in_topdown = crop_img_tensor(merged, expected_shape)\n    return merged, value_to_use_in_topdown\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.GateLayer","title":"<code>GateLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Layer class that implements a gating mechanism.</p> <p>Double the number of channels through a convolutional layer, then use half the channels as gate for the other half.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class GateLayer(nn.Module):\n    \"\"\"\n    Layer class that implements a gating mechanism.\n\n    Double the number of channels through a convolutional layer, then use\n    half the channels as gate for the other half.\n    \"\"\"\n\n    def __init__(\n        self,\n        channels: int,\n        conv_strides: tuple[int] = (2, 2),\n        kernel_size: int = 3,\n        nonlin: Callable = nn.LeakyReLU(),\n    ):\n        super().__init__()\n        assert kernel_size % 2 == 1\n        pad = kernel_size // 2\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        self.conv = conv_layer(channels, 2 * channels, kernel_size, padding=pad)\n        self.nonlin = nonlin\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output # TODO add shape\n        \"\"\"\n        x = self.conv(x)\n        x, gate = torch.chunk(x, 2, dim=1)\n        x = self.nonlin(x)  # TODO remove this?\n        gate = torch.sigmoid(gate)\n        return x * gate\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.GateLayer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output # TODO add shape\n    \"\"\"\n    x = self.conv(x)\n    x, gate = torch.chunk(x, 2, dim=1)\n    x = self.nonlin(x)  # TODO remove this?\n    gate = torch.sigmoid(gate)\n    return x * gate\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLayer","title":"<code>MergeLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Layer class that merges two or more input tensors.</p> <p>Merges two or more (B, C, [Z], Y, X) input tensors by concatenating them along dim=1 and passes the result through: a) a convolutional 1x1 layer (<code>merge_type == \"linear\"</code>), or b) a convolutional 1x1 layer and then a gated residual block (<code>merge_type == \"residual\"</code>), or c) a convolutional 1x1 layer and then an ungated residual block (<code>merge_type == \"residual_ungated\"</code>).</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class MergeLayer(nn.Module):\n    \"\"\"\n    Layer class that merges two or more input tensors.\n\n    Merges two or more (B, C, [Z], Y, X) input tensors by concatenating\n    them along dim=1 and passes the result through:\n    a) a convolutional 1x1 layer (`merge_type == \"linear\"`), or\n    b) a convolutional 1x1 layer and then a gated residual block (`merge_type == \"residual\"`), or\n    c) a convolutional 1x1 layer and then an ungated residual block (`merge_type == \"residual_ungated\"`).\n    \"\"\"\n\n    def __init__(\n        self,\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"],\n        channels: Union[int, Iterable[int]],\n        conv_strides: tuple[int] = (2, 2),\n        nonlin: Callable = nn.LeakyReLU(),\n        batchnorm: bool = True,\n        dropout: Optional[float] = None,\n        res_block_type: Optional[str] = None,\n        res_block_kernel: Optional[int] = None,\n        conv2d_bias: Optional[bool] = True,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n            The type of merge done in the layer. It can be chosen between \"linear\",\n            \"residual\", and \"residual_ungated\". Check the class docstring for more\n            information about the behaviour of different merge modalities.\n        channels: Union[int, Iterable[int]]\n            The number of channels used in the convolutional blocks of this layer.\n            If it is an `int`:\n                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n                - (Optional) ResBlock: in_channels=channels, out_channels=channels\n            If it is an Iterable (must have `len(channels)==3`):\n                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),\n                out_channels=channels[-1]\n                - (Optional) ResBlock: in_channels=channels[-1],\n                out_channels=channels[-1]\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared\n            kernel.\n            Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n        try:\n            iter(channels)\n        except TypeError:  # it is not iterable\n            channels = [channels] * 3\n        else:  # it is iterable\n            if len(channels) == 1:\n                channels = [channels[0]] * 3\n\n        self.conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n\n        if merge_type == \"linear\":\n            self.layer = self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, bias=conv2d_bias\n            )\n        elif merge_type == \"residual\":\n            self.layer = nn.Sequential(\n                self.conv_layer(\n                    sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n                ),\n                ResidualGatedBlock(\n                    conv_strides=conv_strides,\n                    channels=channels[-1],\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    block_type=res_block_type,\n                    kernel=res_block_kernel,\n                    conv2d_bias=conv2d_bias,\n                ),\n            )\n        elif merge_type == \"residual_ungated\":\n            self.layer = nn.Sequential(\n                self.conv_layer(\n                    sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n                ),\n                ResidualBlock(\n                    conv_strides=conv_strides,\n                    channels=channels[-1],\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    block_type=res_block_type,\n                    kernel=res_block_kernel,\n                    conv2d_bias=conv2d_bias,\n                ),\n            )\n\n    def forward(self, *args) -&gt; torch.Tensor:\n\n        # Concatenate the input tensors along dim=1\n        x = torch.cat(args, dim=1)\n\n        # Pass the concatenated tensor through the conv layer\n        x = self.layer(x)\n\n        return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLayer.__init__","title":"<code>__init__(merge_type, channels, conv_strides=(2, 2), nonlin=nn.LeakyReLU(), batchnorm=True, dropout=None, res_block_type=None, res_block_kernel=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>merge_type</code> <code>Literal['linear', 'residual', 'residual_ungated']</code> <p>The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\". Check the class docstring for more information about the behaviour of different merge modalities.</p> required <code>channels</code> <code>Union[int, Iterable[int]]</code> <p>The number of channels used in the convolutional blocks of this layer. If it is an <code>int</code>:     - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels     - (Optional) ResBlock: in_channels=channels, out_channels=channels If it is an Iterable (must have <code>len(channels)==3</code>):     - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),     out_channels=channels[-1]     - (Optional) ResBlock: in_channels=channels[-1],     out_channels=channels[-1]</p> required <code>conv_strides</code> <code>tuple[int]</code> <p>The strides used in the convolutions. Default is <code>(2, 2)</code>.</p> <code>(2, 2)</code> <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> <code>LeakyReLU()</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Optional[float]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>res_block_type</code> <code>Optional[str]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Optional[int]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>Optional[bool]</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"],\n    channels: Union[int, Iterable[int]],\n    conv_strides: tuple[int] = (2, 2),\n    nonlin: Callable = nn.LeakyReLU(),\n    batchnorm: bool = True,\n    dropout: Optional[float] = None,\n    res_block_type: Optional[str] = None,\n    res_block_kernel: Optional[int] = None,\n    conv2d_bias: Optional[bool] = True,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n        The type of merge done in the layer. It can be chosen between \"linear\",\n        \"residual\", and \"residual_ungated\". Check the class docstring for more\n        information about the behaviour of different merge modalities.\n    channels: Union[int, Iterable[int]]\n        The number of channels used in the convolutional blocks of this layer.\n        If it is an `int`:\n            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n            - (Optional) ResBlock: in_channels=channels, out_channels=channels\n        If it is an Iterable (must have `len(channels)==3`):\n            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]),\n            out_channels=channels[-1]\n            - (Optional) ResBlock: in_channels=channels[-1],\n            out_channels=channels[-1]\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared\n        kernel.\n        Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n    try:\n        iter(channels)\n    except TypeError:  # it is not iterable\n        channels = [channels] * 3\n    else:  # it is iterable\n        if len(channels) == 1:\n            channels = [channels[0]] * 3\n\n    self.conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n\n    if merge_type == \"linear\":\n        self.layer = self.conv_layer(\n            sum(channels[:-1]), channels[-1], 1, bias=conv2d_bias\n        )\n    elif merge_type == \"residual\":\n        self.layer = nn.Sequential(\n            self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n            ),\n            ResidualGatedBlock(\n                conv_strides=conv_strides,\n                channels=channels[-1],\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                block_type=res_block_type,\n                kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            ),\n        )\n    elif merge_type == \"residual_ungated\":\n        self.layer = nn.Sequential(\n            self.conv_layer(\n                sum(channels[:-1]), channels[-1], 1, padding=0, bias=conv2d_bias\n            ),\n            ResidualBlock(\n                conv_strides=conv_strides,\n                channels=channels[-1],\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                block_type=res_block_type,\n                kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            ),\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLowRes","title":"<code>MergeLowRes</code>","text":"<p>               Bases: <code>MergeLayer</code></p> <p>Child class of <code>MergeLayer</code>.</p> <p>Specifically designed to merge the low-resolution patches that are used in Lateral Contextualization approach.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class MergeLowRes(MergeLayer):\n    \"\"\"\n    Child class of `MergeLayer`.\n\n    Specifically designed to merge the low-resolution patches\n    that are used in Lateral Contextualization approach.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.retain_spatial_dims = kwargs.pop(\"multiscale_retain_spatial_dims\")\n        self.multiscale_lowres_size_factor = kwargs.pop(\"multiscale_lowres_size_factor\")\n        super().__init__(*args, **kwargs)\n\n    def forward(self, latent: torch.Tensor, lowres: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        latent: torch.Tensor\n            The output latent tensor from previous layer in the LVAE hierarchy.\n        lowres: torch.Tensor\n            The low-res patch image to be merged to increase the context.\n        \"\"\"\n        # TODO: treat (X, Y) and Z differently (e.g., line 762)\n        if self.retain_spatial_dims:\n            # Pad latent tensor to match lowres tensor's shape\n            # Output.shape == Lowres.shape (== Input.shape),\n            # where Input is the input to the BU layer\n            latent = pad_img_tensor(latent, lowres.shape[2:])\n        else:\n            # Crop lowres tensor to match latent tensor's shape\n            lz, ly, lx = lowres.shape[2:]\n            z = lz // self.multiscale_lowres_size_factor\n            y = ly // self.multiscale_lowres_size_factor\n            x = lx // self.multiscale_lowres_size_factor\n            z_pad = (lz - z) // 2\n            y_pad = (ly - y) // 2\n            x_pad = (lx - x) // 2\n            lowres = lowres[:, :, z_pad:-z_pad, y_pad:-y_pad, x_pad:-x_pad]\n\n        return super().forward(latent, lowres)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.MergeLowRes.forward","title":"<code>forward(latent, lowres)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>latent</code> <code>Tensor</code> <p>The output latent tensor from previous layer in the LVAE hierarchy.</p> required <code>lowres</code> <code>Tensor</code> <p>The low-res patch image to be merged to increase the context.</p> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, latent: torch.Tensor, lowres: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    latent: torch.Tensor\n        The output latent tensor from previous layer in the LVAE hierarchy.\n    lowres: torch.Tensor\n        The low-res patch image to be merged to increase the context.\n    \"\"\"\n    # TODO: treat (X, Y) and Z differently (e.g., line 762)\n    if self.retain_spatial_dims:\n        # Pad latent tensor to match lowres tensor's shape\n        # Output.shape == Lowres.shape (== Input.shape),\n        # where Input is the input to the BU layer\n        latent = pad_img_tensor(latent, lowres.shape[2:])\n    else:\n        # Crop lowres tensor to match latent tensor's shape\n        lz, ly, lx = lowres.shape[2:]\n        z = lz // self.multiscale_lowres_size_factor\n        y = ly // self.multiscale_lowres_size_factor\n        x = lx // self.multiscale_lowres_size_factor\n        z_pad = (lz - z) // 2\n        y_pad = (ly - y) // 2\n        x_pad = (lx - x) // 2\n        lowres = lowres[:, :, z_pad:-z_pad, y_pad:-y_pad, x_pad:-x_pad]\n\n    return super().forward(latent, lowres)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling","title":"<code>ResBlockWithResampling</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual block with resampling.</p> <p>Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2). It is structured as follows:     1. <code>pre_conv</code>: a downsampling or upsampling strided convolutional layer in case of resampling, or         a 1x1 convolutional layer that maps the number of channels of the input to <code>inner_channels</code>.     2. <code>ResidualBlock</code>     3. <code>post_conv</code>: a 1x1 convolutional layer that maps the number of channels to <code>c_out</code>.</p> <p>Some implementation notes: - Resampling is performed through a strided convolution layer at the beginning of the block. - The strided convolution block has fixed kernel size of 3x3 and 1 layer of padding with zeros. - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers. - The number of internal channels is by default the same as the number of output channels, but   min_inner_channels can override the behaviour.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResBlockWithResampling(nn.Module):\n    \"\"\"\n    Residual block with resampling.\n\n    Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2).\n    It is structured as follows:\n        1. `pre_conv`: a downsampling or upsampling strided convolutional layer in case of resampling, or\n            a 1x1 convolutional layer that maps the number of channels of the input to `inner_channels`.\n        2. `ResidualBlock`\n        3. `post_conv`: a 1x1 convolutional layer that maps the number of channels to `c_out`.\n\n    Some implementation notes:\n    - Resampling is performed through a strided convolution layer at the beginning of the block.\n    - The strided convolution block has fixed kernel size of 3x3 and 1 layer of padding with zeros.\n    - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers.\n    - The number of internal channels is by default the same as the number of output channels, but\n      min_inner_channels can override the behaviour.\n    \"\"\"\n\n    def __init__(\n        self,\n        mode: Literal[\"top-down\", \"bottom-up\"],\n        c_in: int,\n        c_out: int,\n        conv_strides: tuple[int],\n        min_inner_channels: Union[int, None] = None,\n        nonlin: Callable = nn.LeakyReLU(),\n        resample: bool = False,\n        res_block_kernel: Optional[Union[int, Iterable[int]]] = None,\n        groups: int = 1,\n        batchnorm: bool = True,\n        res_block_type: Union[str, None] = None,\n        dropout: Union[float, None] = None,\n        gated: Union[bool, None] = None,\n        conv2d_bias: bool = True,\n        # lowres_input: bool = False,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        mode: Literal[\"top-down\", \"bottom-up\"]\n            The type of resampling performed in the initial strided convolution of the block.\n            If \"bottom-up\" downsampling of a factor 2 is done.\n            If \"top-down\" upsampling of a factor 2 is done.\n        c_in: int\n            The number of input channels.\n        c_out: int\n            The number of output channels.\n        min_inner_channels: int, optional\n            The number of channels used in the inner layer of this module.\n            Default is `None`, meaning that the number of inner channels is set to `c_out`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        resample: bool, optional\n            Whether to perform resampling in the first convolutional layer.\n            If `False`, the first convolutional layer just maps the input to a tensor with\n            `inner_channels` channels through 1x1 convolution. Default is `False`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n            Default is `None`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n        assert mode in [\"top-down\", \"bottom-up\"]\n\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        transp_conv_layer: ConvType = getattr(nn, f\"ConvTranspose{len(conv_strides)}d\")\n\n        if min_inner_channels is None:\n            min_inner_channels = 0\n        # inner_channels is the number of channels used in the inner layers\n        # of ResBlockWithResampling\n        inner_channels = max(c_out, min_inner_channels)\n\n        # Define first conv layer to change num channels and/or up/downsample\n        if resample:\n            if mode == \"bottom-up\":  # downsample\n                self.pre_conv = conv_layer(\n                    in_channels=c_in,\n                    out_channels=inner_channels,\n                    kernel_size=3,\n                    padding=1,\n                    stride=conv_strides,\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n            elif mode == \"top-down\":  # upsample\n                self.pre_conv = transp_conv_layer(\n                    in_channels=c_in,\n                    kernel_size=3,\n                    out_channels=inner_channels,\n                    padding=1,  # TODO maybe don't hardcode this?\n                    stride=conv_strides,\n                    groups=groups,\n                    output_padding=1 if len(conv_strides) == 2 else (0, 1, 1),\n                    bias=conv2d_bias,\n                )\n        elif c_in != inner_channels:\n            self.pre_conv = conv_layer(\n                c_in, inner_channels, 1, groups=groups, bias=conv2d_bias\n            )\n        else:\n            self.pre_conv = None\n\n        # Residual block\n        self.res = ResidualBlock(\n            channels=inner_channels,\n            conv_strides=conv_strides,\n            nonlin=nonlin,\n            kernel=res_block_kernel,\n            groups=groups,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            gated=gated,\n            block_type=res_block_type,\n            conv2d_bias=conv2d_bias,\n        )\n\n        # Define last conv layer to get correct num output channels\n        if inner_channels != c_out:\n            self.post_conv = conv_layer(\n                inner_channels, c_out, 1, groups=groups, bias=conv2d_bias\n            )\n        else:\n            self.post_conv = None\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output # TODO add shape\n        \"\"\"\n        if self.pre_conv is not None:\n            x = self.pre_conv(x)\n\n        x = self.res(x)\n\n        if self.post_conv is not None:\n            x = self.post_conv(x)\n        return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling.__init__","title":"<code>__init__(mode, c_in, c_out, conv_strides, min_inner_channels=None, nonlin=nn.LeakyReLU(), resample=False, res_block_kernel=None, groups=1, batchnorm=True, res_block_type=None, dropout=None, gated=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['top-down', 'bottom-up']</code> <p>The type of resampling performed in the initial strided convolution of the block. If \"bottom-up\" downsampling of a factor 2 is done. If \"top-down\" upsampling of a factor 2 is done.</p> required <code>c_in</code> <code>int</code> <p>The number of input channels.</p> required <code>c_out</code> <code>int</code> <p>The number of output channels.</p> required <code>min_inner_channels</code> <code>Union[int, None]</code> <p>The number of channels used in the inner layer of this module. Default is <code>None</code>, meaning that the number of inner channels is set to <code>c_out</code>.</p> <code>None</code> <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> <code>LeakyReLU()</code> <code>resample</code> <code>bool</code> <p>Whether to perform resampling in the first convolutional layer. If <code>False</code>, the first convolutional layer just maps the input to a tensor with <code>inner_channels</code> channels through 1x1 convolution. Default is <code>False</code>.</p> <code>False</code> <code>res_block_kernel</code> <code>Optional[Union[int, Iterable[int]]]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>res_block_type</code> <code>Union[str, None]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> <code>None</code> <code>dropout</code> <code>Union[float, None]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>Union[bool, None]</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    mode: Literal[\"top-down\", \"bottom-up\"],\n    c_in: int,\n    c_out: int,\n    conv_strides: tuple[int],\n    min_inner_channels: Union[int, None] = None,\n    nonlin: Callable = nn.LeakyReLU(),\n    resample: bool = False,\n    res_block_kernel: Optional[Union[int, Iterable[int]]] = None,\n    groups: int = 1,\n    batchnorm: bool = True,\n    res_block_type: Union[str, None] = None,\n    dropout: Union[float, None] = None,\n    gated: Union[bool, None] = None,\n    conv2d_bias: bool = True,\n    # lowres_input: bool = False,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    mode: Literal[\"top-down\", \"bottom-up\"]\n        The type of resampling performed in the initial strided convolution of the block.\n        If \"bottom-up\" downsampling of a factor 2 is done.\n        If \"top-down\" upsampling of a factor 2 is done.\n    c_in: int\n        The number of input channels.\n    c_out: int\n        The number of output channels.\n    min_inner_channels: int, optional\n        The number of channels used in the inner layer of this module.\n        Default is `None`, meaning that the number of inner channels is set to `c_out`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    resample: bool, optional\n        Whether to perform resampling in the first convolutional layer.\n        If `False`, the first convolutional layer just maps the input to a tensor with\n        `inner_channels` channels through 1x1 convolution. Default is `False`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n        Default is `None`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n    assert mode in [\"top-down\", \"bottom-up\"]\n\n    conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n    transp_conv_layer: ConvType = getattr(nn, f\"ConvTranspose{len(conv_strides)}d\")\n\n    if min_inner_channels is None:\n        min_inner_channels = 0\n    # inner_channels is the number of channels used in the inner layers\n    # of ResBlockWithResampling\n    inner_channels = max(c_out, min_inner_channels)\n\n    # Define first conv layer to change num channels and/or up/downsample\n    if resample:\n        if mode == \"bottom-up\":  # downsample\n            self.pre_conv = conv_layer(\n                in_channels=c_in,\n                out_channels=inner_channels,\n                kernel_size=3,\n                padding=1,\n                stride=conv_strides,\n                groups=groups,\n                bias=conv2d_bias,\n            )\n        elif mode == \"top-down\":  # upsample\n            self.pre_conv = transp_conv_layer(\n                in_channels=c_in,\n                kernel_size=3,\n                out_channels=inner_channels,\n                padding=1,  # TODO maybe don't hardcode this?\n                stride=conv_strides,\n                groups=groups,\n                output_padding=1 if len(conv_strides) == 2 else (0, 1, 1),\n                bias=conv2d_bias,\n            )\n    elif c_in != inner_channels:\n        self.pre_conv = conv_layer(\n            c_in, inner_channels, 1, groups=groups, bias=conv2d_bias\n        )\n    else:\n        self.pre_conv = None\n\n    # Residual block\n    self.res = ResidualBlock(\n        channels=inner_channels,\n        conv_strides=conv_strides,\n        nonlin=nonlin,\n        kernel=res_block_kernel,\n        groups=groups,\n        batchnorm=batchnorm,\n        dropout=dropout,\n        gated=gated,\n        block_type=res_block_type,\n        conv2d_bias=conv2d_bias,\n    )\n\n    # Define last conv layer to get correct num output channels\n    if inner_channels != c_out:\n        self.post_conv = conv_layer(\n            inner_channels, c_out, 1, groups=groups, bias=conv2d_bias\n        )\n    else:\n        self.post_conv = None\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResBlockWithResampling.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output # TODO add shape\n    \"\"\"\n    if self.pre_conv is not None:\n        x = self.pre_conv(x)\n\n    x = self.res(x)\n\n    if self.post_conv is not None:\n        x = self.post_conv(x)\n    return x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock","title":"<code>ResidualBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Residual block with 2 convolutional layers.</p> <p>Some architectural notes:     - The number of input, intermediate, and output channels is the same,     - Padding is always 'same',     - The 2 convolutional layers have the same groups,     - No stride allowed,     - Kernel sizes must be odd.</p> <p>The output isgiven by: <code>out = gate(f(x)) + x</code>. The presence of the gating mechanism is optional, and f(x) has different structures depending on the <code>block_type</code> argument. Specifically, <code>block_type</code> is a string specifying the block's structure, with:     a = activation     b = batch norm     c = conv layer     d = dropout. For example, \"bacdbacd\" defines a block with 2x[batchnorm, activation, conv, dropout].</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResidualBlock(nn.Module):\n    \"\"\"\n    Residual block with 2 convolutional layers.\n\n    Some architectural notes:\n        - The number of input, intermediate, and output channels is the same,\n        - Padding is always 'same',\n        - The 2 convolutional layers have the same groups,\n        - No stride allowed,\n        - Kernel sizes must be odd.\n\n    The output isgiven by: `out = gate(f(x)) + x`.\n    The presence of the gating mechanism is optional, and f(x) has different\n    structures depending on the `block_type` argument.\n    Specifically, `block_type` is a string specifying the block's structure, with:\n        a = activation\n        b = batch norm\n        c = conv layer\n        d = dropout.\n    For example, \"bacdbacd\" defines a block with 2x[batchnorm, activation, conv, dropout].\n    \"\"\"\n\n    default_kernel_size = (3, 3)\n\n    def __init__(\n        self,\n        channels: int,\n        nonlin: Callable,\n        conv_strides: tuple[int] = (2, 2),\n        kernel: Union[int, Iterable[int], None] = None,\n        groups: int = 1,\n        batchnorm: bool = True,\n        block_type: str = None,\n        dropout: float = None,\n        gated: bool = None,\n        conv2d_bias: bool = True,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        channels: int\n            The number of input and output channels (they are the same).\n        nonlin: Callable\n            The non-linearity function used in the block (e.g., `nn.ReLU`).\n        kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        block_type: str, optional\n            A string specifying the block structure, check class docstring for more info.\n            Default is `None`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        gated: bool, optional\n            Whether to use gated layer. Default is `None`.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        \"\"\"\n        super().__init__()\n\n        # Set kernel size &amp; padding\n        if kernel is None:\n            kernel = self.default_kernel_size\n        elif isinstance(kernel, int):\n            kernel = (kernel, kernel)\n        elif len(kernel) != 2:\n            raise ValueError(\"kernel has to be None, int, or an iterable of length 2\")\n        assert all(k % 2 == 1 for k in kernel), \"kernel sizes have to be odd\"\n        kernel = list(kernel)\n\n        # Define modules\n        conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n        norm_layer: NormType = getattr(nn, f\"BatchNorm{len(conv_strides)}d\")\n        dropout_layer: DropoutType = getattr(nn, f\"Dropout{len(conv_strides)}d\")\n        # TODO: same comment as in lvae.py, would be more readable to have `conv_dims`\n\n        modules = []\n        if block_type == \"cabdcabd\":\n            for i in range(2):\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                modules.append(nonlin)\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                if dropout is not None:\n                    modules.append(dropout_layer(dropout))\n        elif block_type == \"bacdbac\":\n            for i in range(2):\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                modules.append(nonlin)\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                if dropout is not None and i == 0:\n                    modules.append(dropout_layer(dropout))\n        elif block_type == \"bacdbacd\":\n            for i in range(2):\n                if batchnorm:\n                    modules.append(norm_layer(channels))\n                modules.append(nonlin)\n                conv = conv_layer(\n                    channels,\n                    channels,\n                    kernel[i],\n                    padding=\"same\",\n                    groups=groups,\n                    bias=conv2d_bias,\n                )\n                modules.append(conv)\n                modules.append(dropout_layer(dropout))\n\n        else:\n            raise ValueError(f\"unrecognized block type '{block_type}'\")\n\n        self.gated = gated\n        if gated:\n            modules.append(\n                GateLayer(\n                    channels=channels,\n                    conv_strides=conv_strides,\n                    kernel_size=1,\n                    nonlin=nonlin,\n                )\n            )\n\n        self.block = nn.Sequential(*modules)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            input tensor # TODO add shape\n\n        Returns\n        -------\n        torch.Tensor\n            output tensor # TODO add shape\n        \"\"\"\n        out = self.block(x)\n        assert (\n            out.shape == x.shape\n        ), f\"output shape: {out.shape} != input shape: {x.shape}\"\n        return out + x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock.__init__","title":"<code>__init__(channels, nonlin, conv_strides=(2, 2), kernel=None, groups=1, batchnorm=True, block_type=None, dropout=None, gated=None, conv2d_bias=True)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>The number of input and output channels (they are the same).</p> required <code>nonlin</code> <code>Callable</code> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>).</p> required <code>kernel</code> <code>Union[int, Iterable[int], None]</code> <p>The kernel size used in the convolutions of the block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>block_type</code> <code>str</code> <p>A string specifying the block structure, check class docstring for more info. Default is <code>None</code>.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>gated</code> <code>bool</code> <p>Whether to use gated layer. Default is <code>None</code>.</p> <code>None</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> <code>True</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    channels: int,\n    nonlin: Callable,\n    conv_strides: tuple[int] = (2, 2),\n    kernel: Union[int, Iterable[int], None] = None,\n    groups: int = 1,\n    batchnorm: bool = True,\n    block_type: str = None,\n    dropout: float = None,\n    gated: bool = None,\n    conv2d_bias: bool = True,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    channels: int\n        The number of input and output channels (they are the same).\n    nonlin: Callable\n        The non-linearity function used in the block (e.g., `nn.ReLU`).\n    kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    block_type: str, optional\n        A string specifying the block structure, check class docstring for more info.\n        Default is `None`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    gated: bool, optional\n        Whether to use gated layer. Default is `None`.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    \"\"\"\n    super().__init__()\n\n    # Set kernel size &amp; padding\n    if kernel is None:\n        kernel = self.default_kernel_size\n    elif isinstance(kernel, int):\n        kernel = (kernel, kernel)\n    elif len(kernel) != 2:\n        raise ValueError(\"kernel has to be None, int, or an iterable of length 2\")\n    assert all(k % 2 == 1 for k in kernel), \"kernel sizes have to be odd\"\n    kernel = list(kernel)\n\n    # Define modules\n    conv_layer: ConvType = getattr(nn, f\"Conv{len(conv_strides)}d\")\n    norm_layer: NormType = getattr(nn, f\"BatchNorm{len(conv_strides)}d\")\n    dropout_layer: DropoutType = getattr(nn, f\"Dropout{len(conv_strides)}d\")\n    # TODO: same comment as in lvae.py, would be more readable to have `conv_dims`\n\n    modules = []\n    if block_type == \"cabdcabd\":\n        for i in range(2):\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            modules.append(nonlin)\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            if dropout is not None:\n                modules.append(dropout_layer(dropout))\n    elif block_type == \"bacdbac\":\n        for i in range(2):\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            modules.append(nonlin)\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            if dropout is not None and i == 0:\n                modules.append(dropout_layer(dropout))\n    elif block_type == \"bacdbacd\":\n        for i in range(2):\n            if batchnorm:\n                modules.append(norm_layer(channels))\n            modules.append(nonlin)\n            conv = conv_layer(\n                channels,\n                channels,\n                kernel[i],\n                padding=\"same\",\n                groups=groups,\n                bias=conv2d_bias,\n            )\n            modules.append(conv)\n            modules.append(dropout_layer(dropout))\n\n    else:\n        raise ValueError(f\"unrecognized block type '{block_type}'\")\n\n    self.gated = gated\n    if gated:\n        modules.append(\n            GateLayer(\n                channels=channels,\n                conv_strides=conv_strides,\n                kernel_size=1,\n                nonlin=nonlin,\n            )\n        )\n\n    self.block = nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualBlock.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input tensor # TODO add shape</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor # TODO add shape</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        input tensor # TODO add shape\n\n    Returns\n    -------\n    torch.Tensor\n        output tensor # TODO add shape\n    \"\"\"\n    out = self.block(x)\n    assert (\n        out.shape == x.shape\n    ), f\"output shape: {out.shape} != input shape: {x.shape}\"\n    return out + x\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.ResidualGatedBlock","title":"<code>ResidualGatedBlock</code>","text":"<p>               Bases: <code>ResidualBlock</code></p> <p>Layer class that implements a residual block with a gating mechanism.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class ResidualGatedBlock(ResidualBlock):\n    \"\"\"Layer class that implements a residual block with a gating mechanism.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, gated=True)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.SkipConnectionMerger","title":"<code>SkipConnectionMerger</code>","text":"<p>               Bases: <code>MergeLayer</code></p> <p>Specialized <code>MergeLayer</code> module, handles skip connections in the model.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class SkipConnectionMerger(MergeLayer):\n    \"\"\"Specialized `MergeLayer` module, handles skip connections in the model.\"\"\"\n\n    def __init__(\n        self,\n        nonlin: Callable,\n        channels: Union[int, Iterable[int]],\n        batchnorm: bool,\n        dropout: float,\n        res_block_type: str,\n        conv_strides: tuple[int] = (2, 2),\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"] = \"residual\",\n        conv2d_bias: bool = True,\n        res_block_kernel: Optional[int] = None,\n    ):\n        \"\"\"\n        Constructor.\n\n        nonlin: Callable, optional\n            The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n        channels: Union[int, Iterable[int]]\n            The number of channels used in the convolutional blocks of this layer.\n            If it is an `int`:\n                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n                - (Optional) ResBlock: in_channels=channels, out_channels=channels\n            If it is an Iterable (must have `len(channels)==3`):\n                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]\n                - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]\n        batchnorm: bool\n            Whether to use batchnorm layers.\n        dropout: float\n            The dropout probability in dropout layers. If `None` dropout is not used.\n        res_block_type: str\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` doscstring for more information.\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n            The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".\n            Check the class docstring for more information about the behaviour of different merge modalities.\n        conv2d_bias: bool, optional\n            Whether to use bias term in convolutions. Default is `True`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        \"\"\"\n        super().__init__(\n            conv_strides=conv_strides,\n            channels=channels,\n            nonlin=nonlin,\n            merge_type=merge_type,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            conv2d_bias=conv2d_bias,\n        )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.SkipConnectionMerger.__init__","title":"<code>__init__(nonlin, channels, batchnorm, dropout, res_block_type, conv_strides=(2, 2), merge_type='residual', conv2d_bias=True, res_block_kernel=None)</code>","text":"<p>Constructor.</p> <p>nonlin: Callable, optional     The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>. channels: Union[int, Iterable[int]]     The number of channels used in the convolutional blocks of this layer.     If it is an <code>int</code>:         - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels         - (Optional) ResBlock: in_channels=channels, out_channels=channels     If it is an Iterable (must have <code>len(channels)==3</code>):         - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]         - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1] batchnorm: bool     Whether to use batchnorm layers. dropout: float     The dropout probability in dropout layers. If <code>None</code> dropout is not used. res_block_type: str     A string specifying the structure of residual block.     Check <code>ResidualBlock</code> doscstring for more information. conv_strides: tuple, optional     The strides used in the convolutions. Default is <code>(2, 2)</code>. merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]     The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".     Check the class docstring for more information about the behaviour of different merge modalities. conv2d_bias: bool, optional     Whether to use bias term in convolutions. Default is <code>True</code>. res_block_kernel: Union[int, Iterable[int]], optional     The kernel size used in the convolutions of the residual block.     It can be either a single integer or a pair of integers defining the squared kernel.     Default is <code>None</code>.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    nonlin: Callable,\n    channels: Union[int, Iterable[int]],\n    batchnorm: bool,\n    dropout: float,\n    res_block_type: str,\n    conv_strides: tuple[int] = (2, 2),\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"] = \"residual\",\n    conv2d_bias: bool = True,\n    res_block_kernel: Optional[int] = None,\n):\n    \"\"\"\n    Constructor.\n\n    nonlin: Callable, optional\n        The non-linearity function used in the block. Default is `nn.LeakyReLU`.\n    channels: Union[int, Iterable[int]]\n        The number of channels used in the convolutional blocks of this layer.\n        If it is an `int`:\n            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels\n            - (Optional) ResBlock: in_channels=channels, out_channels=channels\n        If it is an Iterable (must have `len(channels)==3`):\n            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]\n            - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]\n    batchnorm: bool\n        Whether to use batchnorm layers.\n    dropout: float\n        The dropout probability in dropout layers. If `None` dropout is not used.\n    res_block_type: str\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` doscstring for more information.\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"]\n        The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\".\n        Check the class docstring for more information about the behaviour of different merge modalities.\n    conv2d_bias: bool, optional\n        Whether to use bias term in convolutions. Default is `True`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    \"\"\"\n    super().__init__(\n        conv_strides=conv_strides,\n        channels=channels,\n        nonlin=nonlin,\n        merge_type=merge_type,\n        batchnorm=batchnorm,\n        dropout=dropout,\n        res_block_type=res_block_type,\n        res_block_kernel=res_block_kernel,\n        conv2d_bias=conv2d_bias,\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownDeterministicResBlock","title":"<code>TopDownDeterministicResBlock</code>","text":"<p>               Bases: <code>ResBlockWithResampling</code></p> <p>Resnet block for top-down deterministic layers.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class TopDownDeterministicResBlock(ResBlockWithResampling):\n    \"\"\"Resnet block for top-down deterministic layers.\"\"\"\n\n    def __init__(self, *args, upsample: bool = False, **kwargs):\n        kwargs[\"resample\"] = upsample\n        super().__init__(\"top-down\", *args, **kwargs)\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer","title":"<code>TopDownLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Top-down inference layer.</p> <p>It includes:     - Stochastic sampling,     - Computation of KL divergence,     - A small deterministic ResNet that performs upsampling.</p> <p>NOTE 1:     The algorithm for generative inference approximately works as follows:         - p_params = output of top-down layer above         - bu = inferred bottom-up value at this layer         - q_params = merge(bu, p_params)         - z = stochastic_layer(q_params)         - (optional) get and merge skip connection from prev top-down layer         - top-down deterministic ResNet</p> <p>NOTE 2:     The Top-Down layer can work in two modes: inference and prediction/generative.     Depending on the particular mode, it follows distinct behaviours:     - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path,     by merging outcomes of bottom-up and top-down passes. The exception is the top layer,     in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer.     - On the contrary in predicition/generative mode, parameters of q(z_i|z_i+1) can be obtained     once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is     possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).</p> <p>NOTE 3:     When doing unconditional generation, bu_value is not available. Hence the     merge layer is not used, and z is sampled directly from p_params.</p> <p>NOTE 4:     If this is the top layer, at inference time, the uppermost bottom-up value     is used directly as q_params, and p_params are defined in this layer     (while they are usually taken from the previous layer), and can be learned.</p> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>class TopDownLayer(nn.Module):\n    \"\"\"Top-down inference layer.\n\n    It includes:\n        - Stochastic sampling,\n        - Computation of KL divergence,\n        - A small deterministic ResNet that performs upsampling.\n\n    NOTE 1:\n        The algorithm for generative inference approximately works as follows:\n            - p_params = output of top-down layer above\n            - bu = inferred bottom-up value at this layer\n            - q_params = merge(bu, p_params)\n            - z = stochastic_layer(q_params)\n            - (optional) get and merge skip connection from prev top-down layer\n            - top-down deterministic ResNet\n\n    NOTE 2:\n        The Top-Down layer can work in two modes: inference and prediction/generative.\n        Depending on the particular mode, it follows distinct behaviours:\n        - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path,\n        by merging outcomes of bottom-up and top-down passes. The exception is the top layer,\n        in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer.\n        - On the contrary in predicition/generative mode, parameters of q(z_i|z_i+1) can be obtained\n        once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is\n        possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).\n\n    NOTE 3:\n        When doing unconditional generation, bu_value is not available. Hence the\n        merge layer is not used, and z is sampled directly from p_params.\n\n    NOTE 4:\n        If this is the top layer, at inference time, the uppermost bottom-up value\n        is used directly as q_params, and p_params are defined in this layer\n        (while they are usually taken from the previous layer), and can be learned.\n    \"\"\"\n\n    def __init__(\n        self,\n        z_dim: int,\n        n_res_blocks: int,\n        n_filters: int,\n        conv_strides: tuple[int],\n        is_top_layer: bool = False,\n        upsampling_steps: Union[int, None] = None,\n        nonlin: Union[Callable, None] = None,\n        merge_type: Union[\n            Literal[\"linear\", \"residual\", \"residual_ungated\"], None\n        ] = None,\n        batchnorm: bool = True,\n        dropout: Union[float, None] = None,\n        stochastic_skip: bool = False,\n        res_block_type: Union[str, None] = None,\n        res_block_kernel: Union[int, None] = None,\n        groups: int = 1,\n        gated: Union[bool, None] = None,\n        learn_top_prior: bool = False,\n        top_prior_param_shape: Union[Iterable[int], None] = None,\n        analytical_kl: bool = False,\n        retain_spatial_dims: bool = False,\n        vanilla_latent_hw: Union[Iterable[int], None] = None,\n        input_image_shape: Union[tuple[int, int], None] = None,\n        normalize_latent_factor: float = 1.0,\n        conv2d_bias: bool = True,\n        stochastic_use_naive_exponential: bool = False,\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        z_dim: int\n            The size of the latent space.\n        n_res_blocks: int\n            The number of TopDownDeterministicResBlock blocks\n        n_filters: int\n            The number of channels present through out the layers of this block.\n        conv_strides: tuple, optional\n            The strides used in the convolutions. Default is `(2, 2)`.\n        is_top_layer: bool, optional\n            Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.\n        upsampling_steps: int, optional\n            The number of upsampling steps that has to be done in this layer (typically 1).\n            Default is `None`.\n        nonlin: Callable, optional\n            The non-linearity function used in the block (e.g., `nn.ReLU`). Default is `None`.\n        merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"], optional\n            The type of merge done in the layer. It can be chosen between \"linear\", \"residual\",\n            and \"residual_ungated\". Check the `MergeLayer` class docstring for more information\n            about the behaviour of different merging modalities. Default is `None`.\n        batchnorm: bool, optional\n            Whether to use batchnorm layers. Default is `True`.\n        dropout: float, optional\n            The dropout probability in dropout layers. If `None` dropout is not used.\n            Default is `None`.\n        stochastic_skip: bool, optional\n            Whether to use skip connections between previous top-down layer's output and this layer's stochastic output.\n            Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical\n            level, hence facilitating the gradient flow during backpropagation. Default is `False`.\n        res_block_type: str, optional\n            A string specifying the structure of residual block.\n            Check `ResidualBlock` documentation for more information.\n            Default is `None`.\n        res_block_kernel: Union[int, Iterable[int]], optional\n            The kernel size used in the convolutions of the residual block.\n            It can be either a single integer or a pair of integers defining the squared kernel.\n            Default is `None`.\n        groups: int, optional\n            The number of groups to consider in the convolutions. Default is 1.\n        gated: bool, optional\n            Whether to use gated layer in `ResidualBlock`. Default is `None`.\n        learn_top_prior:\n            Whether to set the top prior as learnable.\n            If this is set to `False`, in the top-most layer the prior will be N(0,1).\n            Otherwise, we will still have a normal distribution whose parameters will be learnt.\n            Default is `False`.\n        top_prior_param_shape: Iterable[int], optional\n            The size of the tensor which expresses the mean and the variance\n            of the prior for the top most layer. Default is `None`.\n        analytical_kl: bool, optional\n            If True, KL divergence is calculated according to the analytical formula.\n            Otherwise, an MC approximation using sampled latents is calculated.\n            Default is `False`.\n        retain_spatial_dims: bool, optional\n            If `True`, the size of Encoder's latent space is kept to `input_image_shape` within the topdown layer.\n            This implies that the oput spatial size equals the input spatial size.\n            To achieve this, we centercrop the intermediate representation.\n            Default is `False`.\n        vanilla_latent_hw: Iterable[int], optional\n            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n            Default is `None`.\n        input_image_shape: Tuple[int, int], optionalut\n            The shape of the input image tensor.\n            When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer\n            output has the same shape as the input. Default is `None`.\n        normalize_latent_factor: float, optional\n            A factor used to normalize the latent tensors `q_params`.\n            Specifically, normalization is done by dividing the latent tensor by this factor.\n            Default is 1.0.\n        conv2d_bias: bool, optional\n            Whether to use bias term is the convolutional blocks of this layer.\n            Default is `True`.\n        stochastic_use_naive_exponential: bool, optional\n            If `False`, in the NormalStochasticBlock2d exponentials are computed according\n            to the alternative definition provided by `StableExponential` class.\n            This should improve numerical stability in the training process.\n            Default is `False`.\n        \"\"\"\n        super().__init__()\n\n        self.is_top_layer = is_top_layer\n        self.z_dim = z_dim\n        self.stochastic_skip = stochastic_skip\n        self.learn_top_prior = learn_top_prior\n        self.analytical_kl = analytical_kl\n        self.retain_spatial_dims = retain_spatial_dims\n        self.input_image_shape = (\n            input_image_shape if len(conv_strides) == 3 else input_image_shape[1:]\n        )\n        self.latent_shape = self.input_image_shape if self.retain_spatial_dims else None\n        self.normalize_latent_factor = normalize_latent_factor\n        self._vanilla_latent_hw = vanilla_latent_hw  # TODO: check this, it is not used\n\n        # Define top layer prior parameters, possibly learnable\n        if is_top_layer:\n            self.top_prior_params = nn.Parameter(\n                torch.zeros(top_prior_param_shape), requires_grad=learn_top_prior\n            )\n\n        # Upsampling steps left to do in this layer\n        ups_left = upsampling_steps\n\n        # Define deterministic top-down block, which is a sequence of deterministic\n        # residual blocks with (optional) upsampling.\n        block_list = []\n        for _ in range(n_res_blocks):\n            do_resample = False\n            if ups_left &gt; 0:\n                do_resample = True\n                ups_left -= 1\n            block_list.append(\n                TopDownDeterministicResBlock(\n                    c_in=n_filters,\n                    c_out=n_filters,\n                    conv_strides=conv_strides,\n                    nonlin=nonlin,\n                    upsample=do_resample,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    res_block_type=res_block_type,\n                    res_block_kernel=res_block_kernel,\n                    gated=gated,\n                    conv2d_bias=conv2d_bias,\n                    groups=groups,\n                )\n            )\n        self.deterministic_block = nn.Sequential(*block_list)\n\n        # Define stochastic block with convolutions\n\n        self.stochastic = NormalStochasticBlock(\n            c_in=n_filters,\n            c_vars=z_dim,\n            c_out=n_filters,\n            conv_dims=len(conv_strides),\n            transform_p_params=(not is_top_layer),\n            vanilla_latent_hw=vanilla_latent_hw,\n            use_naive_exponential=stochastic_use_naive_exponential,\n        )\n\n        if not is_top_layer:\n            # Merge layer: it combines bottom-up inference and top-down\n            # generative outcomes to give posterior parameters\n            self.merge = MergeLayer(\n                channels=n_filters,\n                conv_strides=conv_strides,\n                merge_type=merge_type,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                conv2d_bias=conv2d_bias,\n            )\n\n            # Skip connection that goes around the stochastic top-down layer\n            if stochastic_skip:\n                self.skip_connection_merger = SkipConnectionMerger(\n                    channels=n_filters,\n                    conv_strides=conv_strides,\n                    nonlin=nonlin,\n                    batchnorm=batchnorm,\n                    dropout=dropout,\n                    res_block_type=res_block_type,\n                    merge_type=merge_type,\n                    conv2d_bias=conv2d_bias,\n                    res_block_kernel=res_block_kernel,\n                )\n\n    def sample_from_q(\n        self,\n        input_: torch.Tensor,\n        bu_value: torch.Tensor,\n        var_clip_max: Optional[float] = None,\n        mask: torch.Tensor = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Method computes the latent inference distribution q(z_i|z_{i+1}).\n\n        Used for sampling a latent tensor from it.\n\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The input tensor to the layer, which is the output of the top-down layer.\n        bu_value: torch.Tensor\n            The tensor defining the parameters /mu_q and /sigma_q computed during the\n            bottom-up deterministic pass at the correspondent hierarchical layer.\n        var_clip_max: float, optional\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped. Default is `None`.\n        mask: Union[None, torch.Tensor], optional\n            A tensor that is used to mask the sampled latent tensor. Default is `None`.\n        \"\"\"\n        if self.is_top_layer:  # In top layer, we don't merge bu_value with p_params\n            q_params = bu_value\n        else:\n            # NOTE: Here the assumption is that the vampprior is only applied on the top layer.\n            n_img_prior = None\n            p_params = self.get_p_params(input_, n_img_prior)\n            q_params = self.merge(bu_value, p_params)\n\n        sample = self.stochastic.sample_from_q(q_params, var_clip_max)\n\n        if mask:\n            return sample[mask]\n\n        return sample\n\n    def get_p_params(\n        self,\n        input_: torch.Tensor,\n        n_img_prior: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Return the parameters of the prior distribution p(z_i|z_{i+1}).\n\n        The parameters depend on the hierarchical level of the layer:\n        - if it is the topmost level, parameters are the ones of the prior.\n        - else, the input from the layer above is the parameters itself.\n\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The input tensor to the layer, which is the output of the top-down layer above.\n        n_img_prior: int\n            The number of images to be generated from the unconditional prior distribution p(z_L).\n        \"\"\"\n        p_params = None\n\n        # If top layer, define p_params as the ones of the prior p(z_L)\n        if self.is_top_layer:\n            p_params = self.top_prior_params\n\n            # Sample specific number of images by expanding the prior\n            if n_img_prior is not None:\n                p_params = p_params.expand(n_img_prior, -1, -1, -1)\n\n        # Else the input from the layer above is p_params itself\n        else:\n            p_params = input_\n\n        return p_params\n\n    def forward(\n        self,\n        input_: Union[torch.Tensor, None] = None,\n        skip_connection_input: Union[torch.Tensor, None] = None,\n        inference_mode: bool = False,\n        bu_value: Union[torch.Tensor, None] = None,\n        n_img_prior: Union[int, None] = None,\n        forced_latent: Union[torch.Tensor, None] = None,\n        force_constant_output: bool = False,\n        mode_pred: bool = False,\n        use_uncond_mode: bool = False,\n        var_clip_max: Union[float, None] = None,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"Forward pass.\n\n        Parameters\n        ----------\n        input_: torch.Tensor, optional\n            The input tensor to the layer, which is the output of the top-down layer.\n            Default is `None`.\n        skip_connection_input: torch.Tensor, optional\n            The tensor brought by the skip connection between the current and the\n            previous top-down layer.\n            Default is `None`.\n        inference_mode: bool, optional\n            Whether the layer is in inference mode. See NOTE 2 in class description\n            for more info.\n            Default is `False`.\n        bu_value: torch.Tensor, optional\n            The tensor defining the parameters /mu_q and /sigma_q computed during the\n            bottom-up deterministic pass\n            at the correspondent hierarchical layer. Default is `None`.\n        n_img_prior: int, optional\n            The number of images to be generated from the unconditional prior\n            distribution p(z_L).\n            Default is `None`.\n        forced_latent: torch.Tensor, optional\n            A pre-defined latent tensor. If it is not `None`, than it is used as the\n            actual latent tensor and,\n            hence, sampling does not happen. Default is `None`.\n        force_constant_output: bool, optional\n            Whether to copy the first sample (and rel. distrib parameters) over the\n            whole batch.\n            This is used when doing experiment from the prior - q is not used.\n            Default is `False`.\n        mode_pred: bool, optional\n            Whether the model is in prediction mode. Default is `False`.\n        use_uncond_mode: bool, optional\n            Whether to use the uncoditional distribution p(z) to sample latents in\n            prediction mode.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        # Check consistency of arguments\n        inputs_none = input_ is None and skip_connection_input is None\n        if self.is_top_layer and not inputs_none:\n            raise ValueError(\"In top layer, inputs should be None\")\n\n        p_params = self.get_p_params(input_, n_img_prior)\n\n        # Get the parameters for the latent distribution to sample from\n        if inference_mode:  # TODO What's this ? reuse Fede's code?\n            if self.is_top_layer:\n                q_params = bu_value\n                if mode_pred is False:\n                    assert p_params.shape[2:] == bu_value.shape[2:], (\n                        \"Spatial dimensions of p_params and bu_value should match. \"\n                        f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                        f\"bu_value={bu_value.shape[2:]}.\"\n                    )\n            else:\n                if use_uncond_mode:\n                    q_params = p_params\n                else:\n                    assert p_params.shape[2:] == bu_value.shape[2:], (\n                        \"Spatial dimensions of p_params and bu_value should match. \"\n                        f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                        f\"bu_value={bu_value.shape[2:]}.\"\n                    )\n                    q_params = self.merge(bu_value, p_params)\n        else:  # generative mode, q is not used, we sample from p(z_i | z_{i+1})\n            q_params = None\n\n        # NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})\n        # depending on the mode (hence, in practice, by checking whether q_params is None).\n\n        # Normalization of latent space parameters for stablity.\n        # See Very deep VAEs generalize autoregressive models.\n        if self.normalize_latent_factor:\n            q_params = q_params / self.normalize_latent_factor\n\n        # Sample (and process) a latent tensor in the stochastic layer\n        x, data_stoch = self.stochastic(\n            p_params=p_params,\n            q_params=q_params,\n            forced_latent=forced_latent,\n            force_constant_output=force_constant_output,\n            analytical_kl=self.analytical_kl,\n            mode_pred=mode_pred,\n            use_uncond_mode=use_uncond_mode,\n            var_clip_max=var_clip_max,\n        )\n        # Merge skip connection from previous layer\n        if self.stochastic_skip and not self.is_top_layer:\n            x = self.skip_connection_merger(x, skip_connection_input)\n        if self.retain_spatial_dims:\n            # NOTE: we assume that one topdown layer will have exactly one upscaling layer.\n\n            # NOTE: in case, in the Bottom-Up layer, LC retains spatial dimensions,\n            # we have the following (see `MergeLowRes`):\n            # - the \"primary-flow\" tensor is padded to match the low-res patch size\n            #   (e.g., from 32x32 to 64x64)\n            # - padded tensor is then merged with the low-res patch (concatenation\n            #   along dim=1 + convolution)\n            # Therefore, we need to do the symmetric operation here, that is to\n            # crop `x` for the same amount we padded it in the correspondent BU layer.\n\n            # NOTE: cropping is done to retain the shape of the input in the output.\n            # Therefore we need it only in the case `x` is the same shape of the input,\n            # because that's the only case in which we need to retain the shape.\n            # Here, it must be strictly greater than half the input shape, which is\n            # the case if and only if `x.shape == self.latent_shape`.\n            rescale = (\n                np.array((1, 2, 2)) if len(self.latent_shape) == 3 else np.array((2, 2))\n            )  # TODO better way?\n            new_latent_shape = tuple(np.array(self.latent_shape) // rescale)\n            if x.shape[-1] &gt; new_latent_shape[-1]:\n                x = crop_img_tensor(x, new_latent_shape)\n        # TODO: `retain_spatial_dims` is the same for all the TD layers.\n        # How to handle the case in which we do not have LC for all layers?\n        # The answer is in `self.latent_shape`, which is equal to `input_image_shape`\n        # (e.g., (64, 64)) if `retain_spatial_dims` is `True`, else it is `None`.\n        # Last top-down block (sequence of residual blocks w\\ upsampling)\n        x = self.deterministic_block(x)\n        # Save some metrics that will be used in the loss computation\n        keys = [\n            \"z\",\n            \"kl_samplewise\",\n            \"kl_samplewise_restricted\",\n            \"kl_spatial\",\n            \"kl_channelwise\",\n            \"logprob_q\",\n            \"qvar_max\",\n        ]\n        data = {k: data_stoch.get(k, None) for k in keys}\n        data[\"q_mu\"] = None\n        data[\"q_lv\"] = None\n        if data_stoch[\"q_params\"] is not None:\n            q_mu, q_lv = data_stoch[\"q_params\"]\n            data[\"q_mu\"] = q_mu\n            data[\"q_lv\"] = q_lv\n        return x, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.__init__","title":"<code>__init__(z_dim, n_res_blocks, n_filters, conv_strides, is_top_layer=False, upsampling_steps=None, nonlin=None, merge_type=None, batchnorm=True, dropout=None, stochastic_skip=False, res_block_type=None, res_block_kernel=None, groups=1, gated=None, learn_top_prior=False, top_prior_param_shape=None, analytical_kl=False, retain_spatial_dims=False, vanilla_latent_hw=None, input_image_shape=None, normalize_latent_factor=1.0, conv2d_bias=True, stochastic_use_naive_exponential=False)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>z_dim</code> <code>int</code> <p>The size of the latent space.</p> required <code>n_res_blocks</code> <code>int</code> <p>The number of TopDownDeterministicResBlock blocks</p> required <code>n_filters</code> <code>int</code> <p>The number of channels present through out the layers of this block.</p> required <code>conv_strides</code> <code>tuple[int]</code> <p>The strides used in the convolutions. Default is <code>(2, 2)</code>.</p> required <code>is_top_layer</code> <code>bool</code> <p>Whether the current layer is at the top of the Decoder hierarchy. Default is <code>False</code>.</p> <code>False</code> <code>upsampling_steps</code> <code>Union[int, None]</code> <p>The number of upsampling steps that has to be done in this layer (typically 1). Default is <code>None</code>.</p> <code>None</code> <code>nonlin</code> <code>Union[Callable, None]</code> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>). Default is <code>None</code>.</p> <code>None</code> <code>merge_type</code> <code>Union[Literal['linear', 'residual', 'residual_ungated'], None]</code> <p>The type of merge done in the layer. It can be chosen between \"linear\", \"residual\", and \"residual_ungated\". Check the <code>MergeLayer</code> class docstring for more information about the behaviour of different merging modalities. Default is <code>None</code>.</p> <code>None</code> <code>batchnorm</code> <code>bool</code> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> <code>True</code> <code>dropout</code> <code>Union[float, None]</code> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> <code>None</code> <code>stochastic_skip</code> <code>bool</code> <p>Whether to use skip connections between previous top-down layer's output and this layer's stochastic output. Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical level, hence facilitating the gradient flow during backpropagation. Default is <code>False</code>.</p> <code>False</code> <code>res_block_type</code> <code>Union[str, None]</code> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> documentation for more information. Default is <code>None</code>.</p> <code>None</code> <code>res_block_kernel</code> <code>Union[int, None]</code> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> <code>None</code> <code>groups</code> <code>int</code> <p>The number of groups to consider in the convolutions. Default is 1.</p> <code>1</code> <code>gated</code> <code>Union[bool, None]</code> <p>Whether to use gated layer in <code>ResidualBlock</code>. Default is <code>None</code>.</p> <code>None</code> <code>learn_top_prior</code> <code>bool</code> <p>Whether to set the top prior as learnable. If this is set to <code>False</code>, in the top-most layer the prior will be N(0,1). Otherwise, we will still have a normal distribution whose parameters will be learnt. Default is <code>False</code>.</p> <code>False</code> <code>top_prior_param_shape</code> <code>Union[Iterable[int], None]</code> <p>The size of the tensor which expresses the mean and the variance of the prior for the top most layer. Default is <code>None</code>.</p> <code>None</code> <code>analytical_kl</code> <code>bool</code> <p>If True, KL divergence is calculated according to the analytical formula. Otherwise, an MC approximation using sampled latents is calculated. Default is <code>False</code>.</p> <code>False</code> <code>retain_spatial_dims</code> <code>bool</code> <p>If <code>True</code>, the size of Encoder's latent space is kept to <code>input_image_shape</code> within the topdown layer. This implies that the oput spatial size equals the input spatial size. To achieve this, we centercrop the intermediate representation. Default is <code>False</code>.</p> <code>False</code> <code>vanilla_latent_hw</code> <code>Union[Iterable[int], None]</code> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> <code>None</code> <code>input_image_shape</code> <code>Union[tuple[int, int], None]</code> <p>The shape of the input image tensor. When <code>retain_spatial_dims</code> is set to <code>True</code>, this is used to ensure that the shape of this layer output has the same shape as the input. Default is <code>None</code>.</p> <code>None</code> <code>normalize_latent_factor</code> <code>float</code> <p>A factor used to normalize the latent tensors <code>q_params</code>. Specifically, normalization is done by dividing the latent tensor by this factor. Default is 1.0.</p> <code>1.0</code> <code>conv2d_bias</code> <code>bool</code> <p>Whether to use bias term is the convolutional blocks of this layer. Default is <code>True</code>.</p> <code>True</code> <code>stochastic_use_naive_exponential</code> <code>bool</code> <p>If <code>False</code>, in the NormalStochasticBlock2d exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> <code>False</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def __init__(\n    self,\n    z_dim: int,\n    n_res_blocks: int,\n    n_filters: int,\n    conv_strides: tuple[int],\n    is_top_layer: bool = False,\n    upsampling_steps: Union[int, None] = None,\n    nonlin: Union[Callable, None] = None,\n    merge_type: Union[\n        Literal[\"linear\", \"residual\", \"residual_ungated\"], None\n    ] = None,\n    batchnorm: bool = True,\n    dropout: Union[float, None] = None,\n    stochastic_skip: bool = False,\n    res_block_type: Union[str, None] = None,\n    res_block_kernel: Union[int, None] = None,\n    groups: int = 1,\n    gated: Union[bool, None] = None,\n    learn_top_prior: bool = False,\n    top_prior_param_shape: Union[Iterable[int], None] = None,\n    analytical_kl: bool = False,\n    retain_spatial_dims: bool = False,\n    vanilla_latent_hw: Union[Iterable[int], None] = None,\n    input_image_shape: Union[tuple[int, int], None] = None,\n    normalize_latent_factor: float = 1.0,\n    conv2d_bias: bool = True,\n    stochastic_use_naive_exponential: bool = False,\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    z_dim: int\n        The size of the latent space.\n    n_res_blocks: int\n        The number of TopDownDeterministicResBlock blocks\n    n_filters: int\n        The number of channels present through out the layers of this block.\n    conv_strides: tuple, optional\n        The strides used in the convolutions. Default is `(2, 2)`.\n    is_top_layer: bool, optional\n        Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.\n    upsampling_steps: int, optional\n        The number of upsampling steps that has to be done in this layer (typically 1).\n        Default is `None`.\n    nonlin: Callable, optional\n        The non-linearity function used in the block (e.g., `nn.ReLU`). Default is `None`.\n    merge_type: Literal[\"linear\", \"residual\", \"residual_ungated\"], optional\n        The type of merge done in the layer. It can be chosen between \"linear\", \"residual\",\n        and \"residual_ungated\". Check the `MergeLayer` class docstring for more information\n        about the behaviour of different merging modalities. Default is `None`.\n    batchnorm: bool, optional\n        Whether to use batchnorm layers. Default is `True`.\n    dropout: float, optional\n        The dropout probability in dropout layers. If `None` dropout is not used.\n        Default is `None`.\n    stochastic_skip: bool, optional\n        Whether to use skip connections between previous top-down layer's output and this layer's stochastic output.\n        Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical\n        level, hence facilitating the gradient flow during backpropagation. Default is `False`.\n    res_block_type: str, optional\n        A string specifying the structure of residual block.\n        Check `ResidualBlock` documentation for more information.\n        Default is `None`.\n    res_block_kernel: Union[int, Iterable[int]], optional\n        The kernel size used in the convolutions of the residual block.\n        It can be either a single integer or a pair of integers defining the squared kernel.\n        Default is `None`.\n    groups: int, optional\n        The number of groups to consider in the convolutions. Default is 1.\n    gated: bool, optional\n        Whether to use gated layer in `ResidualBlock`. Default is `None`.\n    learn_top_prior:\n        Whether to set the top prior as learnable.\n        If this is set to `False`, in the top-most layer the prior will be N(0,1).\n        Otherwise, we will still have a normal distribution whose parameters will be learnt.\n        Default is `False`.\n    top_prior_param_shape: Iterable[int], optional\n        The size of the tensor which expresses the mean and the variance\n        of the prior for the top most layer. Default is `None`.\n    analytical_kl: bool, optional\n        If True, KL divergence is calculated according to the analytical formula.\n        Otherwise, an MC approximation using sampled latents is calculated.\n        Default is `False`.\n    retain_spatial_dims: bool, optional\n        If `True`, the size of Encoder's latent space is kept to `input_image_shape` within the topdown layer.\n        This implies that the oput spatial size equals the input spatial size.\n        To achieve this, we centercrop the intermediate representation.\n        Default is `False`.\n    vanilla_latent_hw: Iterable[int], optional\n        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n        Default is `None`.\n    input_image_shape: Tuple[int, int], optionalut\n        The shape of the input image tensor.\n        When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer\n        output has the same shape as the input. Default is `None`.\n    normalize_latent_factor: float, optional\n        A factor used to normalize the latent tensors `q_params`.\n        Specifically, normalization is done by dividing the latent tensor by this factor.\n        Default is 1.0.\n    conv2d_bias: bool, optional\n        Whether to use bias term is the convolutional blocks of this layer.\n        Default is `True`.\n    stochastic_use_naive_exponential: bool, optional\n        If `False`, in the NormalStochasticBlock2d exponentials are computed according\n        to the alternative definition provided by `StableExponential` class.\n        This should improve numerical stability in the training process.\n        Default is `False`.\n    \"\"\"\n    super().__init__()\n\n    self.is_top_layer = is_top_layer\n    self.z_dim = z_dim\n    self.stochastic_skip = stochastic_skip\n    self.learn_top_prior = learn_top_prior\n    self.analytical_kl = analytical_kl\n    self.retain_spatial_dims = retain_spatial_dims\n    self.input_image_shape = (\n        input_image_shape if len(conv_strides) == 3 else input_image_shape[1:]\n    )\n    self.latent_shape = self.input_image_shape if self.retain_spatial_dims else None\n    self.normalize_latent_factor = normalize_latent_factor\n    self._vanilla_latent_hw = vanilla_latent_hw  # TODO: check this, it is not used\n\n    # Define top layer prior parameters, possibly learnable\n    if is_top_layer:\n        self.top_prior_params = nn.Parameter(\n            torch.zeros(top_prior_param_shape), requires_grad=learn_top_prior\n        )\n\n    # Upsampling steps left to do in this layer\n    ups_left = upsampling_steps\n\n    # Define deterministic top-down block, which is a sequence of deterministic\n    # residual blocks with (optional) upsampling.\n    block_list = []\n    for _ in range(n_res_blocks):\n        do_resample = False\n        if ups_left &gt; 0:\n            do_resample = True\n            ups_left -= 1\n        block_list.append(\n            TopDownDeterministicResBlock(\n                c_in=n_filters,\n                c_out=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                upsample=do_resample,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                res_block_kernel=res_block_kernel,\n                gated=gated,\n                conv2d_bias=conv2d_bias,\n                groups=groups,\n            )\n        )\n    self.deterministic_block = nn.Sequential(*block_list)\n\n    # Define stochastic block with convolutions\n\n    self.stochastic = NormalStochasticBlock(\n        c_in=n_filters,\n        c_vars=z_dim,\n        c_out=n_filters,\n        conv_dims=len(conv_strides),\n        transform_p_params=(not is_top_layer),\n        vanilla_latent_hw=vanilla_latent_hw,\n        use_naive_exponential=stochastic_use_naive_exponential,\n    )\n\n    if not is_top_layer:\n        # Merge layer: it combines bottom-up inference and top-down\n        # generative outcomes to give posterior parameters\n        self.merge = MergeLayer(\n            channels=n_filters,\n            conv_strides=conv_strides,\n            merge_type=merge_type,\n            nonlin=nonlin,\n            batchnorm=batchnorm,\n            dropout=dropout,\n            res_block_type=res_block_type,\n            res_block_kernel=res_block_kernel,\n            conv2d_bias=conv2d_bias,\n        )\n\n        # Skip connection that goes around the stochastic top-down layer\n        if stochastic_skip:\n            self.skip_connection_merger = SkipConnectionMerger(\n                channels=n_filters,\n                conv_strides=conv_strides,\n                nonlin=nonlin,\n                batchnorm=batchnorm,\n                dropout=dropout,\n                res_block_type=res_block_type,\n                merge_type=merge_type,\n                conv2d_bias=conv2d_bias,\n                res_block_kernel=res_block_kernel,\n            )\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.forward","title":"<code>forward(input_=None, skip_connection_input=None, inference_mode=False, bu_value=None, n_img_prior=None, forced_latent=None, force_constant_output=False, mode_pred=False, use_uncond_mode=False, var_clip_max=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Union[Tensor, None]</code> <p>The input tensor to the layer, which is the output of the top-down layer. Default is <code>None</code>.</p> <code>None</code> <code>skip_connection_input</code> <code>Union[Tensor, None]</code> <p>The tensor brought by the skip connection between the current and the previous top-down layer. Default is <code>None</code>.</p> <code>None</code> <code>inference_mode</code> <code>bool</code> <p>Whether the layer is in inference mode. See NOTE 2 in class description for more info. Default is <code>False</code>.</p> <code>False</code> <code>bu_value</code> <code>Union[Tensor, None]</code> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer. Default is <code>None</code>.</p> <code>None</code> <code>n_img_prior</code> <code>Union[int, None]</code> <p>The number of images to be generated from the unconditional prior distribution p(z_L). Default is <code>None</code>.</p> <code>None</code> <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> <code>None</code> <code>force_constant_output</code> <code>bool</code> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> <code>False</code> <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> <code>False</code> <code>var_clip_max</code> <code>Union[float, None]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def forward(\n    self,\n    input_: Union[torch.Tensor, None] = None,\n    skip_connection_input: Union[torch.Tensor, None] = None,\n    inference_mode: bool = False,\n    bu_value: Union[torch.Tensor, None] = None,\n    n_img_prior: Union[int, None] = None,\n    forced_latent: Union[torch.Tensor, None] = None,\n    force_constant_output: bool = False,\n    mode_pred: bool = False,\n    use_uncond_mode: bool = False,\n    var_clip_max: Union[float, None] = None,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"Forward pass.\n\n    Parameters\n    ----------\n    input_: torch.Tensor, optional\n        The input tensor to the layer, which is the output of the top-down layer.\n        Default is `None`.\n    skip_connection_input: torch.Tensor, optional\n        The tensor brought by the skip connection between the current and the\n        previous top-down layer.\n        Default is `None`.\n    inference_mode: bool, optional\n        Whether the layer is in inference mode. See NOTE 2 in class description\n        for more info.\n        Default is `False`.\n    bu_value: torch.Tensor, optional\n        The tensor defining the parameters /mu_q and /sigma_q computed during the\n        bottom-up deterministic pass\n        at the correspondent hierarchical layer. Default is `None`.\n    n_img_prior: int, optional\n        The number of images to be generated from the unconditional prior\n        distribution p(z_L).\n        Default is `None`.\n    forced_latent: torch.Tensor, optional\n        A pre-defined latent tensor. If it is not `None`, than it is used as the\n        actual latent tensor and,\n        hence, sampling does not happen. Default is `None`.\n    force_constant_output: bool, optional\n        Whether to copy the first sample (and rel. distrib parameters) over the\n        whole batch.\n        This is used when doing experiment from the prior - q is not used.\n        Default is `False`.\n    mode_pred: bool, optional\n        Whether the model is in prediction mode. Default is `False`.\n    use_uncond_mode: bool, optional\n        Whether to use the uncoditional distribution p(z) to sample latents in\n        prediction mode.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    # Check consistency of arguments\n    inputs_none = input_ is None and skip_connection_input is None\n    if self.is_top_layer and not inputs_none:\n        raise ValueError(\"In top layer, inputs should be None\")\n\n    p_params = self.get_p_params(input_, n_img_prior)\n\n    # Get the parameters for the latent distribution to sample from\n    if inference_mode:  # TODO What's this ? reuse Fede's code?\n        if self.is_top_layer:\n            q_params = bu_value\n            if mode_pred is False:\n                assert p_params.shape[2:] == bu_value.shape[2:], (\n                    \"Spatial dimensions of p_params and bu_value should match. \"\n                    f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                    f\"bu_value={bu_value.shape[2:]}.\"\n                )\n        else:\n            if use_uncond_mode:\n                q_params = p_params\n            else:\n                assert p_params.shape[2:] == bu_value.shape[2:], (\n                    \"Spatial dimensions of p_params and bu_value should match. \"\n                    f\"Instead, we got p_params={p_params.shape[2:]} and \"\n                    f\"bu_value={bu_value.shape[2:]}.\"\n                )\n                q_params = self.merge(bu_value, p_params)\n    else:  # generative mode, q is not used, we sample from p(z_i | z_{i+1})\n        q_params = None\n\n    # NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})\n    # depending on the mode (hence, in practice, by checking whether q_params is None).\n\n    # Normalization of latent space parameters for stablity.\n    # See Very deep VAEs generalize autoregressive models.\n    if self.normalize_latent_factor:\n        q_params = q_params / self.normalize_latent_factor\n\n    # Sample (and process) a latent tensor in the stochastic layer\n    x, data_stoch = self.stochastic(\n        p_params=p_params,\n        q_params=q_params,\n        forced_latent=forced_latent,\n        force_constant_output=force_constant_output,\n        analytical_kl=self.analytical_kl,\n        mode_pred=mode_pred,\n        use_uncond_mode=use_uncond_mode,\n        var_clip_max=var_clip_max,\n    )\n    # Merge skip connection from previous layer\n    if self.stochastic_skip and not self.is_top_layer:\n        x = self.skip_connection_merger(x, skip_connection_input)\n    if self.retain_spatial_dims:\n        # NOTE: we assume that one topdown layer will have exactly one upscaling layer.\n\n        # NOTE: in case, in the Bottom-Up layer, LC retains spatial dimensions,\n        # we have the following (see `MergeLowRes`):\n        # - the \"primary-flow\" tensor is padded to match the low-res patch size\n        #   (e.g., from 32x32 to 64x64)\n        # - padded tensor is then merged with the low-res patch (concatenation\n        #   along dim=1 + convolution)\n        # Therefore, we need to do the symmetric operation here, that is to\n        # crop `x` for the same amount we padded it in the correspondent BU layer.\n\n        # NOTE: cropping is done to retain the shape of the input in the output.\n        # Therefore we need it only in the case `x` is the same shape of the input,\n        # because that's the only case in which we need to retain the shape.\n        # Here, it must be strictly greater than half the input shape, which is\n        # the case if and only if `x.shape == self.latent_shape`.\n        rescale = (\n            np.array((1, 2, 2)) if len(self.latent_shape) == 3 else np.array((2, 2))\n        )  # TODO better way?\n        new_latent_shape = tuple(np.array(self.latent_shape) // rescale)\n        if x.shape[-1] &gt; new_latent_shape[-1]:\n            x = crop_img_tensor(x, new_latent_shape)\n    # TODO: `retain_spatial_dims` is the same for all the TD layers.\n    # How to handle the case in which we do not have LC for all layers?\n    # The answer is in `self.latent_shape`, which is equal to `input_image_shape`\n    # (e.g., (64, 64)) if `retain_spatial_dims` is `True`, else it is `None`.\n    # Last top-down block (sequence of residual blocks w\\ upsampling)\n    x = self.deterministic_block(x)\n    # Save some metrics that will be used in the loss computation\n    keys = [\n        \"z\",\n        \"kl_samplewise\",\n        \"kl_samplewise_restricted\",\n        \"kl_spatial\",\n        \"kl_channelwise\",\n        \"logprob_q\",\n        \"qvar_max\",\n    ]\n    data = {k: data_stoch.get(k, None) for k in keys}\n    data[\"q_mu\"] = None\n    data[\"q_lv\"] = None\n    if data_stoch[\"q_params\"] is not None:\n        q_mu, q_lv = data_stoch[\"q_params\"]\n        data[\"q_mu\"] = q_mu\n        data[\"q_lv\"] = q_lv\n    return x, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.get_p_params","title":"<code>get_p_params(input_, n_img_prior)</code>","text":"<p>Return the parameters of the prior distribution p(z_i|z_{i+1}).</p> <p>The parameters depend on the hierarchical level of the layer: - if it is the topmost level, parameters are the ones of the prior. - else, the input from the layer above is the parameters itself.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input tensor to the layer, which is the output of the top-down layer above.</p> required <code>n_img_prior</code> <code>int</code> <p>The number of images to be generated from the unconditional prior distribution p(z_L).</p> required Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def get_p_params(\n    self,\n    input_: torch.Tensor,\n    n_img_prior: int,\n) -&gt; torch.Tensor:\n    \"\"\"Return the parameters of the prior distribution p(z_i|z_{i+1}).\n\n    The parameters depend on the hierarchical level of the layer:\n    - if it is the topmost level, parameters are the ones of the prior.\n    - else, the input from the layer above is the parameters itself.\n\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The input tensor to the layer, which is the output of the top-down layer above.\n    n_img_prior: int\n        The number of images to be generated from the unconditional prior distribution p(z_L).\n    \"\"\"\n    p_params = None\n\n    # If top layer, define p_params as the ones of the prior p(z_L)\n    if self.is_top_layer:\n        p_params = self.top_prior_params\n\n        # Sample specific number of images by expanding the prior\n        if n_img_prior is not None:\n            p_params = p_params.expand(n_img_prior, -1, -1, -1)\n\n    # Else the input from the layer above is p_params itself\n    else:\n        p_params = input_\n\n    return p_params\n</code></pre>"},{"location":"reference/careamics/models/lvae/layers/#careamics.models.lvae.layers.TopDownLayer.sample_from_q","title":"<code>sample_from_q(input_, bu_value, var_clip_max=None, mask=None)</code>","text":"<p>Method computes the latent inference distribution q(z_i|z_{i+1}).</p> <p>Used for sampling a latent tensor from it.</p> <p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The input tensor to the layer, which is the output of the top-down layer.</p> required <code>bu_value</code> <code>Tensor</code> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer.</p> required <code>var_clip_max</code> <code>Optional[float]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> <code>None</code> <code>mask</code> <code>Tensor</code> <p>A tensor that is used to mask the sampled latent tensor. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/layers.py</code> <pre><code>def sample_from_q(\n    self,\n    input_: torch.Tensor,\n    bu_value: torch.Tensor,\n    var_clip_max: Optional[float] = None,\n    mask: torch.Tensor = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Method computes the latent inference distribution q(z_i|z_{i+1}).\n\n    Used for sampling a latent tensor from it.\n\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The input tensor to the layer, which is the output of the top-down layer.\n    bu_value: torch.Tensor\n        The tensor defining the parameters /mu_q and /sigma_q computed during the\n        bottom-up deterministic pass at the correspondent hierarchical layer.\n    var_clip_max: float, optional\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped. Default is `None`.\n    mask: Union[None, torch.Tensor], optional\n        A tensor that is used to mask the sampled latent tensor. Default is `None`.\n    \"\"\"\n    if self.is_top_layer:  # In top layer, we don't merge bu_value with p_params\n        q_params = bu_value\n    else:\n        # NOTE: Here the assumption is that the vampprior is only applied on the top layer.\n        n_img_prior = None\n        p_params = self.get_p_params(input_, n_img_prior)\n        q_params = self.merge(bu_value, p_params)\n\n    sample = self.stochastic.sample_from_q(q_params, var_clip_max)\n\n    if mask:\n        return sample[mask]\n\n    return sample\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/","title":"likelihoods","text":"<p>Script containing modules for defining different likelihood functions (as nn.Module).</p>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood","title":"<code>GaussianLikelihood</code>","text":"<p>               Bases: <code>LikelihoodModule</code></p> <p>A specialized <code>LikelihoodModule</code> for Gaussian likelihood.</p> <p>Specifically, in the LVAE model, the likelihood is defined as:     p(x|z_1) = N(x|\\mu_{p,1}, \\sigma_{p,1}^2)</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class GaussianLikelihood(LikelihoodModule):\n    r\"\"\"A specialized `LikelihoodModule` for Gaussian likelihood.\n\n    Specifically, in the LVAE model, the likelihood is defined as:\n        p(x|z_1) = N(x|\\mu_{p,1}, \\sigma_{p,1}^2)\n    \"\"\"\n\n    def __init__(\n        self,\n        predict_logvar: Union[Literal[\"pixelwise\"], None] = None,\n        logvar_lowerbound: Union[float, None] = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        predict_logvar: Union[Literal[\"pixelwise\"], None], optional\n            If `pixelwise`, log-variance is computed for each pixel, else log-variance\n            is not computed. Default is `None`.\n        logvar_lowerbound: float, optional\n            The lowerbound value for log-variance. Default is `None`.\n        \"\"\"\n        super().__init__()\n\n        self.predict_logvar = predict_logvar\n        self.logvar_lowerbound = logvar_lowerbound\n        assert self.predict_logvar in [None, \"pixelwise\"]\n\n        print(\n            f\"[{self.__class__.__name__}] PredLVar:{self.predict_logvar} LowBLVar:{self.logvar_lowerbound}\"\n        )\n\n    def get_mean_lv(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Given the output of the top-down pass, compute the mean and log-variance of the\n        Gaussian distribution defining the likelihood.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor to the likelihood module, i.e., the output of the top-down\n            pass.\n\n        Returns\n        -------\n        tuple of (torch.tensor, optional torch.tensor)\n            The first element of the tuple is the mean, the second element is the\n            log-variance. If the attribute `predict_logvar` is `None` then the second\n            element will be `None`.\n        \"\"\"\n        # if LadderVAE.predict_logvar is None, dim 1 of `x`` has no. of target channels\n        if self.predict_logvar is None:\n            return x, None\n\n        # Get pixel-wise mean and logvar\n        # if LadderVAE.predict_logvar is not None,\n        #   dim 1 has double no. of target channels\n        mean, lv = x.chunk(2, dim=1)\n\n        # Optionally, clip log-var to a lower bound\n        if self.logvar_lowerbound is not None:\n            lv = torch.clip(lv, min=self.logvar_lowerbound)\n\n        return mean, lv\n\n    def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor to the likelihood module, i.e., the output\n            the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case\n            `predict_logvar` is not None, or (B, C, [Z], Y, X) otherwise.\n        \"\"\"\n        mean, lv = self.get_mean_lv(x)\n        params = {\n            \"mean\": mean,\n            \"logvar\": lv,\n        }\n        return params\n\n    @staticmethod\n    def mean(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def mode(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def sample(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        # p = Normal(params['mean'], (params['logvar'] / 2).exp())\n        # return p.rsample()\n        return params[\"mean\"]\n\n    @staticmethod\n    def logvar(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"logvar\"]\n\n    def log_likelihood(\n        self, x: torch.Tensor, params: dict[str, Union[torch.Tensor, None]]\n    ):\n        \"\"\"Compute Gaussian log-likelihood\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The target tensor. Shape is (B, C, [Z], Y, X).\n        params: dict[str, Union[torch.Tensor, None]]\n            The tensors obtained by chunking the output of the top-down pass,\n            here used as parameters of the Gaussian distribution.\n\n        Returns\n        -------\n        torch.Tensor\n            The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n        \"\"\"\n        if self.predict_logvar is not None:\n            logprob = log_normal(x, params[\"mean\"], params[\"logvar\"])\n        else:\n            logprob = -0.5 * (params[\"mean\"] - x) ** 2\n        return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.__init__","title":"<code>__init__(predict_logvar=None, logvar_lowerbound=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>predict_logvar</code> <code>Union[Literal['pixelwise'], None]</code> <p>If <code>pixelwise</code>, log-variance is computed for each pixel, else log-variance is not computed. Default is <code>None</code>.</p> <code>None</code> <code>logvar_lowerbound</code> <code>Union[float, None]</code> <p>The lowerbound value for log-variance. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def __init__(\n    self,\n    predict_logvar: Union[Literal[\"pixelwise\"], None] = None,\n    logvar_lowerbound: Union[float, None] = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    predict_logvar: Union[Literal[\"pixelwise\"], None], optional\n        If `pixelwise`, log-variance is computed for each pixel, else log-variance\n        is not computed. Default is `None`.\n    logvar_lowerbound: float, optional\n        The lowerbound value for log-variance. Default is `None`.\n    \"\"\"\n    super().__init__()\n\n    self.predict_logvar = predict_logvar\n    self.logvar_lowerbound = logvar_lowerbound\n    assert self.predict_logvar in [None, \"pixelwise\"]\n\n    print(\n        f\"[{self.__class__.__name__}] PredLVar:{self.predict_logvar} LowBLVar:{self.logvar_lowerbound}\"\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.distr_params","title":"<code>distr_params(x)</code>","text":"<p>Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the likelihood module, i.e., the output the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case <code>predict_logvar</code> is not None, or (B, C, [Z], Y, X) otherwise.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Get parameters (mean, log-var) of the Gaussian distribution defined by the likelihood.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor to the likelihood module, i.e., the output\n        the LVAE 'output_layer'. Shape is: (B, 2 * C, [Z], Y, X) in case\n        `predict_logvar` is not None, or (B, C, [Z], Y, X) otherwise.\n    \"\"\"\n    mean, lv = self.get_mean_lv(x)\n    params = {\n        \"mean\": mean,\n        \"logvar\": lv,\n    }\n    return params\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.get_mean_lv","title":"<code>get_mean_lv(x)</code>","text":"<p>Given the output of the top-down pass, compute the mean and log-variance of the Gaussian distribution defining the likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to the likelihood module, i.e., the output of the top-down pass.</p> required <p>Returns:</p> Type Description <code>tuple of (torch.tensor, optional torch.tensor)</code> <p>The first element of the tuple is the mean, the second element is the log-variance. If the attribute <code>predict_logvar</code> is <code>None</code> then the second element will be <code>None</code>.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def get_mean_lv(\n    self, x: torch.Tensor\n) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Given the output of the top-down pass, compute the mean and log-variance of the\n    Gaussian distribution defining the likelihood.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor to the likelihood module, i.e., the output of the top-down\n        pass.\n\n    Returns\n    -------\n    tuple of (torch.tensor, optional torch.tensor)\n        The first element of the tuple is the mean, the second element is the\n        log-variance. If the attribute `predict_logvar` is `None` then the second\n        element will be `None`.\n    \"\"\"\n    # if LadderVAE.predict_logvar is None, dim 1 of `x`` has no. of target channels\n    if self.predict_logvar is None:\n        return x, None\n\n    # Get pixel-wise mean and logvar\n    # if LadderVAE.predict_logvar is not None,\n    #   dim 1 has double no. of target channels\n    mean, lv = x.chunk(2, dim=1)\n\n    # Optionally, clip log-var to a lower bound\n    if self.logvar_lowerbound is not None:\n        lv = torch.clip(lv, min=self.logvar_lowerbound)\n\n    return mean, lv\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.GaussianLikelihood.log_likelihood","title":"<code>log_likelihood(x, params)</code>","text":"<p>Compute Gaussian log-likelihood</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The target tensor. Shape is (B, C, [Z], Y, X).</p> required <code>params</code> <code>dict[str, Union[Tensor, None]]</code> <p>The tensors obtained by chunking the output of the top-down pass, here used as parameters of the Gaussian distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The log-likelihood tensor. Shape is (B, C, [Z], Y, X).</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_likelihood(\n    self, x: torch.Tensor, params: dict[str, Union[torch.Tensor, None]]\n):\n    \"\"\"Compute Gaussian log-likelihood\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The target tensor. Shape is (B, C, [Z], Y, X).\n    params: dict[str, Union[torch.Tensor, None]]\n        The tensors obtained by chunking the output of the top-down pass,\n        here used as parameters of the Gaussian distribution.\n\n    Returns\n    -------\n    torch.Tensor\n        The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n    \"\"\"\n    if self.predict_logvar is not None:\n        logprob = log_normal(x, params[\"mean\"], params[\"logvar\"])\n    else:\n        logprob = -0.5 * (params[\"mean\"] - x) ** 2\n    return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.LikelihoodModule","title":"<code>LikelihoodModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>The base class for all likelihood modules. It defines the fundamental structure and methods for specialized likelihood models.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class LikelihoodModule(nn.Module):\n    \"\"\"\n    The base class for all likelihood modules.\n    It defines the fundamental structure and methods for specialized likelihood models.\n    \"\"\"\n\n    def distr_params(self, x: Any) -&gt; None:\n        return None\n\n    def set_params_to_same_device_as(self, correct_device_tensor: Any) -&gt; None:\n        pass\n\n    @staticmethod\n    def logvar(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def mean(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def mode(params: Any) -&gt; None:\n        return None\n\n    @staticmethod\n    def sample(params: Any) -&gt; None:\n        return None\n\n    def log_likelihood(self, x: Any, params: Any) -&gt; None:\n        return None\n\n    def get_mean_lv(\n        self, x: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]: ...\n\n    def forward(\n        self, input_: torch.Tensor, x: Union[torch.Tensor, None]\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Parameters\n        ----------\n        input_: torch.Tensor\n            The output of the top-down pass (e.g., reconstructed image in HDN,\n            or the unmixed images in 'Split' models).\n        x: Union[torch.Tensor, None]\n            The target tensor. If None, the log-likelihood is not computed.\n        \"\"\"\n        distr_params = self.distr_params(input_)\n        mean = self.mean(distr_params)\n        mode = self.mode(distr_params)\n        sample = self.sample(distr_params)\n        logvar = self.logvar(distr_params)\n\n        if x is None:\n            ll = None\n        else:\n            ll = self.log_likelihood(x, distr_params)\n\n        dct = {\n            \"mean\": mean,\n            \"mode\": mode,\n            \"sample\": sample,\n            \"params\": distr_params,\n            \"logvar\": logvar,\n        }\n\n        return ll, dct\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.LikelihoodModule.forward","title":"<code>forward(input_, x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_</code> <code>Tensor</code> <p>The output of the top-down pass (e.g., reconstructed image in HDN, or the unmixed images in 'Split' models).</p> required <code>x</code> <code>Union[Tensor, None]</code> <p>The target tensor. If None, the log-likelihood is not computed.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def forward(\n    self, input_: torch.Tensor, x: Union[torch.Tensor, None]\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Parameters\n    ----------\n    input_: torch.Tensor\n        The output of the top-down pass (e.g., reconstructed image in HDN,\n        or the unmixed images in 'Split' models).\n    x: Union[torch.Tensor, None]\n        The target tensor. If None, the log-likelihood is not computed.\n    \"\"\"\n    distr_params = self.distr_params(input_)\n    mean = self.mean(distr_params)\n    mode = self.mode(distr_params)\n    sample = self.sample(distr_params)\n    logvar = self.logvar(distr_params)\n\n    if x is None:\n        ll = None\n    else:\n        ll = self.log_likelihood(x, distr_params)\n\n    dct = {\n        \"mean\": mean,\n        \"mode\": mode,\n        \"sample\": sample,\n        \"params\": distr_params,\n        \"logvar\": logvar,\n    }\n\n    return ll, dct\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood","title":"<code>NoiseModelLikelihood</code>","text":"<p>               Bases: <code>LikelihoodModule</code></p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>class NoiseModelLikelihood(LikelihoodModule):\n\n    def __init__(\n        self,\n        data_mean: Union[np.ndarray, torch.Tensor],\n        data_std: Union[np.ndarray, torch.Tensor],\n        noise_model: NoiseModel,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        data_mean: Union[np.ndarray, torch.Tensor]\n            The mean of the data, used to unnormalize data for noise model evaluation.\n        data_std: Union[np.ndarray, torch.Tensor]\n            The standard deviation of the data, used to unnormalize data for noise\n            model evaluation.\n        noiseModel: NoiseModel\n            The noise model instance used to compute the likelihood.\n        \"\"\"\n        super().__init__()\n        self.data_mean = torch.Tensor(data_mean)\n        self.data_std = torch.Tensor(data_std)\n        self.noiseModel = noise_model\n\n    def _set_params_to_same_device_as(\n        self, correct_device_tensor: torch.Tensor\n    ) -&gt; None:\n        \"\"\"Set the parameters to the same device as the input tensor.\n\n        Parameters\n        ----------\n        correct_device_tensor: torch.Tensor\n            The tensor whose device is used to set the parameters.\n        \"\"\"\n        if self.data_mean.device != correct_device_tensor.device:\n            self.data_mean = self.data_mean.to(correct_device_tensor.device)\n            self.data_std = self.data_std.to(correct_device_tensor.device)\n        if correct_device_tensor.device != self.noiseModel.device:\n            self.noiseModel.to_device(correct_device_tensor.device)\n\n    def get_mean_lv(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, None]:\n        return x, None\n\n    def distr_params(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        mean, lv = self.get_mean_lv(x)\n        params = {\n            \"mean\": mean,\n            \"logvar\": lv,\n        }\n        return params\n\n    @staticmethod\n    def mean(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def mode(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    @staticmethod\n    def sample(params: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        return params[\"mean\"]\n\n    def log_likelihood(self, x: torch.Tensor, params: dict[str, torch.Tensor]):\n        \"\"\"Compute the log-likelihood given the parameters `params` obtained\n        from the reconstruction tensor and the target tensor `x`.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The target tensor. Shape is (B, C, [Z], Y, X).\n        params: dict[str, Union[torch.Tensor, None]]\n            The tensors obtained from output of the top-down pass.\n            Here, \"mean\" correspond to the whole output, while logvar is `None`.\n\n        Returns\n        -------\n        torch.Tensor\n            The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n        \"\"\"\n        self._set_params_to_same_device_as(x)\n        predicted_s_denormalized = params[\"mean\"] * self.data_std + self.data_mean\n        x_denormalized = x * self.data_std + self.data_mean\n        likelihoods = self.noiseModel.likelihood(\n            x_denormalized, predicted_s_denormalized\n        )\n        logprob = torch.log(likelihoods)\n        return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.__init__","title":"<code>__init__(data_mean, data_std, noise_model)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>data_mean</code> <code>Union[ndarray, Tensor]</code> <p>The mean of the data, used to unnormalize data for noise model evaluation.</p> required <code>data_std</code> <code>Union[ndarray, Tensor]</code> <p>The standard deviation of the data, used to unnormalize data for noise model evaluation.</p> required <code>noiseModel</code> <p>The noise model instance used to compute the likelihood.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def __init__(\n    self,\n    data_mean: Union[np.ndarray, torch.Tensor],\n    data_std: Union[np.ndarray, torch.Tensor],\n    noise_model: NoiseModel,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    data_mean: Union[np.ndarray, torch.Tensor]\n        The mean of the data, used to unnormalize data for noise model evaluation.\n    data_std: Union[np.ndarray, torch.Tensor]\n        The standard deviation of the data, used to unnormalize data for noise\n        model evaluation.\n    noiseModel: NoiseModel\n        The noise model instance used to compute the likelihood.\n    \"\"\"\n    super().__init__()\n    self.data_mean = torch.Tensor(data_mean)\n    self.data_std = torch.Tensor(data_std)\n    self.noiseModel = noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood._set_params_to_same_device_as","title":"<code>_set_params_to_same_device_as(correct_device_tensor)</code>","text":"<p>Set the parameters to the same device as the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>correct_device_tensor</code> <code>Tensor</code> <p>The tensor whose device is used to set the parameters.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def _set_params_to_same_device_as(\n    self, correct_device_tensor: torch.Tensor\n) -&gt; None:\n    \"\"\"Set the parameters to the same device as the input tensor.\n\n    Parameters\n    ----------\n    correct_device_tensor: torch.Tensor\n        The tensor whose device is used to set the parameters.\n    \"\"\"\n    if self.data_mean.device != correct_device_tensor.device:\n        self.data_mean = self.data_mean.to(correct_device_tensor.device)\n        self.data_std = self.data_std.to(correct_device_tensor.device)\n    if correct_device_tensor.device != self.noiseModel.device:\n        self.noiseModel.to_device(correct_device_tensor.device)\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.NoiseModelLikelihood.log_likelihood","title":"<code>log_likelihood(x, params)</code>","text":"<p>Compute the log-likelihood given the parameters <code>params</code> obtained from the reconstruction tensor and the target tensor <code>x</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The target tensor. Shape is (B, C, [Z], Y, X).</p> required <code>params</code> <code>dict[str, Tensor]</code> <p>The tensors obtained from output of the top-down pass. Here, \"mean\" correspond to the whole output, while logvar is <code>None</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The log-likelihood tensor. Shape is (B, C, [Z], Y, X).</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_likelihood(self, x: torch.Tensor, params: dict[str, torch.Tensor]):\n    \"\"\"Compute the log-likelihood given the parameters `params` obtained\n    from the reconstruction tensor and the target tensor `x`.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The target tensor. Shape is (B, C, [Z], Y, X).\n    params: dict[str, Union[torch.Tensor, None]]\n        The tensors obtained from output of the top-down pass.\n        Here, \"mean\" correspond to the whole output, while logvar is `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        The log-likelihood tensor. Shape is (B, C, [Z], Y, X).\n    \"\"\"\n    self._set_params_to_same_device_as(x)\n    predicted_s_denormalized = params[\"mean\"] * self.data_std + self.data_mean\n    x_denormalized = x * self.data_std + self.data_mean\n    likelihoods = self.noiseModel.likelihood(\n        x_denormalized, predicted_s_denormalized\n    )\n    logprob = torch.log(likelihoods)\n    return logprob\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.likelihood_factory","title":"<code>likelihood_factory(config, noise_model=None)</code>","text":"<p>Factory function for creating likelihood modules.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[GaussianLikelihoodConfig, NMLikelihoodConfig]]</code> <p>The configuration object for the likelihood module.</p> required <code>noise_model</code> <code>Optional[NoiseModel]</code> <p>The noise model instance used to define the <code>NoiseModelLikelihood</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>The likelihood module.</p> Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def likelihood_factory(\n    config: Optional[Union[GaussianLikelihoodConfig, NMLikelihoodConfig]],\n    noise_model: Optional[NoiseModel] = None,\n):\n    \"\"\"\n    Factory function for creating likelihood modules.\n\n    Parameters\n    ----------\n    config: Union[GaussianLikelihoodConfig, NMLikelihoodConfig]\n        The configuration object for the likelihood module.\n    noise_model: Optional[NoiseModel]\n        The noise model instance used to define the `NoiseModelLikelihood`.\n\n    Returns\n    -------\n    nn.Module\n        The likelihood module.\n    \"\"\"\n    if config is None:\n        return None\n\n    if isinstance(config, GaussianLikelihoodConfig):\n        return GaussianLikelihood(\n            predict_logvar=config.predict_logvar,\n            logvar_lowerbound=config.logvar_lowerbound,\n        )\n    elif isinstance(config, NMLikelihoodConfig):\n        return NoiseModelLikelihood(\n            data_mean=config.data_mean,\n            data_std=config.data_std,\n            noise_model=noise_model,\n        )\n    else:\n        raise ValueError(f\"Invalid likelihood model type: {config.model_type}\")\n</code></pre>"},{"location":"reference/careamics/models/lvae/likelihoods/#careamics.models.lvae.likelihoods.log_normal","title":"<code>log_normal(x, mean, logvar)</code>","text":"<p>Compute the log-probability at <code>x</code> of a Gaussian distribution with parameters <code>(mean, exp(logvar))</code>.</p> <p>NOTE: In the case of LVAE, the log-likeihood formula becomes:     \\mathbb{E}{z_1\\sim{q\\phi}}[\\log{p_    heta(x|z_1)}]=-\frac{1}{2}(\\mathbb{E}{z_1\\sim{q\\phi}}[\\log{2\\pi\\sigma_{p,0}^2(z_1)}] +\\mathbb{E}{z_1\\sim{q\\phi}}[\frac{(x-\\mu_{p,0}(z_1))^2}{\\sigma_{p,0}^2(z_1)}])</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The ground-truth tensor. Shape is (batch, channels, dim1, dim2).</p> required <code>mean</code> <code>Tensor</code> <p>The inferred mean of distribution. Shape is (batch, channels, dim1, dim2).</p> required <code>logvar</code> <code>Tensor</code> <p>The inferred log-variance of distribution. Shape has to be either scalar or broadcastable.</p> required Source code in <code>src/careamics/models/lvae/likelihoods.py</code> <pre><code>def log_normal(\n    x: torch.Tensor, mean: torch.Tensor, logvar: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the log-probability at `x` of a Gaussian distribution\n    with parameters `(mean, exp(logvar))`.\n\n    NOTE: In the case of LVAE, the log-likeihood formula becomes:\n        \\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\\\log{p_\\theta(x|z_1)}]=-\\frac{1}{2}(\\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\\\log{2\\\\pi\\\\sigma_{p,0}^2(z_1)}] +\\\\mathbb{E}_{z_1\\\\sim{q_\\\\phi}}[\\frac{(x-\\\\mu_{p,0}(z_1))^2}{\\\\sigma_{p,0}^2(z_1)}])\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The ground-truth tensor. Shape is (batch, channels, dim1, dim2).\n    mean: torch.Tensor\n        The inferred mean of distribution. Shape is (batch, channels, dim1, dim2).\n    logvar: torch.Tensor\n        The inferred log-variance of distribution. Shape has to be either scalar or broadcastable.\n    \"\"\"\n    var = torch.exp(logvar)\n    log_prob = -0.5 * (\n        ((x - mean) ** 2) / var + logvar + torch.tensor(2 * math.pi).log()\n    )\n    return log_prob\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/","title":"lvae","text":"<p>Ladder VAE (LVAE) Model.</p> <p>The current implementation is based on \"Interpretable Unsupervised Diversity Denoising and Artefact Removal, Prakash et al.\"</p>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE","title":"<code>LadderVAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>int</code> <p>The size of the input image.</p> required <code>output_channels</code> <code>int</code> <p>The number of output channels.</p> required <code>multiscale_count</code> <code>int</code> <p>The number of scales for multiscale processing.</p> required <code>z_dims</code> <code>list[int]</code> <p>The dimensions of the latent space for each layer.</p> required <code>encoder_n_filters</code> <code>int</code> <p>The number of filters in the encoder.</p> required <code>decoder_n_filters</code> <code>int</code> <p>The number of filters in the decoder.</p> required <code>encoder_conv_strides</code> <code>list[int]</code> <p>The strides for the conv layers encoder.</p> required <code>decoder_conv_strides</code> <code>list[int]</code> <p>The strides for the conv layers decoder.</p> required <code>encoder_dropout</code> <code>float</code> <p>The dropout rate for the encoder.</p> required <code>decoder_dropout</code> <code>float</code> <p>The dropout rate for the decoder.</p> required <code>nonlinearity</code> <code>str</code> <p>The nonlinearity function to use.</p> required <code>predict_logvar</code> <code>bool</code> <p>Whether to predict the log variance.</p> required <code>analytical_kl</code> <code>bool</code> <p>Whether to use analytical KL divergence.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If only 2D convolutions are supported.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>class LadderVAE(nn.Module):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    input_shape : int\n        The size of the input image.\n    output_channels : int\n        The number of output channels.\n    multiscale_count : int\n        The number of scales for multiscale processing.\n    z_dims : list[int]\n        The dimensions of the latent space for each layer.\n    encoder_n_filters : int\n        The number of filters in the encoder.\n    decoder_n_filters : int\n        The number of filters in the decoder.\n    encoder_conv_strides : list[int]\n        The strides for the conv layers encoder.\n    decoder_conv_strides : list[int]\n        The strides for the conv layers decoder.\n    encoder_dropout : float\n        The dropout rate for the encoder.\n    decoder_dropout : float\n        The dropout rate for the decoder.\n    nonlinearity : str\n        The nonlinearity function to use.\n    predict_logvar : bool\n        Whether to predict the log variance.\n    analytical_kl : bool\n        Whether to use analytical KL divergence.\n\n    Raises\n    ------\n    NotImplementedError\n        If only 2D convolutions are supported.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_shape: int,\n        output_channels: int,\n        multiscale_count: int,\n        z_dims: list[int],\n        encoder_n_filters: int,\n        decoder_n_filters: int,\n        encoder_conv_strides: list[int],\n        decoder_conv_strides: list[int],\n        encoder_dropout: float,\n        decoder_dropout: float,\n        nonlinearity: str,\n        predict_logvar: bool,\n        analytical_kl: bool,\n    ):\n        super().__init__()\n\n        # -------------------------------------------------------\n        # Customizable attributes\n        self.image_size = input_shape\n        \"\"\"Input image size. (Z, Y, X) or (Y, X) if the data is 2D.\"\"\"\n        # TODO: we need to be careful with this since used to be an int.\n        # the tuple of shapes used to be `self.input_shape`.\n        self.target_ch = output_channels\n        self.encoder_conv_strides = encoder_conv_strides\n        self.decoder_conv_strides = decoder_conv_strides\n        self._multiscale_count = multiscale_count\n        self.z_dims = z_dims\n        self.encoder_n_filters = encoder_n_filters\n        self.decoder_n_filters = decoder_n_filters\n        self.encoder_dropout = encoder_dropout\n        self.decoder_dropout = decoder_dropout\n        self.nonlin = nonlinearity\n        self.predict_logvar = predict_logvar\n        self.analytical_kl = analytical_kl\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Model attributes -&gt; Hardcoded\n        self.model_type = ModelType.LadderVae  # TODO remove !\n        self.encoder_blocks_per_layer = 1\n        self.decoder_blocks_per_layer = 1\n        self.bottomup_batchnorm = True\n        self.topdown_batchnorm = True\n        self.topdown_conv2d_bias = True\n        self.gated = True\n        self.encoder_res_block_kernel = 3\n        self.decoder_res_block_kernel = 3\n        self.encoder_res_block_skip_padding = False\n        self.decoder_res_block_skip_padding = False\n        self.merge_type = \"residual\"\n        self.no_initial_downscaling = True\n        self.skip_bottomk_buvalues = 0\n        self.stochastic_skip = True\n        self.learn_top_prior = True\n        self.res_block_type = \"bacdbacd\"  # TODO remove !\n        self.mode_pred = False\n        self.logvar_lowerbound = -5\n        self._var_clip_max = 20\n        self._stochastic_use_naive_exponential = False\n        self._enable_topdown_normalize_factor = True\n\n        # Attributes that handle LC -&gt; Hardcoded\n        self.enable_multiscale = self._multiscale_count &gt; 1\n        self.multiscale_retain_spatial_dims = True\n        self.multiscale_lowres_separate_branch = False\n        self.multiscale_decoder_retain_spatial_dims = (\n            self.multiscale_retain_spatial_dims and self.enable_multiscale\n        )\n\n        # Derived attributes\n        self.n_layers = len(self.z_dims)\n\n        # Others...\n        self._tethered_to_input = False\n        self._tethered_ch1_scalar = self._tethered_ch2_scalar = None\n        if self._tethered_to_input:\n            target_ch = 1\n            requires_grad = False\n            self._tethered_ch1_scalar = nn.Parameter(\n                torch.ones(1) * 0.5, requires_grad=requires_grad\n            )\n            self._tethered_ch2_scalar = nn.Parameter(\n                torch.ones(1) * 2.0, requires_grad=requires_grad\n            )\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Data attributes\n        self.color_ch = 1  # TODO for now we only support 1 channel\n        self.normalized_input = True\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        # Loss attributes\n        # enabling reconstruction loss on mixed input\n        self.mixed_rec_w = 0\n        self.nbr_consistency_w = 0\n\n        # -------------------------------------------------------\n        # 3D related stuff\n        self._mode_3D = len(self.image_size) == 3  # TODO refac\n        self._model_3D_depth = self.image_size[0] if self._mode_3D else 1\n        self._decoder_mode_3D = len(self.decoder_conv_strides) == 3\n        if self._mode_3D and not self._decoder_mode_3D:\n            assert self._model_3D_depth % 2 == 1, \"3D model depth should be odd\"\n        assert (\n            self._mode_3D is True or self._decoder_mode_3D is False\n        ), \"Decoder cannot be 3D when encoder is 2D\"\n        self._squish3d = self._mode_3D and not self._decoder_mode_3D\n        self._3D_squisher = (\n            None\n            if not self._squish3d\n            else nn.ModuleList(\n                [\n                    GateLayer(\n                        channels=self.encoder_n_filters,\n                        conv_strides=self.encoder_conv_strides,\n                    )\n                    for k in range(len(self.z_dims))\n                ]\n            )\n        )\n        # TODO: this bit is in the Ashesh's confusing-hacky style... Can we do better?\n\n        # -------------------------------------------------------\n        # # Training attributes\n        # # can be used to tile the validation predictions\n        # self._val_idx_manager = val_idx_manager\n        # self._val_frame_creator = None\n        # # initialize the learning rate scheduler params.\n        # self.lr_scheduler_monitor = self.lr_scheduler_mode = None\n        # self._init_lr_scheduler_params(config)\n        # self._global_step = 0\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n\n        # Calculate the downsampling happening in the network\n        self.downsample = [1] * self.n_layers\n        self.overall_downscale_factor = np.power(2, sum(self.downsample))\n        if not self.no_initial_downscaling:  # by default do another downscaling\n            self.overall_downscale_factor *= 2\n\n        assert max(self.downsample) &lt;= self.encoder_blocks_per_layer\n        assert len(self.downsample) == self.n_layers\n        # -------------------------------------------------------\n\n        # -------------------------------------------------------\n        ### CREATE MODEL BLOCKS\n        # First bottom-up layer: change num channels + downsample by factor 2\n        # unless we want to prevent this\n        self.encoder_conv_op = getattr(nn, f\"Conv{len(self.encoder_conv_strides)}d\")\n        # TODO these should be defined for all layers here ?\n        self.decoder_conv_op = getattr(nn, f\"Conv{len(self.decoder_conv_strides)}d\")\n        # TODO: would be more readable to have a derived parameters to use like\n        # `conv_dims = len(self.encoder_conv_strides)` and then use `Conv{conv_dims}d`\n        stride = 1 if self.no_initial_downscaling else 2\n        self.first_bottom_up = self.create_first_bottom_up(stride)\n\n        # Input Branches for Lateral Contextualization\n        self.lowres_first_bottom_ups = None\n        self._init_multires()\n\n        # Other bottom-up layers\n        self.bottom_up_layers = self.create_bottom_up_layers(\n            self.multiscale_lowres_separate_branch\n        )\n\n        # Top-down layers\n        self.top_down_layers = self.create_top_down_layers()\n        self.final_top_down = self.create_final_topdown_layer(\n            not self.no_initial_downscaling\n        )\n\n        # Likelihood module\n        # self.likelihood = self.create_likelihood_module()\n\n        # Output layer --&gt; Project to target_ch many channels\n        logvar_ch_needed = self.predict_logvar is not None\n        self.output_layer = self.parameter_net = self.decoder_conv_op(\n            self.decoder_n_filters,\n            self.target_ch * (1 + logvar_ch_needed),\n            kernel_size=3,\n            padding=1,\n            bias=self.topdown_conv2d_bias,\n        )\n\n        # # gradient norms. updated while training. this is also logged.\n        # self.grad_norm_bottom_up = 0.0\n        # self.grad_norm_top_down = 0.0\n        # PSNR computation on validation.\n        # self.label1_psnr = RunningPSNR()\n        # self.label2_psnr = RunningPSNR()\n        # TODO: did you add this?\n\n        # msg =f'[{self.__class__.__name__}] Stoc:{not self.non_stochastic_version} RecMode:{self.reconstruction_mode} TethInput:{self._tethered_to_input}'\n        # msg += f' TargetCh: {self.target_ch}'\n        # print(msg)\n\n    ### SET OF METHODS TO CREATE MODEL BLOCKS\n    def create_first_bottom_up(\n        self,\n        init_stride: int,\n        num_res_blocks: int = 1,\n    ) -&gt; nn.Sequential:\n        \"\"\"\n        Method creates the first bottom-up block of the Encoder.\n\n        Its role is to perform a first image compression step.\n        It is composed by a sequence of nn.Conv2d + non-linearity +\n        BottomUpDeterministicResBlock (1 or more, default is 1).\n\n        Parameters\n        ----------\n        init_stride: int\n            The stride used by the intial Conv2d block.\n        num_res_blocks: int, optional\n            The number of BottomUpDeterministicResBlocks, default is 1.\n        \"\"\"\n        # From what I got from Ashesh, Z should not be touched in any case.\n        nonlin = get_activation(self.nonlin)\n        conv_block = self.encoder_conv_op(\n            in_channels=self.color_ch,\n            out_channels=self.encoder_n_filters,\n            kernel_size=self.encoder_res_block_kernel,\n            padding=(\n                0\n                if self.encoder_res_block_skip_padding\n                else self.encoder_res_block_kernel // 2\n            ),\n            stride=init_stride,\n        )\n\n        modules = [conv_block, nonlin]\n\n        for _ in range(num_res_blocks):\n            modules.append(\n                BottomUpDeterministicResBlock(\n                    conv_strides=self.encoder_conv_strides,\n                    c_in=self.encoder_n_filters,\n                    c_out=self.encoder_n_filters,\n                    nonlin=nonlin,\n                    downsample=False,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.encoder_res_block_kernel,\n                )\n            )\n\n        return nn.Sequential(*modules)\n\n    def create_bottom_up_layers(self, lowres_separate_branch: bool) -&gt; nn.ModuleList:\n        \"\"\"\n        Method creates the stack of bottom-up layers of the Encoder.\n\n        that are used to generate the so-called `bu_values`.\n\n        NOTE:\n            If `self._multiscale_count &lt; self.n_layers`, then LC is done only in the first\n            `self._multiscale_count` bottom-up layers (starting from the bottom).\n\n        Parameters\n        ----------\n        lowres_separate_branch: bool\n            Whether the residual block(s) used for encoding the low-res input are shared\n            (`False`) or not (`True`) with the \"same-size\" residual block(s) in the\n            `BottomUpLayer`'s primary flow.\n        \"\"\"\n        multiscale_lowres_size_factor = 1\n        nonlin = get_activation(self.nonlin)\n\n        bottom_up_layers = nn.ModuleList([])\n        for i in range(self.n_layers):\n            # Whether this is the top layer\n            is_top = i == self.n_layers - 1\n\n            # LC is applied only to the first (_multiscale_count - 1) bottom-up layers\n            layer_enable_multiscale = (\n                self.enable_multiscale and self._multiscale_count &gt; i + 1\n            )\n\n            # This factor determines the factor by which the low-resolution tensor is larger\n            # N.B. Only used if layer_enable_multiscale == True, so we updated it only in that case\n            multiscale_lowres_size_factor *= 1 + int(layer_enable_multiscale)\n\n            # TODO: check correctness of this\n            if self._multiscale_count &gt; 1:\n                output_expected_shape = (dim // 2 ** (i + 1) for dim in self.image_size)\n            else:\n                output_expected_shape = None\n\n            # Add bottom-up deterministic layer at level i.\n            # It's a sequence of residual blocks (BottomUpDeterministicResBlock), possibly with downsampling between them.\n            bottom_up_layers.append(\n                BottomUpLayer(\n                    n_res_blocks=self.encoder_blocks_per_layer,\n                    n_filters=self.encoder_n_filters,\n                    downsampling_steps=self.downsample[i],\n                    nonlin=nonlin,\n                    conv_strides=self.encoder_conv_strides,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.encoder_res_block_kernel,\n                    gated=self.gated,\n                    lowres_separate_branch=lowres_separate_branch,\n                    enable_multiscale=self.enable_multiscale,  # TODO: shouldn't the arg be `layer_enable_multiscale` here?\n                    multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n                    multiscale_lowres_size_factor=multiscale_lowres_size_factor,\n                    decoder_retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                    output_expected_shape=output_expected_shape,\n                )\n            )\n\n        return bottom_up_layers\n\n    def create_top_down_layers(self) -&gt; nn.ModuleList:\n        \"\"\"\n        Method creates the stack of top-down layers of the Decoder.\n\n        In these layer the `bu`_values` from the Encoder are merged with the `p_params` from the previous layer\n        of the Decoder to get `q_params`. Then, a stochastic layer generates a sample from the latent distribution\n        with parameters `q_params`. Finally, this sample is fed through a TopDownDeterministicResBlock to\n        compute the `p_params` for the layer below.\n\n        NOTE 1:\n            The algorithm for generative inference approximately works as follows:\n                - p_params = output of top-down layer above\n                - bu = inferred bottom-up value at this layer\n                - q_params = merge(bu, p_params)\n                - z = stochastic_layer(q_params)\n                - (optional) get and merge skip connection from prev top-down layer\n                - top-down deterministic ResNet\n\n        NOTE 2:\n            When doing unconditional generation, bu_value is not available. Hence the\n            merge layer is not used, and z is sampled directly from p_params.\n\n        \"\"\"\n        top_down_layers = nn.ModuleList([])\n        nonlin = get_activation(self.nonlin)\n        # NOTE: top-down layers are created starting from the bottom-most\n        for i in range(self.n_layers):\n            # Check if this is the top layer\n            is_top = i == self.n_layers - 1\n\n            if self._enable_topdown_normalize_factor:  # TODO: What is this?\n                normalize_latent_factor = (\n                    1 / np.sqrt(2 * (1 + i)) if len(self.z_dims) &gt; 4 else 1.0\n                )\n            else:\n                normalize_latent_factor = 1.0\n\n            top_down_layers.append(\n                TopDownLayer(\n                    z_dim=self.z_dims[i],\n                    n_res_blocks=self.decoder_blocks_per_layer,\n                    n_filters=self.decoder_n_filters,\n                    is_top_layer=is_top,\n                    conv_strides=self.decoder_conv_strides,\n                    upsampling_steps=self.downsample[i],\n                    nonlin=nonlin,\n                    merge_type=self.merge_type,\n                    batchnorm=self.topdown_batchnorm,\n                    dropout=self.decoder_dropout,\n                    stochastic_skip=self.stochastic_skip,\n                    learn_top_prior=self.learn_top_prior,\n                    top_prior_param_shape=self.get_top_prior_param_shape(),\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.decoder_res_block_kernel,\n                    gated=self.gated,\n                    analytical_kl=self.analytical_kl,\n                    vanilla_latent_hw=self.get_latent_spatial_size(i),\n                    retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                    input_image_shape=self.image_size,\n                    normalize_latent_factor=normalize_latent_factor,\n                    conv2d_bias=self.topdown_conv2d_bias,\n                    stochastic_use_naive_exponential=self._stochastic_use_naive_exponential,\n                )\n            )\n        return top_down_layers\n\n    def create_final_topdown_layer(self, upsample: bool) -&gt; nn.Sequential:\n        \"\"\"Create the final top-down layer of the Decoder.\n\n        NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation\n        instead of transposed convolution (like in other TD layers).\n\n        Parameters\n        ----------\n        upsample: bool\n            Whether to upsample the input of the final top-down layer\n            by bilinear interpolation with `scale_factor=2`.\n        \"\"\"\n        # Final top-down layer\n        modules = list()\n\n        if upsample:\n            modules.append(Interpolate(scale=2))\n\n        for i in range(self.decoder_blocks_per_layer):\n            modules.append(\n                TopDownDeterministicResBlock(\n                    c_in=self.decoder_n_filters,\n                    c_out=self.decoder_n_filters,\n                    nonlin=get_activation(self.nonlin),\n                    conv_strides=self.decoder_conv_strides,\n                    batchnorm=self.topdown_batchnorm,\n                    dropout=self.decoder_dropout,\n                    res_block_type=self.res_block_type,\n                    res_block_kernel=self.decoder_res_block_kernel,\n                    gated=self.gated,\n                    conv2d_bias=self.topdown_conv2d_bias,\n                )\n            )\n        return nn.Sequential(*modules)\n\n    def _init_multires(self, config=None) -&gt; nn.ModuleList:\n        \"\"\"\n        Method defines the input block/branch to encode/compress low-res lateral inputs.\n\n        at different hierarchical levels\n        in the multiresolution approach (LC). The role of the input branches is similar\n        to the one of the first bottom-up layer in the primary flow of the Encoder,\n        namely to compress the lateral input image to a degree that is compatible with\n        the one of the primary flow.\n\n        NOTE 1: Each input branch consists of a sequence of Conv2d + non-linearity\n        + BottomUpDeterministicResBlock. It is meaningful to observe that the\n        `BottomUpDeterministicResBlock` shares the same model attributes with the blocks\n        in the primary flow of the Encoder (e.g., c_in, c_out, dropout, etc. etc.).\n        Moreover, it does not perform downsampling.\n\n        NOTE 2: `_multiscale_count` attribute defines the total number of inputs to the\n        bottom-up pass. In other terms if we have the input patch and n_LC additional\n        lateral inputs, we will have a total of (n_LC + 1) inputs.\n        \"\"\"\n        stride = 1 if self.no_initial_downscaling else 2\n        nonlin = get_activation(self.nonlin)\n        if self._multiscale_count is None:\n            self._multiscale_count = 1\n\n        msg = (\n            f\"Multiscale count ({self._multiscale_count}) should not exceed the number\"\n            f\"of bottom up layers ({self.n_layers}) by more than 1.\\n\"\n        )\n        assert (\n            self._multiscale_count &lt;= 1 or self._multiscale_count &lt;= 1 + self.n_layers\n        ), msg  # TODO how ?\n\n        msg = (\n            \"Multiscale approach only supports monocrome images. \"\n            f\"Found instead color_ch={self.color_ch}.\"\n        )\n        # assert self._multiscale_count == 1 or self.color_ch == 1, msg\n\n        lowres_first_bottom_ups = []\n        for _ in range(1, self._multiscale_count):\n            first_bottom_up = nn.Sequential(\n                self.encoder_conv_op(\n                    in_channels=self.color_ch,\n                    out_channels=self.encoder_n_filters,\n                    kernel_size=5,\n                    padding=\"same\",\n                    stride=stride,\n                ),\n                nonlin,\n                BottomUpDeterministicResBlock(\n                    c_in=self.encoder_n_filters,\n                    c_out=self.encoder_n_filters,\n                    conv_strides=self.encoder_conv_strides,\n                    nonlin=nonlin,\n                    downsample=False,\n                    batchnorm=self.bottomup_batchnorm,\n                    dropout=self.encoder_dropout,\n                    res_block_type=self.res_block_type,\n                ),\n            )\n            lowres_first_bottom_ups.append(first_bottom_up)\n\n        self.lowres_first_bottom_ups = (\n            nn.ModuleList(lowres_first_bottom_ups)\n            if len(lowres_first_bottom_ups)\n            else None\n        )\n\n    ### SET OF FORWARD-LIKE METHODS\n    def bottomup_pass(self, inp: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"Wrapper of _bottomup_pass().\"\"\"\n        # TODO Remove wrapper\n        return self._bottomup_pass(\n            inp,\n            self.first_bottom_up,\n            self.lowres_first_bottom_ups,\n            self.bottom_up_layers,\n        )\n\n    def _bottomup_pass(\n        self,\n        inp: torch.Tensor,\n        first_bottom_up: nn.Sequential,\n        lowres_first_bottom_ups: nn.ModuleList,\n        bottom_up_layers: nn.ModuleList,\n    ) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Method defines the forward pass through the LVAE Encoder, the so-called.\n\n        Bottom-Up pass.\n\n        Parameters\n        ----------\n        inp: torch.Tensor\n            The input tensor to the bottom-up pass of shape (B, 1+n_LC, H, W), where n_LC\n            is the number of lateral low-res inputs used in the LC approach.\n            In particular, the first channel corresponds to the input patch, while the\n            remaining ones are associated to the lateral low-res inputs.\n        first_bottom_up: nn.Sequential\n            The module defining the first bottom-up layer of the Encoder.\n        lowres_first_bottom_ups: nn.ModuleList\n            The list of modules defining Lateral Contextualization.\n        bottom_up_layers: nn.ModuleList\n            The list of modules defining the stack of bottom-up layers of the Encoder.\n        \"\"\"\n        if self._multiscale_count &gt; 1:\n            x = first_bottom_up(inp[:, :1])\n        else:\n            x = first_bottom_up(inp)\n\n        # Loop from bottom to top layer, store all deterministic nodes we\n        # need for the top-down pass in bu_values list\n        bu_values = []\n        for i in range(self.n_layers):\n            lowres_x = None\n            if self._multiscale_count &gt; 1 and i + 1 &lt; inp.shape[1]:\n                lowres_x = lowres_first_bottom_ups[i](inp[:, i + 1 : i + 2])\n            x, bu_value = bottom_up_layers[i](x, lowres_x=lowres_x)\n            bu_values.append(bu_value)\n\n        return bu_values\n\n    def topdown_pass(\n        self,\n        bu_values: Union[torch.Tensor, None] = None,\n        n_img_prior: Union[torch.Tensor, None] = None,\n        constant_layers: Union[Iterable[int], None] = None,\n        forced_latent: Union[list[torch.Tensor], None] = None,\n        top_down_layers: Union[nn.ModuleList, None] = None,\n        final_top_down_layer: Union[nn.Sequential, None] = None,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Method defines the forward pass through the LVAE Decoder, the so-called.\n\n        Top-Down pass.\n\n        Parameters\n        ----------\n        bu_values: torch.Tensor, optional\n            Output of the bottom-up pass. It will have values from multiple layers of\n            the ladder.\n        n_img_prior: optional\n            When `bu_values` is `None`, `n_img_prior` indicates the number of images to\n            generate\n            from the prior (so bottom-up pass is not used at all here).\n        constant_layers: Iterable[int], optional\n            A sequence of indexes associated to the layers in which a single instance's\n            z is copied over the entire batch (bottom-up path is not used, so only prior\n            is used here). Set to `None` to avoid this behaviour.\n        forced_latent: list[torch.Tensor], optional\n            A list of tensors that are used as fixed latent variables (hence, sampling\n            doesn't take place in this case).\n        top_down_layers: nn.ModuleList, optional\n            A list of top-down layers to use in the top-down pass. If `None`, the method\n            uses the default layers defined in the constructor.\n        final_top_down_layer: nn.Sequential, optional\n            The last top-down layer of the top-down pass. If `None`, the method uses the\n            default layers defined in the constructor.\n        \"\"\"\n        if top_down_layers is None:\n            top_down_layers = self.top_down_layers\n        if final_top_down_layer is None:\n            final_top_down_layer = self.final_top_down\n\n        # Default: no layer is sampled from the distribution's mode\n        if constant_layers is None:\n            constant_layers = []\n        prior_experiment = len(constant_layers) &gt; 0\n\n        # If the bottom-up inference values are not given, don't do\n        # inference, sample from prior instead\n        inference_mode = bu_values is not None\n\n        # Check consistency of arguments\n        if inference_mode != (n_img_prior is None):\n            msg = (\n                \"Number of images for top-down generation has to be given \"\n                \"if and only if we're not doing inference\"\n            )\n            raise RuntimeError(msg)\n        if inference_mode and prior_experiment:\n            msg = (\n                \"Prior experiments (e.g. sampling from mode) are not\"\n                \" compatible with inference mode\"\n            )\n            raise RuntimeError(msg)\n\n        # Sampled latent variables at each layer\n        z = [None] * self.n_layers\n        # KL divergence of each layer\n        kl = [None] * self.n_layers\n        # Kl divergence restricted, only for the LC enabled setup denoiSplit.\n        kl_restricted = [None] * self.n_layers\n        # mean from which z is sampled.\n        q_mu = [None] * self.n_layers\n        # log(var) from which z is sampled.\n        q_lv = [None] * self.n_layers\n        # Spatial map of KL divergence for each layer\n        kl_spatial = [None] * self.n_layers\n        debug_qvar_max = [None] * self.n_layers\n        kl_channelwise = [None] * self.n_layers\n        if forced_latent is None:\n            forced_latent = [None] * self.n_layers\n\n        # Top-down inference/generation loop\n        out = None\n        for i in reversed(range(self.n_layers)):\n            # If available, get deterministic node from bottom-up inference\n            try:\n                bu_value = bu_values[i]\n            except TypeError:\n                bu_value = None\n\n            # Whether the current layer should be sampled from the mode\n            constant_out = i in constant_layers\n\n            # Input for skip connection\n            skip_input = out\n\n            # Full top-down layer, including sampling and deterministic part\n            out, aux = top_down_layers[i](\n                input_=out,\n                skip_connection_input=skip_input,\n                inference_mode=inference_mode,\n                bu_value=bu_value,\n                n_img_prior=n_img_prior,\n                force_constant_output=constant_out,\n                forced_latent=forced_latent[i],\n                mode_pred=self.mode_pred,\n                var_clip_max=self._var_clip_max,\n            )\n            # Save useful variables\n            z[i] = aux[\"z\"]  # sampled variable at this layer (batch, ch, h, w)\n            kl[i] = aux[\"kl_samplewise\"]  # (batch, )\n            kl_restricted[i] = aux[\"kl_samplewise_restricted\"]\n            kl_spatial[i] = aux[\"kl_spatial\"]  # (batch, h, w)\n            q_mu[i] = aux[\"q_mu\"]\n            q_lv[i] = aux[\"q_lv\"]\n\n            kl_channelwise[i] = aux[\"kl_channelwise\"]\n            debug_qvar_max[i] = aux[\"qvar_max\"]\n            # if self.mode_pred is False:\n            #     logprob_p += aux['logprob_p'].mean()  # mean over batch\n            # else:\n            #     logprob_p = None\n\n        # Final top-down layer\n        out = final_top_down_layer(out)\n\n        # Store useful variables in a dict to return them\n        data = {\n            \"z\": z,  # list of tensors with shape (batch, ch[i], h[i], w[i])\n            \"kl\": kl,  # list of tensors with shape (batch, )\n            \"kl_restricted\": kl_restricted,  # list of tensors with shape (batch, )\n            \"kl_spatial\": kl_spatial,  # list of tensors w shape (batch, h[i], w[i])\n            \"kl_channelwise\": kl_channelwise,  # list of tensors with shape (batch, ch[i])\n            # 'logprob_p': logprob_p,  # scalar, mean over batch\n            \"q_mu\": q_mu,\n            \"q_lv\": q_lv,\n            \"debug_qvar_max\": debug_qvar_max,\n        }\n        return out, data\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Forward pass through the LVAE model.\n\n        Parameters\n        ----------\n        x: torch.Tensor\n            The input tensor of shape (B, C, H, W).\n        \"\"\"\n        img_size = x.size()[2:]\n\n        # Bottom-up inference: return list of length n_layers (bottom to top)\n        bu_values = self.bottomup_pass(x)\n        for i in range(0, self.skip_bottomk_buvalues):\n            bu_values[i] = None\n\n        if self._squish3d:\n            bu_values = [\n                torch.mean(self._3D_squisher[k](bu_value), dim=2)\n                for k, bu_value in enumerate(bu_values)\n            ]\n\n        # Top-down inference/generation\n        out, td_data = self.topdown_pass(bu_values)\n\n        if out.shape[-1] &gt; img_size[-1]:\n            # Restore original image size\n            out = crop_img_tensor(out, img_size)\n\n        out = self.output_layer(out)\n\n        return out, td_data\n\n    ### SET OF GETTERS\n    def get_padded_size(self, size):\n        \"\"\"\n        Returns the smallest size (H, W) of the image with actual size given\n        as input, such that H and W are powers of 2.\n        :param size: input size, tuple either (N, C, H, W) or (H, W)\n        :return: 2-tuple (H, W)\n        \"\"\"\n        # Make size argument into (heigth, width)\n        # assert len(size) in [2, 4, 5] # TODO commented out cuz it's weird\n        # We're only interested in the Y,X dimensions\n        size = size[-2:]\n\n        if self.multiscale_decoder_retain_spatial_dims is True:\n            # In this case, we can go much more deeper and so this is not required\n            # (in the way it is. ;). More work would be needed if this was to be correctly implemented )\n            return list(size)\n\n        # Overall downscale factor from input to top layer (power of 2)\n        dwnsc = self.overall_downscale_factor\n\n        # Output smallest powers of 2 that are larger than current sizes\n        padded_size = [((s - 1) // dwnsc + 1) * dwnsc for s in size]\n        # TODO Needed for pad/crop odd sizes. Move to dataset?\n        return padded_size\n\n    def get_latent_spatial_size(self, level_idx: int):\n        \"\"\"Level_idx: 0 is the bottommost layer, the highest resolution one.\"\"\"\n        actual_downsampling = level_idx + 1\n        dwnsc = 2**actual_downsampling\n        sz = self.get_padded_size(self.image_size)\n        h = sz[0] // dwnsc\n        w = sz[1] // dwnsc\n        assert h == w\n        return h\n\n    def get_top_prior_param_shape(self, n_imgs: int = 1):\n\n        # Compute the total downscaling performed in the Encoder\n        if self.multiscale_decoder_retain_spatial_dims is False:\n            dwnsc = self.overall_downscale_factor\n        else:\n            # LC allow the encoder latents to keep the same (H, W) size at different levels\n            actual_downsampling = self.n_layers + 1 - self._multiscale_count\n            dwnsc = 2**actual_downsampling\n\n        h = self.image_size[-2] // dwnsc\n        w = self.image_size[-1] // dwnsc\n        mu_logvar = self.z_dims[-1] * 2  # mu and logvar\n        top_layer_shape = (n_imgs, mu_logvar, h, w)\n        # TODO refactor!\n        if self._model_3D_depth &gt; 1 and self._decoder_mode_3D is True:\n            # TODO check if model_3D_depth is needed ?\n            top_layer_shape = (n_imgs, mu_logvar, self._model_3D_depth, h, w)\n        return top_layer_shape\n\n    def reset_for_inference(self, tile_size: Optional[tuple[int, int]] = None):\n        \"\"\"Should be called if we want to predict for a different input/output size.\"\"\"\n        self.mode_pred = True\n        if tile_size is None:\n            tile_size = self.image_size\n        self.image_size = tile_size\n        for i in range(self.n_layers):\n            self.bottom_up_layers[i].output_expected_shape = (\n                ts // 2 ** (i + 1) for ts in tile_size\n            )\n            self.top_down_layers[i].latent_shape = tile_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.image_size","title":"<code>image_size = input_shape</code>  <code>instance-attribute</code>","text":"<p>Input image size. (Z, Y, X) or (Y, X) if the data is 2D.</p>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE._bottomup_pass","title":"<code>_bottomup_pass(inp, first_bottom_up, lowres_first_bottom_ups, bottom_up_layers)</code>","text":"<p>Method defines the forward pass through the LVAE Encoder, the so-called.</p> <p>Bottom-Up pass.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>Tensor</code> <p>The input tensor to the bottom-up pass of shape (B, 1+n_LC, H, W), where n_LC is the number of lateral low-res inputs used in the LC approach. In particular, the first channel corresponds to the input patch, while the remaining ones are associated to the lateral low-res inputs.</p> required <code>first_bottom_up</code> <code>Sequential</code> <p>The module defining the first bottom-up layer of the Encoder.</p> required <code>lowres_first_bottom_ups</code> <code>ModuleList</code> <p>The list of modules defining Lateral Contextualization.</p> required <code>bottom_up_layers</code> <code>ModuleList</code> <p>The list of modules defining the stack of bottom-up layers of the Encoder.</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def _bottomup_pass(\n    self,\n    inp: torch.Tensor,\n    first_bottom_up: nn.Sequential,\n    lowres_first_bottom_ups: nn.ModuleList,\n    bottom_up_layers: nn.ModuleList,\n) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Method defines the forward pass through the LVAE Encoder, the so-called.\n\n    Bottom-Up pass.\n\n    Parameters\n    ----------\n    inp: torch.Tensor\n        The input tensor to the bottom-up pass of shape (B, 1+n_LC, H, W), where n_LC\n        is the number of lateral low-res inputs used in the LC approach.\n        In particular, the first channel corresponds to the input patch, while the\n        remaining ones are associated to the lateral low-res inputs.\n    first_bottom_up: nn.Sequential\n        The module defining the first bottom-up layer of the Encoder.\n    lowres_first_bottom_ups: nn.ModuleList\n        The list of modules defining Lateral Contextualization.\n    bottom_up_layers: nn.ModuleList\n        The list of modules defining the stack of bottom-up layers of the Encoder.\n    \"\"\"\n    if self._multiscale_count &gt; 1:\n        x = first_bottom_up(inp[:, :1])\n    else:\n        x = first_bottom_up(inp)\n\n    # Loop from bottom to top layer, store all deterministic nodes we\n    # need for the top-down pass in bu_values list\n    bu_values = []\n    for i in range(self.n_layers):\n        lowres_x = None\n        if self._multiscale_count &gt; 1 and i + 1 &lt; inp.shape[1]:\n            lowres_x = lowres_first_bottom_ups[i](inp[:, i + 1 : i + 2])\n        x, bu_value = bottom_up_layers[i](x, lowres_x=lowres_x)\n        bu_values.append(bu_value)\n\n    return bu_values\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE._init_multires","title":"<code>_init_multires(config=None)</code>","text":"<p>Method defines the input block/branch to encode/compress low-res lateral inputs.</p> <p>at different hierarchical levels in the multiresolution approach (LC). The role of the input branches is similar to the one of the first bottom-up layer in the primary flow of the Encoder, namely to compress the lateral input image to a degree that is compatible with the one of the primary flow.</p> <p>NOTE 1: Each input branch consists of a sequence of Conv2d + non-linearity + BottomUpDeterministicResBlock. It is meaningful to observe that the <code>BottomUpDeterministicResBlock</code> shares the same model attributes with the blocks in the primary flow of the Encoder (e.g., c_in, c_out, dropout, etc. etc.). Moreover, it does not perform downsampling.</p> <p>NOTE 2: <code>_multiscale_count</code> attribute defines the total number of inputs to the bottom-up pass. In other terms if we have the input patch and n_LC additional lateral inputs, we will have a total of (n_LC + 1) inputs.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def _init_multires(self, config=None) -&gt; nn.ModuleList:\n    \"\"\"\n    Method defines the input block/branch to encode/compress low-res lateral inputs.\n\n    at different hierarchical levels\n    in the multiresolution approach (LC). The role of the input branches is similar\n    to the one of the first bottom-up layer in the primary flow of the Encoder,\n    namely to compress the lateral input image to a degree that is compatible with\n    the one of the primary flow.\n\n    NOTE 1: Each input branch consists of a sequence of Conv2d + non-linearity\n    + BottomUpDeterministicResBlock. It is meaningful to observe that the\n    `BottomUpDeterministicResBlock` shares the same model attributes with the blocks\n    in the primary flow of the Encoder (e.g., c_in, c_out, dropout, etc. etc.).\n    Moreover, it does not perform downsampling.\n\n    NOTE 2: `_multiscale_count` attribute defines the total number of inputs to the\n    bottom-up pass. In other terms if we have the input patch and n_LC additional\n    lateral inputs, we will have a total of (n_LC + 1) inputs.\n    \"\"\"\n    stride = 1 if self.no_initial_downscaling else 2\n    nonlin = get_activation(self.nonlin)\n    if self._multiscale_count is None:\n        self._multiscale_count = 1\n\n    msg = (\n        f\"Multiscale count ({self._multiscale_count}) should not exceed the number\"\n        f\"of bottom up layers ({self.n_layers}) by more than 1.\\n\"\n    )\n    assert (\n        self._multiscale_count &lt;= 1 or self._multiscale_count &lt;= 1 + self.n_layers\n    ), msg  # TODO how ?\n\n    msg = (\n        \"Multiscale approach only supports monocrome images. \"\n        f\"Found instead color_ch={self.color_ch}.\"\n    )\n    # assert self._multiscale_count == 1 or self.color_ch == 1, msg\n\n    lowres_first_bottom_ups = []\n    for _ in range(1, self._multiscale_count):\n        first_bottom_up = nn.Sequential(\n            self.encoder_conv_op(\n                in_channels=self.color_ch,\n                out_channels=self.encoder_n_filters,\n                kernel_size=5,\n                padding=\"same\",\n                stride=stride,\n            ),\n            nonlin,\n            BottomUpDeterministicResBlock(\n                c_in=self.encoder_n_filters,\n                c_out=self.encoder_n_filters,\n                conv_strides=self.encoder_conv_strides,\n                nonlin=nonlin,\n                downsample=False,\n                batchnorm=self.bottomup_batchnorm,\n                dropout=self.encoder_dropout,\n                res_block_type=self.res_block_type,\n            ),\n        )\n        lowres_first_bottom_ups.append(first_bottom_up)\n\n    self.lowres_first_bottom_ups = (\n        nn.ModuleList(lowres_first_bottom_ups)\n        if len(lowres_first_bottom_ups)\n        else None\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.bottomup_pass","title":"<code>bottomup_pass(inp)</code>","text":"<p>Wrapper of _bottomup_pass().</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def bottomup_pass(self, inp: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"Wrapper of _bottomup_pass().\"\"\"\n    # TODO Remove wrapper\n    return self._bottomup_pass(\n        inp,\n        self.first_bottom_up,\n        self.lowres_first_bottom_ups,\n        self.bottom_up_layers,\n    )\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_bottom_up_layers","title":"<code>create_bottom_up_layers(lowres_separate_branch)</code>","text":"<p>Method creates the stack of bottom-up layers of the Encoder.</p> <p>that are used to generate the so-called <code>bu_values</code>.</p> <p>NOTE:     If <code>self._multiscale_count &lt; self.n_layers</code>, then LC is done only in the first     <code>self._multiscale_count</code> bottom-up layers (starting from the bottom).</p> <p>Parameters:</p> Name Type Description Default <code>lowres_separate_branch</code> <code>bool</code> <p>Whether the residual block(s) used for encoding the low-res input are shared (<code>False</code>) or not (<code>True</code>) with the \"same-size\" residual block(s) in the <code>BottomUpLayer</code>'s primary flow.</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_bottom_up_layers(self, lowres_separate_branch: bool) -&gt; nn.ModuleList:\n    \"\"\"\n    Method creates the stack of bottom-up layers of the Encoder.\n\n    that are used to generate the so-called `bu_values`.\n\n    NOTE:\n        If `self._multiscale_count &lt; self.n_layers`, then LC is done only in the first\n        `self._multiscale_count` bottom-up layers (starting from the bottom).\n\n    Parameters\n    ----------\n    lowres_separate_branch: bool\n        Whether the residual block(s) used for encoding the low-res input are shared\n        (`False`) or not (`True`) with the \"same-size\" residual block(s) in the\n        `BottomUpLayer`'s primary flow.\n    \"\"\"\n    multiscale_lowres_size_factor = 1\n    nonlin = get_activation(self.nonlin)\n\n    bottom_up_layers = nn.ModuleList([])\n    for i in range(self.n_layers):\n        # Whether this is the top layer\n        is_top = i == self.n_layers - 1\n\n        # LC is applied only to the first (_multiscale_count - 1) bottom-up layers\n        layer_enable_multiscale = (\n            self.enable_multiscale and self._multiscale_count &gt; i + 1\n        )\n\n        # This factor determines the factor by which the low-resolution tensor is larger\n        # N.B. Only used if layer_enable_multiscale == True, so we updated it only in that case\n        multiscale_lowres_size_factor *= 1 + int(layer_enable_multiscale)\n\n        # TODO: check correctness of this\n        if self._multiscale_count &gt; 1:\n            output_expected_shape = (dim // 2 ** (i + 1) for dim in self.image_size)\n        else:\n            output_expected_shape = None\n\n        # Add bottom-up deterministic layer at level i.\n        # It's a sequence of residual blocks (BottomUpDeterministicResBlock), possibly with downsampling between them.\n        bottom_up_layers.append(\n            BottomUpLayer(\n                n_res_blocks=self.encoder_blocks_per_layer,\n                n_filters=self.encoder_n_filters,\n                downsampling_steps=self.downsample[i],\n                nonlin=nonlin,\n                conv_strides=self.encoder_conv_strides,\n                batchnorm=self.bottomup_batchnorm,\n                dropout=self.encoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.encoder_res_block_kernel,\n                gated=self.gated,\n                lowres_separate_branch=lowres_separate_branch,\n                enable_multiscale=self.enable_multiscale,  # TODO: shouldn't the arg be `layer_enable_multiscale` here?\n                multiscale_retain_spatial_dims=self.multiscale_retain_spatial_dims,\n                multiscale_lowres_size_factor=multiscale_lowres_size_factor,\n                decoder_retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                output_expected_shape=output_expected_shape,\n            )\n        )\n\n    return bottom_up_layers\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_final_topdown_layer","title":"<code>create_final_topdown_layer(upsample)</code>","text":"<p>Create the final top-down layer of the Decoder.</p> <p>NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation instead of transposed convolution (like in other TD layers).</p> <p>Parameters:</p> Name Type Description Default <code>upsample</code> <code>bool</code> <p>Whether to upsample the input of the final top-down layer by bilinear interpolation with <code>scale_factor=2</code>.</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_final_topdown_layer(self, upsample: bool) -&gt; nn.Sequential:\n    \"\"\"Create the final top-down layer of the Decoder.\n\n    NOTE: In this layer, (optional) upsampling is performed by bilinear interpolation\n    instead of transposed convolution (like in other TD layers).\n\n    Parameters\n    ----------\n    upsample: bool\n        Whether to upsample the input of the final top-down layer\n        by bilinear interpolation with `scale_factor=2`.\n    \"\"\"\n    # Final top-down layer\n    modules = list()\n\n    if upsample:\n        modules.append(Interpolate(scale=2))\n\n    for i in range(self.decoder_blocks_per_layer):\n        modules.append(\n            TopDownDeterministicResBlock(\n                c_in=self.decoder_n_filters,\n                c_out=self.decoder_n_filters,\n                nonlin=get_activation(self.nonlin),\n                conv_strides=self.decoder_conv_strides,\n                batchnorm=self.topdown_batchnorm,\n                dropout=self.decoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.decoder_res_block_kernel,\n                gated=self.gated,\n                conv2d_bias=self.topdown_conv2d_bias,\n            )\n        )\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_first_bottom_up","title":"<code>create_first_bottom_up(init_stride, num_res_blocks=1)</code>","text":"<p>Method creates the first bottom-up block of the Encoder.</p> <p>Its role is to perform a first image compression step. It is composed by a sequence of nn.Conv2d + non-linearity + BottomUpDeterministicResBlock (1 or more, default is 1).</p> <p>Parameters:</p> Name Type Description Default <code>init_stride</code> <code>int</code> <p>The stride used by the intial Conv2d block.</p> required <code>num_res_blocks</code> <code>int</code> <p>The number of BottomUpDeterministicResBlocks, default is 1.</p> <code>1</code> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_first_bottom_up(\n    self,\n    init_stride: int,\n    num_res_blocks: int = 1,\n) -&gt; nn.Sequential:\n    \"\"\"\n    Method creates the first bottom-up block of the Encoder.\n\n    Its role is to perform a first image compression step.\n    It is composed by a sequence of nn.Conv2d + non-linearity +\n    BottomUpDeterministicResBlock (1 or more, default is 1).\n\n    Parameters\n    ----------\n    init_stride: int\n        The stride used by the intial Conv2d block.\n    num_res_blocks: int, optional\n        The number of BottomUpDeterministicResBlocks, default is 1.\n    \"\"\"\n    # From what I got from Ashesh, Z should not be touched in any case.\n    nonlin = get_activation(self.nonlin)\n    conv_block = self.encoder_conv_op(\n        in_channels=self.color_ch,\n        out_channels=self.encoder_n_filters,\n        kernel_size=self.encoder_res_block_kernel,\n        padding=(\n            0\n            if self.encoder_res_block_skip_padding\n            else self.encoder_res_block_kernel // 2\n        ),\n        stride=init_stride,\n    )\n\n    modules = [conv_block, nonlin]\n\n    for _ in range(num_res_blocks):\n        modules.append(\n            BottomUpDeterministicResBlock(\n                conv_strides=self.encoder_conv_strides,\n                c_in=self.encoder_n_filters,\n                c_out=self.encoder_n_filters,\n                nonlin=nonlin,\n                downsample=False,\n                batchnorm=self.bottomup_batchnorm,\n                dropout=self.encoder_dropout,\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.encoder_res_block_kernel,\n            )\n        )\n\n    return nn.Sequential(*modules)\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.create_top_down_layers","title":"<code>create_top_down_layers()</code>","text":"<p>Method creates the stack of top-down layers of the Decoder.</p> <p>In these layer the <code>bu</code>_values<code>from the Encoder are merged with the</code>p_params<code>from the previous layer of the Decoder to get</code>q_params<code>. Then, a stochastic layer generates a sample from the latent distribution with parameters</code>q_params<code>. Finally, this sample is fed through a TopDownDeterministicResBlock to compute the</code>p_params` for the layer below.</p> <p>NOTE 1:     The algorithm for generative inference approximately works as follows:         - p_params = output of top-down layer above         - bu = inferred bottom-up value at this layer         - q_params = merge(bu, p_params)         - z = stochastic_layer(q_params)         - (optional) get and merge skip connection from prev top-down layer         - top-down deterministic ResNet</p> <p>NOTE 2:     When doing unconditional generation, bu_value is not available. Hence the     merge layer is not used, and z is sampled directly from p_params.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def create_top_down_layers(self) -&gt; nn.ModuleList:\n    \"\"\"\n    Method creates the stack of top-down layers of the Decoder.\n\n    In these layer the `bu`_values` from the Encoder are merged with the `p_params` from the previous layer\n    of the Decoder to get `q_params`. Then, a stochastic layer generates a sample from the latent distribution\n    with parameters `q_params`. Finally, this sample is fed through a TopDownDeterministicResBlock to\n    compute the `p_params` for the layer below.\n\n    NOTE 1:\n        The algorithm for generative inference approximately works as follows:\n            - p_params = output of top-down layer above\n            - bu = inferred bottom-up value at this layer\n            - q_params = merge(bu, p_params)\n            - z = stochastic_layer(q_params)\n            - (optional) get and merge skip connection from prev top-down layer\n            - top-down deterministic ResNet\n\n    NOTE 2:\n        When doing unconditional generation, bu_value is not available. Hence the\n        merge layer is not used, and z is sampled directly from p_params.\n\n    \"\"\"\n    top_down_layers = nn.ModuleList([])\n    nonlin = get_activation(self.nonlin)\n    # NOTE: top-down layers are created starting from the bottom-most\n    for i in range(self.n_layers):\n        # Check if this is the top layer\n        is_top = i == self.n_layers - 1\n\n        if self._enable_topdown_normalize_factor:  # TODO: What is this?\n            normalize_latent_factor = (\n                1 / np.sqrt(2 * (1 + i)) if len(self.z_dims) &gt; 4 else 1.0\n            )\n        else:\n            normalize_latent_factor = 1.0\n\n        top_down_layers.append(\n            TopDownLayer(\n                z_dim=self.z_dims[i],\n                n_res_blocks=self.decoder_blocks_per_layer,\n                n_filters=self.decoder_n_filters,\n                is_top_layer=is_top,\n                conv_strides=self.decoder_conv_strides,\n                upsampling_steps=self.downsample[i],\n                nonlin=nonlin,\n                merge_type=self.merge_type,\n                batchnorm=self.topdown_batchnorm,\n                dropout=self.decoder_dropout,\n                stochastic_skip=self.stochastic_skip,\n                learn_top_prior=self.learn_top_prior,\n                top_prior_param_shape=self.get_top_prior_param_shape(),\n                res_block_type=self.res_block_type,\n                res_block_kernel=self.decoder_res_block_kernel,\n                gated=self.gated,\n                analytical_kl=self.analytical_kl,\n                vanilla_latent_hw=self.get_latent_spatial_size(i),\n                retain_spatial_dims=self.multiscale_decoder_retain_spatial_dims,\n                input_image_shape=self.image_size,\n                normalize_latent_factor=normalize_latent_factor,\n                conv2d_bias=self.topdown_conv2d_bias,\n                stochastic_use_naive_exponential=self._stochastic_use_naive_exponential,\n            )\n        )\n    return top_down_layers\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the LVAE model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape (B, C, H, W).</p> required Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Forward pass through the LVAE model.\n\n    Parameters\n    ----------\n    x: torch.Tensor\n        The input tensor of shape (B, C, H, W).\n    \"\"\"\n    img_size = x.size()[2:]\n\n    # Bottom-up inference: return list of length n_layers (bottom to top)\n    bu_values = self.bottomup_pass(x)\n    for i in range(0, self.skip_bottomk_buvalues):\n        bu_values[i] = None\n\n    if self._squish3d:\n        bu_values = [\n            torch.mean(self._3D_squisher[k](bu_value), dim=2)\n            for k, bu_value in enumerate(bu_values)\n        ]\n\n    # Top-down inference/generation\n    out, td_data = self.topdown_pass(bu_values)\n\n    if out.shape[-1] &gt; img_size[-1]:\n        # Restore original image size\n        out = crop_img_tensor(out, img_size)\n\n    out = self.output_layer(out)\n\n    return out, td_data\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.get_latent_spatial_size","title":"<code>get_latent_spatial_size(level_idx)</code>","text":"<p>Level_idx: 0 is the bottommost layer, the highest resolution one.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def get_latent_spatial_size(self, level_idx: int):\n    \"\"\"Level_idx: 0 is the bottommost layer, the highest resolution one.\"\"\"\n    actual_downsampling = level_idx + 1\n    dwnsc = 2**actual_downsampling\n    sz = self.get_padded_size(self.image_size)\n    h = sz[0] // dwnsc\n    w = sz[1] // dwnsc\n    assert h == w\n    return h\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.get_padded_size","title":"<code>get_padded_size(size)</code>","text":"<p>Returns the smallest size (H, W) of the image with actual size given as input, such that H and W are powers of 2. :param size: input size, tuple either (N, C, H, W) or (H, W) :return: 2-tuple (H, W)</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def get_padded_size(self, size):\n    \"\"\"\n    Returns the smallest size (H, W) of the image with actual size given\n    as input, such that H and W are powers of 2.\n    :param size: input size, tuple either (N, C, H, W) or (H, W)\n    :return: 2-tuple (H, W)\n    \"\"\"\n    # Make size argument into (heigth, width)\n    # assert len(size) in [2, 4, 5] # TODO commented out cuz it's weird\n    # We're only interested in the Y,X dimensions\n    size = size[-2:]\n\n    if self.multiscale_decoder_retain_spatial_dims is True:\n        # In this case, we can go much more deeper and so this is not required\n        # (in the way it is. ;). More work would be needed if this was to be correctly implemented )\n        return list(size)\n\n    # Overall downscale factor from input to top layer (power of 2)\n    dwnsc = self.overall_downscale_factor\n\n    # Output smallest powers of 2 that are larger than current sizes\n    padded_size = [((s - 1) // dwnsc + 1) * dwnsc for s in size]\n    # TODO Needed for pad/crop odd sizes. Move to dataset?\n    return padded_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.reset_for_inference","title":"<code>reset_for_inference(tile_size=None)</code>","text":"<p>Should be called if we want to predict for a different input/output size.</p> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def reset_for_inference(self, tile_size: Optional[tuple[int, int]] = None):\n    \"\"\"Should be called if we want to predict for a different input/output size.\"\"\"\n    self.mode_pred = True\n    if tile_size is None:\n        tile_size = self.image_size\n    self.image_size = tile_size\n    for i in range(self.n_layers):\n        self.bottom_up_layers[i].output_expected_shape = (\n            ts // 2 ** (i + 1) for ts in tile_size\n        )\n        self.top_down_layers[i].latent_shape = tile_size\n</code></pre>"},{"location":"reference/careamics/models/lvae/lvae/#careamics.models.lvae.lvae.LadderVAE.topdown_pass","title":"<code>topdown_pass(bu_values=None, n_img_prior=None, constant_layers=None, forced_latent=None, top_down_layers=None, final_top_down_layer=None)</code>","text":"<p>Method defines the forward pass through the LVAE Decoder, the so-called.</p> <p>Top-Down pass.</p> <p>Parameters:</p> Name Type Description Default <code>bu_values</code> <code>Union[Tensor, None]</code> <p>Output of the bottom-up pass. It will have values from multiple layers of the ladder.</p> <code>None</code> <code>n_img_prior</code> <code>Union[Tensor, None]</code> <p>When <code>bu_values</code> is <code>None</code>, <code>n_img_prior</code> indicates the number of images to generate from the prior (so bottom-up pass is not used at all here).</p> <code>None</code> <code>constant_layers</code> <code>Union[Iterable[int], None]</code> <p>A sequence of indexes associated to the layers in which a single instance's z is copied over the entire batch (bottom-up path is not used, so only prior is used here). Set to <code>None</code> to avoid this behaviour.</p> <code>None</code> <code>forced_latent</code> <code>Union[list[Tensor], None]</code> <p>A list of tensors that are used as fixed latent variables (hence, sampling doesn't take place in this case).</p> <code>None</code> <code>top_down_layers</code> <code>Union[ModuleList, None]</code> <p>A list of top-down layers to use in the top-down pass. If <code>None</code>, the method uses the default layers defined in the constructor.</p> <code>None</code> <code>final_top_down_layer</code> <code>Union[Sequential, None]</code> <p>The last top-down layer of the top-down pass. If <code>None</code>, the method uses the default layers defined in the constructor.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/lvae.py</code> <pre><code>def topdown_pass(\n    self,\n    bu_values: Union[torch.Tensor, None] = None,\n    n_img_prior: Union[torch.Tensor, None] = None,\n    constant_layers: Union[Iterable[int], None] = None,\n    forced_latent: Union[list[torch.Tensor], None] = None,\n    top_down_layers: Union[nn.ModuleList, None] = None,\n    final_top_down_layer: Union[nn.Sequential, None] = None,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Method defines the forward pass through the LVAE Decoder, the so-called.\n\n    Top-Down pass.\n\n    Parameters\n    ----------\n    bu_values: torch.Tensor, optional\n        Output of the bottom-up pass. It will have values from multiple layers of\n        the ladder.\n    n_img_prior: optional\n        When `bu_values` is `None`, `n_img_prior` indicates the number of images to\n        generate\n        from the prior (so bottom-up pass is not used at all here).\n    constant_layers: Iterable[int], optional\n        A sequence of indexes associated to the layers in which a single instance's\n        z is copied over the entire batch (bottom-up path is not used, so only prior\n        is used here). Set to `None` to avoid this behaviour.\n    forced_latent: list[torch.Tensor], optional\n        A list of tensors that are used as fixed latent variables (hence, sampling\n        doesn't take place in this case).\n    top_down_layers: nn.ModuleList, optional\n        A list of top-down layers to use in the top-down pass. If `None`, the method\n        uses the default layers defined in the constructor.\n    final_top_down_layer: nn.Sequential, optional\n        The last top-down layer of the top-down pass. If `None`, the method uses the\n        default layers defined in the constructor.\n    \"\"\"\n    if top_down_layers is None:\n        top_down_layers = self.top_down_layers\n    if final_top_down_layer is None:\n        final_top_down_layer = self.final_top_down\n\n    # Default: no layer is sampled from the distribution's mode\n    if constant_layers is None:\n        constant_layers = []\n    prior_experiment = len(constant_layers) &gt; 0\n\n    # If the bottom-up inference values are not given, don't do\n    # inference, sample from prior instead\n    inference_mode = bu_values is not None\n\n    # Check consistency of arguments\n    if inference_mode != (n_img_prior is None):\n        msg = (\n            \"Number of images for top-down generation has to be given \"\n            \"if and only if we're not doing inference\"\n        )\n        raise RuntimeError(msg)\n    if inference_mode and prior_experiment:\n        msg = (\n            \"Prior experiments (e.g. sampling from mode) are not\"\n            \" compatible with inference mode\"\n        )\n        raise RuntimeError(msg)\n\n    # Sampled latent variables at each layer\n    z = [None] * self.n_layers\n    # KL divergence of each layer\n    kl = [None] * self.n_layers\n    # Kl divergence restricted, only for the LC enabled setup denoiSplit.\n    kl_restricted = [None] * self.n_layers\n    # mean from which z is sampled.\n    q_mu = [None] * self.n_layers\n    # log(var) from which z is sampled.\n    q_lv = [None] * self.n_layers\n    # Spatial map of KL divergence for each layer\n    kl_spatial = [None] * self.n_layers\n    debug_qvar_max = [None] * self.n_layers\n    kl_channelwise = [None] * self.n_layers\n    if forced_latent is None:\n        forced_latent = [None] * self.n_layers\n\n    # Top-down inference/generation loop\n    out = None\n    for i in reversed(range(self.n_layers)):\n        # If available, get deterministic node from bottom-up inference\n        try:\n            bu_value = bu_values[i]\n        except TypeError:\n            bu_value = None\n\n        # Whether the current layer should be sampled from the mode\n        constant_out = i in constant_layers\n\n        # Input for skip connection\n        skip_input = out\n\n        # Full top-down layer, including sampling and deterministic part\n        out, aux = top_down_layers[i](\n            input_=out,\n            skip_connection_input=skip_input,\n            inference_mode=inference_mode,\n            bu_value=bu_value,\n            n_img_prior=n_img_prior,\n            force_constant_output=constant_out,\n            forced_latent=forced_latent[i],\n            mode_pred=self.mode_pred,\n            var_clip_max=self._var_clip_max,\n        )\n        # Save useful variables\n        z[i] = aux[\"z\"]  # sampled variable at this layer (batch, ch, h, w)\n        kl[i] = aux[\"kl_samplewise\"]  # (batch, )\n        kl_restricted[i] = aux[\"kl_samplewise_restricted\"]\n        kl_spatial[i] = aux[\"kl_spatial\"]  # (batch, h, w)\n        q_mu[i] = aux[\"q_mu\"]\n        q_lv[i] = aux[\"q_lv\"]\n\n        kl_channelwise[i] = aux[\"kl_channelwise\"]\n        debug_qvar_max[i] = aux[\"qvar_max\"]\n        # if self.mode_pred is False:\n        #     logprob_p += aux['logprob_p'].mean()  # mean over batch\n        # else:\n        #     logprob_p = None\n\n    # Final top-down layer\n    out = final_top_down_layer(out)\n\n    # Store useful variables in a dict to return them\n    data = {\n        \"z\": z,  # list of tensors with shape (batch, ch[i], h[i], w[i])\n        \"kl\": kl,  # list of tensors with shape (batch, )\n        \"kl_restricted\": kl_restricted,  # list of tensors with shape (batch, )\n        \"kl_spatial\": kl_spatial,  # list of tensors w shape (batch, h[i], w[i])\n        \"kl_channelwise\": kl_channelwise,  # list of tensors with shape (batch, ch[i])\n        # 'logprob_p': logprob_p,  # scalar, mean over batch\n        \"q_mu\": q_mu,\n        \"q_lv\": q_lv,\n        \"debug_qvar_max\": debug_qvar_max,\n    }\n    return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/","title":"noise_models","text":""},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel","title":"<code>GaussianMixtureNoiseModel</code>","text":"<p>               Bases: <code>Module</code></p> <p>Define a noise model parameterized as a mixture of gaussians.</p> <p>If <code>config.path</code> is not provided a new object is initialized from scratch. Otherwise, a model is loaded from <code>config.path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GaussianMixtureNMConfig</code> <p>A <code>pydantic</code> model that defines the configuration of the GMM noise model.</p> required <p>Attributes:</p> Name Type Description <code>min_signal</code> <code>float</code> <p>Minimum signal intensity expected in the image.</p> <code>max_signal</code> <code>float</code> <p>Maximum signal intensity expected in the image.</p> <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory where the trained noise model (*.npz) is saved in the <code>train</code> method.</p> <code>weight</code> <code>Parameter</code> <p>A [3*n_gaussian, n_coeff] sized array containing the values of the weights describing the GMM noise model, with each row corresponding to one parameter of each gaussian, namely [mean, standard deviation and weight]. Specifically, rows are organized as follows: - first n_gaussian rows correspond to the means - next n_gaussian rows correspond to the weights - last n_gaussian rows correspond to the standard deviations If <code>weight=None</code>, the weight array is initialized using the <code>min_signal</code> and <code>max_signal</code> parameters.</p> <code>n_gaussian</code> <code>int</code> <p>Number of gaussians in the mixture.</p> <code>n_coeff</code> <code>int</code> <p>Number of coefficients to describe the functional relationship between gaussian parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic relationship and so on.</p> <code>device</code> <code>device</code> <p>GPU device.</p> <code>min_sigma</code> <code>float</code> <p>All values of <code>standard deviation</code> below this are clamped to this value.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>class GaussianMixtureNoiseModel(nn.Module):\n    \"\"\"Define a noise model parameterized as a mixture of gaussians.\n\n    If `config.path` is not provided a new object is initialized from scratch.\n    Otherwise, a model is loaded from `config.path`.\n\n    Parameters\n    ----------\n    config : GaussianMixtureNMConfig\n        A `pydantic` model that defines the configuration of the GMM noise model.\n\n    Attributes\n    ----------\n    min_signal : float\n        Minimum signal intensity expected in the image.\n    max_signal : float\n        Maximum signal intensity expected in the image.\n    path: Union[str, Path]\n        Path to the directory where the trained noise model (*.npz) is saved in the `train` method.\n    weight : torch.nn.Parameter\n        A [3*n_gaussian, n_coeff] sized array containing the values of the weights\n        describing the GMM noise model, with each row corresponding to one\n        parameter of each gaussian, namely [mean, standard deviation and weight].\n        Specifically, rows are organized as follows:\n        - first n_gaussian rows correspond to the means\n        - next n_gaussian rows correspond to the weights\n        - last n_gaussian rows correspond to the standard deviations\n        If `weight=None`, the weight array is initialized using the `min_signal`\n        and `max_signal` parameters.\n    n_gaussian: int\n        Number of gaussians in the mixture.\n    n_coeff: int\n        Number of coefficients to describe the functional relationship between gaussian\n        parameters and the signal. 2 implies a linear relationship, 3 implies a quadratic\n        relationship and so on.\n    device: device\n        GPU device.\n    min_sigma: float\n        All values of `standard deviation` below this are clamped to this value.\n    \"\"\"\n\n    # TODO training a NM relies on getting a clean data(N2V e.g,)\n    def __init__(self, config: GaussianMixtureNMConfig) -&gt; None:\n        super().__init__()\n        self.device = torch.device(\"cpu\")\n\n        if config.path is not None:\n            params = np.load(config.path)\n        else:\n            params = config.model_dump(exclude_none=True)\n\n        min_sigma = torch.tensor(params[\"min_sigma\"])\n        min_signal = torch.tensor(params[\"min_signal\"])\n        max_signal = torch.tensor(params[\"max_signal\"])\n        self.register_buffer(\"min_signal\", min_signal)\n        self.register_buffer(\"max_signal\", max_signal)\n        self.register_buffer(\"min_sigma\", min_sigma)\n        self.register_buffer(\"tolerance\", torch.tensor([1e-10]))\n\n        if \"trained_weight\" in params:\n            weight = torch.tensor(params[\"trained_weight\"])\n        elif \"weight\" in params and params[\"weight\"] is not None:\n            weight = torch.tensor(params[\"weight\"])\n        else:\n            weight = self._initialize_weights(\n                params[\"n_gaussian\"], params[\"n_coeff\"], max_signal, min_signal\n            )\n\n        self.n_gaussian = weight.shape[0] // 3\n        self.n_coeff = weight.shape[1]\n\n        self.register_parameter(\"weight\", nn.Parameter(weight))\n        self._set_model_mode(mode=\"prediction\")\n\n        print(f\"[{self.__class__.__name__}] min_sigma: {self.min_sigma}\")\n\n    def _initialize_weights(\n        self,\n        n_gaussian: int,\n        n_coeff: int,\n        max_signal: torch.Tensor,\n        min_signal: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create random weight initialization.\"\"\"\n        weight = torch.randn(n_gaussian * 3, n_coeff)\n        weight[n_gaussian : 2 * n_gaussian, 1] = torch.log(\n            max_signal - min_signal\n        ).float()\n        return weight\n\n    def to_device(self, device: torch.device):\n        self.device = device\n        self.to(device)\n\n    def _set_model_mode(self, mode: str) -&gt; None:\n        \"\"\"Move parameters to the device and set weights' requires_grad depending on the mode\"\"\"\n        if mode == \"train\":\n            self.weight.requires_grad = True\n        else:\n            self.weight.requires_grad = False\n\n    def polynomial_regressor(\n        self, weight_params: torch.Tensor, signals: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Combines `weight_params` and signal `signals` to regress for the gaussian parameter values.\n\n        Parameters\n        ----------\n        weight_params : Tensor\n            Corresponds to specific rows of the `self.weight`\n\n        signals : Tensor\n            Signals\n\n        Returns\n        -------\n        value : Tensor\n            Corresponds to either of mean, standard deviation or weight, evaluated at `signals`\n        \"\"\"\n        value = torch.zeros_like(signals)\n        for i in range(weight_params.shape[0]):\n            value += weight_params[i] * (\n                ((signals - self.min_signal) / (self.max_signal - self.min_signal)) ** i\n            )\n        return value\n\n    def normal_density(\n        self, x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the normal probability density at `x` given the mean `mean` and standard deviation `std`.\n\n        Parameters\n        ----------\n        x: Tensor\n            Observations\n        mean: Tensor\n            Mean\n        std: Tensor\n            Standard-deviation\n\n        Returns\n        -------\n        tmp: Tensor\n            Normal probability density of `x` given `mean` and `std`\n        \"\"\"\n        tmp = -((x - mean) ** 2)\n        tmp = tmp / (2.0 * std * std)\n        tmp = torch.exp(tmp)\n        tmp = tmp / torch.sqrt((2.0 * np.pi) * std * std)\n        return tmp\n\n    def likelihood(\n        self, observations: torch.Tensor, signals: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.\n\n        Parameters\n        ----------\n        observations : Tensor\n            Noisy observations\n        signals : Tensor\n            Underlying signals\n\n        Returns\n        -------\n        value: torch.Tensor:\n            Likelihood of observations given the signals and the GMM noise model\n        \"\"\"\n        gaussian_parameters: list[torch.Tensor] = self.get_gaussian_parameters(signals)\n        p = 0\n        for gaussian in range(self.n_gaussian):\n            p += (\n                self.normal_density(\n                    observations,\n                    gaussian_parameters[gaussian],\n                    gaussian_parameters[self.n_gaussian + gaussian],\n                )\n                * gaussian_parameters[2 * self.n_gaussian + gaussian]\n            )\n        return p + self.tolerance\n\n    def get_gaussian_parameters(self, signals: torch.Tensor) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Returns the noise model for given signals\n\n        Parameters\n        ----------\n        signals : Tensor\n            Underlying signals\n\n        Returns\n        -------\n        noise_model: list of Tensor\n            Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n        \"\"\"\n        noise_model = []\n        mu = []\n        sigma = []\n        alpha = []\n        kernels = self.weight.shape[0] // 3\n        for num in range(kernels):\n            mu.append(self.polynomial_regressor(self.weight[num, :], signals))\n            expval = torch.exp(self.weight[kernels + num, :])\n            sigma_temp = self.polynomial_regressor(expval, signals)\n            sigma_temp = torch.clamp(sigma_temp, min=self.min_sigma)\n            sigma.append(torch.sqrt(sigma_temp))\n\n            expval = torch.exp(\n                self.polynomial_regressor(self.weight[2 * kernels + num, :], signals)\n                + self.tolerance\n            )\n            alpha.append(expval)\n\n        sum_alpha = 0\n        for al in range(kernels):\n            sum_alpha = alpha[al] + sum_alpha\n\n        # sum of alpha is forced to be 1.\n        for ker in range(kernels):\n            alpha[ker] = alpha[ker] / sum_alpha\n\n        sum_means = 0\n        # sum_means is the alpha weighted average of the means\n        for ker in range(kernels):\n            sum_means = alpha[ker] * mu[ker] + sum_means\n\n        # subtracting the alpha weighted average of the means from the means\n        # ensures that the GMM has the inclination to have the mean=signals.\n        # its like a residual conection. I don't understand why we need to learn the mean?\n        for ker in range(kernels):\n            mu[ker] = mu[ker] - sum_means + signals\n\n        for i in range(kernels):\n            noise_model.append(mu[i])\n        for j in range(kernels):\n            noise_model.append(sigma[j])\n        for k in range(kernels):\n            noise_model.append(alpha[k])\n\n        return noise_model\n\n    @staticmethod\n    def _fast_shuffle(series: torch.Tensor, num: int) -&gt; torch.Tensor:\n        \"\"\"Shuffle the inputs randomly num times\"\"\"\n        length = series.shape[0]\n        for _ in range(num):\n            idx = torch.randperm(length)\n            series = series[idx, :]\n        return series\n\n    def get_signal_observation_pairs(\n        self,\n        signal: NDArray,\n        observation: NDArray,\n        lower_clip: float,\n        upper_clip: float,\n    ) -&gt; torch.Tensor:\n        \"\"\"Returns the Signal-Observation pixel intensities as a two-column array\n\n        Parameters\n        ----------\n        signal : numpy array\n            Clean Signal Data\n        observation: numpy array\n            Noisy observation Data\n        lower_clip: float\n            Lower percentile bound for clipping.\n        upper_clip: float\n            Upper percentile bound for clipping.\n\n        Returns\n        -------\n        noise_model: list of torch floats\n            Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n        \"\"\"\n        lb = np.percentile(signal, lower_clip)\n        ub = np.percentile(signal, upper_clip)\n        stepsize = observation[0].size\n        n_observations = observation.shape[0]\n        n_signals = signal.shape[0]\n        sig_obs_pairs = np.zeros((n_observations * stepsize, 2))\n\n        for i in range(n_observations):\n            j = i // (n_observations // n_signals)\n            sig_obs_pairs[stepsize * i : stepsize * (i + 1), 0] = signal[j].ravel()\n            sig_obs_pairs[stepsize * i : stepsize * (i + 1), 1] = observation[i].ravel()\n        sig_obs_pairs = sig_obs_pairs[\n            (sig_obs_pairs[:, 0] &gt; lb) &amp; (sig_obs_pairs[:, 0] &lt; ub)\n        ]\n        sig_obs_pairs = sig_obs_pairs.astype(np.float32)\n        sig_obs_pairs = torch.from_numpy(sig_obs_pairs)\n        return self._fast_shuffle(sig_obs_pairs, 2)\n\n    def fit(\n        self,\n        signal: NDArray,\n        observation: NDArray,\n        learning_rate: float = 1e-1,\n        batch_size: int = 250000,\n        n_epochs: int = 2000,\n        lower_clip: float = 0.0,\n        upper_clip: float = 100.0,\n    ) -&gt; list[float]:\n        \"\"\"Training to learn the noise model from signal - observation pairs.\n\n        Parameters\n        ----------\n        signal: numpy array\n            Clean Signal Data\n        observation: numpy array\n            Noisy Observation Data\n        learning_rate: float\n            Learning rate. Default = 1e-1.\n        batch_size: int\n            Nini-batch size. Default = 250000.\n        n_epochs: int\n            Number of epochs. Default = 2000.\n        lower_clip : int\n            Lower percentile for clipping. Default is 0.\n        upper_clip : int\n            Upper percentile for clipping. Default is 100.\n        \"\"\"\n        self._set_model_mode(mode=\"train\")\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.to_device(device)\n        optimizer = torch.optim.Adam([self.weight], lr=learning_rate)\n\n        sig_obs_pairs = self.get_signal_observation_pairs(\n            signal, observation, lower_clip, upper_clip\n        )\n\n        train_losses = []\n        counter = 0\n        for t in range(n_epochs):\n            if (counter + 1) * batch_size &gt;= sig_obs_pairs.shape[0]:\n                counter = 0\n                sig_obs_pairs = self._fast_shuffle(sig_obs_pairs, 1)\n\n            batch_vectors = sig_obs_pairs[\n                counter * batch_size : (counter + 1) * batch_size, :\n            ]\n            observations = batch_vectors[:, 1].to(self.device)\n            signals = batch_vectors[:, 0].to(self.device)\n\n            p = self.likelihood(observations, signals)\n\n            joint_loss = torch.mean(-torch.log(p))\n            train_losses.append(joint_loss.item())\n\n            if self.weight.isnan().any() or self.weight.isinf().any():\n                print(\n                    \"NaN or Inf detected in the weights. Aborting training at epoch: \",\n                    t,\n                )\n                break\n\n            if t % 100 == 0:\n                last_losses = train_losses[-100:]\n                print(t, np.mean(last_losses))\n\n            optimizer.zero_grad()\n            joint_loss.backward()\n            optimizer.step()\n            counter += 1\n\n        self._set_model_mode(mode=\"prediction\")\n        self.to_device(torch.device(\"cpu\"))\n        print(\"===================\\n\")\n        return train_losses\n\n    def sample_observation_from_signal(self, signal: NDArray) -&gt; NDArray:\n        \"\"\"\n        Sample an instance of observation based on an input signal using a\n        learned Gaussian Mixture Model. For each pixel in the input signal,\n        samples a corresponding noisy pixel.\n\n        Parameters\n        ----------\n        signal: numpy array\n            Clean 2D signal data.\n\n        Returns\n        -------\n        observation: numpy array\n            An instance of noisy observation data based on the input signal.\n        \"\"\"\n        assert len(signal.shape) == 2, \"Only 2D inputs are supported.\"\n\n        signal_tensor = torch.from_numpy(signal).to(torch.float32)\n        height, width = signal_tensor.shape\n\n        with torch.no_grad():\n            # Get gaussian parameters for each pixel\n            gaussian_params = self.get_gaussian_parameters(signal_tensor)\n            means = np.array(gaussian_params[: self.n_gaussian])\n            stds = np.array(gaussian_params[self.n_gaussian : self.n_gaussian * 2])\n            alphas = np.array(gaussian_params[self.n_gaussian * 2 :])\n\n            if self.n_gaussian == 1:\n                # Single gaussian case\n                observation = np.random.normal(\n                    loc=means[0], scale=stds[0], size=(height, width)\n                )\n            else:\n                # Multiple gaussians: sample component for each pixel\n                uniform = np.random.rand(1, height, width)\n                # Compute cumulative probabilities for component selection\n                cumulative_alphas = np.cumsum(\n                    alphas, axis=0\n                )  # Shape: (n_gaussian, height, width)\n                selected_component = np.argmax(\n                    uniform &lt; cumulative_alphas, axis=0, keepdims=True\n                )\n\n                # For every pixel, choose the corresponding gaussian\n                # and get the learned mu and sigma\n                selected_mus = np.take_along_axis(means, selected_component, axis=0)\n                selected_stds = np.take_along_axis(stds, selected_component, axis=0)\n                selected_mus = selected_mus.squeeze(0)\n                selected_stds = selected_stds.squeeze(0)\n\n                # Sample from the normal distribution with learned mu and sigma\n                observation = np.random.normal(\n                    selected_mus, selected_stds, size=(height, width)\n                )\n        return observation\n\n    def save(self, path: str, name: str) -&gt; None:\n        \"\"\"Save the trained parameters on the noise model.\n\n        Parameters\n        ----------\n        path : str\n            Path to save the trained parameters.\n        name : str\n            File name to save the trained parameters.\n        \"\"\"\n        os.makedirs(path, exist_ok=True)\n        np.savez(\n            os.path.join(path, name),\n            trained_weight=self.weight.numpy(),\n            min_signal=self.min_signal.numpy(),\n            max_signal=self.max_signal.numpy(),\n            min_sigma=self.min_sigma,\n        )\n        print(\"The trained parameters (\" + name + \") is saved at location: \" + path)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel._fast_shuffle","title":"<code>_fast_shuffle(series, num)</code>  <code>staticmethod</code>","text":"<p>Shuffle the inputs randomly num times</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>@staticmethod\ndef _fast_shuffle(series: torch.Tensor, num: int) -&gt; torch.Tensor:\n    \"\"\"Shuffle the inputs randomly num times\"\"\"\n    length = series.shape[0]\n    for _ in range(num):\n        idx = torch.randperm(length)\n        series = series[idx, :]\n    return series\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel._initialize_weights","title":"<code>_initialize_weights(n_gaussian, n_coeff, max_signal, min_signal)</code>","text":"<p>Create random weight initialization.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def _initialize_weights(\n    self,\n    n_gaussian: int,\n    n_coeff: int,\n    max_signal: torch.Tensor,\n    min_signal: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Create random weight initialization.\"\"\"\n    weight = torch.randn(n_gaussian * 3, n_coeff)\n    weight[n_gaussian : 2 * n_gaussian, 1] = torch.log(\n        max_signal - min_signal\n    ).float()\n    return weight\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel._set_model_mode","title":"<code>_set_model_mode(mode)</code>","text":"<p>Move parameters to the device and set weights' requires_grad depending on the mode</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def _set_model_mode(self, mode: str) -&gt; None:\n    \"\"\"Move parameters to the device and set weights' requires_grad depending on the mode\"\"\"\n    if mode == \"train\":\n        self.weight.requires_grad = True\n    else:\n        self.weight.requires_grad = False\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.fit","title":"<code>fit(signal, observation, learning_rate=0.1, batch_size=250000, n_epochs=2000, lower_clip=0.0, upper_clip=100.0)</code>","text":"<p>Training to learn the noise model from signal - observation pairs.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>NDArray</code> <p>Clean Signal Data</p> required <code>observation</code> <code>NDArray</code> <p>Noisy Observation Data</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate. Default = 1e-1.</p> <code>0.1</code> <code>batch_size</code> <code>int</code> <p>Nini-batch size. Default = 250000.</p> <code>250000</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs. Default = 2000.</p> <code>2000</code> <code>lower_clip</code> <code>int</code> <p>Lower percentile for clipping. Default is 0.</p> <code>0.0</code> <code>upper_clip</code> <code>int</code> <p>Upper percentile for clipping. Default is 100.</p> <code>100.0</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def fit(\n    self,\n    signal: NDArray,\n    observation: NDArray,\n    learning_rate: float = 1e-1,\n    batch_size: int = 250000,\n    n_epochs: int = 2000,\n    lower_clip: float = 0.0,\n    upper_clip: float = 100.0,\n) -&gt; list[float]:\n    \"\"\"Training to learn the noise model from signal - observation pairs.\n\n    Parameters\n    ----------\n    signal: numpy array\n        Clean Signal Data\n    observation: numpy array\n        Noisy Observation Data\n    learning_rate: float\n        Learning rate. Default = 1e-1.\n    batch_size: int\n        Nini-batch size. Default = 250000.\n    n_epochs: int\n        Number of epochs. Default = 2000.\n    lower_clip : int\n        Lower percentile for clipping. Default is 0.\n    upper_clip : int\n        Upper percentile for clipping. Default is 100.\n    \"\"\"\n    self._set_model_mode(mode=\"train\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.to_device(device)\n    optimizer = torch.optim.Adam([self.weight], lr=learning_rate)\n\n    sig_obs_pairs = self.get_signal_observation_pairs(\n        signal, observation, lower_clip, upper_clip\n    )\n\n    train_losses = []\n    counter = 0\n    for t in range(n_epochs):\n        if (counter + 1) * batch_size &gt;= sig_obs_pairs.shape[0]:\n            counter = 0\n            sig_obs_pairs = self._fast_shuffle(sig_obs_pairs, 1)\n\n        batch_vectors = sig_obs_pairs[\n            counter * batch_size : (counter + 1) * batch_size, :\n        ]\n        observations = batch_vectors[:, 1].to(self.device)\n        signals = batch_vectors[:, 0].to(self.device)\n\n        p = self.likelihood(observations, signals)\n\n        joint_loss = torch.mean(-torch.log(p))\n        train_losses.append(joint_loss.item())\n\n        if self.weight.isnan().any() or self.weight.isinf().any():\n            print(\n                \"NaN or Inf detected in the weights. Aborting training at epoch: \",\n                t,\n            )\n            break\n\n        if t % 100 == 0:\n            last_losses = train_losses[-100:]\n            print(t, np.mean(last_losses))\n\n        optimizer.zero_grad()\n        joint_loss.backward()\n        optimizer.step()\n        counter += 1\n\n    self._set_model_mode(mode=\"prediction\")\n    self.to_device(torch.device(\"cpu\"))\n    print(\"===================\\n\")\n    return train_losses\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.get_gaussian_parameters","title":"<code>get_gaussian_parameters(signals)</code>","text":"<p>Returns the noise model for given signals</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Tensor</code> <p>Underlying signals</p> required <p>Returns:</p> Name Type Description <code>noise_model</code> <code>list of Tensor</code> <p>Contains a list of <code>mu</code>, <code>sigma</code> and <code>alpha</code> for the <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def get_gaussian_parameters(self, signals: torch.Tensor) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Returns the noise model for given signals\n\n    Parameters\n    ----------\n    signals : Tensor\n        Underlying signals\n\n    Returns\n    -------\n    noise_model: list of Tensor\n        Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n    \"\"\"\n    noise_model = []\n    mu = []\n    sigma = []\n    alpha = []\n    kernels = self.weight.shape[0] // 3\n    for num in range(kernels):\n        mu.append(self.polynomial_regressor(self.weight[num, :], signals))\n        expval = torch.exp(self.weight[kernels + num, :])\n        sigma_temp = self.polynomial_regressor(expval, signals)\n        sigma_temp = torch.clamp(sigma_temp, min=self.min_sigma)\n        sigma.append(torch.sqrt(sigma_temp))\n\n        expval = torch.exp(\n            self.polynomial_regressor(self.weight[2 * kernels + num, :], signals)\n            + self.tolerance\n        )\n        alpha.append(expval)\n\n    sum_alpha = 0\n    for al in range(kernels):\n        sum_alpha = alpha[al] + sum_alpha\n\n    # sum of alpha is forced to be 1.\n    for ker in range(kernels):\n        alpha[ker] = alpha[ker] / sum_alpha\n\n    sum_means = 0\n    # sum_means is the alpha weighted average of the means\n    for ker in range(kernels):\n        sum_means = alpha[ker] * mu[ker] + sum_means\n\n    # subtracting the alpha weighted average of the means from the means\n    # ensures that the GMM has the inclination to have the mean=signals.\n    # its like a residual conection. I don't understand why we need to learn the mean?\n    for ker in range(kernels):\n        mu[ker] = mu[ker] - sum_means + signals\n\n    for i in range(kernels):\n        noise_model.append(mu[i])\n    for j in range(kernels):\n        noise_model.append(sigma[j])\n    for k in range(kernels):\n        noise_model.append(alpha[k])\n\n    return noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.get_signal_observation_pairs","title":"<code>get_signal_observation_pairs(signal, observation, lower_clip, upper_clip)</code>","text":"<p>Returns the Signal-Observation pixel intensities as a two-column array</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>numpy array</code> <p>Clean Signal Data</p> required <code>observation</code> <code>NDArray</code> <p>Noisy observation Data</p> required <code>lower_clip</code> <code>float</code> <p>Lower percentile bound for clipping.</p> required <code>upper_clip</code> <code>float</code> <p>Upper percentile bound for clipping.</p> required <p>Returns:</p> Name Type Description <code>noise_model</code> <code>list of torch floats</code> <p>Contains a list of <code>mu</code>, <code>sigma</code> and <code>alpha</code> for the <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def get_signal_observation_pairs(\n    self,\n    signal: NDArray,\n    observation: NDArray,\n    lower_clip: float,\n    upper_clip: float,\n) -&gt; torch.Tensor:\n    \"\"\"Returns the Signal-Observation pixel intensities as a two-column array\n\n    Parameters\n    ----------\n    signal : numpy array\n        Clean Signal Data\n    observation: numpy array\n        Noisy observation Data\n    lower_clip: float\n        Lower percentile bound for clipping.\n    upper_clip: float\n        Upper percentile bound for clipping.\n\n    Returns\n    -------\n    noise_model: list of torch floats\n        Contains a list of `mu`, `sigma` and `alpha` for the `signals`\n    \"\"\"\n    lb = np.percentile(signal, lower_clip)\n    ub = np.percentile(signal, upper_clip)\n    stepsize = observation[0].size\n    n_observations = observation.shape[0]\n    n_signals = signal.shape[0]\n    sig_obs_pairs = np.zeros((n_observations * stepsize, 2))\n\n    for i in range(n_observations):\n        j = i // (n_observations // n_signals)\n        sig_obs_pairs[stepsize * i : stepsize * (i + 1), 0] = signal[j].ravel()\n        sig_obs_pairs[stepsize * i : stepsize * (i + 1), 1] = observation[i].ravel()\n    sig_obs_pairs = sig_obs_pairs[\n        (sig_obs_pairs[:, 0] &gt; lb) &amp; (sig_obs_pairs[:, 0] &lt; ub)\n    ]\n    sig_obs_pairs = sig_obs_pairs.astype(np.float32)\n    sig_obs_pairs = torch.from_numpy(sig_obs_pairs)\n    return self._fast_shuffle(sig_obs_pairs, 2)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.likelihood","title":"<code>likelihood(observations, signals)</code>","text":"<p>Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.</p> <p>Parameters:</p> Name Type Description Default <code>observations</code> <code>Tensor</code> <p>Noisy observations</p> required <code>signals</code> <code>Tensor</code> <p>Underlying signals</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>torch.Tensor:</code> <p>Likelihood of observations given the signals and the GMM noise model</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def likelihood(\n    self, observations: torch.Tensor, signals: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the likelihood of observations given the signals and the corresponding gaussian parameters.\n\n    Parameters\n    ----------\n    observations : Tensor\n        Noisy observations\n    signals : Tensor\n        Underlying signals\n\n    Returns\n    -------\n    value: torch.Tensor:\n        Likelihood of observations given the signals and the GMM noise model\n    \"\"\"\n    gaussian_parameters: list[torch.Tensor] = self.get_gaussian_parameters(signals)\n    p = 0\n    for gaussian in range(self.n_gaussian):\n        p += (\n            self.normal_density(\n                observations,\n                gaussian_parameters[gaussian],\n                gaussian_parameters[self.n_gaussian + gaussian],\n            )\n            * gaussian_parameters[2 * self.n_gaussian + gaussian]\n        )\n    return p + self.tolerance\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.normal_density","title":"<code>normal_density(x, mean, std)</code>","text":"<p>Evaluates the normal probability density at <code>x</code> given the mean <code>mean</code> and standard deviation <code>std</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Observations</p> required <code>mean</code> <code>Tensor</code> <p>Mean</p> required <code>std</code> <code>Tensor</code> <p>Standard-deviation</p> required <p>Returns:</p> Name Type Description <code>tmp</code> <code>Tensor</code> <p>Normal probability density of <code>x</code> given <code>mean</code> and <code>std</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def normal_density(\n    self, x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluates the normal probability density at `x` given the mean `mean` and standard deviation `std`.\n\n    Parameters\n    ----------\n    x: Tensor\n        Observations\n    mean: Tensor\n        Mean\n    std: Tensor\n        Standard-deviation\n\n    Returns\n    -------\n    tmp: Tensor\n        Normal probability density of `x` given `mean` and `std`\n    \"\"\"\n    tmp = -((x - mean) ** 2)\n    tmp = tmp / (2.0 * std * std)\n    tmp = torch.exp(tmp)\n    tmp = tmp / torch.sqrt((2.0 * np.pi) * std * std)\n    return tmp\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.polynomial_regressor","title":"<code>polynomial_regressor(weight_params, signals)</code>","text":"<p>Combines <code>weight_params</code> and signal <code>signals</code> to regress for the gaussian parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>weight_params</code> <code>Tensor</code> <p>Corresponds to specific rows of the <code>self.weight</code></p> required <code>signals</code> <code>Tensor</code> <p>Signals</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>Tensor</code> <p>Corresponds to either of mean, standard deviation or weight, evaluated at <code>signals</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def polynomial_regressor(\n    self, weight_params: torch.Tensor, signals: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Combines `weight_params` and signal `signals` to regress for the gaussian parameter values.\n\n    Parameters\n    ----------\n    weight_params : Tensor\n        Corresponds to specific rows of the `self.weight`\n\n    signals : Tensor\n        Signals\n\n    Returns\n    -------\n    value : Tensor\n        Corresponds to either of mean, standard deviation or weight, evaluated at `signals`\n    \"\"\"\n    value = torch.zeros_like(signals)\n    for i in range(weight_params.shape[0]):\n        value += weight_params[i] * (\n            ((signals - self.min_signal) / (self.max_signal - self.min_signal)) ** i\n        )\n    return value\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.sample_observation_from_signal","title":"<code>sample_observation_from_signal(signal)</code>","text":"<p>Sample an instance of observation based on an input signal using a learned Gaussian Mixture Model. For each pixel in the input signal, samples a corresponding noisy pixel.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>NDArray</code> <p>Clean 2D signal data.</p> required <p>Returns:</p> Name Type Description <code>observation</code> <code>numpy array</code> <p>An instance of noisy observation data based on the input signal.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def sample_observation_from_signal(self, signal: NDArray) -&gt; NDArray:\n    \"\"\"\n    Sample an instance of observation based on an input signal using a\n    learned Gaussian Mixture Model. For each pixel in the input signal,\n    samples a corresponding noisy pixel.\n\n    Parameters\n    ----------\n    signal: numpy array\n        Clean 2D signal data.\n\n    Returns\n    -------\n    observation: numpy array\n        An instance of noisy observation data based on the input signal.\n    \"\"\"\n    assert len(signal.shape) == 2, \"Only 2D inputs are supported.\"\n\n    signal_tensor = torch.from_numpy(signal).to(torch.float32)\n    height, width = signal_tensor.shape\n\n    with torch.no_grad():\n        # Get gaussian parameters for each pixel\n        gaussian_params = self.get_gaussian_parameters(signal_tensor)\n        means = np.array(gaussian_params[: self.n_gaussian])\n        stds = np.array(gaussian_params[self.n_gaussian : self.n_gaussian * 2])\n        alphas = np.array(gaussian_params[self.n_gaussian * 2 :])\n\n        if self.n_gaussian == 1:\n            # Single gaussian case\n            observation = np.random.normal(\n                loc=means[0], scale=stds[0], size=(height, width)\n            )\n        else:\n            # Multiple gaussians: sample component for each pixel\n            uniform = np.random.rand(1, height, width)\n            # Compute cumulative probabilities for component selection\n            cumulative_alphas = np.cumsum(\n                alphas, axis=0\n            )  # Shape: (n_gaussian, height, width)\n            selected_component = np.argmax(\n                uniform &lt; cumulative_alphas, axis=0, keepdims=True\n            )\n\n            # For every pixel, choose the corresponding gaussian\n            # and get the learned mu and sigma\n            selected_mus = np.take_along_axis(means, selected_component, axis=0)\n            selected_stds = np.take_along_axis(stds, selected_component, axis=0)\n            selected_mus = selected_mus.squeeze(0)\n            selected_stds = selected_stds.squeeze(0)\n\n            # Sample from the normal distribution with learned mu and sigma\n            observation = np.random.normal(\n                selected_mus, selected_stds, size=(height, width)\n            )\n    return observation\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.GaussianMixtureNoiseModel.save","title":"<code>save(path, name)</code>","text":"<p>Save the trained parameters on the noise model.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to save the trained parameters.</p> required <code>name</code> <code>str</code> <p>File name to save the trained parameters.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def save(self, path: str, name: str) -&gt; None:\n    \"\"\"Save the trained parameters on the noise model.\n\n    Parameters\n    ----------\n    path : str\n        Path to save the trained parameters.\n    name : str\n        File name to save the trained parameters.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    np.savez(\n        os.path.join(path, name),\n        trained_weight=self.weight.numpy(),\n        min_signal=self.min_signal.numpy(),\n        max_signal=self.max_signal.numpy(),\n        min_sigma=self.min_sigma,\n    )\n    print(\"The trained parameters (\" + name + \") is saved at location: \" + path)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel","title":"<code>MultiChannelNoiseModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>class MultiChannelNoiseModel(nn.Module):\n    def __init__(self, nmodels: list[GaussianMixtureNoiseModel]):\n        \"\"\"Constructor.\n\n        To handle noise models and the relative likelihood computation for multiple\n        output channels (e.g., muSplit, denoiseSplit).\n\n        This class:\n        - receives as input a variable number of noise models, one for each channel.\n        - computes the likelihood of observations given signals for each channel.\n        - returns the concatenation of these likelihoods.\n\n        Parameters\n        ----------\n        nmodels : list[GaussianMixtureNoiseModel]\n            List of noise models, one for each output channel.\n        \"\"\"\n        super().__init__()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        for i, nmodel in enumerate(nmodels):  # TODO refactor this !!!\n            if nmodel is not None:\n                self.add_module(\n                    f\"nmodel_{i}\", nmodel\n                )  # TODO: wouldn't be easier to use a list?\n\n        self._nm_cnt = 0\n        for nmodel in nmodels:\n            if nmodel is not None:\n                self._nm_cnt += 1\n\n        print(f\"[{self.__class__.__name__}] Nmodels count:{self._nm_cnt}\")\n\n    def to_device(self, device: torch.device):\n        self.device = device\n        self.to(device)\n        for ch_idx in range(self._nm_cnt):\n            nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n            nmodel.to_device(device)\n\n    def likelihood(self, obs: torch.Tensor, signal: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute the likelihood of observations given signals for each channel.\n\n        Parameters\n        ----------\n        obs : torch.Tensor\n            Noisy observations, i.e., the target(s). Specifically, the input noisy\n            image for HDN, or the noisy unmixed images used for supervision\n            for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of\n            unmixed channels.\n        signal : torch.Tensor\n            Underlying signals, i.e., the (clean) output of the model. Specifically, the\n            denoised image for HDN, or the unmixed images for denoiSplit.\n            Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.\n        \"\"\"\n        # Case 1: obs and signal have a single channel (e.g., denoising)\n        if obs.shape[1] == 1:\n            assert signal.shape[1] == 1\n            return self.nmodel_0.likelihood(obs, signal)\n\n        # Case 2: obs and signal have multiple channels (e.g., denoiSplit)\n        assert obs.shape[1] == self._nm_cnt, (\n            \"The number of channels in `obs` must match the number of noise models.\"\n            f\" Got instead: obs={obs.shape[1]},  nm={self._nm_cnt}\"\n        )\n        ll_list = []\n        for ch_idx in range(obs.shape[1]):\n            nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n            ll_list.append(\n                nmodel.likelihood(\n                    obs[:, ch_idx : ch_idx + 1], signal[:, ch_idx : ch_idx + 1]\n                )  # slicing to keep the channel dimension\n            )\n        return torch.cat(ll_list, dim=1)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel.__init__","title":"<code>__init__(nmodels)</code>","text":"<p>Constructor.</p> <p>To handle noise models and the relative likelihood computation for multiple output channels (e.g., muSplit, denoiseSplit).</p> <p>This class: - receives as input a variable number of noise models, one for each channel. - computes the likelihood of observations given signals for each channel. - returns the concatenation of these likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>nmodels</code> <code>list[GaussianMixtureNoiseModel]</code> <p>List of noise models, one for each output channel.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def __init__(self, nmodels: list[GaussianMixtureNoiseModel]):\n    \"\"\"Constructor.\n\n    To handle noise models and the relative likelihood computation for multiple\n    output channels (e.g., muSplit, denoiseSplit).\n\n    This class:\n    - receives as input a variable number of noise models, one for each channel.\n    - computes the likelihood of observations given signals for each channel.\n    - returns the concatenation of these likelihoods.\n\n    Parameters\n    ----------\n    nmodels : list[GaussianMixtureNoiseModel]\n        List of noise models, one for each output channel.\n    \"\"\"\n    super().__init__()\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for i, nmodel in enumerate(nmodels):  # TODO refactor this !!!\n        if nmodel is not None:\n            self.add_module(\n                f\"nmodel_{i}\", nmodel\n            )  # TODO: wouldn't be easier to use a list?\n\n    self._nm_cnt = 0\n    for nmodel in nmodels:\n        if nmodel is not None:\n            self._nm_cnt += 1\n\n    print(f\"[{self.__class__.__name__}] Nmodels count:{self._nm_cnt}\")\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.MultiChannelNoiseModel.likelihood","title":"<code>likelihood(obs, signal)</code>","text":"<p>Compute the likelihood of observations given signals for each channel.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Tensor</code> <p>Noisy observations, i.e., the target(s). Specifically, the input noisy image for HDN, or the noisy unmixed images used for supervision for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.</p> required <code>signal</code> <code>Tensor</code> <p>Underlying signals, i.e., the (clean) output of the model. Specifically, the denoised image for HDN, or the unmixed images for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.</p> required Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def likelihood(self, obs: torch.Tensor, signal: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the likelihood of observations given signals for each channel.\n\n    Parameters\n    ----------\n    obs : torch.Tensor\n        Noisy observations, i.e., the target(s). Specifically, the input noisy\n        image for HDN, or the noisy unmixed images used for supervision\n        for denoiSplit. Shape: (B, C, [Z], Y, X), where C is the number of\n        unmixed channels.\n    signal : torch.Tensor\n        Underlying signals, i.e., the (clean) output of the model. Specifically, the\n        denoised image for HDN, or the unmixed images for denoiSplit.\n        Shape: (B, C, [Z], Y, X), where C is the number of unmixed channels.\n    \"\"\"\n    # Case 1: obs and signal have a single channel (e.g., denoising)\n    if obs.shape[1] == 1:\n        assert signal.shape[1] == 1\n        return self.nmodel_0.likelihood(obs, signal)\n\n    # Case 2: obs and signal have multiple channels (e.g., denoiSplit)\n    assert obs.shape[1] == self._nm_cnt, (\n        \"The number of channels in `obs` must match the number of noise models.\"\n        f\" Got instead: obs={obs.shape[1]},  nm={self._nm_cnt}\"\n    )\n    ll_list = []\n    for ch_idx in range(obs.shape[1]):\n        nmodel = getattr(self, f\"nmodel_{ch_idx}\")\n        ll_list.append(\n            nmodel.likelihood(\n                obs[:, ch_idx : ch_idx + 1], signal[:, ch_idx : ch_idx + 1]\n            )  # slicing to keep the channel dimension\n        )\n    return torch.cat(ll_list, dim=1)\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.create_histogram","title":"<code>create_histogram(bins, min_val, max_val, observation, signal)</code>","text":"<p>Creates a 2D histogram from 'observation' and 'signal'.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>int</code> <p>Number of bins in x and y.</p> required <code>min_val</code> <code>float</code> <p>Lower bound of the lowest bin in x and y.</p> required <code>max_val</code> <code>float</code> <p>Upper bound of the highest bin in x and y.</p> required <code>observation</code> <code>ndarray</code> <p>3D numpy array (stack of 2D images). Observation.shape[0] must be divisible by signal.shape[0]. Assumes that n subsequent images in observation belong to one image in 'signal'.</p> required <code>signal</code> <code>ndarray</code> <p>3D numpy array (stack of 2D images).</p> required <p>Returns:</p> Name Type Description <code>histogram</code> <code>ndarray</code> <p>A 3D array: - histogram[0]: Normalized 2D counts. - histogram[1]: Lower boundaries of bins along y. - histogram[2]: Upper boundaries of bins along y.</p> <code>The values for x can be obtained by transposing 'histogram[1]' and 'histogram[2]'.</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def create_histogram(\n    bins: int, min_val: float, max_val: float, observation: NDArray, signal: NDArray\n) -&gt; NDArray:\n    \"\"\"\n    Creates a 2D histogram from 'observation' and 'signal'.\n\n    Parameters\n    ----------\n    bins : int\n        Number of bins in x and y.\n    min_val : float\n        Lower bound of the lowest bin in x and y.\n    max_val : float\n        Upper bound of the highest bin in x and y.\n    observation : np.ndarray\n        3D numpy array (stack of 2D images).\n        Observation.shape[0] must be divisible by signal.shape[0].\n        Assumes that n subsequent images in observation belong to one image in 'signal'.\n    signal : np.ndarray\n        3D numpy array (stack of 2D images).\n\n    Returns\n    -------\n    histogram : np.ndarray\n        A 3D array:\n        - histogram[0]: Normalized 2D counts.\n        - histogram[1]: Lower boundaries of bins along y.\n        - histogram[2]: Upper boundaries of bins along y.\n    The values for x can be obtained by transposing 'histogram[1]' and 'histogram[2]'.\n    \"\"\"\n    histogram = np.zeros((3, bins, bins))\n\n    value_range = [min_val, max_val]\n\n    # Compute mapping factor between observation and signal samples\n    obs_to_signal_shape_factor = int(observation.shape[0] / signal.shape[0])\n\n    # Flatten arrays and align signal values\n    signal_indices = np.arange(observation.shape[0]) // obs_to_signal_shape_factor\n    signal_values = signal[signal_indices].ravel()\n    observation_values = observation.ravel()\n\n    count_histogram, signal_edges, _ = np.histogram2d(\n        signal_values, observation_values, bins=bins, range=[value_range, value_range]\n    )\n\n    # Normalize rows to obtain probabilities\n    row_sums = count_histogram.sum(axis=1, keepdims=True)\n    count_histogram /= np.clip(row_sums, a_min=1e-20, a_max=None)\n\n    histogram[0] = count_histogram\n    histogram[1] = signal_edges[:-1][..., np.newaxis]\n    histogram[2] = signal_edges[1:][..., np.newaxis]\n\n    return histogram\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.noise_model_factory","title":"<code>noise_model_factory(model_config)</code>","text":"<p>Noise model factory.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>Optional[MultiChannelNMConfig]</code> <p>Noise model configuration, a <code>MultiChannelNMConfig</code> config that defines noise models for the different output channels.</p> required <p>Returns:</p> Type Description <code>Optional[MultiChannelNoiseModel]</code> <p>A noise model instance.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the chosen noise model <code>model_type</code> is not implemented. Currently only <code>GaussianMixtureNoiseModel</code> is implemented.</p> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def noise_model_factory(\n    model_config: Optional[MultiChannelNMConfig],\n) -&gt; Optional[MultiChannelNoiseModel]:\n    \"\"\"Noise model factory.\n\n    Parameters\n    ----------\n    model_config : Optional[MultiChannelNMConfig]\n        Noise model configuration, a `MultiChannelNMConfig` config that defines\n        noise models for the different output channels.\n\n    Returns\n    -------\n    Optional[MultiChannelNoiseModel]\n        A noise model instance.\n\n    Raises\n    ------\n    NotImplementedError\n        If the chosen noise model `model_type` is not implemented.\n        Currently only `GaussianMixtureNoiseModel` is implemented.\n    \"\"\"\n    if model_config:\n        noise_models = []\n        for nm in model_config.noise_models:\n            if nm.path:\n                if nm.model_type == \"GaussianMixtureNoiseModel\":\n                    noise_models.append(GaussianMixtureNoiseModel(nm))\n                else:\n                    raise NotImplementedError(\n                        f\"Model {nm.model_type} is not implemented\"\n                    )\n\n            else:  # TODO this means signal/obs are provided. Controlled in pydantic model\n                # TODO train a new model. Config should always be provided?\n                if nm.model_type == \"GaussianMixtureNoiseModel\":\n                    # TODO one model for each channel all make this choise inside the model?\n                    # trained_nm = train_gm_noise_model(nm)\n                    # noise_models.append(trained_nm)\n                    raise NotImplementedError(\n                        \"GaussianMixtureNoiseModel model training is not implemented.\"\n                    )\n                else:\n                    raise NotImplementedError(\n                        f\"Model {nm.model_type} is not implemented\"\n                    )\n        return MultiChannelNoiseModel(noise_models)\n    return None\n</code></pre>"},{"location":"reference/careamics/models/lvae/noise_models/#careamics.models.lvae.noise_models.train_gm_noise_model","title":"<code>train_gm_noise_model(model_config, signal, observation)</code>","text":"<p>Train a Gaussian mixture noise model.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>GaussianMixtureNoiseModel</code> <p>description</p> required <p>Returns:</p> Type Description <code>_description_</code> Source code in <code>src/careamics/models/lvae/noise_models.py</code> <pre><code>def train_gm_noise_model(\n    model_config: GaussianMixtureNMConfig,\n    signal: np.ndarray,\n    observation: np.ndarray,\n) -&gt; GaussianMixtureNoiseModel:\n    \"\"\"Train a Gaussian mixture noise model.\n\n    Parameters\n    ----------\n    model_config : GaussianMixtureNoiseModel\n        _description_\n\n    Returns\n    -------\n    _description_\n    \"\"\"\n    # TODO where to put train params?\n    # TODO any training params ? Different channels ?\n    noise_model = GaussianMixtureNoiseModel(model_config)\n    # TODO revisit config unpacking\n    noise_model.fit(signal, observation)\n    return noise_model\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/","title":"stochastic","text":"<p>Script containing the common basic blocks (nn.Module) reused by the LadderVAE architecture.</p>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock","title":"<code>NormalStochasticBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Stochastic block used in the Top-Down inference pass.</p> <p>Algorithm:     - map input parameters to q(z) and (optionally) p(z) via convolution     - sample a latent tensor z ~ q(z)     - feed z to convolution and return.</p> <p>NOTE 1:     If parameters for q are not given, sampling is done from p(z).</p> <p>NOTE 2:     The restricted KL divergence is obtained by first computing the element-wise KL divergence     (i.e., the KL computed for each element of the latent tensors). Then, the restricted version     is computed by summing over the channels and the spatial dimensions associated only to the     portion of the latent tensor that is used for prediction.</p> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>class NormalStochasticBlock(nn.Module):\n    \"\"\"\n    Stochastic block used in the Top-Down inference pass.\n\n    Algorithm:\n        - map input parameters to q(z) and (optionally) p(z) via convolution\n        - sample a latent tensor z ~ q(z)\n        - feed z to convolution and return.\n\n    NOTE 1:\n        If parameters for q are not given, sampling is done from p(z).\n\n    NOTE 2:\n        The restricted KL divergence is obtained by first computing the element-wise KL divergence\n        (i.e., the KL computed for each element of the latent tensors). Then, the restricted version\n        is computed by summing over the channels and the spatial dimensions associated only to the\n        portion of the latent tensor that is used for prediction.\n    \"\"\"\n\n    def __init__(\n        self,\n        c_in: int,\n        c_vars: int,\n        c_out: int,\n        conv_dims: int = 2,\n        kernel: int = 3,\n        transform_p_params: bool = True,\n        vanilla_latent_hw: int = None,\n        use_naive_exponential: bool = False,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        c_in: int\n            The number of channels of the input tensor.\n        c_vars: int\n            The number of channels of the latent space tensor.\n        c_out:  int\n            The output of the stochastic layer.\n            Note that this is different from the sampled latent z.\n        conv_dims: int, optional\n            The number of dimensions of the convolutional layers (2D or 3D).\n            Default is 2.\n        kernel: int, optional\n            The size of the kernel used in convolutional layers.\n            Default is 3.\n        transform_p_params: bool, optional\n            Whether a transformation should be applied to the `p_params` tensor.\n            The transformation consists in a 2D convolution ()`conv_in_p()`) that\n            maps the input to a larger number of channels.\n            Default is `True`.\n        vanilla_latent_hw: int, optional\n            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n            Default is `None`.\n        use_naive_exponential: bool, optional\n            If `False`, exponentials are computed according to the alternative definition\n            provided by `StableExponential` class. This should improve numerical stability\n            in the training process. Default is `False`.\n        \"\"\"\n        super().__init__()\n        assert kernel % 2 == 1\n        pad = kernel // 2\n        self.transform_p_params = transform_p_params\n        self.c_in = c_in\n        self.c_out = c_out\n        self.c_vars = c_vars\n        self.conv_dims = conv_dims\n        self._use_naive_exponential = use_naive_exponential\n        self._vanilla_latent_hw = vanilla_latent_hw\n\n        conv_layer: ConvType = getattr(nn, f\"Conv{conv_dims}d\")\n\n        if transform_p_params:\n            self.conv_in_p = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n        self.conv_in_q = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n        self.conv_out = conv_layer(c_vars, c_out, kernel, padding=pad)\n\n    def get_z(\n        self,\n        sampling_distrib: torch.distributions.normal.Normal,\n        forced_latent: Union[torch.Tensor, None],\n        mode_pred: bool,\n        use_uncond_mode: bool,\n    ) -&gt; torch.Tensor:\n        \"\"\"Sample a latent tensor from the given latent distribution.\n\n        Latent tensor can be obtained is several ways:\n            - Sampled from the (Gaussian) latent distribution.\n            - Taken as a pre-defined forced latent.\n            - Taken as the mode (mean) of the latent distribution.\n            - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.\n\n        Parameters\n        ----------\n        sampling_distrib: torch.distributions.normal.Normal\n            The Gaussian distribution from which latent tensor is sampled.\n        forced_latent: torch.Tensor\n            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,\n            hence, sampling does not happen.\n        mode_pred: bool\n            Whether the model is prediction mode.\n        use_uncond_mode: bool\n            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n        \"\"\"\n        if forced_latent is None:\n            if mode_pred:\n                if use_uncond_mode:\n                    z = sampling_distrib.mean\n                else:\n                    z = sampling_distrib.rsample()\n            else:\n                z = sampling_distrib.rsample()\n        else:\n            z = forced_latent\n        return z\n\n    def sample_from_q(\n        self, q_params: torch.Tensor, var_clip_max: float\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Given an input parameter tensor defining q(z),\n        it processes it by calling `process_q_params()` method and\n        sample a latent tensor from the resulting distribution.\n\n        Parameters\n        ----------\n        q_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        _, _, q = self.process_q_params(q_params, var_clip_max)\n        return q.rsample()\n\n    def compute_kl_metrics(\n        self,\n        p: torch.distributions.normal.Normal,\n        p_params: torch.Tensor,\n        q: torch.distributions.normal.Normal,\n        q_params: torch.Tensor,\n        mode_pred: bool,\n        analytical_kl: bool,\n        z: torch.Tensor,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.\n        Specifically, the different versions of the KL loss terms are:\n            - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].\n            - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].\n            - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is\n            used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].\n            - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].\n            - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]\n\n        Parameters\n        ----------\n        p: torch.distributions.normal.Normal\n            The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).\n        p_params: torch.Tensor\n            The parameters of the prior generative distribution.\n        q: torch.distributions.normal.Normal\n            The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).\n        q_params: torch.Tensor\n            The parameters of the inference distribution.\n        mode_pred: bool\n            Whether the model is in prediction mode.\n        analytical_kl: bool\n            Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n        z: torch.Tensor\n            The sampled latent tensor.\n        \"\"\"\n        kl_samplewise_restricted = None\n        if mode_pred is False:  # if not predicting\n            if analytical_kl:\n                kl_elementwise = kl_divergence(q, p)\n            else:\n                kl_elementwise = kl_normal_mc(z, p_params, q_params)\n\n            all_dims = tuple(range(len(kl_elementwise.shape)))\n            kl_samplewise = kl_elementwise.sum(all_dims[1:])\n            kl_channelwise = kl_elementwise.sum(all_dims[2:])\n\n            # compute KL only on the portion of the latent space that is used for prediction.\n            pad = (kl_elementwise.shape[-1] - self._vanilla_latent_hw) // 2\n            if pad &gt; 0:\n                tmp = kl_elementwise[..., pad:-pad, pad:-pad]\n                kl_samplewise_restricted = tmp.sum(all_dims[1:])\n            else:\n                kl_samplewise_restricted = kl_samplewise\n\n            # Compute spatial KL analytically (but conditioned on samples from\n            # previous layers)\n            kl_spatial = kl_elementwise.sum(1)\n        else:  # if predicting, no need to compute KL\n            kl_elementwise = kl_samplewise = kl_spatial = kl_channelwise = None\n\n        kl_dict = {\n            \"kl_elementwise\": kl_elementwise,  # (batch, ch, h, w)\n            \"kl_samplewise\": kl_samplewise,  # (batch, )\n            \"kl_samplewise_restricted\": kl_samplewise_restricted,  # (batch, )\n            \"kl_spatial\": kl_spatial,  # (batch, h, w)\n            \"kl_channelwise\": kl_channelwise,  # (batch, ch)\n        }  # TODO revisit, check dims\n        return kl_dict\n\n    def process_p_params(\n        self, p_params: torch.Tensor, var_clip_max: float\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n        \"\"\"Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).\n\n        Processing consists in:\n            - (optionally) 2D convolution on the input tensor to increase number of channels.\n            - split the resulting tensor into two chunks, the mean and the log-variance.\n            - (optionally) clip the log-variance to an upper threshold.\n            - define the normal distribution p(z) given the parameter tensors above.\n\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        if self.transform_p_params:\n            p_params = self.conv_in_p(p_params)\n        else:\n            assert p_params.size(1) == 2 * self.c_vars\n\n        # Define p(z)\n        p_mu, p_lv = p_params.chunk(2, dim=1)\n        if var_clip_max is not None:\n            p_lv = torch.clip(p_lv, max=var_clip_max)\n\n        p_mu = StableMean(p_mu)\n        p_lv = StableLogVar(p_lv, enable_stable=not self._use_naive_exponential)\n        p = Normal(p_mu.get(), p_lv.get_std())\n        return p_mu, p_lv, p\n\n    def process_q_params(\n        self, q_params: torch.Tensor, var_clip_max: float, allow_oddsizes: bool = False\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n        \"\"\"\n        Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).\n\n        Processing consists in:\n            - convolution on the input tensor to double the number of channels.\n            - split the resulting tensor into 2 chunks, respectively mean and log-var.\n            - (optionally) clip the log-variance to an upper threshold.\n            - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.\n            - define the normal distribution q(z) given the parameter tensors above.\n\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The input tensor to be processed.\n        var_clip_max: float\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped.\n        \"\"\"\n        q_params = self.conv_in_q(q_params)\n\n        q_mu, q_lv = q_params.chunk(2, dim=1)\n        if var_clip_max is not None:\n            q_lv = torch.clip(q_lv, max=var_clip_max)\n\n        if q_mu.shape[-1] % 2 == 1 and allow_oddsizes is False:\n            q_mu = F.center_crop(q_mu, q_mu.shape[-1] - 1)\n            q_lv = F.center_crop(q_lv, q_lv.shape[-1] - 1)\n            # TODO revisit ?!\n        q_mu = StableMean(q_mu)\n        q_lv = StableLogVar(q_lv, enable_stable=not self._use_naive_exponential)\n        q = Normal(q_mu.get(), q_lv.get_std())\n        return q_mu, q_lv, q\n\n    def forward(\n        self,\n        p_params: torch.Tensor,\n        q_params: Union[torch.Tensor, None] = None,\n        forced_latent: Union[torch.Tensor, None] = None,\n        force_constant_output: bool = False,\n        analytical_kl: bool = False,\n        mode_pred: bool = False,\n        use_uncond_mode: bool = False,\n        var_clip_max: Union[float, None] = None,\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Parameters\n        ----------\n        p_params: torch.Tensor\n            The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).\n        q_params: torch.Tensor, optional\n            The tensor resulting from merging the bu_value tensor at the same hierarchical level\n            from the bottom-up pass and the `p_params` tensor. Default is `None`.\n        forced_latent: torch.Tensor, optional\n            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent\n            tensor and, hence, sampling does not happen. Default is `None`.\n        force_constant_output: bool, optional\n            Whether to copy the first sample (and rel. distrib parameters) over the whole batch.\n            This is used when doing experiment from the prior - q is not used.\n            Default is `False`.\n        analytical_kl: bool, optional\n            Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n            Default is `False`.\n        mode_pred: bool, optional\n            Whether the model is in prediction mode. Default is `False`.\n        use_uncond_mode: bool, optional\n            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n            Default is `False`.\n        var_clip_max: float, optional\n            The maximum value reachable by the log-variance of the latent distribution.\n            Values exceeding this threshold are clipped. Default is `None`.\n        \"\"\"\n        debug_qvar_max = 0\n\n        # Check sampling options consistency\n        assert forced_latent is None\n\n        # Get generative distribution p(z_i|z_{i+1})\n        p_mu, p_lv, p = self.process_p_params(p_params, var_clip_max)\n        p_params = (p_mu, p_lv)\n\n        if q_params is not None:\n            # Get inference distribution q(z_i|z_{i+1})\n            q_mu, q_lv, q = self.process_q_params(q_params, var_clip_max)\n            q_params = (q_mu, q_lv)\n            debug_qvar_max = torch.max(q_lv.get())\n            sampling_distrib = q\n            q_size = q_mu.get().shape[-1]\n            if p_mu.get().shape[-1] != q_size and mode_pred is False:\n                p_mu.centercrop_to_size(q_size)\n                p_lv.centercrop_to_size(q_size)\n        else:\n            sampling_distrib = p\n\n        # Sample latent variable\n        z = self.get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)\n\n        # TODO: not necessary, remove\n        # Copy one sample (and distrib parameters) over the whole batch.\n        # This is used when doing experiment from the prior - q is not used.\n        if force_constant_output:\n            z = z[0:1].expand_as(z).clone()\n            p_params = (\n                p_params[0][0:1].expand_as(p_params[0]).clone(),\n                p_params[1][0:1].expand_as(p_params[1]).clone(),\n            )\n\n        # Pass the sampled latent through the output convolution of stochastic block\n        out = self.conv_out(z)\n\n        if q_params is not None:\n            # Compute log q(z)\n            logprob_q = q.log_prob(z).sum(tuple(range(1, z.dim())))\n            # Compute KL divergence metrics\n            kl_dict = self.compute_kl_metrics(\n                p, p_params, q, q_params, mode_pred, analytical_kl, z\n            )\n        else:\n            kl_dict = {}\n            logprob_q = None\n\n        # Store meaningful quantities for later computation\n        data = kl_dict\n        data[\"z\"] = z  # sampled variable at this layer (B, C, [Z], Y, X)\n        data[\"p_params\"] = p_params  # (B, C, [Z], Y, X) where B is 1 or batch size\n        data[\"q_params\"] = q_params  # (B, C, [Z], Y, X)\n        data[\"logprob_q\"] = logprob_q  # (B, )\n        data[\"qvar_max\"] = debug_qvar_max\n        return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.__init__","title":"<code>__init__(c_in, c_vars, c_out, conv_dims=2, kernel=3, transform_p_params=True, vanilla_latent_hw=None, use_naive_exponential=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>c_in</code> <code>int</code> <p>The number of channels of the input tensor.</p> required <code>c_vars</code> <code>int</code> <p>The number of channels of the latent space tensor.</p> required <code>c_out</code> <code>int</code> <p>The output of the stochastic layer. Note that this is different from the sampled latent z.</p> required <code>conv_dims</code> <code>int</code> <p>The number of dimensions of the convolutional layers (2D or 3D). Default is 2.</p> <code>2</code> <code>kernel</code> <code>int</code> <p>The size of the kernel used in convolutional layers. Default is 3.</p> <code>3</code> <code>transform_p_params</code> <code>bool</code> <p>Whether a transformation should be applied to the <code>p_params</code> tensor. The transformation consists in a 2D convolution ()<code>conv_in_p()</code>) that maps the input to a larger number of channels. Default is <code>True</code>.</p> <code>True</code> <code>vanilla_latent_hw</code> <code>int</code> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> <code>None</code> <code>use_naive_exponential</code> <code>bool</code> <p>If <code>False</code>, exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> <code>False</code> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def __init__(\n    self,\n    c_in: int,\n    c_vars: int,\n    c_out: int,\n    conv_dims: int = 2,\n    kernel: int = 3,\n    transform_p_params: bool = True,\n    vanilla_latent_hw: int = None,\n    use_naive_exponential: bool = False,\n):\n    \"\"\"\n    Parameters\n    ----------\n    c_in: int\n        The number of channels of the input tensor.\n    c_vars: int\n        The number of channels of the latent space tensor.\n    c_out:  int\n        The output of the stochastic layer.\n        Note that this is different from the sampled latent z.\n    conv_dims: int, optional\n        The number of dimensions of the convolutional layers (2D or 3D).\n        Default is 2.\n    kernel: int, optional\n        The size of the kernel used in convolutional layers.\n        Default is 3.\n    transform_p_params: bool, optional\n        Whether a transformation should be applied to the `p_params` tensor.\n        The transformation consists in a 2D convolution ()`conv_in_p()`) that\n        maps the input to a larger number of channels.\n        Default is `True`.\n    vanilla_latent_hw: int, optional\n        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).\n        Default is `None`.\n    use_naive_exponential: bool, optional\n        If `False`, exponentials are computed according to the alternative definition\n        provided by `StableExponential` class. This should improve numerical stability\n        in the training process. Default is `False`.\n    \"\"\"\n    super().__init__()\n    assert kernel % 2 == 1\n    pad = kernel // 2\n    self.transform_p_params = transform_p_params\n    self.c_in = c_in\n    self.c_out = c_out\n    self.c_vars = c_vars\n    self.conv_dims = conv_dims\n    self._use_naive_exponential = use_naive_exponential\n    self._vanilla_latent_hw = vanilla_latent_hw\n\n    conv_layer: ConvType = getattr(nn, f\"Conv{conv_dims}d\")\n\n    if transform_p_params:\n        self.conv_in_p = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n    self.conv_in_q = conv_layer(c_in, 2 * c_vars, kernel, padding=pad)\n    self.conv_out = conv_layer(c_vars, c_out, kernel, padding=pad)\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.compute_kl_metrics","title":"<code>compute_kl_metrics(p, p_params, q, q_params, mode_pred, analytical_kl, z)</code>","text":"<p>Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric. Specifically, the different versions of the KL loss terms are:     - <code>kl_elementwise</code>: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].     - <code>kl_samplewise</code>: KL term associated to each sample in the batch [Shape: (batch, )].     - <code>kl_samplewise_restricted</code>: KL term only associated to the portion of the latent tensor that is     used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].     - <code>kl_channelwise</code>: KL term associated to each sample and each channel [Shape: (batch, ch, )].     - <code>kl_spatial</code>: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Normal</code> <p>The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</p> required <code>p_params</code> <code>Tensor</code> <p>The parameters of the prior generative distribution.</p> required <code>q</code> <code>Normal</code> <p>The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</p> required <code>q_params</code> <code>Tensor</code> <p>The parameters of the inference distribution.</p> required <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode.</p> required <code>analytical_kl</code> <code>bool</code> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation.</p> required <code>z</code> <code>Tensor</code> <p>The sampled latent tensor.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def compute_kl_metrics(\n    self,\n    p: torch.distributions.normal.Normal,\n    p_params: torch.Tensor,\n    q: torch.distributions.normal.Normal,\n    q_params: torch.Tensor,\n    mode_pred: bool,\n    analytical_kl: bool,\n    z: torch.Tensor,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.\n    Specifically, the different versions of the KL loss terms are:\n        - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].\n        - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].\n        - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is\n        used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].\n        - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].\n        - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]\n\n    Parameters\n    ----------\n    p: torch.distributions.normal.Normal\n        The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).\n    p_params: torch.Tensor\n        The parameters of the prior generative distribution.\n    q: torch.distributions.normal.Normal\n        The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).\n    q_params: torch.Tensor\n        The parameters of the inference distribution.\n    mode_pred: bool\n        Whether the model is in prediction mode.\n    analytical_kl: bool\n        Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n    z: torch.Tensor\n        The sampled latent tensor.\n    \"\"\"\n    kl_samplewise_restricted = None\n    if mode_pred is False:  # if not predicting\n        if analytical_kl:\n            kl_elementwise = kl_divergence(q, p)\n        else:\n            kl_elementwise = kl_normal_mc(z, p_params, q_params)\n\n        all_dims = tuple(range(len(kl_elementwise.shape)))\n        kl_samplewise = kl_elementwise.sum(all_dims[1:])\n        kl_channelwise = kl_elementwise.sum(all_dims[2:])\n\n        # compute KL only on the portion of the latent space that is used for prediction.\n        pad = (kl_elementwise.shape[-1] - self._vanilla_latent_hw) // 2\n        if pad &gt; 0:\n            tmp = kl_elementwise[..., pad:-pad, pad:-pad]\n            kl_samplewise_restricted = tmp.sum(all_dims[1:])\n        else:\n            kl_samplewise_restricted = kl_samplewise\n\n        # Compute spatial KL analytically (but conditioned on samples from\n        # previous layers)\n        kl_spatial = kl_elementwise.sum(1)\n    else:  # if predicting, no need to compute KL\n        kl_elementwise = kl_samplewise = kl_spatial = kl_channelwise = None\n\n    kl_dict = {\n        \"kl_elementwise\": kl_elementwise,  # (batch, ch, h, w)\n        \"kl_samplewise\": kl_samplewise,  # (batch, )\n        \"kl_samplewise_restricted\": kl_samplewise_restricted,  # (batch, )\n        \"kl_spatial\": kl_spatial,  # (batch, h, w)\n        \"kl_channelwise\": kl_channelwise,  # (batch, ch)\n    }  # TODO revisit, check dims\n    return kl_dict\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.forward","title":"<code>forward(p_params, q_params=None, forced_latent=None, force_constant_output=False, analytical_kl=False, mode_pred=False, use_uncond_mode=False, var_clip_max=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>p_params</code> <code>Tensor</code> <p>The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</p> required <code>q_params</code> <code>Union[Tensor, None]</code> <p>The tensor resulting from merging the bu_value tensor at the same hierarchical level from the bottom-up pass and the <code>p_params</code> tensor. Default is <code>None</code>.</p> <code>None</code> <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> <code>None</code> <code>force_constant_output</code> <code>bool</code> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> <code>False</code> <code>analytical_kl</code> <code>bool</code> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation. Default is <code>False</code>.</p> <code>False</code> <code>mode_pred</code> <code>bool</code> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode. Default is <code>False</code>.</p> <code>False</code> <code>var_clip_max</code> <code>Union[float, None]</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> <code>None</code> Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def forward(\n    self,\n    p_params: torch.Tensor,\n    q_params: Union[torch.Tensor, None] = None,\n    forced_latent: Union[torch.Tensor, None] = None,\n    force_constant_output: bool = False,\n    analytical_kl: bool = False,\n    mode_pred: bool = False,\n    use_uncond_mode: bool = False,\n    var_clip_max: Union[float, None] = None,\n) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).\n    q_params: torch.Tensor, optional\n        The tensor resulting from merging the bu_value tensor at the same hierarchical level\n        from the bottom-up pass and the `p_params` tensor. Default is `None`.\n    forced_latent: torch.Tensor, optional\n        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent\n        tensor and, hence, sampling does not happen. Default is `None`.\n    force_constant_output: bool, optional\n        Whether to copy the first sample (and rel. distrib parameters) over the whole batch.\n        This is used when doing experiment from the prior - q is not used.\n        Default is `False`.\n    analytical_kl: bool, optional\n        Whether to compute the KL divergence analytically or using Monte Carlo estimation.\n        Default is `False`.\n    mode_pred: bool, optional\n        Whether the model is in prediction mode. Default is `False`.\n    use_uncond_mode: bool, optional\n        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n        Default is `False`.\n    var_clip_max: float, optional\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped. Default is `None`.\n    \"\"\"\n    debug_qvar_max = 0\n\n    # Check sampling options consistency\n    assert forced_latent is None\n\n    # Get generative distribution p(z_i|z_{i+1})\n    p_mu, p_lv, p = self.process_p_params(p_params, var_clip_max)\n    p_params = (p_mu, p_lv)\n\n    if q_params is not None:\n        # Get inference distribution q(z_i|z_{i+1})\n        q_mu, q_lv, q = self.process_q_params(q_params, var_clip_max)\n        q_params = (q_mu, q_lv)\n        debug_qvar_max = torch.max(q_lv.get())\n        sampling_distrib = q\n        q_size = q_mu.get().shape[-1]\n        if p_mu.get().shape[-1] != q_size and mode_pred is False:\n            p_mu.centercrop_to_size(q_size)\n            p_lv.centercrop_to_size(q_size)\n    else:\n        sampling_distrib = p\n\n    # Sample latent variable\n    z = self.get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)\n\n    # TODO: not necessary, remove\n    # Copy one sample (and distrib parameters) over the whole batch.\n    # This is used when doing experiment from the prior - q is not used.\n    if force_constant_output:\n        z = z[0:1].expand_as(z).clone()\n        p_params = (\n            p_params[0][0:1].expand_as(p_params[0]).clone(),\n            p_params[1][0:1].expand_as(p_params[1]).clone(),\n        )\n\n    # Pass the sampled latent through the output convolution of stochastic block\n    out = self.conv_out(z)\n\n    if q_params is not None:\n        # Compute log q(z)\n        logprob_q = q.log_prob(z).sum(tuple(range(1, z.dim())))\n        # Compute KL divergence metrics\n        kl_dict = self.compute_kl_metrics(\n            p, p_params, q, q_params, mode_pred, analytical_kl, z\n        )\n    else:\n        kl_dict = {}\n        logprob_q = None\n\n    # Store meaningful quantities for later computation\n    data = kl_dict\n    data[\"z\"] = z  # sampled variable at this layer (B, C, [Z], Y, X)\n    data[\"p_params\"] = p_params  # (B, C, [Z], Y, X) where B is 1 or batch size\n    data[\"q_params\"] = q_params  # (B, C, [Z], Y, X)\n    data[\"logprob_q\"] = logprob_q  # (B, )\n    data[\"qvar_max\"] = debug_qvar_max\n    return out, data\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.get_z","title":"<code>get_z(sampling_distrib, forced_latent, mode_pred, use_uncond_mode)</code>","text":"<p>Sample a latent tensor from the given latent distribution.</p> <p>Latent tensor can be obtained is several ways:     - Sampled from the (Gaussian) latent distribution.     - Taken as a pre-defined forced latent.     - Taken as the mode (mean) of the latent distribution.     - In prediction mode (<code>mode_pred==True</code>), can be either sample or taken as the distribution mode.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_distrib</code> <code>Normal</code> <p>The Gaussian distribution from which latent tensor is sampled.</p> required <code>forced_latent</code> <code>Union[Tensor, None]</code> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen.</p> required <code>mode_pred</code> <code>bool</code> <p>Whether the model is prediction mode.</p> required <code>use_uncond_mode</code> <code>bool</code> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def get_z(\n    self,\n    sampling_distrib: torch.distributions.normal.Normal,\n    forced_latent: Union[torch.Tensor, None],\n    mode_pred: bool,\n    use_uncond_mode: bool,\n) -&gt; torch.Tensor:\n    \"\"\"Sample a latent tensor from the given latent distribution.\n\n    Latent tensor can be obtained is several ways:\n        - Sampled from the (Gaussian) latent distribution.\n        - Taken as a pre-defined forced latent.\n        - Taken as the mode (mean) of the latent distribution.\n        - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.\n\n    Parameters\n    ----------\n    sampling_distrib: torch.distributions.normal.Normal\n        The Gaussian distribution from which latent tensor is sampled.\n    forced_latent: torch.Tensor\n        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,\n        hence, sampling does not happen.\n    mode_pred: bool\n        Whether the model is prediction mode.\n    use_uncond_mode: bool\n        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.\n    \"\"\"\n    if forced_latent is None:\n        if mode_pred:\n            if use_uncond_mode:\n                z = sampling_distrib.mean\n            else:\n                z = sampling_distrib.rsample()\n        else:\n            z = sampling_distrib.rsample()\n    else:\n        z = forced_latent\n    return z\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.process_p_params","title":"<code>process_p_params(p_params, var_clip_max)</code>","text":"<p>Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).</p> <p>Processing consists in:     - (optionally) 2D convolution on the input tensor to increase number of channels.     - split the resulting tensor into two chunks, the mean and the log-variance.     - (optionally) clip the log-variance to an upper threshold.     - define the normal distribution p(z) given the parameter tensors above.</p> <p>Parameters:</p> Name Type Description Default <code>p_params</code> <code>Tensor</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def process_p_params(\n    self, p_params: torch.Tensor, var_clip_max: float\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n    \"\"\"Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).\n\n    Processing consists in:\n        - (optionally) 2D convolution on the input tensor to increase number of channels.\n        - split the resulting tensor into two chunks, the mean and the log-variance.\n        - (optionally) clip the log-variance to an upper threshold.\n        - define the normal distribution p(z) given the parameter tensors above.\n\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    if self.transform_p_params:\n        p_params = self.conv_in_p(p_params)\n    else:\n        assert p_params.size(1) == 2 * self.c_vars\n\n    # Define p(z)\n    p_mu, p_lv = p_params.chunk(2, dim=1)\n    if var_clip_max is not None:\n        p_lv = torch.clip(p_lv, max=var_clip_max)\n\n    p_mu = StableMean(p_mu)\n    p_lv = StableLogVar(p_lv, enable_stable=not self._use_naive_exponential)\n    p = Normal(p_mu.get(), p_lv.get_std())\n    return p_mu, p_lv, p\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.process_q_params","title":"<code>process_q_params(q_params, var_clip_max, allow_oddsizes=False)</code>","text":"<p>Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).</p> <p>Processing consists in:     - convolution on the input tensor to double the number of channels.     - split the resulting tensor into 2 chunks, respectively mean and log-var.     - (optionally) clip the log-variance to an upper threshold.     - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.     - define the normal distribution q(z) given the parameter tensors above.</p> <p>Parameters:</p> Name Type Description Default <code>p_params</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def process_q_params(\n    self, q_params: torch.Tensor, var_clip_max: float, allow_oddsizes: bool = False\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.distributions.normal.Normal]:\n    \"\"\"\n    Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).\n\n    Processing consists in:\n        - convolution on the input tensor to double the number of channels.\n        - split the resulting tensor into 2 chunks, respectively mean and log-var.\n        - (optionally) clip the log-variance to an upper threshold.\n        - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.\n        - define the normal distribution q(z) given the parameter tensors above.\n\n    Parameters\n    ----------\n    p_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    q_params = self.conv_in_q(q_params)\n\n    q_mu, q_lv = q_params.chunk(2, dim=1)\n    if var_clip_max is not None:\n        q_lv = torch.clip(q_lv, max=var_clip_max)\n\n    if q_mu.shape[-1] % 2 == 1 and allow_oddsizes is False:\n        q_mu = F.center_crop(q_mu, q_mu.shape[-1] - 1)\n        q_lv = F.center_crop(q_lv, q_lv.shape[-1] - 1)\n        # TODO revisit ?!\n    q_mu = StableMean(q_mu)\n    q_lv = StableLogVar(q_lv, enable_stable=not self._use_naive_exponential)\n    q = Normal(q_mu.get(), q_lv.get_std())\n    return q_mu, q_lv, q\n</code></pre>"},{"location":"reference/careamics/models/lvae/stochastic/#careamics.models.lvae.stochastic.NormalStochasticBlock.sample_from_q","title":"<code>sample_from_q(q_params, var_clip_max)</code>","text":"<p>Given an input parameter tensor defining q(z), it processes it by calling <code>process_q_params()</code> method and sample a latent tensor from the resulting distribution.</p> <p>Parameters:</p> Name Type Description Default <code>q_params</code> <code>Tensor</code> <p>The input tensor to be processed.</p> required <code>var_clip_max</code> <code>float</code> <p>The maximum value reachable by the log-variance of the latent distribution. Values exceeding this threshold are clipped.</p> required Source code in <code>src/careamics/models/lvae/stochastic.py</code> <pre><code>def sample_from_q(\n    self, q_params: torch.Tensor, var_clip_max: float\n) -&gt; torch.Tensor:\n    \"\"\"\n    Given an input parameter tensor defining q(z),\n    it processes it by calling `process_q_params()` method and\n    sample a latent tensor from the resulting distribution.\n\n    Parameters\n    ----------\n    q_params: torch.Tensor\n        The input tensor to be processed.\n    var_clip_max: float\n        The maximum value reachable by the log-variance of the latent distribution.\n        Values exceeding this threshold are clipped.\n    \"\"\"\n    _, _, q = self.process_q_params(q_params, var_clip_max)\n    return q.rsample()\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/","title":"utils","text":"<p>Script for utility functions needed by the LVAE model.</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.Interpolate","title":"<code>Interpolate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper for torch.nn.functional.interpolate.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class Interpolate(nn.Module):\n    \"\"\"Wrapper for torch.nn.functional.interpolate.\"\"\"\n\n    def __init__(self, size=None, scale=None, mode=\"bilinear\", align_corners=False):\n        super().__init__()\n        assert (size is None) == (scale is not None)\n        self.size = size\n        self.scale = scale\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        out = F.interpolate(\n            x,\n            size=self.size,\n            scale_factor=self.scale,\n            mode=self.mode,\n            align_corners=self.align_corners,\n        )\n        return out\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableExponential","title":"<code>StableExponential</code>","text":"<p>Class that redefines the definition of exp() to increase numerical stability. Naturally, also the definition of log() must change accordingly. However, it is worth noting that the two operations remain one the inverse of the other, meaning that x = log(exp(x)) and x = exp(log(x)) are always true.</p> <p>Definition:     exp(x) = {         exp(x) if x&lt;=0         x+1    if x&gt;0     }</p> <pre><code>log(x) = {\n    x        if x&lt;=0\n    log(1+x) if x&gt;0\n}\n</code></pre> <p>NOTE 1:     Within the class everything is done on the tensor given as input to the constructor.     Therefore, when exp() is called, self._tensor.exp() is computed.     When log() is called, torch.log(self._tensor.exp()) is computed instead.</p> <p>NOTE 2:     Given the output from exp(), torch.log() or the log() method of the class give identical results.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableExponential:\n    \"\"\"\n    Class that redefines the definition of exp() to increase numerical stability.\n    Naturally, also the definition of log() must change accordingly.\n    However, it is worth noting that the two operations remain one the inverse of the other,\n    meaning that x = log(exp(x)) and x = exp(log(x)) are always true.\n\n    Definition:\n        exp(x) = {\n            exp(x) if x&lt;=0\n            x+1    if x&gt;0\n        }\n\n        log(x) = {\n            x        if x&lt;=0\n            log(1+x) if x&gt;0\n        }\n\n    NOTE 1:\n        Within the class everything is done on the tensor given as input to the constructor.\n        Therefore, when exp() is called, self._tensor.exp() is computed.\n        When log() is called, torch.log(self._tensor.exp()) is computed instead.\n\n    NOTE 2:\n        Given the output from exp(), torch.log() or the log() method of the class give identical results.\n    \"\"\"\n\n    def __init__(self, tensor):\n        self._raw_tensor = tensor\n        posneg_dic = self.posneg_separation(self._raw_tensor)\n        self.pos_f, self.neg_f = posneg_dic[\"filter\"]\n        self.pos_data, self.neg_data = posneg_dic[\"value\"]\n\n    def posneg_separation(self, tensor):\n        pos = tensor &gt; 0\n        pos_tensor = torch.clip(tensor, min=0)\n\n        neg = tensor &lt;= 0\n        neg_tensor = torch.clip(tensor, max=0)\n\n        return {\"filter\": [pos, neg], \"value\": [pos_tensor, neg_tensor]}\n\n    def exp(self):\n        return torch.exp(self.neg_data) * self.neg_f + (1 + self.pos_data) * self.pos_f\n\n    def log(self):\n        return self.neg_data * self.neg_f + torch.log(1 + self.pos_data) * self.pos_f\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar","title":"<code>StableLogVar</code>","text":"<p>Class that provides a numerically stable implementation of Log-Variance. Specifically, it uses the exp() and log() formulas defined in <code>StableExponential</code> class.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableLogVar:\n    \"\"\"\n    Class that provides a numerically stable implementation of Log-Variance.\n    Specifically, it uses the exp() and log() formulas defined in `StableExponential` class.\n    \"\"\"\n\n    def __init__(\n        self, logvar: torch.Tensor, enable_stable: bool = True, var_eps: float = 1e-6\n    ):\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        logvar: torch.Tensor\n            The input (true) logvar vector, to be converted in the Stable version.\n        enable_stable: bool, optional\n            Whether to compute the stable version of log-variance. Default is `True`.\n        var_eps: float, optional\n            The minimum value attainable by the variance. Default is `1e-6`.\n        \"\"\"\n        self._lv = logvar\n        self._enable_stable = enable_stable\n        self._eps = var_eps\n\n    def get(self) -&gt; torch.Tensor:\n        if self._enable_stable is False:\n            return self._lv\n\n        return torch.log(self.get_var())\n\n    def get_var(self) -&gt; torch.Tensor:\n        \"\"\"\n        Get Variance from Log-Variance.\n        \"\"\"\n        if self._enable_stable is False:\n            return torch.exp(self._lv)\n        return StableExponential(self._lv).exp() + self._eps\n\n    def get_std(self) -&gt; torch.Tensor:\n        return torch.sqrt(self.get_var())\n\n    @property\n    def is_3D(self) -&gt; bool:\n        \"\"\"Check if the _lv tensor is 3D.\n\n        Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).\n        \"\"\"\n        return self._lv.dim() == 5\n\n    def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n        \"\"\"\n        Centercrop the log-variance tensor to the desired size.\n\n        Parameters\n        ----------\n        size: torch.Tensor\n            The desired size of the log-variance tensor.\n        \"\"\"\n        assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n        if self._lv.shape[-1] == size:\n            return\n\n        diff = self._lv.shape[-1] - size\n        assert diff &gt; 0 and diff % 2 == 0\n        self._lv = F.center_crop(self._lv, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.is_3D","title":"<code>is_3D</code>  <code>property</code>","text":"<p>Check if the _lv tensor is 3D.</p> <p>Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.__init__","title":"<code>__init__(logvar, enable_stable=True, var_eps=1e-06)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>logvar</code> <code>Tensor</code> <p>The input (true) logvar vector, to be converted in the Stable version.</p> required <code>enable_stable</code> <code>bool</code> <p>Whether to compute the stable version of log-variance. Default is <code>True</code>.</p> <code>True</code> <code>var_eps</code> <code>float</code> <p>The minimum value attainable by the variance. Default is <code>1e-6</code>.</p> <code>1e-06</code> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def __init__(\n    self, logvar: torch.Tensor, enable_stable: bool = True, var_eps: float = 1e-6\n):\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    logvar: torch.Tensor\n        The input (true) logvar vector, to be converted in the Stable version.\n    enable_stable: bool, optional\n        Whether to compute the stable version of log-variance. Default is `True`.\n    var_eps: float, optional\n        The minimum value attainable by the variance. Default is `1e-6`.\n    \"\"\"\n    self._lv = logvar\n    self._enable_stable = enable_stable\n    self._eps = var_eps\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.centercrop_to_size","title":"<code>centercrop_to_size(size)</code>","text":"<p>Centercrop the log-variance tensor to the desired size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Sequence[int]</code> <p>The desired size of the log-variance tensor.</p> required Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n    \"\"\"\n    Centercrop the log-variance tensor to the desired size.\n\n    Parameters\n    ----------\n    size: torch.Tensor\n        The desired size of the log-variance tensor.\n    \"\"\"\n    assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n    if self._lv.shape[-1] == size:\n        return\n\n    diff = self._lv.shape[-1] - size\n    assert diff &gt; 0 and diff % 2 == 0\n    self._lv = F.center_crop(self._lv, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableLogVar.get_var","title":"<code>get_var()</code>","text":"<p>Get Variance from Log-Variance.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def get_var(self) -&gt; torch.Tensor:\n    \"\"\"\n    Get Variance from Log-Variance.\n    \"\"\"\n    if self._enable_stable is False:\n        return torch.exp(self._lv)\n    return StableExponential(self._lv).exp() + self._eps\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean","title":"<code>StableMean</code>","text":"Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>class StableMean:\n\n    def __init__(self, mean):\n        self._mean = mean\n\n    def get(self) -&gt; torch.Tensor:\n        return self._mean\n\n    @property\n    def is_3D(self) -&gt; bool:\n        \"\"\"Check if the _mean tensor is 3D.\n\n        Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).\n        \"\"\"\n        return self._mean.dim() == 5\n\n    def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n        \"\"\"Centercrop the mean tensor to the desired size.\n\n        Implemented only in the case of 2D tensors.\n\n        Parameters\n        ----------\n        size: torch.Tensor\n            The desired size of the log-variance tensor.\n        \"\"\"\n        assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n        if self._mean.shape[-1] == size:\n            return\n\n        diff = self._mean.shape[-1] - size\n        assert diff &gt; 0 and diff % 2 == 0\n        self._mean = F.center_crop(self._mean, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean.is_3D","title":"<code>is_3D</code>  <code>property</code>","text":"<p>Check if the _mean tensor is 3D.</p> <p>Recall that, in this framework, tensors have shape (B, C, [Z], Y, X).</p>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.StableMean.centercrop_to_size","title":"<code>centercrop_to_size(size)</code>","text":"<p>Centercrop the mean tensor to the desired size.</p> <p>Implemented only in the case of 2D tensors.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>Sequence[int]</code> <p>The desired size of the log-variance tensor.</p> required Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def centercrop_to_size(self, size: Sequence[int]) -&gt; None:\n    \"\"\"Centercrop the mean tensor to the desired size.\n\n    Implemented only in the case of 2D tensors.\n\n    Parameters\n    ----------\n    size: torch.Tensor\n        The desired size of the log-variance tensor.\n    \"\"\"\n    assert not self.is_3D, \"Centercrop is implemented only for 2D tensors.\"\n\n    if self._mean.shape[-1] == size:\n        return\n\n    diff = self._mean.shape[-1] - size\n    assert diff &gt; 0 and diff % 2 == 0\n    self._mean = F.center_crop(self._mean, (size, size))\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils._pad_crop_img","title":"<code>_pad_crop_img(x, size, mode)</code>","text":"<p>Pads or crops a tensor.</p> <p>Pads or crops a tensor of shape (B, C, [Z], Y, X) to new shape.</p> Parameters: <p>x: torch.Tensor     Input image of shape (B, C, [Z], Y, X) size: Sequence[int]     Desired size ([Z], Y, X*) mode: Literal[\"crop\", \"pad\"]     Mode, either 'pad' or 'crop'</p> Returns: <p>torch.Tensor:     The padded or cropped tensor</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def _pad_crop_img(\n    x: torch.Tensor, size: Sequence[int], mode: Literal[\"crop\", \"pad\"]\n) -&gt; torch.Tensor:\n    \"\"\"Pads or crops a tensor.\n\n    Pads or crops a tensor of shape (B, C, [Z], Y, X) to new shape.\n\n    Parameters:\n    -----------\n    x: torch.Tensor\n        Input image of shape (B, C, [Z], Y, X)\n    size: Sequence[int]\n        Desired size ([Z*], Y*, X*)\n    mode: Literal[\"crop\", \"pad\"]\n        Mode, either 'pad' or 'crop'\n\n    Returns:\n    --------\n    torch.Tensor:\n        The padded or cropped tensor\n    \"\"\"\n    # TODO: Support cropping/padding on selected dimensions\n    assert (x.dim() == 4 and len(size) == 2) or (x.dim() == 5 and len(size) == 3)\n\n    size = tuple(size)\n    x_size = x.size()[2:]\n\n    if mode == \"pad\":\n        cond = any(x_size[i] &gt; size[i] for i in range(len(size)))\n    elif mode == \"crop\":\n        cond = any(x_size[i] &lt; size[i] for i in range(len(size)))\n\n    if cond:\n        raise ValueError(f\"Trying to {mode} from size {x_size} to size {size}\")\n\n    diffs = [abs(x - s) for x, s in zip(x_size, size)]\n    d1 = [d // 2 for d in diffs]\n    d2 = [d - (d // 2) for d in diffs]\n\n    if mode == \"pad\":\n        if x.dim() == 4:\n            padding = [d1[1], d2[1], d1[0], d2[0], 0, 0, 0, 0]\n        elif x.dim() == 5:\n            padding = [d1[2], d2[2], d1[1], d2[1], d1[0], d2[0], 0, 0, 0, 0]\n        return nn.functional.pad(x, padding)\n    elif mode == \"crop\":\n        if x.dim() == 4:\n            return x[:, :, d1[0] : (x_size[0] - d2[0]), d1[1] : (x_size[1] - d2[1])]\n        elif x.dim() == 5:\n            return x[\n                :,\n                :,\n                d1[0] : (x_size[0] - d2[0]),\n                d1[1] : (x_size[1] - d2[1]),\n                d1[2] : (x_size[2] - d2[2]),\n            ]\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.allow_numpy","title":"<code>allow_numpy(func)</code>","text":"<p>All optional arguments are passed as is. positional arguments are checked. if they are numpy array, they are converted to torch Tensor.</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def allow_numpy(func):\n    \"\"\"\n    All optional arguments are passed as is. positional arguments are checked. if they are numpy array,\n    they are converted to torch Tensor.\n    \"\"\"\n\n    def numpy_wrapper(*args, **kwargs):\n        new_args = []\n        for arg in args:\n            if isinstance(arg, np.ndarray):\n                arg = torch.Tensor(arg)\n            new_args.append(arg)\n        new_args = tuple(new_args)\n\n        output = func(*new_args, **kwargs)\n        return output\n\n    return numpy_wrapper\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.crop_img_tensor","title":"<code>crop_img_tensor(x, size)</code>","text":"<p>Crops a tensor. Crops a tensor of shape (batch, channels, h, w) to a desired height and width given by a tuple. Args:     x (torch.Tensor): Input image     size (list or tuple): Desired size (height, width)</p> <p>Returns:</p> Type Description <code>    The cropped tensor</code> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def crop_img_tensor(x, size) -&gt; torch.Tensor:\n    \"\"\"Crops a tensor.\n    Crops a tensor of shape (batch, channels, h, w) to a desired height and width\n    given by a tuple.\n    Args:\n        x (torch.Tensor): Input image\n        size (list or tuple): Desired size (height, width)\n\n    Returns\n    -------\n        The cropped tensor\n    \"\"\"\n    return _pad_crop_img(x, size, \"crop\")\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.kl_normal_mc","title":"<code>kl_normal_mc(z, p_mulv, q_mulv)</code>","text":"<p>One-sample estimation of element-wise KL between two diagonal multivariate normal distributions. Any number of dimensions, broadcasting supported (be careful). :param z: :param p_mulv: :param q_mulv: :return:</p> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def kl_normal_mc(z, p_mulv, q_mulv):\n    \"\"\"\n    One-sample estimation of element-wise KL between two diagonal\n    multivariate normal distributions. Any number of dimensions,\n    broadcasting supported (be careful).\n    :param z:\n    :param p_mulv:\n    :param q_mulv:\n    :return:\n    \"\"\"\n    assert isinstance(p_mulv, tuple)\n    assert isinstance(q_mulv, tuple)\n    p_mu, p_lv = p_mulv\n    q_mu, q_lv = q_mulv\n\n    p_std = p_lv.get_std()\n    q_std = q_lv.get_std()\n\n    p_distrib = Normal(p_mu.get(), p_std)\n    q_distrib = Normal(q_mu.get(), q_std)\n    return q_distrib.log_prob(z) - p_distrib.log_prob(z)\n</code></pre>"},{"location":"reference/careamics/models/lvae/utils/#careamics.models.lvae.utils.pad_img_tensor","title":"<code>pad_img_tensor(x, size)</code>","text":"<p>Pads a tensor</p> <p>Pads a tensor of shape (B, C, [Z], Y, X) to desired spatial dimensions.</p> Parameters: <pre><code>x (torch.Tensor): Input image of shape (B, C, [Z], Y, X)\nsize (list or tuple): Desired size  ([Z*], Y*, X*)\n</code></pre> Returns: <pre><code>The padded tensor\n</code></pre> Source code in <code>src/careamics/models/lvae/utils.py</code> <pre><code>def pad_img_tensor(x: torch.Tensor, size: Sequence[int]) -&gt; torch.Tensor:\n    \"\"\"Pads a tensor\n\n    Pads a tensor of shape (B, C, [Z], Y, X) to desired spatial dimensions.\n\n    Parameters:\n    -----------\n        x (torch.Tensor): Input image of shape (B, C, [Z], Y, X)\n        size (list or tuple): Desired size  ([Z*], Y*, X*)\n\n    Returns:\n    --------\n        The padded tensor\n    \"\"\"\n    return _pad_crop_img(x, size, \"pad\")\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/","title":"lvae_prediction","text":"<p>Module containing pytorch implementations for obtaining predictions from an LVAE.</p>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_mmse_tiled_batch","title":"<code>lvae_predict_mmse_tiled_batch(model, likelihood_obj, input, mmse_count)</code>","text":"<p>Generate the MMSE (minimum mean squared error) prediction, for a given input.</p> <p>This is calculated from the mean of multiple single sample predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>torch.tensor | tuple of (torch.tensor, Any, ...)</code> <p>Input to generate prediction for. This can include auxilary inputs such as <code>TileInformation</code>, but the model input is always the first item of the tuple. Expected shape of the model input is (S, C, Y, X).</p> required <code>mmse_count</code> <code>int</code> <p>Number of samples to generate to calculate MMSE (minimum mean squared error).</p> required <p>Returns:</p> Type Description <code>tuple of (tuple of (torch.Tensor[Any], Any, ...))</code> <p>A tuple of 3 elements. The first element contains the MMSE prediction, the second contains the standard deviation of the samples used to create the MMSE prediction. Finally the last element contains the log-variance of the likelihood, this will be <code>None</code> if <code>likelihood.predict_logvar</code> is <code>None</code>. Any auxillary data included in the input will also be include with all of the MMSE prediction, the standard deviation, and the log-variance.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_mmse_tiled_batch(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: tuple[Any],\n    mmse_count: int,\n) -&gt; tuple[tuple[Any], tuple[Any], Optional[tuple[Any]]]:\n    # TODO: fix docstring return types, ... hard to make readable\n    \"\"\"\n    Generate the MMSE (minimum mean squared error) prediction, for a given input.\n\n    This is calculated from the mean of multiple single sample predictions.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor | tuple of (torch.tensor, Any, ...)\n        Input to generate prediction for. This can include auxilary inputs such as\n        `TileInformation`, but the model input is always the first item of the tuple.\n        Expected shape of the model input is (S, C, Y, X).\n    mmse_count : int\n        Number of samples to generate to calculate MMSE (minimum mean squared error).\n\n    Returns\n    -------\n    tuple of (tuple of (torch.Tensor[Any], Any, ...))\n        A tuple of 3 elements. The first element contains the MMSE prediction, the\n        second contains the standard deviation of the samples used to create the MMSE\n        prediction. Finally the last element contains the log-variance of the\n        likelihood, this will be `None` if `likelihood.predict_logvar` is `None`.\n        Any auxillary data included in the input will also be include with all of the\n        MMSE prediction, the standard deviation, and the log-variance.\n    \"\"\"\n    if mmse_count &lt;= 0:\n        raise ValueError(\"MMSE count must be greater than zero.\")\n\n    x: torch.Tensor\n    aux: list[Any]\n    x, *aux = input\n\n    input_shape = x.shape\n    output_shape = (input_shape[0], model.target_ch, *input_shape[2:])\n    log_var: Optional[torch.Tensor] = None\n    # pre-declare empty array to fill with individual sample predictions\n    sample_predictions = torch.zeros(size=(mmse_count, *output_shape))\n    for mmse_idx in range(mmse_count):\n        sample_prediction, lv = lvae_predict_single_sample(\n            model=model, likelihood_obj=likelihood_obj, input=x\n        )\n        # only keep the log variance of the first sample prediction\n        if mmse_idx == 0:\n            log_var = lv\n\n        # store sample predictions\n        sample_predictions[mmse_idx, ...] = sample_prediction\n\n    mmse_prediction = torch.mean(sample_predictions, dim=0)\n    mmse_prediction_std = torch.std(sample_predictions, dim=0)\n\n    log_var_output = (log_var, *aux) if log_var is not None else None\n    return (mmse_prediction, *aux), (mmse_prediction_std, *aux), log_var_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_single_sample","title":"<code>lvae_predict_single_sample(model, likelihood_obj, input)</code>","text":"<p>Generate a single sample prediction from an LVAE model, for a given input.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>tensor</code> <p>Input to generate prediction for. Expected shape is (S, C, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of (torch.tensor, optional torch.tensor)</code> <p>The first element is the sample prediction, and the second element is the log-variance. The log-variance will be None if <code>model.predict_logvar is None</code>.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_single_sample(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: torch.Tensor,\n) -&gt; tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Generate a single sample prediction from an LVAE model, for a given input.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor\n        Input to generate prediction for. Expected shape is (S, C, Y, X).\n\n    Returns\n    -------\n    tuple of (torch.tensor, optional torch.tensor)\n        The first element is the sample prediction, and the second element is the\n        log-variance. The log-variance will be None if `model.predict_logvar is None`.\n    \"\"\"\n    model.eval()  # Not in original predict code: effects batch_norm and dropout layers\n    with torch.no_grad():\n        output: torch.Tensor\n        output, _ = model(input)  # 2nd item is top-down data dict\n\n    # presently, get_mean_lv just splits the output in 2 if predict_logvar=True,\n    #   optionally clips the logvavr if logvar_lowerbound is not None\n    # TODO: consider refactoring to remove use of the likelihood object\n    sample_prediction, log_var = likelihood_obj.get_mean_lv(output)\n\n    # TODO: output denormalization using target stats that will be saved in data config\n    # -&gt; Don't think we need this, saw it in a random bit of code somewhere.\n\n    return sample_prediction, log_var\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_prediction/#careamics.prediction_utils.lvae_prediction.lvae_predict_tiled_batch","title":"<code>lvae_predict_tiled_batch(model, likelihood_obj, input)</code>","text":"<p>Generate a single sample prediction from an LVAE model, for a given input.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>LadderVAE</code> <p>Trained LVAE model.</p> required <code>likelihood_obj</code> <code>LikelihoodModule</code> <p>Instance of a likelihood class.</p> required <code>input</code> <code>torch.tensor | tuple of (torch.tensor, Any, ...)</code> <p>Input to generate prediction for. This can include auxilary inputs such as <code>TileInformation</code>, but the model input is always the first item of the tuple. Expected shape of the model input is (S, C, Y, X).</p> required <p>Returns:</p> Type Description <code>tuple of ((torch.tensor, Any, ...), optional tuple of (torch.tensor, Any, ...))</code> <p>The first element is the sample prediction, and the second element is the log-variance. The log-variance will be None if <code>model.predict_logvar is None</code>. Any auxillary data included in the input will also be include with both the sample prediction and the log-variance.</p> Source code in <code>src/careamics/prediction_utils/lvae_prediction.py</code> <pre><code>def lvae_predict_tiled_batch(\n    model: LVAE,\n    likelihood_obj: LikelihoodModule,\n    input: tuple[Any],\n) -&gt; tuple[tuple[Any], Optional[tuple[Any]]]:\n    # TODO: fix docstring return types, ... too many output options\n    \"\"\"\n    Generate a single sample prediction from an LVAE model, for a given input.\n\n    Parameters\n    ----------\n    model : LVAE\n        Trained LVAE model.\n    likelihood_obj : LikelihoodModule\n        Instance of a likelihood class.\n    input : torch.tensor | tuple of (torch.tensor, Any, ...)\n        Input to generate prediction for. This can include auxilary inputs such as\n        `TileInformation`, but the model input is always the first item of the tuple.\n        Expected shape of the model input is (S, C, Y, X).\n\n    Returns\n    -------\n    tuple of ((torch.tensor, Any, ...), optional tuple of (torch.tensor, Any, ...))\n        The first element is the sample prediction, and the second element is the\n        log-variance. The log-variance will be None if `model.predict_logvar is None`.\n        Any auxillary data included in the input will also be include with both the\n        sample prediction and the log-variance.\n    \"\"\"\n    x: torch.Tensor\n    aux: list[Any]\n    x, *aux = input\n\n    sample_prediction, log_var = lvae_predict_single_sample(\n        model=model, likelihood_obj=likelihood_obj, input=x\n    )\n\n    log_var_output = (log_var, *aux) if log_var is not None else None\n    return (sample_prediction, *aux), log_var_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/lvae_tiling_manager/","title":"lvae_tiling_manager","text":"<p>Module contiaing tiling manager class.</p>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/","title":"prediction_outputs","text":"<p>Module containing functions to convert prediction outputs to desired form.</p>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs._combine_array_batches","title":"<code>_combine_array_batches(predictions)</code>","text":"<p>Combine batches of arrays.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>Prediction arrays that are output from <code>Trainer.predict</code>. A list of arrays that have dimensions (B, C, (Z), Y, X), where B is batch size.</p> required <p>Returns:</p> Type Description <code>list of numpy.ndarray</code> <p>A list of arrays with dimensions (1, C, (Z), Y, X).</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def _combine_array_batches(predictions: list[NDArray]) -&gt; list[NDArray]:\n    \"\"\"\n    Combine batches of arrays.\n\n    Parameters\n    ----------\n    predictions : list\n        Prediction arrays that are output from `Trainer.predict`. A list of arrays that\n        have dimensions (B, C, (Z), Y, X), where B is batch size.\n\n    Returns\n    -------\n    list of numpy.ndarray\n        A list of arrays with dimensions (1, C, (Z), Y, X).\n    \"\"\"\n    prediction_concat: NDArray = np.concatenate(predictions, axis=0)\n    prediction_split = np.split(prediction_concat, prediction_concat.shape[0], axis=0)\n    return prediction_split\n</code></pre>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs._combine_tiled_batches","title":"<code>_combine_tiled_batches(predictions)</code>","text":"<p>Combine batches from tiled output.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list of (numpy.ndarray, list of TileInformation)</code> <p>Predictions that are output from <code>Trainer.predict</code>. For tiled batches, this is a list of tuples. The first element of the tuples is the prediction output of tiles with dimension (B, C, (Z), Y, X), where B is batch size. The second element of the tuples is a list of TileInformation objects of length B.</p> required <p>Returns:</p> Type Description <code>tuple of (list of numpy.ndarray, list of TileInformation)</code> <p>Combined batches.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def _combine_tiled_batches(\n    predictions: list[tuple[NDArray, list[TileInformation]]]\n) -&gt; tuple[list[NDArray], list[TileInformation]]:\n    \"\"\"\n    Combine batches from tiled output.\n\n    Parameters\n    ----------\n    predictions : list of (numpy.ndarray, list of TileInformation)\n        Predictions that are output from `Trainer.predict`. For tiled batches, this is\n        a list of tuples. The first element of the tuples is the prediction output of\n        tiles with dimension (B, C, (Z), Y, X), where B is batch size. The second\n        element of the tuples is a list of TileInformation objects of length B.\n\n    Returns\n    -------\n    tuple of (list of numpy.ndarray, list of TileInformation)\n        Combined batches.\n    \"\"\"\n    # turn list of lists into single list\n    tile_infos = [\n        tile_info for _, tile_info_list in predictions for tile_info in tile_info_list\n    ]\n    prediction_tiles: list[NDArray] = _combine_array_batches(\n        [preds for preds, _ in predictions]\n    )\n    return prediction_tiles, tile_infos\n</code></pre>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.combine_batches","title":"<code>combine_batches(predictions, tiled)</code>","text":"<pre><code>combine_batches(predictions: list[Any], tiled: Literal[True]) -&gt; tuple[list[NDArray], list[TileInformation]]\n</code></pre><pre><code>combine_batches(predictions: list[Any], tiled: Literal[False]) -&gt; list[NDArray]\n</code></pre><pre><code>combine_batches(predictions: list[Any], tiled: Union[bool, Literal[True], Literal[False]]) -&gt; Union[list[NDArray], tuple[list[NDArray], list[TileInformation]]]\n</code></pre> <p>If predictions are in batches, they will be combined.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>Predictions that are output from <code>Trainer.predict</code>.</p> required <code>tiled</code> <code>bool</code> <p>Whether the predictions are tiled.</p> required <p>Returns:</p> Type Description <code>(list of numpy.ndarray) or tuple of (list of numpy.ndarray, list of TileInformation)</code> <p>Combined batches.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def combine_batches(\n    predictions: list[Any], tiled: bool\n) -&gt; Union[list[NDArray], tuple[list[NDArray], list[TileInformation]]]:\n    \"\"\"\n    If predictions are in batches, they will be combined.\n\n    Parameters\n    ----------\n    predictions : list\n        Predictions that are output from `Trainer.predict`.\n    tiled : bool\n        Whether the predictions are tiled.\n\n    Returns\n    -------\n    (list of numpy.ndarray) or tuple of (list of numpy.ndarray, list of TileInformation)\n        Combined batches.\n    \"\"\"\n    if tiled:\n        return _combine_tiled_batches(predictions)\n    else:\n        return _combine_array_batches(predictions)\n</code></pre>"},{"location":"reference/careamics/prediction_utils/prediction_outputs/#careamics.prediction_utils.prediction_outputs.convert_outputs","title":"<code>convert_outputs(predictions, tiled)</code>","text":"<p>Convert the Lightning trainer outputs to the desired form.</p> <p>This method allows stitching back together tiled predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>Predictions that are output from <code>Trainer.predict</code>.</p> required <code>tiled</code> <code>bool</code> <p>Whether the predictions are tiled.</p> required <p>Returns:</p> Type Description <code>list of numpy.ndarray or numpy.ndarray</code> <p>list of arrays with the axes SC(Z)YX. If there is only 1 output it will not be in a list.</p> Source code in <code>src/careamics/prediction_utils/prediction_outputs.py</code> <pre><code>def convert_outputs(predictions: list[Any], tiled: bool) -&gt; list[NDArray]:\n    \"\"\"\n    Convert the Lightning trainer outputs to the desired form.\n\n    This method allows stitching back together tiled predictions.\n\n    Parameters\n    ----------\n    predictions : list\n        Predictions that are output from `Trainer.predict`.\n    tiled : bool\n        Whether the predictions are tiled.\n\n    Returns\n    -------\n    list of numpy.ndarray or numpy.ndarray\n        list of arrays with the axes SC(Z)YX. If there is only 1 output it will not\n        be in a list.\n    \"\"\"\n    if len(predictions) == 0:\n        return predictions\n\n    # this layout is to stop mypy complaining\n    if tiled:\n        predictions_comb = combine_batches(predictions, tiled)\n        predictions_output = stitch_prediction(*predictions_comb)\n    else:\n        predictions_output = combine_batches(predictions, tiled)\n\n    return predictions_output\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/","title":"stitch_prediction","text":"<p>Prediction utility functions.</p>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.stitch_prediction","title":"<code>stitch_prediction(tiles, tile_infos)</code>","text":"<p>Stitch tiles back together to form a full image(s).</p> <p>Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a singleton dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list of numpy.ndarray</code> <p>Cropped tiles and their respective stitching coordinates. Can contain tiles from multiple images.</p> required <code>tile_infos</code> <code>list of TileInformation</code> <p>List of information and coordinates obtained from <code>dataset.tiled_patching.extract_tiles</code>.</p> required <p>Returns:</p> Type Description <code>list of numpy.ndarray</code> <p>Full image(s).</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>def stitch_prediction(\n    tiles: list[np.ndarray],\n    tile_infos: list[TileInformation],\n) -&gt; list[np.ndarray]:\n    \"\"\"\n    Stitch tiles back together to form a full image(s).\n\n    Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a\n    singleton dimension.\n\n    Parameters\n    ----------\n    tiles : list of numpy.ndarray\n        Cropped tiles and their respective stitching coordinates. Can contain tiles\n        from multiple images.\n    tile_infos : list of TileInformation\n        List of information and coordinates obtained from\n        `dataset.tiled_patching.extract_tiles`.\n\n    Returns\n    -------\n    list of numpy.ndarray\n        Full image(s).\n    \"\"\"\n    # Find where to split the lists so that only info from one image is contained.\n    # Do this by locating the last tiles of each image.\n    last_tiles = [tile_info.last_tile for tile_info in tile_infos]\n    last_tile_position = np.where(last_tiles)[0]\n    image_slices = [\n        slice(\n            None if i == 0 else last_tile_position[i - 1] + 1, last_tile_position[i] + 1\n        )\n        for i in range(len(last_tile_position))\n    ]\n    image_predictions = []\n    # slice the lists and apply stitch_prediction_single to each in turn.\n    for image_slice in image_slices:\n        image_predictions.append(\n            stitch_prediction_single(tiles[image_slice], tile_infos[image_slice])\n        )\n    return image_predictions\n</code></pre>"},{"location":"reference/careamics/prediction_utils/stitch_prediction/#careamics.prediction_utils.stitch_prediction.stitch_prediction_single","title":"<code>stitch_prediction_single(tiles, tile_infos)</code>","text":"<p>Stitch tiles back together to form a full image.</p> <p>Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a singleton dimension.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list of numpy.ndarray</code> <p>Cropped tiles and their respective stitching coordinates.</p> required <code>tile_infos</code> <code>list of TileInformation</code> <p>List of information and coordinates obtained from <code>dataset.tiled_patching.extract_tiles</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Full image, with dimensions SC(Z)YX.</p> Source code in <code>src/careamics/prediction_utils/stitch_prediction.py</code> <pre><code>def stitch_prediction_single(\n    tiles: list[NDArray],\n    tile_infos: list[TileInformation],\n) -&gt; NDArray:\n    \"\"\"\n    Stitch tiles back together to form a full image.\n\n    Tiles are of dimensions SC(Z)YX, where C is the number of channels and can be a\n    singleton dimension.\n\n    Parameters\n    ----------\n    tiles : list of numpy.ndarray\n        Cropped tiles and their respective stitching coordinates.\n    tile_infos : list of TileInformation\n        List of information and coordinates obtained from\n        `dataset.tiled_patching.extract_tiles`.\n\n    Returns\n    -------\n    numpy.ndarray\n        Full image, with dimensions SC(Z)YX.\n    \"\"\"\n    # TODO: this is hacky... need a better way to deal with when input channels and\n    #   target channels do not match\n    if len(tile_infos[0].array_shape) == 4:\n        # 4 dimensions =&gt; 3 spatial dimensions so -4 is channel dimension\n        tile_channels = tiles[0].shape[-4]\n    elif len(tile_infos[0].array_shape) == 3:\n        # 3 dimensions =&gt; 2 spatial dimensions so -3 is channel dimension\n        tile_channels = tiles[0].shape[-3]\n    else:\n        # Note pretty sure this is unreachable because array shape is already\n        #   validated by TileInformation\n        raise ValueError(\n            f\"Unsupported number of output dimension {len(tile_infos[0].array_shape)}\"\n        )\n    # retrieve whole array size, add S dim and use number of channels in tile\n    input_shape = (1, tile_channels, *tile_infos[0].array_shape[1:])\n    predicted_image = np.zeros(input_shape, dtype=np.float32)\n\n    for tile, tile_info in zip(tiles, tile_infos):\n\n        # Compute coordinates for cropping predicted tile\n        crop_slices: tuple[Union[builtins.ellipsis, slice], ...] = (\n            ...,\n            *[slice(c[0], c[1]) for c in tile_info.overlap_crop_coords],\n        )\n\n        # Crop predited tile according to overlap coordinates\n        cropped_tile = tile[crop_slices]\n\n        # Insert cropped tile into predicted image using stitch coordinates\n        image_slices = (..., *[slice(c[0], c[1]) for c in tile_info.stitch_coords])\n        predicted_image[image_slices] = cropped_tile.astype(np.float32)\n\n    return predicted_image\n</code></pre>"},{"location":"reference/careamics/transforms/compose/","title":"compose","text":"<p>A class chaining transforms together.</p>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose","title":"<code>Compose</code>","text":"<p>A class chaining transforms together.</p> <p>Parameters:</p> Name Type Description Default <code>transform_list</code> <code>list[TransformModel]</code> <p>A list of dictionaries where each dictionary contains the name of a transform and its parameters.</p> required <p>Attributes:</p> Name Type Description <code>_callable_transforms</code> <code>Callable</code> <p>A callable that applies the transforms to the input data.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>class Compose:\n    \"\"\"A class chaining transforms together.\n\n    Parameters\n    ----------\n    transform_list : list[TransformModel]\n        A list of dictionaries where each dictionary contains the name of a\n        transform and its parameters.\n\n    Attributes\n    ----------\n    _callable_transforms : Callable\n        A callable that applies the transforms to the input data.\n    \"\"\"\n\n    def __init__(self, transform_list: list[NORM_AND_SPATIAL_UNION]) -&gt; None:\n        \"\"\"Instantiate a Compose object.\n\n        Parameters\n        ----------\n        transform_list : list[NORM_AND_SPATIAL_UNION]\n            A list of dictionaries where each dictionary contains the name of a\n            transform and its parameters.\n        \"\"\"\n        # retrieve all available transforms\n        # TODO: correctly type hint get_all_transforms function output\n        all_transforms: dict[str, type[Transform]] = get_all_transforms()\n\n        # instantiate all transforms\n        self.transforms: list[Transform] = [\n            all_transforms[t.name](**t.model_dump()) for t in transform_list\n        ]\n\n    def _chain_transforms(\n        self, patch: NDArray, target: Optional[NDArray]\n    ) -&gt; tuple[Optional[NDArray], ...]:\n        \"\"\"Chain transforms on the input data.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Input data.\n        target : Optional[np.ndarray]\n            Target data, by default None.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            The output of the transformations.\n        \"\"\"\n        params: Union[tuple[NDArray, Optional[NDArray]],] = (patch, target)\n\n        for t in self.transforms:\n            *params, _ = t(*params)  # ignore additional_arrays dict\n\n        # avoid None values that create problems for collating\n        return tuple(p for p in params if p is not None)\n\n    def _chain_transforms_additional_arrays(\n        self,\n        patch: NDArray,\n        target: Optional[NDArray],\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n        \"\"\"Chain transforms on the input data, with additional arrays.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Input data.\n        target : Optional[np.ndarray]\n            Target data, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            The output of the transformations.\n        \"\"\"\n        params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n        for t in self.transforms:\n            patch, target, additional_arrays = t(**params)\n            params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n        return patch, target, additional_arrays\n\n    def __call__(\n        self, patch: NDArray, target: Optional[NDArray] = None\n    ) -&gt; tuple[NDArray, ...]:\n        \"\"\"Apply the transforms to the input data.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            The input data.\n        target : Optional[np.ndarray], optional\n            Target data, by default None.\n\n        Returns\n        -------\n        tuple[np.ndarray, ...]\n            The output of the transformations.\n        \"\"\"\n        # TODO: solve casting Compose.__call__ ouput\n        return cast(tuple[NDArray, ...], self._chain_transforms(patch, target))\n\n    def transform_with_additional_arrays(\n        self,\n        patch: NDArray,\n        target: Optional[NDArray] = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n        \"\"\"Apply the transforms to the input data, including additional arrays.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            The input data.\n        target : Optional[np.ndarray], optional\n            Target data, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        NDArray\n            The transformed patch.\n        NDArray | None\n            The transformed target.\n        dict of {str, NDArray}\n            Transformed additional arrays. Keys correspond to the keyword argument\n            names.\n        \"\"\"\n        return self._chain_transforms_additional_arrays(\n            patch, target, **additional_arrays\n        )\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.__call__","title":"<code>__call__(patch, target=None)</code>","text":"<p>Apply the transforms to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>The input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ...]</code> <p>The output of the transformations.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def __call__(\n    self, patch: NDArray, target: Optional[NDArray] = None\n) -&gt; tuple[NDArray, ...]:\n    \"\"\"Apply the transforms to the input data.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        The input data.\n    target : Optional[np.ndarray], optional\n        Target data, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray, ...]\n        The output of the transformations.\n    \"\"\"\n    # TODO: solve casting Compose.__call__ ouput\n    return cast(tuple[NDArray, ...], self._chain_transforms(patch, target))\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.__init__","title":"<code>__init__(transform_list)</code>","text":"<p>Instantiate a Compose object.</p> <p>Parameters:</p> Name Type Description Default <code>transform_list</code> <code>list[NORM_AND_SPATIAL_UNION]</code> <p>A list of dictionaries where each dictionary contains the name of a transform and its parameters.</p> required Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def __init__(self, transform_list: list[NORM_AND_SPATIAL_UNION]) -&gt; None:\n    \"\"\"Instantiate a Compose object.\n\n    Parameters\n    ----------\n    transform_list : list[NORM_AND_SPATIAL_UNION]\n        A list of dictionaries where each dictionary contains the name of a\n        transform and its parameters.\n    \"\"\"\n    # retrieve all available transforms\n    # TODO: correctly type hint get_all_transforms function output\n    all_transforms: dict[str, type[Transform]] = get_all_transforms()\n\n    # instantiate all transforms\n    self.transforms: list[Transform] = [\n        all_transforms[t.name](**t.model_dump()) for t in transform_list\n    ]\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose._chain_transforms","title":"<code>_chain_transforms(patch, target)</code>","text":"<p>Chain transforms on the input data.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>The output of the transformations.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def _chain_transforms(\n    self, patch: NDArray, target: Optional[NDArray]\n) -&gt; tuple[Optional[NDArray], ...]:\n    \"\"\"Chain transforms on the input data.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Input data.\n    target : Optional[np.ndarray]\n        Target data, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        The output of the transformations.\n    \"\"\"\n    params: Union[tuple[NDArray, Optional[NDArray]],] = (patch, target)\n\n    for t in self.transforms:\n        *params, _ = t(*params)  # ignore additional_arrays dict\n\n    # avoid None values that create problems for collating\n    return tuple(p for p in params if p is not None)\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose._chain_transforms_additional_arrays","title":"<code>_chain_transforms_additional_arrays(patch, target, **additional_arrays)</code>","text":"<p>Chain transforms on the input data, with additional arrays.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> required <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>The output of the transformations.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def _chain_transforms_additional_arrays(\n    self,\n    patch: NDArray,\n    target: Optional[NDArray],\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n    \"\"\"Chain transforms on the input data, with additional arrays.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Input data.\n    target : Optional[np.ndarray]\n        Target data, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        The output of the transformations.\n    \"\"\"\n    params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n    for t in self.transforms:\n        patch, target, additional_arrays = t(**params)\n        params = {\"patch\": patch, \"target\": target, **additional_arrays}\n\n    return patch, target, additional_arrays\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.Compose.transform_with_additional_arrays","title":"<code>transform_with_additional_arrays(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transforms to the input data, including additional arrays.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>The input data.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target data, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>NDArray</code> <p>The transformed patch.</p> <code>NDArray | None</code> <p>The transformed target.</p> <code>dict of {str, NDArray}</code> <p>Transformed additional arrays. Keys correspond to the keyword argument names.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def transform_with_additional_arrays(\n    self,\n    patch: NDArray,\n    target: Optional[NDArray] = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n    \"\"\"Apply the transforms to the input data, including additional arrays.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        The input data.\n    target : Optional[np.ndarray], optional\n        Target data, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    NDArray\n        The transformed patch.\n    NDArray | None\n        The transformed target.\n    dict of {str, NDArray}\n        Transformed additional arrays. Keys correspond to the keyword argument\n        names.\n    \"\"\"\n    return self._chain_transforms_additional_arrays(\n        patch, target, **additional_arrays\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/compose/#careamics.transforms.compose.get_all_transforms","title":"<code>get_all_transforms()</code>","text":"<p>Return all the transforms accepted by CAREamics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with all the transforms accepted by CAREamics, where the keys are the transform names and the values are the transform classes.</p> Source code in <code>src/careamics/transforms/compose.py</code> <pre><code>def get_all_transforms() -&gt; dict[str, type]:\n    \"\"\"Return all the transforms accepted by CAREamics.\n\n    Returns\n    -------\n    dict\n        A dictionary with all the transforms accepted by CAREamics, where the keys are\n        the transform names and the values are the transform classes.\n    \"\"\"\n    return ALL_TRANSFORMS\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/","title":"n2v_manipulate","text":"<p>N2V manipulation transform.</p>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate","title":"<code>N2VManipulate</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Default augmentation for the N2V model.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <code>int</code> <p>Size of the replacement area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask, by default 0.2.</p> <code>0.2</code> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median, by default uniform.</p> <code>value</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch, by default True.</p> <code>True</code> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>StructN2V mask axis, by default \"none\".</p> <code>'none'</code> <code>struct_mask_span</code> <code>int</code> <p>StructN2V mask span, by default 5.</p> <code>5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask.</p> <code>roi_size</code> <code>int</code> <p>Size of the replacement area.</p> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median.</p> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch.</p> <code>struct_mask</code> <code>Optional[StructMaskParameters]</code> <p>StructN2V mask parameters.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>class N2VManipulate(Transform):\n    \"\"\"\n    Default augmentation for the N2V model.\n\n    This transform expects C(Z)YX dimensions.\n\n    Parameters\n    ----------\n    roi_size : int, optional\n        Size of the replacement area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels to mask, by default 0.2.\n    strategy : Literal[ \"uniform\", \"median\" ], optional\n        Replaccement strategy, uniform or median, by default uniform.\n    remove_center : bool, optional\n        Whether to remove central pixel from patch, by default True.\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        StructN2V mask axis, by default \"none\".\n    struct_mask_span : int, optional\n        StructN2V mask span, by default 5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n\n    Attributes\n    ----------\n    masked_pixel_percentage : float\n        Percentage of pixels to mask.\n    roi_size : int\n        Size of the replacement area.\n    strategy : Literal[ \"uniform\", \"median\" ]\n        Replaccement strategy, uniform or median.\n    remove_center : bool\n        Whether to remove central pixel from patch.\n    struct_mask : Optional[StructMaskParameters]\n        StructN2V mask parameters.\n    rng : Generator\n        Random number generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        roi_size: int = 11,\n        masked_pixel_percentage: float = 0.2,\n        strategy: Literal[\n            \"uniform\", \"median\"\n        ] = SupportedPixelManipulation.UNIFORM.value,\n        remove_center: bool = True,\n        struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n        struct_mask_span: int = 5,\n        seed: Optional[int] = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        roi_size : int, optional\n            Size of the replacement area, by default 11.\n        masked_pixel_percentage : float, optional\n            Percentage of pixels to mask, by default 0.2.\n        strategy : Literal[ \"uniform\", \"median\" ], optional\n            Replaccement strategy, uniform or median, by default uniform.\n        remove_center : bool, optional\n            Whether to remove central pixel from patch, by default True.\n        struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n            StructN2V mask axis, by default \"none\".\n        struct_mask_span : int, optional\n            StructN2V mask span, by default 5.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        \"\"\"\n        self.masked_pixel_percentage = masked_pixel_percentage\n        self.roi_size = roi_size\n        self.strategy = strategy\n        self.remove_center = remove_center  # TODO is this ever used?\n\n        if struct_mask_axis == SupportedStructAxis.NONE:\n            self.struct_mask: Optional[StructMaskParameters] = None\n        else:\n            self.struct_mask = StructMaskParameters(\n                axis=0 if struct_mask_axis == SupportedStructAxis.HORIZONTAL else 1,\n                span=struct_mask_span,\n            )\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self, patch: NDArray, *args: Any, **kwargs: Any\n    ) -&gt; tuple[NDArray, NDArray, NDArray]:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        *args : Any\n            Additional arguments, unused.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Returns\n        -------\n        tuple[np.ndarray, np.ndarray, np.ndarray]\n            Masked patch, original patch, and mask.\n        \"\"\"\n        masked = np.zeros_like(patch)\n        mask = np.zeros_like(patch)\n        if self.strategy == SupportedPixelManipulation.UNIFORM:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(patch.shape[0]):\n                masked[c, ...], mask[c, ...] = uniform_manipulate(\n                    patch=patch[c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    remove_center=self.remove_center,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        elif self.strategy == SupportedPixelManipulation.MEDIAN:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(patch.shape[0]):\n                masked[c, ...], mask[c, ...] = median_manipulate(\n                    patch=patch[c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        else:\n            raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n        # TODO: Output does not match other transforms, how to resolve?\n        #     - Don't include in Compose and apply after if algorithm is N2V?\n        #     - or just don't return patch? but then mask is in the target position\n        # TODO why return patch?\n        return masked, patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate.__call__","title":"<code>__call__(patch, *args, **kwargs)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape C(Z)YX.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments, unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray]</code> <p>Masked patch, original patch, and mask.</p> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>def __call__(\n    self, patch: NDArray, *args: Any, **kwargs: Any\n) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape C(Z)YX.\n    *args : Any\n        Additional arguments, unused.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Returns\n    -------\n    tuple[np.ndarray, np.ndarray, np.ndarray]\n        Masked patch, original patch, and mask.\n    \"\"\"\n    masked = np.zeros_like(patch)\n    mask = np.zeros_like(patch)\n    if self.strategy == SupportedPixelManipulation.UNIFORM:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(patch.shape[0]):\n            masked[c, ...], mask[c, ...] = uniform_manipulate(\n                patch=patch[c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                remove_center=self.remove_center,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    elif self.strategy == SupportedPixelManipulation.MEDIAN:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(patch.shape[0]):\n            masked[c, ...], mask[c, ...] = median_manipulate(\n                patch=patch[c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    else:\n        raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n    # TODO: Output does not match other transforms, how to resolve?\n    #     - Don't include in Compose and apply after if algorithm is N2V?\n    #     - or just don't return patch? but then mask is in the target position\n    # TODO why return patch?\n    return masked, patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate/#careamics.transforms.n2v_manipulate.N2VManipulate.__init__","title":"<code>__init__(roi_size=11, masked_pixel_percentage=0.2, strategy=SupportedPixelManipulation.UNIFORM.value, remove_center=True, struct_mask_axis='none', struct_mask_span=5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>roi_size</code> <code>int</code> <p>Size of the replacement area, by default 11.</p> <code>11</code> <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask, by default 0.2.</p> <code>0.2</code> <code>strategy</code> <code>Literal['uniform', 'median']</code> <p>Replaccement strategy, uniform or median, by default uniform.</p> <code>value</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch, by default True.</p> <code>True</code> <code>struct_mask_axis</code> <code>Literal['horizontal', 'vertical', 'none']</code> <p>StructN2V mask axis, by default \"none\".</p> <code>'none'</code> <code>struct_mask_span</code> <code>int</code> <p>StructN2V mask span, by default 5.</p> <code>5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/n2v_manipulate.py</code> <pre><code>def __init__(\n    self,\n    roi_size: int = 11,\n    masked_pixel_percentage: float = 0.2,\n    strategy: Literal[\n        \"uniform\", \"median\"\n    ] = SupportedPixelManipulation.UNIFORM.value,\n    remove_center: bool = True,\n    struct_mask_axis: Literal[\"horizontal\", \"vertical\", \"none\"] = \"none\",\n    struct_mask_span: int = 5,\n    seed: Optional[int] = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    roi_size : int, optional\n        Size of the replacement area, by default 11.\n    masked_pixel_percentage : float, optional\n        Percentage of pixels to mask, by default 0.2.\n    strategy : Literal[ \"uniform\", \"median\" ], optional\n        Replaccement strategy, uniform or median, by default uniform.\n    remove_center : bool, optional\n        Whether to remove central pixel from patch, by default True.\n    struct_mask_axis : Literal[\"horizontal\", \"vertical\", \"none\"], optional\n        StructN2V mask axis, by default \"none\".\n    struct_mask_span : int, optional\n        StructN2V mask span, by default 5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n    self.masked_pixel_percentage = masked_pixel_percentage\n    self.roi_size = roi_size\n    self.strategy = strategy\n    self.remove_center = remove_center  # TODO is this ever used?\n\n    if struct_mask_axis == SupportedStructAxis.NONE:\n        self.struct_mask: Optional[StructMaskParameters] = None\n    else:\n        self.struct_mask = StructMaskParameters(\n            axis=0 if struct_mask_axis == SupportedStructAxis.HORIZONTAL else 1,\n            span=struct_mask_span,\n        )\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/","title":"n2v_manipulate_torch","text":"<p>N2V manipulation transform for PyTorch.</p>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch","title":"<code>N2VManipulateTorch</code>","text":"<p>Default augmentation for the N2V model.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>n2v_manipulate_config</code> <code>N2VManipulateConfig</code> <p>N2V manipulation configuration.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>masked_pixel_percentage</code> <code>float</code> <p>Percentage of pixels to mask.</p> <code>roi_size</code> <code>int</code> <p>Size of the replacement area.</p> <code>strategy</code> <code>Literal[uniform, median]</code> <p>Replacement strategy, uniform or median.</p> <code>remove_center</code> <code>bool</code> <p>Whether to remove central pixel from patch.</p> <code>struct_mask</code> <code>Optional[StructMaskParameters]</code> <p>StructN2V mask parameters.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>class N2VManipulateTorch:\n    \"\"\"\n    Default augmentation for the N2V model.\n\n    This transform expects C(Z)YX dimensions.\n\n    Parameters\n    ----------\n    n2v_manipulate_config : N2VManipulateConfig\n        N2V manipulation configuration.\n    seed : Optional[int], optional\n        Random seed, by default None.\n\n    Attributes\n    ----------\n    masked_pixel_percentage : float\n        Percentage of pixels to mask.\n    roi_size : int\n        Size of the replacement area.\n    strategy : Literal[ \"uniform\", \"median\" ]\n        Replacement strategy, uniform or median.\n    remove_center : bool\n        Whether to remove central pixel from patch.\n    struct_mask : Optional[StructMaskParameters]\n        StructN2V mask parameters.\n    rng : Generator\n        Random number generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        n2v_manipulate_config: N2VManipulateModel,\n        seed: Optional[int] = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        n2v_manipulate_config : N2VManipulateModel\n            N2V manipulation configuration.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        \"\"\"\n        self.masked_pixel_percentage = n2v_manipulate_config.masked_pixel_percentage\n        self.roi_size = n2v_manipulate_config.roi_size\n        self.strategy = n2v_manipulate_config.strategy\n        self.remove_center = n2v_manipulate_config.remove_center\n\n        if n2v_manipulate_config.struct_mask_axis == SupportedStructAxis.NONE:\n            self.struct_mask: Optional[StructMaskParameters] = None\n        else:\n            self.struct_mask = StructMaskParameters(\n                axis=(\n                    0\n                    if n2v_manipulate_config.struct_mask_axis\n                    == SupportedStructAxis.HORIZONTAL\n                    else 1\n                ),\n                span=n2v_manipulate_config.struct_mask_span,\n            )\n\n        # PyTorch random generator\n        # TODO refactor into careamics.utils.torch_utils.get_device\n        if torch.cuda.is_available():\n            device = \"cuda\"\n        elif torch.backends.mps.is_available() and platform.processor() in (\n            \"arm\",\n            \"arm64\",\n        ):\n            device = \"mps\"\n        else:\n            device = \"cpu\"\n\n        self.rng = (\n            torch.Generator(device=device).manual_seed(seed)\n            if seed is not None\n            else torch.Generator(device=device)\n        )\n\n    def __call__(\n        self, batch: torch.Tensor, *args: Any, **kwargs: Any\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        batch : torch.Tensor\n            Batch if image patches, 2D or 3D, shape BC(Z)YX.\n        *args : Any\n            Additional arguments, unused.\n        **kwargs : Any\n            Additional keyword arguments, unused.\n\n        Returns\n        -------\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n            Masked patch, original patch, and mask.\n        \"\"\"\n        masked = torch.zeros_like(batch)\n        mask = torch.zeros_like(batch, dtype=torch.uint8)\n\n        if self.strategy == SupportedPixelManipulation.UNIFORM:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(batch.shape[1]):\n                masked[:, c, ...], mask[:, c, ...] = uniform_manipulate_torch(\n                    patch=batch[:, c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    remove_center=self.remove_center,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        elif self.strategy == SupportedPixelManipulation.MEDIAN:\n            # Iterate over the channels to apply manipulation separately\n            for c in range(batch.shape[1]):\n                masked[:, c, ...], mask[:, c, ...] = median_manipulate_torch(\n                    batch=batch[:, c, ...],\n                    mask_pixel_percentage=self.masked_pixel_percentage,\n                    subpatch_size=self.roi_size,\n                    struct_params=self.struct_mask,\n                    rng=self.rng,\n                )\n        else:\n            raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n        return masked, batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch.__call__","title":"<code>__call__(batch, *args, **kwargs)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Batch if image patches, 2D or 3D, shape BC(Z)YX.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments, unused.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, unused.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>Masked patch, original patch, and mask.</p> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>def __call__(\n    self, batch: torch.Tensor, *args: Any, **kwargs: Any\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Batch if image patches, 2D or 3D, shape BC(Z)YX.\n    *args : Any\n        Additional arguments, unused.\n    **kwargs : Any\n        Additional keyword arguments, unused.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n        Masked patch, original patch, and mask.\n    \"\"\"\n    masked = torch.zeros_like(batch)\n    mask = torch.zeros_like(batch, dtype=torch.uint8)\n\n    if self.strategy == SupportedPixelManipulation.UNIFORM:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(batch.shape[1]):\n            masked[:, c, ...], mask[:, c, ...] = uniform_manipulate_torch(\n                patch=batch[:, c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                remove_center=self.remove_center,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    elif self.strategy == SupportedPixelManipulation.MEDIAN:\n        # Iterate over the channels to apply manipulation separately\n        for c in range(batch.shape[1]):\n            masked[:, c, ...], mask[:, c, ...] = median_manipulate_torch(\n                batch=batch[:, c, ...],\n                mask_pixel_percentage=self.masked_pixel_percentage,\n                subpatch_size=self.roi_size,\n                struct_params=self.struct_mask,\n                rng=self.rng,\n            )\n    else:\n        raise ValueError(f\"Unknown masking strategy ({self.strategy}).\")\n\n    return masked, batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/n2v_manipulate_torch/#careamics.transforms.n2v_manipulate_torch.N2VManipulateTorch.__init__","title":"<code>__init__(n2v_manipulate_config, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>n2v_manipulate_config</code> <code>N2VManipulateModel</code> <p>N2V manipulation configuration.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/n2v_manipulate_torch.py</code> <pre><code>def __init__(\n    self,\n    n2v_manipulate_config: N2VManipulateModel,\n    seed: Optional[int] = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    n2v_manipulate_config : N2VManipulateModel\n        N2V manipulation configuration.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n    self.masked_pixel_percentage = n2v_manipulate_config.masked_pixel_percentage\n    self.roi_size = n2v_manipulate_config.roi_size\n    self.strategy = n2v_manipulate_config.strategy\n    self.remove_center = n2v_manipulate_config.remove_center\n\n    if n2v_manipulate_config.struct_mask_axis == SupportedStructAxis.NONE:\n        self.struct_mask: Optional[StructMaskParameters] = None\n    else:\n        self.struct_mask = StructMaskParameters(\n            axis=(\n                0\n                if n2v_manipulate_config.struct_mask_axis\n                == SupportedStructAxis.HORIZONTAL\n                else 1\n            ),\n            span=n2v_manipulate_config.struct_mask_span,\n        )\n\n    # PyTorch random generator\n    # TODO refactor into careamics.utils.torch_utils.get_device\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif torch.backends.mps.is_available() and platform.processor() in (\n        \"arm\",\n        \"arm64\",\n    ):\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    self.rng = (\n        torch.Generator(device=device).manual_seed(seed)\n        if seed is not None\n        else torch.Generator(device=device)\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/","title":"normalize","text":"<p>Normalization and denormalization transforms for image patches.</p>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize","title":"<code>Denormalize</code>","text":"<p>Denormalize an image.</p> <p>Denormalization is performed expecting a zero mean and unit variance input. This transform expects C(Z)YX dimensions.</p> <p>Note that an epsilon value of 1e-6 is added to the standard deviation to avoid division by zero during the normalization step, which is taken into account during denormalization.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list or tuple of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list or tuple of float</code> <p>Standard deviation value per channel.</p> required Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>class Denormalize:\n    \"\"\"\n    Denormalize an image.\n\n    Denormalization is performed expecting a zero mean and unit variance input. This\n    transform expects C(Z)YX dimensions.\n\n    Note that an epsilon value of 1e-6 is added to the standard deviation to avoid\n    division by zero during the normalization step, which is taken into account during\n    denormalization.\n\n    Parameters\n    ----------\n    image_means : list or tuple of float\n        Mean value per channel.\n    image_stds : list or tuple of float\n        Standard deviation value per channel.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        image_means: list[float],\n        image_stds: list[float],\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        image_means : list of float\n            Mean value per channel.\n        image_stds : list of float\n            Standard deviation value per channel.\n        \"\"\"\n        self.image_means = image_means\n        self.image_stds = image_stds\n\n        self.eps = 1e-6\n\n    def __call__(self, patch: NDArray) -&gt; NDArray:\n        \"\"\"Reverse the normalization operation for a batch of patches.\n\n        Parameters\n        ----------\n        patch : NDArray\n            Patch, 2D or 3D, shape BC(Z)YX.\n\n        Returns\n        -------\n        NDArray\n            Transformed array.\n        \"\"\"\n        if len(self.image_means) != patch.shape[1]:\n            raise ValueError(\n                f\"Number of means (got a list of size {len(self.image_means)}) and \"\n                f\"number of channels (got shape {patch.shape} for BC(Z)YX) do not \"\n                f\"match.\"\n            )\n\n        means = _reshape_stats(self.image_means, patch.ndim)\n        stds = _reshape_stats(self.image_stds, patch.ndim)\n\n        denorm_array = self._apply(\n            patch,\n            np.swapaxes(means, 0, 1),  # swap axes as C channel is axis 1\n            np.swapaxes(stds, 0, 1),\n        )\n\n        return denorm_array.astype(np.float32)\n\n    def _apply(self, array: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n        \"\"\"\n        Apply the transform to the image.\n\n        Parameters\n        ----------\n        array : NDArray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        mean : NDArray\n            Mean values.\n        std : NDArray\n            Standard deviations.\n\n        Returns\n        -------\n        NDArray\n            Denormalized image array.\n        \"\"\"\n        return array * (std + self.eps) + mean\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize.__call__","title":"<code>__call__(patch)</code>","text":"<p>Reverse the normalization operation for a batch of patches.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>NDArray</code> <p>Patch, 2D or 3D, shape BC(Z)YX.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Transformed array.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __call__(self, patch: NDArray) -&gt; NDArray:\n    \"\"\"Reverse the normalization operation for a batch of patches.\n\n    Parameters\n    ----------\n    patch : NDArray\n        Patch, 2D or 3D, shape BC(Z)YX.\n\n    Returns\n    -------\n    NDArray\n        Transformed array.\n    \"\"\"\n    if len(self.image_means) != patch.shape[1]:\n        raise ValueError(\n            f\"Number of means (got a list of size {len(self.image_means)}) and \"\n            f\"number of channels (got shape {patch.shape} for BC(Z)YX) do not \"\n            f\"match.\"\n        )\n\n    means = _reshape_stats(self.image_means, patch.ndim)\n    stds = _reshape_stats(self.image_stds, patch.ndim)\n\n    denorm_array = self._apply(\n        patch,\n        np.swapaxes(means, 0, 1),  # swap axes as C channel is axis 1\n        np.swapaxes(stds, 0, 1),\n    )\n\n    return denorm_array.astype(np.float32)\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize.__init__","title":"<code>__init__(image_means, image_stds)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __init__(\n    self,\n    image_means: list[float],\n    image_stds: list[float],\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    \"\"\"\n    self.image_means = image_means\n    self.image_stds = image_stds\n\n    self.eps = 1e-6\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Denormalize._apply","title":"<code>_apply(array, mean, std)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>NDArray</code> <p>Image patch, 2D or 3D, shape C(Z)YX.</p> required <code>mean</code> <code>NDArray</code> <p>Mean values.</p> required <code>std</code> <code>NDArray</code> <p>Standard deviations.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Denormalized image array.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def _apply(self, array: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n    \"\"\"\n    Apply the transform to the image.\n\n    Parameters\n    ----------\n    array : NDArray\n        Image patch, 2D or 3D, shape C(Z)YX.\n    mean : NDArray\n        Mean values.\n    std : NDArray\n        Standard deviations.\n\n    Returns\n    -------\n    NDArray\n        Denormalized image array.\n    \"\"\"\n    return array * (std + self.eps) + mean\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize","title":"<code>Normalize</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Normalize an image or image patch.</p> <p>Normalization is a zero mean and unit variance. This transform expects C(Z)YX dimensions.</p> <p>Not that an epsilon value of 1e-6 is added to the standard deviation to avoid division by zero and that it returns a float32 image.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required <code>target_means</code> <code>list of float</code> <p>Target mean value per channel, by default None.</p> <code>None</code> <code>target_stds</code> <code>list of float</code> <p>Target standard deviation value per channel, by default None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> <code>target_means</code> <code>list of float, optional</code> <p>Target mean value per channel, by default None.</p> <code>target_stds</code> <code>list of float, optional</code> <p>Target standard deviation value per channel, by default None.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>class Normalize(Transform):\n    \"\"\"\n    Normalize an image or image patch.\n\n    Normalization is a zero mean and unit variance. This transform expects C(Z)YX\n    dimensions.\n\n    Not that an epsilon value of 1e-6 is added to the standard deviation to avoid\n    division by zero and that it returns a float32 image.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means : list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n\n    Attributes\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means :list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_means: list[float],\n        image_stds: list[float],\n        target_means: Optional[list[float]] = None,\n        target_stds: Optional[list[float]] = None,\n    ):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        image_means : list of float\n            Mean value per channel.\n        image_stds : list of float\n            Standard deviation value per channel.\n        target_means : list of float, optional\n            Target mean value per channel, by default None.\n        target_stds : list of float, optional\n            Target standard deviation value per channel, by default None.\n        \"\"\"\n        self.image_means = image_means\n        self.image_stds = image_stds\n        self.target_means = target_means\n        self.target_stds = target_stds\n\n        self.eps = 1e-6\n\n    def __call__(\n        self,\n        patch: np.ndarray,\n        target: Optional[NDArray] = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : NDArray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : NDArray, optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple of NDArray\n            Transformed patch and target, the target can be returned as `None`.\n        \"\"\"\n        if len(self.image_means) != patch.shape[0]:\n            raise ValueError(\n                f\"Number of means (got a list of size {len(self.image_means)}) and \"\n                f\"number of channels (got shape {patch.shape} for C(Z)YX) do not match.\"\n            )\n        if len(additional_arrays) != 0:\n            raise NotImplementedError(\n                \"Transforming additional arrays is currently not supported for \"\n                \"`Normalize`.\"\n            )\n\n        # reshape mean and std and apply the normalization to the patch\n        means = _reshape_stats(self.image_means, patch.ndim)\n        stds = _reshape_stats(self.image_stds, patch.ndim)\n        norm_patch = self._apply(patch, means, stds)\n\n        # same for the target patch\n        if (\n            target is not None\n            and self.target_means is not None\n            and self.target_stds is not None\n        ):\n            target_means = _reshape_stats(self.target_means, target.ndim)\n            target_stds = _reshape_stats(self.target_stds, target.ndim)\n            norm_target = self._apply(target, target_means, target_stds)\n        else:\n            norm_target = None\n\n        return norm_patch, norm_target, additional_arrays\n\n    def _apply(self, patch: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n        \"\"\"\n        Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : NDArray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        mean : NDArray\n            Mean values.\n        std : NDArray\n            Standard deviations.\n\n        Returns\n        -------\n        NDArray\n            Normalized image patch.\n        \"\"\"\n        return ((patch - mean) / (std + self.eps)).astype(np.float32)\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>NDArray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>NDArray</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple of NDArray</code> <p>Transformed patch and target, the target can be returned as <code>None</code>.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __call__(\n    self,\n    patch: np.ndarray,\n    target: Optional[NDArray] = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : NDArray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : NDArray, optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    tuple of NDArray\n        Transformed patch and target, the target can be returned as `None`.\n    \"\"\"\n    if len(self.image_means) != patch.shape[0]:\n        raise ValueError(\n            f\"Number of means (got a list of size {len(self.image_means)}) and \"\n            f\"number of channels (got shape {patch.shape} for C(Z)YX) do not match.\"\n        )\n    if len(additional_arrays) != 0:\n        raise NotImplementedError(\n            \"Transforming additional arrays is currently not supported for \"\n            \"`Normalize`.\"\n        )\n\n    # reshape mean and std and apply the normalization to the patch\n    means = _reshape_stats(self.image_means, patch.ndim)\n    stds = _reshape_stats(self.image_stds, patch.ndim)\n    norm_patch = self._apply(patch, means, stds)\n\n    # same for the target patch\n    if (\n        target is not None\n        and self.target_means is not None\n        and self.target_stds is not None\n    ):\n        target_means = _reshape_stats(self.target_means, target.ndim)\n        target_stds = _reshape_stats(self.target_stds, target.ndim)\n        norm_target = self._apply(target, target_means, target_stds)\n    else:\n        norm_target = None\n\n    return norm_patch, norm_target, additional_arrays\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize.__init__","title":"<code>__init__(image_means, image_stds, target_means=None, target_stds=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>image_means</code> <code>list of float</code> <p>Mean value per channel.</p> required <code>image_stds</code> <code>list of float</code> <p>Standard deviation value per channel.</p> required <code>target_means</code> <code>list of float</code> <p>Target mean value per channel, by default None.</p> <code>None</code> <code>target_stds</code> <code>list of float</code> <p>Target standard deviation value per channel, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def __init__(\n    self,\n    image_means: list[float],\n    image_stds: list[float],\n    target_means: Optional[list[float]] = None,\n    target_stds: Optional[list[float]] = None,\n):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    image_means : list of float\n        Mean value per channel.\n    image_stds : list of float\n        Standard deviation value per channel.\n    target_means : list of float, optional\n        Target mean value per channel, by default None.\n    target_stds : list of float, optional\n        Target standard deviation value per channel, by default None.\n    \"\"\"\n    self.image_means = image_means\n    self.image_stds = image_stds\n    self.target_means = target_means\n    self.target_stds = target_stds\n\n    self.eps = 1e-6\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize.Normalize._apply","title":"<code>_apply(patch, mean, std)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>NDArray</code> <p>Image patch, 2D or 3D, shape C(Z)YX.</p> required <code>mean</code> <code>NDArray</code> <p>Mean values.</p> required <code>std</code> <code>NDArray</code> <p>Standard deviations.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Normalized image patch.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def _apply(self, patch: NDArray, mean: NDArray, std: NDArray) -&gt; NDArray:\n    \"\"\"\n    Apply the transform to the image.\n\n    Parameters\n    ----------\n    patch : NDArray\n        Image patch, 2D or 3D, shape C(Z)YX.\n    mean : NDArray\n        Mean values.\n    std : NDArray\n        Standard deviations.\n\n    Returns\n    -------\n    NDArray\n        Normalized image patch.\n    \"\"\"\n    return ((patch - mean) / (std + self.eps)).astype(np.float32)\n</code></pre>"},{"location":"reference/careamics/transforms/normalize/#careamics.transforms.normalize._reshape_stats","title":"<code>_reshape_stats(stats, ndim)</code>","text":"<p>Reshape stats to match the number of dimensions of the input image.</p> <p>This allows to broadcast the stats (mean or std) to the image dimensions, and thus directly perform a vectorial calculation.</p> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>list of float</code> <p>List of stats, mean or standard deviation.</p> required <code>ndim</code> <code>int</code> <p>Number of dimensions of the image, including the C channel.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>Reshaped stats.</p> Source code in <code>src/careamics/transforms/normalize.py</code> <pre><code>def _reshape_stats(stats: list[float], ndim: int) -&gt; NDArray:\n    \"\"\"Reshape stats to match the number of dimensions of the input image.\n\n    This allows to broadcast the stats (mean or std) to the image dimensions, and\n    thus directly perform a vectorial calculation.\n\n    Parameters\n    ----------\n    stats : list of float\n        List of stats, mean or standard deviation.\n    ndim : int\n        Number of dimensions of the image, including the C channel.\n\n    Returns\n    -------\n    NDArray\n        Reshaped stats.\n    \"\"\"\n    return np.array(stats)[(..., *[np.newaxis] * (ndim - 1))]\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/","title":"pixel_manipulation","text":"<p>Pixel manipulation methods.</p> <p>Pixel manipulation is used in N2V and similar algorithm to replace the value of masked pixels.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation._apply_struct_mask","title":"<code>_apply_struct_mask(patch, coords, struct_params, rng=None)</code>","text":"<p>Apply structN2V masks to patch.</p> <p>Each point in <code>coords</code> corresponds to the center of a mask, masks are paremeterized by <code>struct_params</code> and pixels in the mask (with respect to <code>coords</code>) are replaced by a random value.</p> <p>Note that the structN2V mask is applied in 2D at the coordinates given by <code>coords</code>.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Patch to be manipulated, 2D or 3D.</p> required <code>coords</code> <code>ndarray</code> <p>Coordinates of the ROI(subpatch) centers.</p> required <code>struct_params</code> <code>StructMaskParameters</code> <p>Parameters for the structN2V mask (axis and span).</p> required <code>rng</code> <code>Generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Patch with the structN2V mask applied.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def _apply_struct_mask(\n    patch: np.ndarray,\n    coords: np.ndarray,\n    struct_params: StructMaskParameters,\n    rng: Optional[np.random.Generator] = None,\n) -&gt; np.ndarray:\n    \"\"\"Apply structN2V masks to patch.\n\n    Each point in `coords` corresponds to the center of a mask, masks are paremeterized\n    by `struct_params` and pixels in the mask (with respect to `coords`) are replaced by\n    a random value.\n\n    Note that the structN2V mask is applied in 2D at the coordinates given by `coords`.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Patch to be manipulated, 2D or 3D.\n    coords : np.ndarray\n        Coordinates of the ROI(subpatch) centers.\n    struct_params : StructMaskParameters\n        Parameters for the structN2V mask (axis and span).\n    rng : np.random.Generator or None\n        Random number generator.\n\n    Returns\n    -------\n    np.ndarray\n        Patch with the structN2V mask applied.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    # relative axis\n    moving_axis = -1 - struct_params.axis\n\n    # Create a mask array\n    mask = np.expand_dims(\n        np.ones(struct_params.span), axis=list(range(len(patch.shape) - 1))\n    )  # (1, 1, span) or (1, span)\n\n    # Move the moving axis to the correct position\n    # i.e. the axis along which the coordinates should change\n    mask = np.moveaxis(mask, -1, moving_axis)\n    center = np.array(mask.shape) // 2\n\n    # Mark the center\n    mask[tuple(center.T)] = 0\n\n    # displacements from center\n    dx = np.indices(mask.shape)[:, mask == 1] - center[:, None]\n\n    # combine all coords (ndim, npts,) with all displacements (ncoords,ndim,)\n    mix = dx.T[..., None] + coords.T[None]\n    mix = mix.transpose([1, 0, 2]).reshape([mask.ndim, -1]).T\n\n    # delete entries that are out of bounds\n    mix = np.delete(mix, mix[:, moving_axis] &lt; 0, axis=0)\n\n    max_bound = patch.shape[moving_axis] - 1\n    mix = np.delete(mix, mix[:, moving_axis] &gt; max_bound, axis=0)\n\n    # replace neighbouring pixels with random values from flat dist\n    patch[tuple(mix.T)] = rng.uniform(patch.min(), patch.max(), size=mix.shape[0])\n\n    return patch\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation._create_subpatch_center_mask","title":"<code>_create_subpatch_center_mask(subpatch, center_coords)</code>","text":"<p>Create a mask with the center of the subpatch masked.</p> <p>Parameters:</p> Name Type Description Default <code>subpatch</code> <code>ndarray</code> <p>Subpatch to be manipulated.</p> required <code>center_coords</code> <code>ndarray</code> <p>Coordinates of the original center before possible crop.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Mask with the center of the subpatch masked.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def _create_subpatch_center_mask(\n    subpatch: np.ndarray, center_coords: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Create a mask with the center of the subpatch masked.\n\n    Parameters\n    ----------\n    subpatch : np.ndarray\n        Subpatch to be manipulated.\n    center_coords : np.ndarray\n        Coordinates of the original center before possible crop.\n\n    Returns\n    -------\n    np.ndarray\n        Mask with the center of the subpatch masked.\n    \"\"\"\n    mask = np.ones(subpatch.shape)\n    mask[tuple(center_coords)] = 0\n    return np.ma.make_mask(mask)  # type: ignore\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation._create_subpatch_struct_mask","title":"<code>_create_subpatch_struct_mask(subpatch, center_coords, struct_params)</code>","text":"<p>Create a structN2V mask for the subpatch.</p> <p>Parameters:</p> Name Type Description Default <code>subpatch</code> <code>ndarray</code> <p>Subpatch to be manipulated.</p> required <code>center_coords</code> <code>ndarray</code> <p>Coordinates of the original center before possible crop.</p> required <code>struct_params</code> <code>StructMaskParameters</code> <p>Parameters for the structN2V mask (axis and span).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>StructN2V mask for the subpatch.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def _create_subpatch_struct_mask(\n    subpatch: np.ndarray, center_coords: np.ndarray, struct_params: StructMaskParameters\n) -&gt; np.ndarray:\n    \"\"\"Create a structN2V mask for the subpatch.\n\n    Parameters\n    ----------\n    subpatch : np.ndarray\n        Subpatch to be manipulated.\n    center_coords : np.ndarray\n        Coordinates of the original center before possible crop.\n    struct_params : StructMaskParameters\n        Parameters for the structN2V mask (axis and span).\n\n    Returns\n    -------\n    np.ndarray\n        StructN2V mask for the subpatch.\n    \"\"\"\n    # TODO no test for this function!\n    # Create a mask with the center of the subpatch masked\n    mask_placeholder = np.ones(subpatch.shape)\n\n    # reshape to move the struct axis to the first position\n    mask_reshaped = np.moveaxis(mask_placeholder, struct_params.axis, 0)\n\n    # create the mask index for the struct axis\n    mask_index = slice(\n        max(0, center_coords.take(struct_params.axis) - (struct_params.span - 1) // 2),\n        min(\n            1 + center_coords.take(struct_params.axis) + (struct_params.span - 1) // 2,\n            subpatch.shape[struct_params.axis],\n        ),\n    )\n    mask_reshaped[struct_params.axis][mask_index] = 0\n\n    # reshape back to the original shape\n    mask = np.moveaxis(mask_reshaped, 0, struct_params.axis)\n\n    return np.ma.make_mask(mask)  # type: ignore\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation._get_stratified_coords","title":"<code>_get_stratified_coords(mask_pixel_perc, shape, rng=None)</code>","text":"<p>Generate coordinates of the pixels to mask.</p> <p>Randomly selects the coordinates of the pixels to mask in a stratified way, i.e. the distance between masked pixels is approximately the same.</p> <p>Parameters:</p> Name Type Description Default <code>mask_pixel_perc</code> <code>float</code> <p>Actual (quasi) percentage of masked pixels across the whole image. Used in calculating the distance between masked pixels across each axis.</p> required <code>shape</code> <code>tuple[int, ...]</code> <p>Shape of the input patch.</p> required <code>rng</code> <code>Generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of coordinates of the masked pixels.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def _get_stratified_coords(\n    mask_pixel_perc: float,\n    shape: tuple[int, ...],\n    rng: Optional[np.random.Generator] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Generate coordinates of the pixels to mask.\n\n    Randomly selects the coordinates of the pixels to mask in a stratified way, i.e.\n    the distance between masked pixels is approximately the same.\n\n    Parameters\n    ----------\n    mask_pixel_perc : float\n        Actual (quasi) percentage of masked pixels across the whole image. Used in\n        calculating the distance between masked pixels across each axis.\n    shape : tuple[int, ...]\n        Shape of the input patch.\n    rng : np.random.Generator or None\n        Random number generator.\n\n    Returns\n    -------\n    np.ndarray\n        Array of coordinates of the masked pixels.\n    \"\"\"\n    if len(shape) &lt; 2 or len(shape) &gt; 3:\n        raise ValueError(\n            \"Calculating coordinates is only possible for 2D and 3D patches\"\n        )\n\n    if rng is None:\n        rng = np.random.default_rng()\n\n    mask_pixel_distance = np.round((100 / mask_pixel_perc) ** (1 / len(shape))).astype(\n        np.int32\n    )\n\n    # Define a grid of coordinates for each axis in the input patch and the step size\n    pixel_coords = []\n    steps = []\n    for axis_size in shape:\n        # make sure axis size is evenly divisible by box size\n        num_pixels = int(np.ceil(axis_size / mask_pixel_distance))\n        axis_pixel_coords, step = np.linspace(\n            0, axis_size, num_pixels, dtype=np.int32, endpoint=False, retstep=True\n        )\n        # explain\n        pixel_coords.append(axis_pixel_coords.T)\n        steps.append(step)\n\n    # Create a meshgrid of coordinates for each axis in the input patch\n    coordinate_grid_list = np.meshgrid(*pixel_coords)\n    coordinate_grid = np.array(coordinate_grid_list).reshape(len(shape), -1).T\n\n    grid_random_increment = rng.integers(\n        _odd_jitter_func(float(max(steps)), rng)  # type: ignore\n        * np.ones_like(coordinate_grid).astype(np.int32)\n        - 1,\n        size=coordinate_grid.shape,\n        endpoint=True,\n    )\n    coordinate_grid += grid_random_increment\n    coordinate_grid = np.clip(coordinate_grid, 0, np.array(shape) - 1)\n    return coordinate_grid\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation._odd_jitter_func","title":"<code>_odd_jitter_func(step, rng)</code>","text":"<p>Randomly sample a jitter to be applied to the masking grid.</p> <p>This is done to account for cases where the step size is not an integer.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>float</code> <p>Step size of the grid, output of np.linspace.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of random jitter to be added to the grid.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def _odd_jitter_func(step: float, rng: np.random.Generator) -&gt; np.ndarray:\n    \"\"\"\n    Randomly sample a jitter to be applied to the masking grid.\n\n    This is done to account for cases where the step size is not an integer.\n\n    Parameters\n    ----------\n    step : float\n        Step size of the grid, output of np.linspace.\n    rng : np.random.Generator\n        Random number generator.\n\n    Returns\n    -------\n    np.ndarray\n        Array of random jitter to be added to the grid.\n    \"\"\"\n    # Define the random jitter to be added to the grid\n    odd_jitter = np.where(np.floor(step) == step, 0, rng.integers(0, 2))\n\n    # Round the step size to the nearest integer depending on the jitter\n    return np.floor(step) if odd_jitter == 0 else np.ceil(step)\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation.median_manipulate","title":"<code>median_manipulate(patch, mask_pixel_percentage, subpatch_size=11, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with the median of their surrounding subpatch.</p> <p>N2V2 version, manipulated pixels are selected randomly away from a grid with an approximate uniform probability to be selected across the whole patch.</p> <p>If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>floar</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>Generator or None</code> <p>Random number generato, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>tuple containing the manipulated patch, the original patch and the mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def median_manipulate(\n    patch: np.ndarray,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    struct_params: Optional[StructMaskParameters] = None,\n    rng: Optional[np.random.Generator] = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Manipulate pixels by replacing them with the median of their surrounding subpatch.\n\n    N2V2 version, manipulated pixels are selected randomly away from a grid with an\n    approximate uniform probability to be selected across the whole patch.\n\n    If `struct_params` is not None, an additional structN2V mask is applied to the data,\n    replacing the pixels in the mask with random values (excluding the pixel already\n    manipulated).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : floar\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    struct_params : StructMaskParameters or None, optional\n        Parameters for the structN2V mask (axis and span).\n    rng : np.random.Generator or None, optional\n        Random number generato, by default None.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n           tuple containing the manipulated patch, the original patch and the mask.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    transformed_patch = patch.copy()\n\n    # Get the coordinates of the pixels to be replaced\n    subpatch_centers = _get_stratified_coords(mask_pixel_percentage, patch.shape, rng)\n\n    # Generate coordinate grid for subpatch\n    roi_span = np.array(\n        [-np.floor(subpatch_size / 2), np.ceil(subpatch_size / 2)]\n    ).astype(np.int32)\n\n    subpatch_crops_span_full = subpatch_centers[np.newaxis, ...].T + roi_span\n\n    # Dimensions n dims, n centers, (min, max)\n    subpatch_crops_span_clipped = np.clip(\n        subpatch_crops_span_full,\n        a_min=np.zeros_like(patch.shape)[:, np.newaxis, np.newaxis],\n        a_max=np.array(patch.shape)[:, np.newaxis, np.newaxis],\n    )\n\n    for idx in range(subpatch_crops_span_clipped.shape[1]):\n        subpatch_coords = subpatch_crops_span_clipped[:, idx, ...]\n        idxs = [\n            slice(x[0], x[1]) if x[1] - x[0] &gt; 0 else slice(0, 1)\n            for x in subpatch_coords\n        ]\n        subpatch = patch[tuple(idxs)]\n        subpatch_center_adjusted = subpatch_centers[idx] - subpatch_coords[:, 0]\n\n        if struct_params is None:\n            subpatch_mask = _create_subpatch_center_mask(\n                subpatch, subpatch_center_adjusted\n            )\n        else:\n            subpatch_mask = _create_subpatch_struct_mask(\n                subpatch, subpatch_center_adjusted, struct_params\n            )\n        transformed_patch[tuple(subpatch_centers[idx])] = np.median(\n            subpatch[subpatch_mask]\n        )\n\n    mask = np.where(transformed_patch != patch, 1, 0).astype(np.uint8)\n\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask(\n            transformed_patch, subpatch_centers, struct_params\n        )\n\n    return (\n        transformed_patch,\n        mask,\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation/#careamics.transforms.pixel_manipulation.uniform_manipulate","title":"<code>uniform_manipulate(patch, mask_pixel_percentage, subpatch_size=11, remove_center=True, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with a neighbor values.</p> <p>Manipulated pixels are selected unformly selected in a subpatch, away from a grid with an approximate uniform probability to be selected across the whole patch. If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove the center pixel from the subpatch, by default False.</p> <code>True</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>Generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray]</code> <p>tuple containing the manipulated patch and the corresponding mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation.py</code> <pre><code>def uniform_manipulate(\n    patch: np.ndarray,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    remove_center: bool = True,\n    struct_params: Optional[StructMaskParameters] = None,\n    rng: Optional[np.random.Generator] = None,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Manipulate pixels by replacing them with a neighbor values.\n\n    Manipulated pixels are selected unformly selected in a subpatch, away from a grid\n    with an approximate uniform probability to be selected across the whole patch.\n    If `struct_params` is not None, an additional structN2V mask is applied to the\n    data, replacing the pixels in the mask with random values (excluding the pixel\n    already manipulated).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    remove_center : bool\n        Whether to remove the center pixel from the subpatch, by default False.\n    struct_params : StructMaskParameters or None\n        Parameters for the structN2V mask (axis and span).\n    rng : np.random.Generator or None\n        Random number generator.\n\n    Returns\n    -------\n    tuple[np.ndarray]\n        tuple containing the manipulated patch and the corresponding mask.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    # Get the coordinates of the pixels to be replaced\n    transformed_patch = patch.copy()\n\n    subpatch_centers = _get_stratified_coords(mask_pixel_percentage, patch.shape, rng)\n\n    # Generate coordinate grid for subpatch\n    roi_span_full = np.arange(\n        -np.floor(subpatch_size / 2), np.ceil(subpatch_size / 2)\n    ).astype(np.int32)\n\n    # Remove the center pixel from the grid if needed\n    roi_span = roi_span_full[roi_span_full != 0] if remove_center else roi_span_full\n\n    # Randomly select coordinates from the grid\n    random_increment = rng.choice(roi_span, size=subpatch_centers.shape)\n\n    # Clip the coordinates to the patch size\n    replacement_coords = np.clip(\n        subpatch_centers + random_increment,\n        0,\n        [patch.shape[i] - 1 for i in range(len(patch.shape))],\n    )\n\n    # Get the replacement pixels from all subpatchs\n    replacement_pixels = patch[tuple(replacement_coords.T.tolist())]\n\n    # Replace the original pixels with the replacement pixels\n    transformed_patch[tuple(subpatch_centers.T.tolist())] = replacement_pixels\n    mask = np.where(transformed_patch != patch, 1, 0).astype(np.uint8)\n\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask(\n            transformed_patch, subpatch_centers, struct_params\n        )\n\n    return (\n        transformed_patch,\n        mask,\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/","title":"pixel_manipulation_torch","text":"<p>N2V manipulation functions for PyTorch.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch._apply_struct_mask_torch","title":"<code>_apply_struct_mask_torch(patch, coords, struct_params, rng=None)</code>","text":"<p>Apply structN2V masks to patch.</p> <p>Each point in <code>coords</code> corresponds to the center of a mask. Masks are parameterized by <code>struct_params</code>, and pixels in the mask (with respect to <code>coords</code>) are replaced by a random value.</p> <p>Note that the structN2V mask is applied in 2D at the coordinates given by <code>coords</code>.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>Tensor</code> <p>Patch to be manipulated, (batch, y, x) or (batch, z, y, x).</p> required <code>coords</code> <code>Tensor</code> <p>Coordinates of the ROI (subpatch) centers.</p> required <code>struct_params</code> <code>StructMaskParameters</code> <p>Parameters for the structN2V mask (axis and span).</p> required <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Patch with the structN2V mask applied.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def _apply_struct_mask_torch(\n    patch: torch.Tensor,\n    coords: torch.Tensor,\n    struct_params: StructMaskParameters,\n    rng: Optional[torch.Generator] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Apply structN2V masks to patch.\n\n    Each point in `coords` corresponds to the center of a mask. Masks are parameterized\n    by `struct_params`, and pixels in the mask (with respect to `coords`) are replaced\n    by a random value.\n\n    Note that the structN2V mask is applied in 2D at the coordinates given by `coords`.\n\n    Parameters\n    ----------\n    patch : torch.Tensor\n        Patch to be manipulated, (batch, y, x) or (batch, z, y, x).\n    coords : torch.Tensor\n        Coordinates of the ROI (subpatch) centers.\n    struct_params : StructMaskParameters\n        Parameters for the structN2V mask (axis and span).\n    rng : torch.Generator, optional\n        Random number generator.\n\n    Returns\n    -------\n    torch.Tensor\n        Patch with the structN2V mask applied.\n    \"\"\"\n    if rng is None:\n        rng = torch.Generator(device=patch.device)\n\n    # Relative axis\n    moving_axis = -1 - struct_params.axis\n\n    # Create a mask array\n    mask_shape = [1] * len(patch.shape)\n    mask_shape[moving_axis] = struct_params.span\n    mask = torch.ones(mask_shape, device=patch.device)\n\n    center = torch.tensor(mask.shape, device=patch.device) // 2\n\n    # Mark the center\n    mask[tuple(center)] = 0\n\n    # Displacements from center\n    displacements = torch.stack(torch.where(mask == 1)) - center.unsqueeze(1)\n\n    # Combine all coords (ndim, npts) with all displacements (ncoords, ndim)\n    mix = displacements.T.unsqueeze(-1) + coords.T.unsqueeze(0)\n    mix = mix.permute([1, 0, 2]).reshape([mask.ndim, -1]).T\n\n    # Filter out invalid indices\n    valid_indices = (mix[:, moving_axis] &gt;= 0) &amp; (\n        mix[:, moving_axis] &lt; patch.shape[moving_axis]\n    )\n    mix = mix[valid_indices]\n\n    # Replace neighboring pixels with random values from a uniform distribution\n    random_values = torch.empty(len(mix), device=patch.device).uniform_(\n        patch.min().item(), patch.max().item(), generator=rng\n    )\n    patch[tuple(mix.T.tolist())] = random_values\n\n    return patch\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch._get_stratified_coords_torch","title":"<code>_get_stratified_coords_torch(mask_pixel_perc, shape, rng=None)</code>","text":"<p>Generate coordinates of the pixels to mask.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch._get_stratified_coords_torch--todo-add-more-details","title":"TODO add more details","text":"<p>Randomly selects the coordinates of the pixels to mask in a stratified way, i.e. the distance between masked pixels is approximately the same.</p> <p>Parameters:</p> Name Type Description Default <code>mask_pixel_perc</code> <code>float</code> <p>Actual (quasi) percentage of masked pixels across the whole image. Used in calculating the distance between masked pixels across each axis.</p> required <code>shape</code> <code>tuple[int, ...]</code> <p>Shape of the input patch.</p> required <code>rng</code> <code>Generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of coordinates of the masked pixels.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def _get_stratified_coords_torch(\n    mask_pixel_perc: float,\n    shape: tuple[int, ...],\n    rng: Optional[torch.Generator] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Generate coordinates of the pixels to mask.\n\n    # TODO add more details\n    Randomly selects the coordinates of the pixels to mask in a stratified way, i.e.\n    the distance between masked pixels is approximately the same.\n\n    Parameters\n    ----------\n    mask_pixel_perc : float\n        Actual (quasi) percentage of masked pixels across the whole image. Used in\n        calculating the distance between masked pixels across each axis.\n    shape : tuple[int, ...]\n        Shape of the input patch.\n    rng : torch.Generator or None\n        Random number generator.\n\n    Returns\n    -------\n    np.ndarray\n        Array of coordinates of the masked pixels.\n    \"\"\"\n    if rng is None:\n        rng = torch.Generator()\n\n    # Calculate the maximum distance between masked pixels. Inversely proportional to\n    # the percentage of masked pixels.\n    mask_pixel_distance = round((100 / mask_pixel_perc) ** (1 / len(shape)))\n\n    pixel_coords = []\n    steps = []\n\n    # loop over dimensions\n    for axis_size in shape:\n        # number of pixels to mask along the axis\n        num_pixels = int(torch.ceil(torch.tensor(axis_size / mask_pixel_distance)))\n\n        # create 1D grid of coordinates for the axis\n        axis_pixel_coords = torch.linspace(\n            0,\n            axis_size - (axis_size // num_pixels),\n            num_pixels,\n            dtype=torch.int32,\n        )\n\n        # calculate the step size between coordinates\n        step = (\n            axis_pixel_coords[1] - axis_pixel_coords[0]\n            if len(axis_pixel_coords) &gt; 1\n            else axis_size\n        )\n\n        pixel_coords.append(axis_pixel_coords)\n        steps.append(step)\n\n    # create a 2D meshgrid of coordinates\n    coordinate_grid_list = torch.meshgrid(*pixel_coords, indexing=\"ij\")\n    coordinate_grid = torch.stack(\n        [g.flatten() for g in coordinate_grid_list], dim=-1\n    ).to(rng.device)\n\n    # add a random jitter increment so that the coordinates do not lie on the grid\n    random_increment = torch.randint(\n        high=int(_odd_jitter_func_torch(float(max(steps)), rng)),\n        size=torch.tensor(coordinate_grid.shape).to(rng.device).tolist(),\n        generator=rng,\n        device=rng.device,\n    )\n    coordinate_grid += random_increment\n\n    # make sure no coordinate lie outside the range\n    return torch.clamp(\n        coordinate_grid,\n        torch.zeros_like(torch.tensor(shape)).to(device=rng.device),\n        torch.tensor([v - 1 for v in shape]).to(device=rng.device),\n    )\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch._odd_jitter_func_torch","title":"<code>_odd_jitter_func_torch(step, rng)</code>","text":"<p>Randomly sample a jitter to be applied to the masking grid.</p> <p>This is done to account for cases where the step size is not an integer.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>float</code> <p>Step size of the grid, output of np.linspace.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Array of random jitter to be added to the grid.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def _odd_jitter_func_torch(step: float, rng: torch.Generator) -&gt; torch.Tensor:\n    \"\"\"\n    Randomly sample a jitter to be applied to the masking grid.\n\n    This is done to account for cases where the step size is not an integer.\n\n    Parameters\n    ----------\n    step : float\n        Step size of the grid, output of np.linspace.\n    rng : torch.Generator\n        Random number generator.\n\n    Returns\n    -------\n    torch.Tensor\n        Array of random jitter to be added to the grid.\n    \"\"\"\n    step_floor = torch.floor(torch.tensor(step))\n    odd_jitter = (\n        step_floor\n        if step_floor == step\n        else torch.randint(high=2, size=(1,), generator=rng)\n    )\n    return step_floor if odd_jitter == 0 else step_floor + 1\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.median_manipulate_torch","title":"<code>median_manipulate_torch(batch, mask_pixel_percentage, subpatch_size=11, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with the median of their surrounding subpatch.</p> <p>N2V2 version, manipulated pixels are selected randomly away from a grid with an approximate uniform probability to be selected across the whole patch.</p> <p>If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tensor</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x).</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>default_generator or None</code> <p>Random number generato, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple containing the manipulated patch, the original patch and the mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def median_manipulate_torch(\n    batch: torch.Tensor,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    struct_params: Optional[StructMaskParameters] = None,\n    rng: Optional[torch.Generator] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Manipulate pixels by replacing them with the median of their surrounding subpatch.\n\n    N2V2 version, manipulated pixels are selected randomly away from a grid with an\n    approximate uniform probability to be selected across the whole patch.\n\n    If `struct_params` is not None, an additional structN2V mask is applied to the data,\n    replacing the pixels in the mask with random values (excluding the pixel already\n    manipulated).\n\n    Parameters\n    ----------\n    batch : torch.Tensor\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x).\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    struct_params : StructMaskParameters or None, optional\n        Parameters for the structN2V mask (axis and span).\n    rng : torch.default_generator or None, optional\n        Random number generato, by default None.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n           tuple containing the manipulated patch, the original patch and the mask.\n    \"\"\"\n    # get the coordinates of the future ROI centers\n    subpatch_center_coordinates = _get_stratified_coords_torch(\n        mask_pixel_percentage, batch.shape, rng\n    ).to(\n        device=batch.device\n    )  # (num_coordinates, batch + num_spatial_dims)\n\n    # Calculate the padding value for the input tensor\n    pad_value = subpatch_size // 2\n\n    # Generate all offsets for the ROIs. Iteration starting from 1 to skip the batch\n    offsets = torch.meshgrid(\n        [\n            torch.arange(-pad_value, pad_value + 1, device=batch.device)\n            for _ in range(1, subpatch_center_coordinates.shape[1])\n        ],\n        indexing=\"ij\",\n    )\n    offsets = torch.stack(\n        [axis_offset.flatten() for axis_offset in offsets], dim=1\n    )  # (subpatch_size**2, num_spatial_dims)\n\n    # Create the list to assemble coordinates of the ROIs centers for each axis\n    coords_axes = []\n    # Create the list to assemble the span of coordinates defining the ROIs for each\n    # axis\n    coords_expands = []\n    for d in range(subpatch_center_coordinates.shape[1]):\n        coords_axes.append(subpatch_center_coordinates[:, d])\n        if d == 0:\n            # For batch dimension coordinates are not expanded (no offsets)\n            coords_expands.append(\n                subpatch_center_coordinates[:, d]\n                .unsqueeze(1)\n                .expand(-1, subpatch_size ** offsets.shape[1])\n            )  # (num_coordinates, subpatch_size**num_spacial_dims)\n        else:\n            # For spatial dimensions, coordinates are expanded with offsets, creating\n            # spans\n            coords_expands.append(\n                (\n                    subpatch_center_coordinates[:, d].unsqueeze(1) + offsets[:, d - 1]\n                ).clamp(0, batch.shape[d] - 1)\n            )  # (num_coordinates, subpatch_size**num_spacial_dims)\n\n    # create array of rois by indexing the batch with gathered coordinates\n    rois = batch[\n        tuple(coords_expands)\n    ]  # (num_coordinates, subpatch_size**num_spacial_dims)\n\n    if struct_params is not None:\n        # Create the structN2V mask\n        h, w = torch.meshgrid(\n            torch.arange(subpatch_size), torch.arange(subpatch_size), indexing=\"ij\"\n        )\n        center_idx = subpatch_size // 2\n        halfspan = (struct_params.span - 1) // 2\n\n        # Determine the axis along which to apply the mask\n        if struct_params.axis == 0:\n            center_axis = h\n            span_axis = w\n        else:\n            center_axis = w\n            span_axis = h\n\n        # Create the mask\n        struct_mask = (\n            ~(\n                (center_axis == center_idx)\n                &amp; (span_axis &gt;= center_idx - halfspan)\n                &amp; (span_axis &lt;= center_idx + halfspan)\n            )\n        ).flatten()\n        rois_filtered = rois[:, struct_mask]\n    else:\n        # Remove the center pixel value from the rois\n        center_idx = (subpatch_size ** offsets.shape[1]) // 2\n        rois_filtered = torch.cat(\n            [rois[:, :center_idx], rois[:, center_idx + 1 :]], dim=1\n        )\n\n    # compute the medians.\n    medians = rois_filtered.median(dim=1).values  # (num_coordinates,)\n\n    # Update the output tensor with medians\n    output_batch = batch.clone()\n    output_batch[tuple(coords_axes)] = medians\n    mask = torch.where(output_batch != batch, 1, 0).to(torch.uint8)\n\n    if struct_params is not None:\n        output_batch = _apply_struct_mask_torch(\n            output_batch, subpatch_center_coordinates, struct_params\n        )\n\n    return output_batch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.uniform_manipulate_torch","title":"<code>uniform_manipulate_torch(patch, mask_pixel_percentage, subpatch_size=11, remove_center=True, struct_params=None, rng=None)</code>","text":"<p>Manipulate pixels by replacing them with a neighbor values.</p>"},{"location":"reference/careamics/transforms/pixel_manipulation_torch/#careamics.transforms.pixel_manipulation_torch.uniform_manipulate_torch--todo-add-more-details-especially-about-batch","title":"TODO add more details, especially about batch","text":"<p>Manipulated pixels are selected unformly selected in a subpatch, away from a grid with an approximate uniform probability to be selected across the whole patch. If <code>struct_params</code> is not None, an additional structN2V mask is applied to the data, replacing the pixels in the mask with random values (excluding the pixel already manipulated).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>Tensor</code> <p>Image patch, 2D or 3D, shape (y, x) or (z, y, x). # TODO batch and channel.</p> required <code>mask_pixel_percentage</code> <code>float</code> <p>Approximate percentage of pixels to be masked.</p> required <code>subpatch_size</code> <code>int</code> <p>Size of the subpatch the new pixel value is sampled from, by default 11.</p> <code>11</code> <code>remove_center</code> <code>bool</code> <p>Whether to remove the center pixel from the subpatch, by default False.</p> <code>True</code> <code>struct_params</code> <code>StructMaskParameters or None</code> <p>Parameters for the structN2V mask (axis and span).</p> <code>None</code> <code>rng</code> <code>default_generator or None</code> <p>Random number generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple containing the manipulated patch and the corresponding mask.</p> Source code in <code>src/careamics/transforms/pixel_manipulation_torch.py</code> <pre><code>def uniform_manipulate_torch(\n    patch: torch.Tensor,\n    mask_pixel_percentage: float,\n    subpatch_size: int = 11,\n    remove_center: bool = True,\n    struct_params: Optional[StructMaskParameters] = None,\n    rng: Optional[torch.Generator] = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Manipulate pixels by replacing them with a neighbor values.\n\n    # TODO add more details, especially about batch\n\n    Manipulated pixels are selected unformly selected in a subpatch, away from a grid\n    with an approximate uniform probability to be selected across the whole patch.\n    If `struct_params` is not None, an additional structN2V mask is applied to the\n    data, replacing the pixels in the mask with random values (excluding the pixel\n    already manipulated).\n\n    Parameters\n    ----------\n    patch : torch.Tensor\n        Image patch, 2D or 3D, shape (y, x) or (z, y, x). # TODO batch and channel.\n    mask_pixel_percentage : float\n        Approximate percentage of pixels to be masked.\n    subpatch_size : int\n        Size of the subpatch the new pixel value is sampled from, by default 11.\n    remove_center : bool\n        Whether to remove the center pixel from the subpatch, by default False.\n    struct_params : StructMaskParameters or None\n        Parameters for the structN2V mask (axis and span).\n    rng : torch.default_generator or None\n        Random number generator.\n\n    Returns\n    -------\n    tuple[torch.Tensor, torch.Tensor]\n        tuple containing the manipulated patch and the corresponding mask.\n    \"\"\"\n    if rng is None:\n        rng = torch.Generator(device=patch.device)\n        # TODO do we need seed ?\n\n    # create a copy of the patch\n    transformed_patch = patch.clone()\n\n    # get the coordinates of the pixels to be masked\n    subpatch_centers = _get_stratified_coords_torch(\n        mask_pixel_percentage, patch.shape, rng\n    )\n    subpatch_centers = subpatch_centers.to(device=patch.device)\n\n    # TODO refactor with non negative indices?\n    # arrange the list of indices to represent the ROI around the pixel to be masked\n    roi_span_full = torch.arange(\n        -(subpatch_size // 2),\n        subpatch_size // 2 + 1,\n        dtype=torch.int32,\n        device=patch.device,\n    )\n\n    # remove the center pixel from the ROI\n    roi_span = roi_span_full[roi_span_full != 0] if remove_center else roi_span_full\n\n    # create a random increment to select the replacement value\n    # this increment is added to the center coordinates\n    random_increment = roi_span[\n        torch.randint(\n            low=min(roi_span),\n            high=max(roi_span) + 1,  # TODO check this, it may exclude one value\n            size=subpatch_centers.shape,\n            generator=rng,\n            device=patch.device,\n        )\n    ]\n\n    # compute the replacement pixel coordinates\n    replacement_coords = torch.clamp(\n        subpatch_centers + random_increment,\n        torch.zeros_like(torch.tensor(patch.shape)).to(device=patch.device),\n        torch.tensor([v - 1 for v in patch.shape]).to(device=patch.device),\n    )\n\n    # replace the pixels in the patch\n    # tuples and transpose are needed for proper indexing\n    replacement_pixels = patch[tuple(replacement_coords.T)]\n    transformed_patch[tuple(subpatch_centers.T)] = replacement_pixels\n\n    # create a mask representing the masked pixels\n    mask = (transformed_patch != patch).to(dtype=torch.uint8)\n\n    # apply structN2V mask if needed\n    if struct_params is not None:\n        transformed_patch = _apply_struct_mask_torch(\n            transformed_patch, subpatch_centers, struct_params, rng\n        )\n\n    return transformed_patch, mask\n</code></pre>"},{"location":"reference/careamics/transforms/struct_mask_parameters/","title":"struct_mask_parameters","text":"<p>Class representing the parameters of structN2V masks.</p>"},{"location":"reference/careamics/transforms/struct_mask_parameters/#careamics.transforms.struct_mask_parameters.StructMaskParameters","title":"<code>StructMaskParameters</code>  <code>dataclass</code>","text":"<p>Parameters of structN2V masks.</p> <p>Attributes:</p> Name Type Description <code>axis</code> <code>Literal[0, 1]</code> <p>Axis along which to apply the mask, horizontal (0) or vertical (1).</p> <code>span</code> <code>int</code> <p>Span of the mask.</p> Source code in <code>src/careamics/transforms/struct_mask_parameters.py</code> <pre><code>@dataclass\nclass StructMaskParameters:\n    \"\"\"Parameters of structN2V masks.\n\n    Attributes\n    ----------\n    axis : Literal[0, 1]\n        Axis along which to apply the mask, horizontal (0) or vertical (1).\n    span : int\n        Span of the mask.\n    \"\"\"\n\n    axis: Literal[0, 1]\n    span: int\n</code></pre>"},{"location":"reference/careamics/transforms/transform/","title":"transform","text":"<p>A general parent class for transforms.</p>"},{"location":"reference/careamics/transforms/transform/#careamics.transforms.transform.Transform","title":"<code>Transform</code>","text":"<p>A general parent class for transforms.</p> Source code in <code>src/careamics/transforms/transform.py</code> <pre><code>class Transform:\n    \"\"\"A general parent class for transforms.\"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Apply the transform.\n\n        Parameters\n        ----------\n        *args : Any\n            Arguments.\n        **kwargs : Any\n            Keyword arguments.\n\n        Returns\n        -------\n        Any\n            Transformed data.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/careamics/transforms/transform/#careamics.transforms.transform.Transform.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Apply the transform.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Transformed data.</p> Source code in <code>src/careamics/transforms/transform.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Apply the transform.\n\n    Parameters\n    ----------\n    *args : Any\n        Arguments.\n    **kwargs : Any\n        Keyword arguments.\n\n    Returns\n    -------\n    Any\n        Transformed data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics/transforms/tta/","title":"tta","text":"<p>Test-time augmentations.</p>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA","title":"<code>ImageRestorationTTA</code>","text":"<p>Test-time augmentation for image restoration tasks.</p> <p>The augmentation is performed using all 90 deg rotations and their flipped version, as well as the original image flipped.</p> <p>Tensors should be of shape SC(Z)YX.</p> <p>This transformation is used in the LightningModule in order to perform test-time augmentation.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>class ImageRestorationTTA:\n    \"\"\"\n    Test-time augmentation for image restoration tasks.\n\n    The augmentation is performed using all 90 deg rotations and their flipped version,\n    as well as the original image flipped.\n\n    Tensors should be of shape SC(Z)YX.\n\n    This transformation is used in the LightningModule in order to perform test-time\n    augmentation.\n    \"\"\"\n\n    def forward(self, input_tensor: Tensor) -&gt; list[Tensor]:\n        \"\"\"\n        Apply test-time augmentation to the input tensor.\n\n        Parameters\n        ----------\n        input_tensor : Tensor\n            Input tensor, shape SC(Z)YX.\n\n        Returns\n        -------\n        list of torch.Tensor\n            List of augmented tensors.\n        \"\"\"\n        # axes: only applies to YX axes\n        axes = (-2, -1)\n\n        augmented = [\n            # original\n            input_tensor,\n            # rotations\n            rot90(input_tensor, 1, dims=axes),\n            rot90(input_tensor, 2, dims=axes),\n            rot90(input_tensor, 3, dims=axes),\n            # original flipped\n            flip(input_tensor, dims=(axes[0],)),\n            flip(input_tensor, dims=(axes[1],)),\n        ]\n\n        # rotated once, flipped\n        augmented.extend(\n            [\n                flip(augmented[1], dims=(axes[0],)),\n                flip(augmented[1], dims=(axes[1],)),\n            ]\n        )\n\n        return augmented\n\n    def backward(self, x: list[Tensor]) -&gt; Tensor:\n        \"\"\"Undo the test-time augmentation.\n\n        Parameters\n        ----------\n        x : Any\n            List of augmented tensors of shape SC(Z)YX.\n\n        Returns\n        -------\n        Any\n            Original tensor.\n        \"\"\"\n        axes = (-2, -1)\n\n        reverse = [\n            # original\n            x[0],\n            # rotated\n            rot90(x[1], -1, dims=axes),\n            rot90(x[2], -2, dims=axes),\n            rot90(x[3], -3, dims=axes),\n            # original flipped\n            flip(x[4], dims=(axes[0],)),\n            flip(x[5], dims=(axes[1],)),\n            # rotated once, flipped\n            rot90(flip(x[6], dims=(axes[0],)), -1, dims=axes),\n            rot90(flip(x[7], dims=(axes[1],)), -1, dims=axes),\n        ]\n\n        return mean(stack(reverse), dim=0)\n</code></pre>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA.backward","title":"<code>backward(x)</code>","text":"<p>Undo the test-time augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>List of augmented tensors of shape SC(Z)YX.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Original tensor.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>def backward(self, x: list[Tensor]) -&gt; Tensor:\n    \"\"\"Undo the test-time augmentation.\n\n    Parameters\n    ----------\n    x : Any\n        List of augmented tensors of shape SC(Z)YX.\n\n    Returns\n    -------\n    Any\n        Original tensor.\n    \"\"\"\n    axes = (-2, -1)\n\n    reverse = [\n        # original\n        x[0],\n        # rotated\n        rot90(x[1], -1, dims=axes),\n        rot90(x[2], -2, dims=axes),\n        rot90(x[3], -3, dims=axes),\n        # original flipped\n        flip(x[4], dims=(axes[0],)),\n        flip(x[5], dims=(axes[1],)),\n        # rotated once, flipped\n        rot90(flip(x[6], dims=(axes[0],)), -1, dims=axes),\n        rot90(flip(x[7], dims=(axes[1],)), -1, dims=axes),\n    ]\n\n    return mean(stack(reverse), dim=0)\n</code></pre>"},{"location":"reference/careamics/transforms/tta/#careamics.transforms.tta.ImageRestorationTTA.forward","title":"<code>forward(input_tensor)</code>","text":"<p>Apply test-time augmentation to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor, shape SC(Z)YX.</p> required <p>Returns:</p> Type Description <code>list of torch.Tensor</code> <p>List of augmented tensors.</p> Source code in <code>src/careamics/transforms/tta.py</code> <pre><code>def forward(self, input_tensor: Tensor) -&gt; list[Tensor]:\n    \"\"\"\n    Apply test-time augmentation to the input tensor.\n\n    Parameters\n    ----------\n    input_tensor : Tensor\n        Input tensor, shape SC(Z)YX.\n\n    Returns\n    -------\n    list of torch.Tensor\n        List of augmented tensors.\n    \"\"\"\n    # axes: only applies to YX axes\n    axes = (-2, -1)\n\n    augmented = [\n        # original\n        input_tensor,\n        # rotations\n        rot90(input_tensor, 1, dims=axes),\n        rot90(input_tensor, 2, dims=axes),\n        rot90(input_tensor, 3, dims=axes),\n        # original flipped\n        flip(input_tensor, dims=(axes[0],)),\n        flip(input_tensor, dims=(axes[1],)),\n    ]\n\n    # rotated once, flipped\n    augmented.extend(\n        [\n            flip(augmented[1], dims=(axes[0],)),\n            flip(augmented[1], dims=(axes[1],)),\n        ]\n    )\n\n    return augmented\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/","title":"xy_flip","text":"<p>XY flip transform.</p>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip","title":"<code>XYFlip</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Flip image along X and Y axis, one at a time.</p> <p>This transform randomly flips one of the last two axes.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Attributes:</p> Name Type Description <code>axis_indices</code> <code>List[int]</code> <p>Indices of the axes that can be flipped.</p> <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <p>Parameters:</p> Name Type Description Default <code>flip_x</code> <code>bool</code> <p>Whether to flip along the X axis, by default True.</p> <code>True</code> <code>flip_y</code> <code>bool</code> <p>Whether to flip along the Y axis, by default True.</p> <code>True</code> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>class XYFlip(Transform):\n    \"\"\"Flip image along X and Y axis, one at a time.\n\n    This transform randomly flips one of the last two axes.\n\n    This transform expects C(Z)YX dimensions.\n\n    Attributes\n    ----------\n    axis_indices : List[int]\n        Indices of the axes that can be flipped.\n    rng : np.random.Generator\n        Random number generator.\n    p : float\n        Probability of applying the transform.\n    seed : Optional[int]\n        Random seed.\n\n    Parameters\n    ----------\n    flip_x : bool, optional\n        Whether to flip along the X axis, by default True.\n    flip_y : bool, optional\n        Whether to flip along the Y axis, by default True.\n    p : float, optional\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n\n    def __init__(\n        self,\n        flip_x: bool = True,\n        flip_y: bool = True,\n        p: float = 0.5,\n        seed: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        flip_x : bool, optional\n            Whether to flip along the X axis, by default True.\n        flip_y : bool, optional\n            Whether to flip along the Y axis, by default True.\n        p : float\n            Probability of applying the transform, by default 0.5.\n        seed : Optional[int], optional\n            Random seed, by default None.\n        \"\"\"\n        if p &lt; 0 or p &gt; 1:\n            raise ValueError(\"Probability must be in [0, 1].\")\n\n        if not flip_x and not flip_y:\n            raise ValueError(\"At least one axis must be flippable.\")\n\n        # probability to apply the transform\n        self.p = p\n\n        # \"flippable\" axes\n        self.axis_indices = []\n\n        if flip_y:\n            self.axis_indices.append(-2)\n        if flip_x:\n            self.axis_indices.append(-1)\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self,\n        patch: NDArray,\n        target: Optional[NDArray] = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : Optional[np.ndarray], optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        Tuple[np.ndarray, Optional[np.ndarray]]\n            Transformed patch and target.\n        \"\"\"\n        if self.rng.random() &gt; self.p:\n            return patch, target, additional_arrays\n\n        # choose an axis to flip\n        axis = self.rng.choice(self.axis_indices)\n\n        patch_transformed = self._apply(patch, axis)\n        target_transformed = self._apply(target, axis) if target is not None else None\n        additional_transformed = {\n            key: self._apply(array, axis) for key, array in additional_arrays.items()\n        }\n\n        return patch_transformed, target_transformed, additional_transformed\n\n    def _apply(self, patch: NDArray, axis: int) -&gt; NDArray:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image patch, 2D or 3D, shape C(Z)YX.\n        axis : int\n            Axis to flip.\n\n        Returns\n        -------\n        np.ndarray\n            Flipped image patch.\n        \"\"\"\n        return np.ascontiguousarray(np.flip(patch, axis=axis))\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Optional[ndarray]]</code> <p>Transformed patch and target.</p> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>def __call__(\n    self,\n    patch: NDArray,\n    target: Optional[NDArray] = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : Optional[np.ndarray], optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    Tuple[np.ndarray, Optional[np.ndarray]]\n        Transformed patch and target.\n    \"\"\"\n    if self.rng.random() &gt; self.p:\n        return patch, target, additional_arrays\n\n    # choose an axis to flip\n    axis = self.rng.choice(self.axis_indices)\n\n    patch_transformed = self._apply(patch, axis)\n    target_transformed = self._apply(target, axis) if target is not None else None\n    additional_transformed = {\n        key: self._apply(array, axis) for key, array in additional_arrays.items()\n    }\n\n    return patch_transformed, target_transformed, additional_transformed\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip.__init__","title":"<code>__init__(flip_x=True, flip_y=True, p=0.5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>flip_x</code> <code>bool</code> <p>Whether to flip along the X axis, by default True.</p> <code>True</code> <code>flip_y</code> <code>bool</code> <p>Whether to flip along the Y axis, by default True.</p> <code>True</code> <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>def __init__(\n    self,\n    flip_x: bool = True,\n    flip_y: bool = True,\n    p: float = 0.5,\n    seed: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    flip_x : bool, optional\n        Whether to flip along the X axis, by default True.\n    flip_y : bool, optional\n        Whether to flip along the Y axis, by default True.\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int], optional\n        Random seed, by default None.\n    \"\"\"\n    if p &lt; 0 or p &gt; 1:\n        raise ValueError(\"Probability must be in [0, 1].\")\n\n    if not flip_x and not flip_y:\n        raise ValueError(\"At least one axis must be flippable.\")\n\n    # probability to apply the transform\n    self.p = p\n\n    # \"flippable\" axes\n    self.axis_indices = []\n\n    if flip_y:\n        self.axis_indices.append(-2)\n    if flip_x:\n        self.axis_indices.append(-1)\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/transforms/xy_flip/#careamics.transforms.xy_flip.XYFlip._apply","title":"<code>_apply(patch, axis)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image patch, 2D or 3D, shape C(Z)YX.</p> required <code>axis</code> <code>int</code> <p>Axis to flip.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Flipped image patch.</p> Source code in <code>src/careamics/transforms/xy_flip.py</code> <pre><code>def _apply(self, patch: NDArray, axis: int) -&gt; NDArray:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image patch, 2D or 3D, shape C(Z)YX.\n    axis : int\n        Axis to flip.\n\n    Returns\n    -------\n    np.ndarray\n        Flipped image patch.\n    \"\"\"\n    return np.ascontiguousarray(np.flip(patch, axis=axis))\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/","title":"xy_random_rotate90","text":"<p>Patch transform applying XY random 90 degrees rotations.</p>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90","title":"<code>XYRandomRotate90</code>","text":"<p>               Bases: <code>Transform</code></p> <p>Applies random 90 degree rotations to the YX axis.</p> <p>This transform expects C(Z)YX dimensions.</p> <p>Attributes:</p> Name Type Description <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>p</code> <code>float</code> <p>Probability of applying the transform.</p> <code>seed</code> <code>Optional[int]</code> <p>Random seed.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>class XYRandomRotate90(Transform):\n    \"\"\"Applies random 90 degree rotations to the YX axis.\n\n    This transform expects C(Z)YX dimensions.\n\n    Attributes\n    ----------\n    rng : np.random.Generator\n        Random number generator.\n    p : float\n        Probability of applying the transform.\n    seed : Optional[int]\n        Random seed.\n\n    Parameters\n    ----------\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Random seed, by default None.\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, seed: Optional[int] = None):\n        \"\"\"Constructor.\n\n        Parameters\n        ----------\n        p : float\n            Probability of applying the transform, by default 0.5.\n        seed : Optional[int]\n            Random seed, by default None.\n        \"\"\"\n        if p &lt; 0 or p &gt; 1:\n            raise ValueError(\"Probability must be in [0, 1].\")\n\n        # probability to apply the transform\n        self.p = p\n\n        # numpy random generator\n        self.rng = np.random.default_rng(seed=seed)\n\n    def __call__(\n        self,\n        patch: NDArray,\n        target: Optional[NDArray] = None,\n        **additional_arrays: NDArray,\n    ) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n        \"\"\"Apply the transform to the source patch and the target (optional).\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Patch, 2D or 3D, shape C(Z)YX.\n        target : Optional[np.ndarray], optional\n            Target for the patch, by default None.\n        **additional_arrays : NDArray\n            Additional arrays that will be transformed identically to `patch` and\n            `target`.\n\n        Returns\n        -------\n        tuple[np.ndarray, Optional[np.ndarray]]\n            Transformed patch and target.\n        \"\"\"\n        if self.rng.random() &gt; self.p:\n            return patch, target, additional_arrays\n\n        # number of rotations\n        n_rot = self.rng.integers(1, 4)\n\n        axes = (-2, -1)\n        patch_transformed = self._apply(patch, n_rot, axes)\n        target_transformed = (\n            self._apply(target, n_rot, axes) if target is not None else None\n        )\n        additional_transformed = {\n            key: self._apply(array, n_rot, axes)\n            for key, array in additional_arrays.items()\n        }\n\n        return patch_transformed, target_transformed, additional_transformed\n\n    def _apply(self, patch: NDArray, n_rot: int, axes: tuple[int, int]) -&gt; NDArray:\n        \"\"\"Apply the transform to the image.\n\n        Parameters\n        ----------\n        patch : np.ndarray\n            Image or image patch, 2D or 3D, shape C(Z)YX.\n        n_rot : int\n            Number of 90 degree rotations.\n        axes : tuple[int, int]\n            Axes along which to rotate the patch.\n\n        Returns\n        -------\n        np.ndarray\n            Transformed patch.\n        \"\"\"\n        return np.ascontiguousarray(np.rot90(patch, k=n_rot, axes=axes))\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90.__call__","title":"<code>__call__(patch, target=None, **additional_arrays)</code>","text":"<p>Apply the transform to the source patch and the target (optional).</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Patch, 2D or 3D, shape C(Z)YX.</p> required <code>target</code> <code>Optional[ndarray]</code> <p>Target for the patch, by default None.</p> <code>None</code> <code>**additional_arrays</code> <code>NDArray</code> <p>Additional arrays that will be transformed identically to <code>patch</code> and <code>target</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[ndarray, Optional[ndarray]]</code> <p>Transformed patch and target.</p> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>def __call__(\n    self,\n    patch: NDArray,\n    target: Optional[NDArray] = None,\n    **additional_arrays: NDArray,\n) -&gt; tuple[NDArray, Optional[NDArray], dict[str, NDArray]]:\n    \"\"\"Apply the transform to the source patch and the target (optional).\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Patch, 2D or 3D, shape C(Z)YX.\n    target : Optional[np.ndarray], optional\n        Target for the patch, by default None.\n    **additional_arrays : NDArray\n        Additional arrays that will be transformed identically to `patch` and\n        `target`.\n\n    Returns\n    -------\n    tuple[np.ndarray, Optional[np.ndarray]]\n        Transformed patch and target.\n    \"\"\"\n    if self.rng.random() &gt; self.p:\n        return patch, target, additional_arrays\n\n    # number of rotations\n    n_rot = self.rng.integers(1, 4)\n\n    axes = (-2, -1)\n    patch_transformed = self._apply(patch, n_rot, axes)\n    target_transformed = (\n        self._apply(target, n_rot, axes) if target is not None else None\n    )\n    additional_transformed = {\n        key: self._apply(array, n_rot, axes)\n        for key, array in additional_arrays.items()\n    }\n\n    return patch_transformed, target_transformed, additional_transformed\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90.__init__","title":"<code>__init__(p=0.5, seed=None)</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>Probability of applying the transform, by default 0.5.</p> <code>0.5</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed, by default None.</p> <code>None</code> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>def __init__(self, p: float = 0.5, seed: Optional[int] = None):\n    \"\"\"Constructor.\n\n    Parameters\n    ----------\n    p : float\n        Probability of applying the transform, by default 0.5.\n    seed : Optional[int]\n        Random seed, by default None.\n    \"\"\"\n    if p &lt; 0 or p &gt; 1:\n        raise ValueError(\"Probability must be in [0, 1].\")\n\n    # probability to apply the transform\n    self.p = p\n\n    # numpy random generator\n    self.rng = np.random.default_rng(seed=seed)\n</code></pre>"},{"location":"reference/careamics/transforms/xy_random_rotate90/#careamics.transforms.xy_random_rotate90.XYRandomRotate90._apply","title":"<code>_apply(patch, n_rot, axes)</code>","text":"<p>Apply the transform to the image.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>ndarray</code> <p>Image or image patch, 2D or 3D, shape C(Z)YX.</p> required <code>n_rot</code> <code>int</code> <p>Number of 90 degree rotations.</p> required <code>axes</code> <code>tuple[int, int]</code> <p>Axes along which to rotate the patch.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Transformed patch.</p> Source code in <code>src/careamics/transforms/xy_random_rotate90.py</code> <pre><code>def _apply(self, patch: NDArray, n_rot: int, axes: tuple[int, int]) -&gt; NDArray:\n    \"\"\"Apply the transform to the image.\n\n    Parameters\n    ----------\n    patch : np.ndarray\n        Image or image patch, 2D or 3D, shape C(Z)YX.\n    n_rot : int\n        Number of 90 degree rotations.\n    axes : tuple[int, int]\n        Axes along which to rotate the patch.\n\n    Returns\n    -------\n    np.ndarray\n        Transformed patch.\n    \"\"\"\n    return np.ascontiguousarray(np.rot90(patch, k=n_rot, axes=axes))\n</code></pre>"},{"location":"reference/careamics/utils/autocorrelation/","title":"autocorrelation","text":"<p>Autocorrelation function.</p>"},{"location":"reference/careamics/utils/autocorrelation/#careamics.utils.autocorrelation.autocorrelation","title":"<code>autocorrelation(image)</code>","text":"<p>Compute the autocorrelation of an image.</p> <p>This method is used to explore spatial correlations in images, in particular in the noise.</p> <p>The autocorrelation is normalized to the zero-shift value, which is centered in the resulting images.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray</code> <p>Input image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Autocorrelation of the input image.</p> Source code in <code>src/careamics/utils/autocorrelation.py</code> <pre><code>def autocorrelation(image: NDArray) -&gt; NDArray:\n    \"\"\"Compute the autocorrelation of an image.\n\n    This method is used to explore spatial correlations in images,\n    in particular in the noise.\n\n    The autocorrelation is normalized to the zero-shift value, which is centered in\n    the resulting images.\n\n    Parameters\n    ----------\n    image : NDArray\n        Input image.\n\n    Returns\n    -------\n    numpy.ndarray\n        Autocorrelation of the input image.\n    \"\"\"\n    # normalize image\n    image = (image - np.mean(image)) / np.std(image)\n\n    # compute autocorrelation in fourier space\n    image = np.fft.fftn(image)\n    image = np.abs(image) ** 2\n    image = np.fft.ifftn(image).real\n\n    # normalize to zero shift value\n    image = image / image.flat[0]\n\n    # shift zero frequency to center\n    image = np.fft.fftshift(image)\n\n    return image\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/","title":"base_enum","text":"<p>A base class for Enum that allows checking if a value is in the Enum.</p>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum","title":"<code>BaseEnum</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Base Enum class, allowing checking if a value is in the enum.</p> Example <p>from careamics.utils.base_enum import BaseEnum</p> Source code in <code>src/careamics/utils/base_enum.py</code> <pre><code>class BaseEnum(Enum, metaclass=_ContainerEnum):\n    \"\"\"Base Enum class, allowing checking if a value is in the enum.\n\n    Example\n    -------\n    &gt;&gt;&gt; from careamics.utils.base_enum import BaseEnum\n    &gt;&gt;&gt; # Define a new enum\n    &gt;&gt;&gt; class BaseEnumExtension(BaseEnum):\n    ...     VALUE = \"value\"\n    &gt;&gt;&gt; # Check if value is in the enum\n    &gt;&gt;&gt; \"value\" in BaseEnumExtension\n    True\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum--define-a-new-enum","title":"Define a new enum","text":"<p>class BaseEnumExtension(BaseEnum): ...     VALUE = \"value\"</p>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum.BaseEnum--check-if-value-is-in-the-enum","title":"Check if value is in the enum","text":"<p>\"value\" in BaseEnumExtension True</p>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum._ContainerEnum","title":"<code>_ContainerEnum</code>","text":"<p>               Bases: <code>EnumMeta</code></p> <p>Metaclass for Enum with contains method.</p> Source code in <code>src/careamics/utils/base_enum.py</code> <pre><code>class _ContainerEnum(EnumMeta):\n    \"\"\"Metaclass for Enum with __contains__ method.\"\"\"\n\n    def __contains__(cls, item: Any) -&gt; bool:\n        \"\"\"Check if an item is in the Enum.\n\n        Parameters\n        ----------\n        item : Any\n            Item to check.\n\n        Returns\n        -------\n        bool\n            True if the item is in the Enum, False otherwise.\n        \"\"\"\n        try:\n            cls(item)\n        except ValueError:\n            return False\n        return True\n\n    @classmethod\n    def has_value(cls, value: Any) -&gt; bool:\n        \"\"\"Check if a value is in the Enum.\n\n        Parameters\n        ----------\n        value : Any\n            Value to check.\n\n        Returns\n        -------\n        bool\n            True if the value is in the Enum, False otherwise.\n        \"\"\"\n        return value in cls._value2member_map_\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum._ContainerEnum.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Check if an item is in the Enum.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> <p>Item to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the item is in the Enum, False otherwise.</p> Source code in <code>src/careamics/utils/base_enum.py</code> <pre><code>def __contains__(cls, item: Any) -&gt; bool:\n    \"\"\"Check if an item is in the Enum.\n\n    Parameters\n    ----------\n    item : Any\n        Item to check.\n\n    Returns\n    -------\n    bool\n        True if the item is in the Enum, False otherwise.\n    \"\"\"\n    try:\n        cls(item)\n    except ValueError:\n        return False\n    return True\n</code></pre>"},{"location":"reference/careamics/utils/base_enum/#careamics.utils.base_enum._ContainerEnum.has_value","title":"<code>has_value(value)</code>  <code>classmethod</code>","text":"<p>Check if a value is in the Enum.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Value to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the value is in the Enum, False otherwise.</p> Source code in <code>src/careamics/utils/base_enum.py</code> <pre><code>@classmethod\ndef has_value(cls, value: Any) -&gt; bool:\n    \"\"\"Check if a value is in the Enum.\n\n    Parameters\n    ----------\n    value : Any\n        Value to check.\n\n    Returns\n    -------\n    bool\n        True if the value is in the Enum, False otherwise.\n    \"\"\"\n    return value in cls._value2member_map_\n</code></pre>"},{"location":"reference/careamics/utils/context/","title":"context","text":"<p>Context submodule.</p> <p>A convenience function to change the working directory in order to save data.</p>"},{"location":"reference/careamics/utils/context/#careamics.utils.context.cwd","title":"<code>cwd(path)</code>","text":"<p>Change the current working directory to the given path.</p> <p>This method can be used to generate files in a specific directory, once out of the context, the working directory is set back to the original one.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>New working directory path.</p> required <p>Returns:</p> Type Description <code>Iterator[None]</code> <p>None values.</p> <p>Examples:</p> <p>The context is whcnaged within the block and then restored to the original one.</p> <pre><code>&gt;&gt;&gt; with cwd(my_path):\n...     pass # do something\n</code></pre> Source code in <code>src/careamics/utils/context.py</code> <pre><code>@contextmanager\ndef cwd(path: Union[str, Path]) -&gt; Iterator[None]:\n    \"\"\"\n    Change the current working directory to the given path.\n\n    This method can be used to generate files in a specific directory, once out of the\n    context, the working directory is set back to the original one.\n\n    Parameters\n    ----------\n    path : Union[str,Path]\n        New working directory path.\n\n    Returns\n    -------\n    Iterator[None]\n        None values.\n\n    Examples\n    --------\n    The context is whcnaged within the block and then restored to the original one.\n\n    &gt;&gt;&gt; with cwd(my_path):\n    ...     pass # do something\n    \"\"\"\n    path = Path(path)\n\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n\n    old_pwd = Path(\".\").absolute()\n    os.chdir(path)\n    try:\n        yield\n    finally:\n        os.chdir(old_pwd)\n</code></pre>"},{"location":"reference/careamics/utils/context/#careamics.utils.context.get_careamics_home","title":"<code>get_careamics_home()</code>","text":"<p>Return the CAREamics home directory.</p> <p>CAREamics home directory is a hidden folder in home.</p> <p>Returns:</p> Type Description <code>Path</code> <p>CAREamics home directory path.</p> Source code in <code>src/careamics/utils/context.py</code> <pre><code>def get_careamics_home() -&gt; Path:\n    \"\"\"Return the CAREamics home directory.\n\n    CAREamics home directory is a hidden folder in home.\n\n    Returns\n    -------\n    Path\n        CAREamics home directory path.\n    \"\"\"\n    home = Path.home() / \".careamics\"\n\n    if not home.exists():\n        home.mkdir(parents=True, exist_ok=True)\n\n    return home\n</code></pre>"},{"location":"reference/careamics/utils/lightning_utils/","title":"lightning_utils","text":"<p>PyTorch lightning utilities.</p>"},{"location":"reference/careamics/utils/lightning_utils/#careamics.utils.lightning_utils.read_csv_logger","title":"<code>read_csv_logger(experiment_name, log_folder)</code>","text":"<p>Return the loss curves from the csv logs.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment.</p> required <code>log_folder</code> <code>Path or str</code> <p>Path to the folder containing the csv logs.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the loss curves, with keys \"train_epoch\", \"val_epoch\", \"train_loss\" and \"val_loss\".</p> Source code in <code>src/careamics/utils/lightning_utils.py</code> <pre><code>def read_csv_logger(experiment_name: str, log_folder: Union[str, Path]) -&gt; dict:\n    \"\"\"Return the loss curves from the csv logs.\n\n    Parameters\n    ----------\n    experiment_name : str\n        Name of the experiment.\n    log_folder : Path or str\n        Path to the folder containing the csv logs.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the loss curves, with keys \"train_epoch\", \"val_epoch\",\n        \"train_loss\" and \"val_loss\".\n    \"\"\"\n    path = Path(log_folder) / experiment_name\n\n    # find the most recent of version_* folders\n    versions = [int(v.name.split(\"_\")[-1]) for v in path.iterdir() if v.is_dir()]\n    version = max(versions)\n\n    path_log = path / f\"version_{version}\" / \"metrics.csv\"\n\n    epochs = []\n    train_losses_tmp = []\n    val_losses_tmp = []\n    with open(path_log) as f:\n        lines = f.readlines()\n\n        for single_line in lines[1:]:\n            epoch, _, train_loss, _, val_loss = single_line.strip().split(\",\")\n\n            epochs.append(epoch)\n            train_losses_tmp.append(train_loss)\n            val_losses_tmp.append(val_loss)\n\n    # train and val are not logged on the same row and can have different lengths\n    train_epoch = [\n        int(epochs[i]) for i in range(len(epochs)) if train_losses_tmp[i] != \"\"\n    ]\n    val_epoch = [int(epochs[i]) for i in range(len(epochs)) if val_losses_tmp[i] != \"\"]\n    train_losses = [float(loss) for loss in train_losses_tmp if loss != \"\"]\n    val_losses = [float(loss) for loss in val_losses_tmp if loss != \"\"]\n\n    return {\n        \"train_epoch\": train_epoch,\n        \"val_epoch\": val_epoch,\n        \"train_loss\": train_losses,\n        \"val_loss\": val_losses,\n    }\n</code></pre>"},{"location":"reference/careamics/utils/logging/","title":"logging","text":"<p>Logging submodule.</p> <p>The methods are responsible for the in-console logger.</p>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar","title":"<code>ProgressBar</code>","text":"<p>Keras style progress bar.</p> <p>Adapted from https://github.com/yueyericardo/pkbar.</p> <p>Parameters:</p> Name Type Description Default <code>max_value</code> <code>Optional[int]</code> <p>Maximum progress bar value, by default None.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>Zero-indexed current epoch, by default None.</p> <code>None</code> <code>num_epochs</code> <code>Optional[int]</code> <p>Total number of epochs, by default None.</p> <code>None</code> <code>stateful_metrics</code> <code>Optional[list]</code> <p>Iterable of string names of metrics that should not be averaged over time. Metrics in this list will be displayed as-is. All others will be averaged by the progress bar before display, by default None.</p> <code>None</code> <code>always_stateful</code> <code>bool</code> <pre><code>Whether to set all metrics to be stateful, by default False.\n</code></pre> <code>False</code> <code>mode</code> <code>str</code> <p>Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".</p> <code>'train'</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>class ProgressBar:\n    \"\"\"\n    Keras style progress bar.\n\n    Adapted from https://github.com/yueyericardo/pkbar.\n\n    Parameters\n    ----------\n    max_value : Optional[int], optional\n        Maximum progress bar value, by default None.\n    epoch : Optional[int], optional\n        Zero-indexed current epoch, by default None.\n    num_epochs : Optional[int], optional\n        Total number of epochs, by default None.\n    stateful_metrics : Optional[list], optional\n        Iterable of string names of metrics that should *not* be averaged over time.\n        Metrics in this list will be displayed as-is. All others will be averaged by\n        the progress bar before display, by default None.\n    always_stateful : bool, optional\n            Whether to set all metrics to be stateful, by default False.\n    mode : str, optional\n        Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n    \"\"\"\n\n    def __init__(\n        self,\n        max_value: Optional[int] = None,\n        epoch: Optional[int] = None,\n        num_epochs: Optional[int] = None,\n        stateful_metrics: Optional[list] = None,\n        always_stateful: bool = False,\n        mode: str = \"train\",\n    ) -&gt; None:\n        \"\"\"\n        Constructor.\n\n        Parameters\n        ----------\n        max_value : Optional[int], optional\n            Maximum progress bar value, by default None.\n        epoch : Optional[int], optional\n            Zero-indexed current epoch, by default None.\n        num_epochs : Optional[int], optional\n            Total number of epochs, by default None.\n        stateful_metrics : Optional[list], optional\n            Iterable of string names of metrics that should *not* be averaged over time.\n            Metrics in this list will be displayed as-is. All others will be averaged by\n            the progress bar before display, by default None.\n        always_stateful : bool, optional\n             Whether to set all metrics to be stateful, by default False.\n        mode : str, optional\n            Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n        \"\"\"\n        self.max_value = max_value\n        # Width of the progress bar\n        self.width = 30\n        self.always_stateful = always_stateful\n\n        if (epoch is not None) and (num_epochs is not None):\n            print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()\n\n        self._dynamic_display = (\n            (hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty())\n            or \"ipykernel\" in sys.modules\n            or \"posix\" in sys.modules\n        )\n        self._total_width = 0\n        self._seen_so_far = 0\n        # We use a dict + list to avoid garbage collection\n        # issues found in OrderedDict\n        self._values: dict[Any, Any] = {}\n        self._values_order: list[Any] = []\n        self._start = time.time()\n        self._last_update = 0.0\n        self.spin = self.spinning_cursor() if self.max_value is None else None\n        if mode == \"train\" and self.max_value is None:\n            self.message = \"Estimating dataset size\"\n        elif mode == \"val\":\n            self.message = \"Validating\"\n        elif mode == \"predict\":\n            self.message = \"Denoising\"\n\n    def update(\n        self, current_step: int, batch_size: int = 1, values: Optional[list] = None\n    ) -&gt; None:\n        \"\"\"\n        Update the progress bar.\n\n        Parameters\n        ----------\n        current_step : int\n            Index of the current step.\n        batch_size : int, optional\n            Batch size, by default 1.\n        values : Optional[list], optional\n            Updated metrics values, by default None.\n        \"\"\"\n        values = values or []\n        for k, v in values:\n            # if torch tensor, convert it to numpy\n            if str(type(v)) == \"&lt;class 'torch.Tensor'&gt;\":\n                v = v.detach().cpu().numpy()\n\n            if k not in self._values_order:\n                self._values_order.append(k)\n            if k not in self.stateful_metrics and not self.always_stateful:\n                if k not in self._values:\n                    self._values[k] = [\n                        v * (current_step - self._seen_so_far),\n                        current_step - self._seen_so_far,\n                    ]\n                else:\n                    self._values[k][0] += v * (current_step - self._seen_so_far)\n                    self._values[k][1] += current_step - self._seen_so_far\n            else:\n                # Stateful metrics output a numeric value. This representation\n                # means \"take an average from a single value\" but keeps the\n                # numeric formatting.\n                self._values[k] = [v, 1]\n\n        self._seen_so_far = current_step\n\n        now = time.time()\n        info = f\" - {(now - self._start):.0f}s\"\n\n        prev_total_width = self._total_width\n        if self._dynamic_display:\n            sys.stdout.write(\"\\b\" * prev_total_width)\n            sys.stdout.write(\"\\r\")\n        else:\n            sys.stdout.write(\"\\n\")\n\n        if self.max_value is not None:\n            bar = f\"{current_step}/{self.max_value} [\"\n            progress = float(current_step) / self.max_value\n            progress_width = int(self.width * progress)\n            if progress_width &gt; 0:\n                bar += \"=\" * (progress_width - 1)\n                if current_step &lt; self.max_value:\n                    bar += \"&gt;\"\n                else:\n                    bar += \"=\"\n            bar += \".\" * (self.width - progress_width)\n            bar += \"]\"\n        else:\n            bar = (\n                f\"{self.message} {next(self.spin)}, tile \"  # type: ignore\n                f\"No. {current_step * batch_size}\"\n            )\n\n        self._total_width = len(bar)\n        sys.stdout.write(bar)\n\n        if current_step &gt; 0:\n            time_per_unit = (now - self._start) / current_step\n        else:\n            time_per_unit = 0\n\n        if time_per_unit &gt;= 1 or time_per_unit == 0:\n            info += f\" {time_per_unit:.0f}s/step\"\n        elif time_per_unit &gt;= 1e-3:\n            info += f\" {time_per_unit * 1e3:.0f}ms/step\"\n        else:\n            info += f\" {time_per_unit * 1e6:.0f}us/step\"\n\n        for k in self._values_order:\n            info += f\" - {k}:\"\n            if isinstance(self._values[k], list):\n                avg = self._values[k][0] / max(1, self._values[k][1])\n                if abs(avg) &gt; 1e-3:\n                    info += f\" {avg:.4f}\"\n                else:\n                    info += f\" {avg:.4e}\"\n            else:\n                info += f\" {self._values[k]}s\"\n\n        self._total_width += len(info)\n        if prev_total_width &gt; self._total_width:\n            info += \" \" * (prev_total_width - self._total_width)\n\n        if self.max_value is not None and current_step &gt;= self.max_value:\n            info += \"\\n\"\n\n        sys.stdout.write(info)\n        sys.stdout.flush()\n\n        self._last_update = now\n\n    def add(self, n: int, values: Optional[list] = None) -&gt; None:\n        \"\"\"\n        Update the progress bar by n steps.\n\n        Parameters\n        ----------\n        n : int\n            Number of steps to increase the progress bar with.\n        values : Optional[list], optional\n            Updated metrics values, by default None.\n        \"\"\"\n        self.update(self._seen_so_far + n, 1, values=values)\n\n    def spinning_cursor(self) -&gt; Generator:\n        \"\"\"\n        Generate a spinning cursor animation.\n\n        Taken from https://github.com/manrajgrover/py-spinners/tree/master.\n\n        Returns\n        -------\n        Generator\n            Generator of animation frames.\n        \"\"\"\n        while True:\n            yield from [\n                \"\u2593 ----- \u2592\",\n                \"\u2593 ----- \u2592\",\n                \"\u2593 ----- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 -&gt;--- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 --&gt;-- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ---&gt;- \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2593 ----&gt; \u2592\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 ----- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 -&gt;--- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 --&gt;-- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ---&gt;- \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n                \"\u2592 ----&gt; \u2591\",\n            ]\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.__init__","title":"<code>__init__(max_value=None, epoch=None, num_epochs=None, stateful_metrics=None, always_stateful=False, mode='train')</code>","text":"<p>Constructor.</p> <p>Parameters:</p> Name Type Description Default <code>max_value</code> <code>Optional[int]</code> <p>Maximum progress bar value, by default None.</p> <code>None</code> <code>epoch</code> <code>Optional[int]</code> <p>Zero-indexed current epoch, by default None.</p> <code>None</code> <code>num_epochs</code> <code>Optional[int]</code> <p>Total number of epochs, by default None.</p> <code>None</code> <code>stateful_metrics</code> <code>Optional[list]</code> <p>Iterable of string names of metrics that should not be averaged over time. Metrics in this list will be displayed as-is. All others will be averaged by the progress bar before display, by default None.</p> <code>None</code> <code>always_stateful</code> <code>bool</code> <p>Whether to set all metrics to be stateful, by default False.</p> <code>False</code> <code>mode</code> <code>str</code> <p>Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".</p> <code>'train'</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def __init__(\n    self,\n    max_value: Optional[int] = None,\n    epoch: Optional[int] = None,\n    num_epochs: Optional[int] = None,\n    stateful_metrics: Optional[list] = None,\n    always_stateful: bool = False,\n    mode: str = \"train\",\n) -&gt; None:\n    \"\"\"\n    Constructor.\n\n    Parameters\n    ----------\n    max_value : Optional[int], optional\n        Maximum progress bar value, by default None.\n    epoch : Optional[int], optional\n        Zero-indexed current epoch, by default None.\n    num_epochs : Optional[int], optional\n        Total number of epochs, by default None.\n    stateful_metrics : Optional[list], optional\n        Iterable of string names of metrics that should *not* be averaged over time.\n        Metrics in this list will be displayed as-is. All others will be averaged by\n        the progress bar before display, by default None.\n    always_stateful : bool, optional\n         Whether to set all metrics to be stateful, by default False.\n    mode : str, optional\n        Mode, one of \"train\", \"val\", or \"predict\", by default \"train\".\n    \"\"\"\n    self.max_value = max_value\n    # Width of the progress bar\n    self.width = 30\n    self.always_stateful = always_stateful\n\n    if (epoch is not None) and (num_epochs is not None):\n        print(f\"Epoch: {epoch + 1}/{num_epochs}\")\n\n    if stateful_metrics:\n        self.stateful_metrics = set(stateful_metrics)\n    else:\n        self.stateful_metrics = set()\n\n    self._dynamic_display = (\n        (hasattr(sys.stdout, \"isatty\") and sys.stdout.isatty())\n        or \"ipykernel\" in sys.modules\n        or \"posix\" in sys.modules\n    )\n    self._total_width = 0\n    self._seen_so_far = 0\n    # We use a dict + list to avoid garbage collection\n    # issues found in OrderedDict\n    self._values: dict[Any, Any] = {}\n    self._values_order: list[Any] = []\n    self._start = time.time()\n    self._last_update = 0.0\n    self.spin = self.spinning_cursor() if self.max_value is None else None\n    if mode == \"train\" and self.max_value is None:\n        self.message = \"Estimating dataset size\"\n    elif mode == \"val\":\n        self.message = \"Validating\"\n    elif mode == \"predict\":\n        self.message = \"Denoising\"\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.add","title":"<code>add(n, values=None)</code>","text":"<p>Update the progress bar by n steps.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of steps to increase the progress bar with.</p> required <code>values</code> <code>Optional[list]</code> <p>Updated metrics values, by default None.</p> <code>None</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def add(self, n: int, values: Optional[list] = None) -&gt; None:\n    \"\"\"\n    Update the progress bar by n steps.\n\n    Parameters\n    ----------\n    n : int\n        Number of steps to increase the progress bar with.\n    values : Optional[list], optional\n        Updated metrics values, by default None.\n    \"\"\"\n    self.update(self._seen_so_far + n, 1, values=values)\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.spinning_cursor","title":"<code>spinning_cursor()</code>","text":"<p>Generate a spinning cursor animation.</p> <p>Taken from https://github.com/manrajgrover/py-spinners/tree/master.</p> <p>Returns:</p> Type Description <code>Generator</code> <p>Generator of animation frames.</p> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def spinning_cursor(self) -&gt; Generator:\n    \"\"\"\n    Generate a spinning cursor animation.\n\n    Taken from https://github.com/manrajgrover/py-spinners/tree/master.\n\n    Returns\n    -------\n    Generator\n        Generator of animation frames.\n    \"\"\"\n    while True:\n        yield from [\n            \"\u2593 ----- \u2592\",\n            \"\u2593 ----- \u2592\",\n            \"\u2593 ----- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 -&gt;--- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 --&gt;-- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ---&gt;- \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2593 ----&gt; \u2592\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 ----- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 -&gt;--- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 --&gt;-- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ---&gt;- \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n            \"\u2592 ----&gt; \u2591\",\n        ]\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.ProgressBar.update","title":"<code>update(current_step, batch_size=1, values=None)</code>","text":"<p>Update the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>current_step</code> <code>int</code> <p>Index of the current step.</p> required <code>batch_size</code> <code>int</code> <p>Batch size, by default 1.</p> <code>1</code> <code>values</code> <code>Optional[list]</code> <p>Updated metrics values, by default None.</p> <code>None</code> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def update(\n    self, current_step: int, batch_size: int = 1, values: Optional[list] = None\n) -&gt; None:\n    \"\"\"\n    Update the progress bar.\n\n    Parameters\n    ----------\n    current_step : int\n        Index of the current step.\n    batch_size : int, optional\n        Batch size, by default 1.\n    values : Optional[list], optional\n        Updated metrics values, by default None.\n    \"\"\"\n    values = values or []\n    for k, v in values:\n        # if torch tensor, convert it to numpy\n        if str(type(v)) == \"&lt;class 'torch.Tensor'&gt;\":\n            v = v.detach().cpu().numpy()\n\n        if k not in self._values_order:\n            self._values_order.append(k)\n        if k not in self.stateful_metrics and not self.always_stateful:\n            if k not in self._values:\n                self._values[k] = [\n                    v * (current_step - self._seen_so_far),\n                    current_step - self._seen_so_far,\n                ]\n            else:\n                self._values[k][0] += v * (current_step - self._seen_so_far)\n                self._values[k][1] += current_step - self._seen_so_far\n        else:\n            # Stateful metrics output a numeric value. This representation\n            # means \"take an average from a single value\" but keeps the\n            # numeric formatting.\n            self._values[k] = [v, 1]\n\n    self._seen_so_far = current_step\n\n    now = time.time()\n    info = f\" - {(now - self._start):.0f}s\"\n\n    prev_total_width = self._total_width\n    if self._dynamic_display:\n        sys.stdout.write(\"\\b\" * prev_total_width)\n        sys.stdout.write(\"\\r\")\n    else:\n        sys.stdout.write(\"\\n\")\n\n    if self.max_value is not None:\n        bar = f\"{current_step}/{self.max_value} [\"\n        progress = float(current_step) / self.max_value\n        progress_width = int(self.width * progress)\n        if progress_width &gt; 0:\n            bar += \"=\" * (progress_width - 1)\n            if current_step &lt; self.max_value:\n                bar += \"&gt;\"\n            else:\n                bar += \"=\"\n        bar += \".\" * (self.width - progress_width)\n        bar += \"]\"\n    else:\n        bar = (\n            f\"{self.message} {next(self.spin)}, tile \"  # type: ignore\n            f\"No. {current_step * batch_size}\"\n        )\n\n    self._total_width = len(bar)\n    sys.stdout.write(bar)\n\n    if current_step &gt; 0:\n        time_per_unit = (now - self._start) / current_step\n    else:\n        time_per_unit = 0\n\n    if time_per_unit &gt;= 1 or time_per_unit == 0:\n        info += f\" {time_per_unit:.0f}s/step\"\n    elif time_per_unit &gt;= 1e-3:\n        info += f\" {time_per_unit * 1e3:.0f}ms/step\"\n    else:\n        info += f\" {time_per_unit * 1e6:.0f}us/step\"\n\n    for k in self._values_order:\n        info += f\" - {k}:\"\n        if isinstance(self._values[k], list):\n            avg = self._values[k][0] / max(1, self._values[k][1])\n            if abs(avg) &gt; 1e-3:\n                info += f\" {avg:.4f}\"\n            else:\n                info += f\" {avg:.4e}\"\n        else:\n            info += f\" {self._values[k]}s\"\n\n    self._total_width += len(info)\n    if prev_total_width &gt; self._total_width:\n        info += \" \" * (prev_total_width - self._total_width)\n\n    if self.max_value is not None and current_step &gt;= self.max_value:\n        info += \"\\n\"\n\n    sys.stdout.write(info)\n    sys.stdout.flush()\n\n    self._last_update = now\n</code></pre>"},{"location":"reference/careamics/utils/logging/#careamics.utils.logging.get_logger","title":"<code>get_logger(name, log_level=logging.INFO, log_path=None)</code>","text":"<p>Create a python logger instance with configured handlers.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger.</p> required <code>log_level</code> <code>int</code> <p>Log level (info, error etc.), by default logging.INFO.</p> <code>INFO</code> <code>log_path</code> <code>Optional[Union[str, Path]]</code> <p>Path in which to save the log, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Logger.</p> Source code in <code>src/careamics/utils/logging.py</code> <pre><code>def get_logger(\n    name: str,\n    log_level: int = logging.INFO,\n    log_path: Optional[Union[str, Path]] = None,\n) -&gt; logging.Logger:\n    \"\"\"\n    Create a python logger instance with configured handlers.\n\n    Parameters\n    ----------\n    name : str\n        Name of the logger.\n    log_level : int, optional\n        Log level (info, error etc.), by default logging.INFO.\n    log_path : Optional[Union[str, Path]], optional\n        Path in which to save the log, by default None.\n\n    Returns\n    -------\n    logging.Logger\n        Logger.\n    \"\"\"\n    logger = logging.getLogger(name)\n    logger.propagate = False\n\n    if name in LOGGERS:\n        return logger\n\n    for logger_name in LOGGERS:\n        if name.startswith(logger_name):\n            return logger\n\n    logger.propagate = False\n\n    if log_path:\n        handlers = [\n            logging.StreamHandler(),\n            logging.FileHandler(log_path),\n        ]\n    else:\n        handlers = [logging.StreamHandler()]\n\n    formatter = logging.Formatter(\"%(message)s\")\n\n    for handler in handlers:\n        handler.setFormatter(formatter)  # type: ignore\n        handler.setLevel(log_level)  # type: ignore\n        logger.addHandler(handler)  # type: ignore\n\n    logger.setLevel(log_level)\n    LOGGERS[name] = True\n\n    logger.propagate = False\n\n    return logger\n</code></pre>"},{"location":"reference/careamics/utils/metrics/","title":"metrics","text":"<p>Metrics submodule.</p> <p>This module contains various metrics and a metrics tracking class.</p>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR","title":"<code>RunningPSNR</code>","text":"<p>Compute the running PSNR during validation step in training.</p> <p>This class allows to compute the PSNR on the entire validation set one batch at the time.</p> <p>Attributes:</p> Name Type Description <code>N</code> <code>int</code> <p>Number of elements seen so far during the epoch.</p> <code>mse_sum</code> <code>float</code> <p>Running sum of the MSE over the N elements seen so far.</p> <code>max</code> <code>float</code> <p>Running max value of the N target images seen so far.</p> <code>min</code> <code>float</code> <p>Running min value of the N target images seen so far.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>class RunningPSNR:\n    \"\"\"Compute the running PSNR during validation step in training.\n\n    This class allows to compute the PSNR on the entire validation set\n    one batch at the time.\n\n    Attributes\n    ----------\n    N : int\n        Number of elements seen so far during the epoch.\n    mse_sum : float\n        Running sum of the MSE over the N elements seen so far.\n    max : float\n        Running max value of the N target images seen so far.\n    min : float\n        Running min value of the N target images seen so far.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Constructor.\"\"\"\n        self.N = None\n        self.mse_sum = None\n        self.max = self.min = None\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the running PSNR computation.\n\n        Usually called at the end of each epoch.\n        \"\"\"\n        self.mse_sum = 0\n        self.N = 0\n        self.max = self.min = None\n\n    def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n        \"\"\"Update the running PSNR statistics given a new batch.\n\n        Parameters\n        ----------\n        rec : torch.Tensor\n            Reconstructed batch.\n        tar : torch.Tensor\n            Target batch.\n        \"\"\"\n        ins_max = torch.max(tar).item()\n        ins_min = torch.min(tar).item()\n        if self.max is None:\n            assert self.min is None\n            self.max = ins_max\n            self.min = ins_min\n        else:\n            self.max = max(self.max, ins_max)\n            self.min = min(self.min, ins_min)\n\n        mse = (rec - tar) ** 2\n        elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n        self.mse_sum += torch.nansum(elementwise_mse)\n        self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n\n    def get(self) -&gt; Optional[torch.Tensor]:\n        \"\"\"Get the actual PSNR value given the running statistics.\n\n        Returns\n        -------\n        Optional[torch.Tensor]\n            PSNR value.\n        \"\"\"\n        if self.N == 0 or self.N is None:\n            return None\n        rmse = torch.sqrt(self.mse_sum / self.N)\n        return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.__init__","title":"<code>__init__()</code>","text":"<p>Constructor.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    self.N = None\n    self.mse_sum = None\n    self.max = self.min = None\n    self.reset()\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.get","title":"<code>get()</code>","text":"<p>Get the actual PSNR value given the running statistics.</p> <p>Returns:</p> Type Description <code>Optional[Tensor]</code> <p>PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def get(self) -&gt; Optional[torch.Tensor]:\n    \"\"\"Get the actual PSNR value given the running statistics.\n\n    Returns\n    -------\n    Optional[torch.Tensor]\n        PSNR value.\n    \"\"\"\n    if self.N == 0 or self.N is None:\n        return None\n    rmse = torch.sqrt(self.mse_sum / self.N)\n    return 20 * torch.log10((self.max - self.min) / rmse)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.reset","title":"<code>reset()</code>","text":"<p>Reset the running PSNR computation.</p> <p>Usually called at the end of each epoch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def reset(self):\n    \"\"\"Reset the running PSNR computation.\n\n    Usually called at the end of each epoch.\n    \"\"\"\n    self.mse_sum = 0\n    self.N = 0\n    self.max = self.min = None\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.RunningPSNR.update","title":"<code>update(rec, tar)</code>","text":"<p>Update the running PSNR statistics given a new batch.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>Tensor</code> <p>Reconstructed batch.</p> required <code>tar</code> <code>Tensor</code> <p>Target batch.</p> required Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def update(self, rec: torch.Tensor, tar: torch.Tensor) -&gt; None:\n    \"\"\"Update the running PSNR statistics given a new batch.\n\n    Parameters\n    ----------\n    rec : torch.Tensor\n        Reconstructed batch.\n    tar : torch.Tensor\n        Target batch.\n    \"\"\"\n    ins_max = torch.max(tar).item()\n    ins_min = torch.min(tar).item()\n    if self.max is None:\n        assert self.min is None\n        self.max = ins_max\n        self.min = ins_min\n    else:\n        self.max = max(self.max, ins_max)\n        self.min = min(self.min, ins_min)\n\n    mse = (rec - tar) ** 2\n    elementwise_mse = torch.mean(mse.view(len(mse), -1), dim=1)\n    self.mse_sum += torch.nansum(elementwise_mse)\n    self.N += len(elementwise_mse) - torch.sum(torch.isnan(elementwise_mse))\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._avg_psnr","title":"<code>_avg_psnr(target, prediction, psnr_fn)</code>","text":"<p>Compute the average PSNR over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <code>psnr_fn</code> <code>Callable</code> <p>PSNR function to use.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average PSNR value over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def _avg_psnr(target: np.ndarray, prediction: np.ndarray, psnr_fn: Callable) -&gt; float:\n    \"\"\"Compute the average PSNR over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n    psnr_fn : Callable\n        PSNR function to use.\n\n    Returns\n    -------\n    float\n        Average PSNR value over the batch.\n    \"\"\"\n    return np.mean(\n        [\n            psnr_fn(target[i : i + 1], prediction[i : i + 1]).item()\n            for i in range(len(prediction))\n        ]\n    )\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._fix","title":"<code>_fix(gt, x)</code>","text":"<p>Zero mean a groud truth array and adjust the range of the array.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>Union[ndarray, Tensor]</code> <p>Ground truth image.</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Zero-mean and range-adjusted array.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def _fix(\n    gt: Union[np.ndarray, torch.Tensor], x: Union[np.ndarray, torch.Tensor]\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Zero mean a groud truth array and adjust the range of the array.\n\n    Parameters\n    ----------\n    gt : Union[np.ndarray, torch.Tensor]\n        Ground truth image.\n    x : Union[np.ndarray, torch.Tensor]\n        Input array.\n\n    Returns\n    -------\n    Union[np.ndarray, torch.Tensor]\n        Zero-mean and range-adjusted array.\n    \"\"\"\n    gt_ = _zero_mean(gt)\n    return _fix_range(gt_, _zero_mean(x))\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._fix_range","title":"<code>_fix_range(gt, x)</code>","text":"<p>Adjust the range of an array based on a reference ground-truth array.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>Union[ndarray, Tensor]</code> <p>Ground truth array.</p> required <code>x</code> <code>Union[ndarray, Tensor]</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>Union[ndarray, Tensor]</code> <p>Range-adjusted array.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def _fix_range(\n    gt: Union[np.ndarray, torch.Tensor], x: Union[np.ndarray, torch.Tensor]\n) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Adjust the range of an array based on a reference ground-truth array.\n\n    Parameters\n    ----------\n    gt : Union[np.ndarray, torch.Tensor]\n        Ground truth array.\n    x : Union[np.ndarray, torch.Tensor]\n        Input array.\n\n    Returns\n    -------\n    Union[np.ndarray, torch.Tensor]\n        Range-adjusted array.\n    \"\"\"\n    a = (gt * x).sum() / (x * x).sum()\n    return x * a\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._range_invariant_multiscale_ssim","title":"<code>_range_invariant_multiscale_ssim(gt_, pred_)</code>","text":"<p>Compute range invariant multiscale SSIM for a single channel.</p> <p>The advantage of this metric in comparison to commonly used SSIM is that it is invariant to scalar multiplications in the prediction.</p>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._range_invariant_multiscale_ssim--todo-add-reference-to-the-paper","title":"TODO: Add reference to the paper.","text":"<p>NOTE: images fed to this function should have channels dimension as the last one.</p> <p>Parameters:</p> Name Type Description Default <code>gt_</code> <code>Union[ndarray, Tensor]</code> <p>Ground truth image with shape (N, H, W).</p> required <code>pred_</code> <code>Union[ndarray, Tensor]</code> <p>Predicted image with shape (N, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Range invariant multiscale SSIM value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def _range_invariant_multiscale_ssim(\n    gt_: Union[np.ndarray, torch.Tensor], pred_: Union[np.ndarray, torch.Tensor]\n) -&gt; float:\n    \"\"\"Compute range invariant multiscale SSIM for a single channel.\n\n    The advantage of this metric in comparison to commonly used SSIM is that\n    it is invariant to scalar multiplications in the prediction.\n    # TODO: Add reference to the paper.\n\n    NOTE: images fed to this function should have channels dimension as the last one.\n\n    Parameters\n    ----------\n    gt_ : Union[np.ndarray, torch.Tensor]\n        Ground truth image with shape (N, H, W).\n    pred_ : Union[np.ndarray, torch.Tensor]\n        Predicted image with shape (N, H, W).\n\n    Returns\n    -------\n    float\n        Range invariant multiscale SSIM value.\n    \"\"\"\n    shape = gt_.shape\n    gt_ = torch.Tensor(gt_.reshape((shape[0], -1)))\n    pred_ = torch.Tensor(pred_.reshape((shape[0], -1)))\n    gt_ = _zero_mean(gt_)\n    pred_ = _zero_mean(pred_)\n    pred_ = _fix(gt_, pred_)\n    pred_ = pred_.reshape(shape)\n    gt_ = gt_.reshape(shape)\n\n    ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n        data_range=gt_.max() - gt_.min()\n    )\n    return ms_ssim(torch.Tensor(pred_[:, None]), torch.Tensor(gt_[:, None])).item()\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics._zero_mean","title":"<code>_zero_mean(x)</code>","text":"<p>Zero the mean of an array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray or Tensor</code> <p>Input array.</p> required <p>Returns:</p> Type Description <code>ndarray or Tensor</code> <p>Zero-mean array.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def _zero_mean(x: Union[np.ndarray, torch.Tensor]) -&gt; Union[np.ndarray, torch.Tensor]:\n    \"\"\"\n    Zero the mean of an array.\n\n    Parameters\n    ----------\n    x : numpy.ndarray or torch.Tensor\n        Input array.\n\n    Returns\n    -------\n    numpy.ndarray or torch.Tensor\n        Zero-mean array.\n    \"\"\"\n    return x - x.mean()\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_psnr","title":"<code>avg_psnr(target, prediction)</code>","text":"<p>Compute the average PSNR over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average PSNR value over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_psnr(target: np.ndarray, prediction: np.ndarray) -&gt; float:\n    \"\"\"Compute the average PSNR over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    float\n        Average PSNR value over the batch.\n    \"\"\"\n    return _avg_psnr(target, prediction, psnr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_range_inv_psnr","title":"<code>avg_range_inv_psnr(target, prediction)</code>","text":"<p>Compute the average range-invariant PSNR over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average range-invariant PSNR value over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_range_inv_psnr(target: np.ndarray, prediction: np.ndarray) -&gt; float:\n    \"\"\"Compute the average range-invariant PSNR over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    float\n        Average range-invariant PSNR value over the batch.\n    \"\"\"\n    return _avg_psnr(target, prediction, scale_invariant_psnr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_range_invariant_psnr","title":"<code>avg_range_invariant_psnr(pred, target)</code>","text":"<p>Compute the average range-invariant PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>ndarray</code> <p>Predicted images.</p> required <code>target</code> <code>ndarray</code> <p>Target images.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average range-invariant PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_range_invariant_psnr(\n    pred: np.ndarray,\n    target: np.ndarray,\n) -&gt; float:\n    \"\"\"Compute the average range-invariant PSNR.\n\n    Parameters\n    ----------\n    pred : np.ndarray\n        Predicted images.\n    target : np.ndarray\n        Target images.\n\n    Returns\n    -------\n    float\n        Average range-invariant PSNR value.\n    \"\"\"\n    psnr_arr = []\n    for i in range(pred.shape[0]):\n        psnr_arr.append(scale_invariant_psnr(pred[i], target[i]))\n    return np.mean(psnr_arr)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.avg_ssim","title":"<code>avg_ssim(target, prediction)</code>","text":"<p>Compute the average Structural Similarity (SSIM) over a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>ndarray</code> <p>Array of ground truth images, shape is (N, C, H, W).</p> required <code>prediction</code> <code>ndarray</code> <p>Array of predicted images, shape is (N, C, H, W).</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>Mean and standard deviation of SSIM values over the batch.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def avg_ssim(\n    target: Union[np.ndarray, torch.Tensor], prediction: Union[np.ndarray, torch.Tensor]\n) -&gt; tuple[float, float]:\n    \"\"\"Compute the average Structural Similarity (SSIM) over a batch of images.\n\n    Parameters\n    ----------\n    target : np.ndarray\n        Array of ground truth images, shape is (N, C, H, W).\n    prediction : np.ndarray\n        Array of predicted images, shape is (N, C, H, W).\n\n    Returns\n    -------\n    tuple[float, float]\n        Mean and standard deviation of SSIM values over the batch.\n    \"\"\"\n    ssim = [\n        structural_similarity(\n            target[i], prediction[i], data_range=(target[i].max() - target[i].min())\n        )\n        for i in range(len(target))\n    ]\n    return np.mean(ssim), np.std(ssim)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.multiscale_ssim","title":"<code>multiscale_ssim(gt_, pred_, range_invariant=True)</code>","text":"<p>Compute channel-wise multiscale SSIM for each channel.</p> <p>It allows to use either standard multiscale SSIM or its range-invariant version.</p> <p>NOTE: images fed to this function should have channels dimension as the last one.</p>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.multiscale_ssim--todo-do-we-want-to-allow-this-behavior-or-we-want-the-usual-n-c-h-w","title":"TODO: do we want to allow this behavior? or we want the usual (N, C, H, W)?","text":"<p>Parameters:</p> Name Type Description Default <code>gt_</code> <code>Union[ndarray, Tensor]</code> <p>Ground truth image with shape (N, H, W, C).</p> required <code>pred_</code> <code>Union[ndarray, Tensor]</code> <p>Predicted image with shape (N, H, W, C).</p> required <code>range_invariant</code> <code>bool</code> <p>Whether to use standard or range invariant multiscale SSIM.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of SSIM values for each channel.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def multiscale_ssim(\n    gt_: Union[np.ndarray, torch.Tensor],\n    pred_: Union[np.ndarray, torch.Tensor],\n    range_invariant: bool = True,\n) -&gt; list[Union[float, None]]:\n    \"\"\"Compute channel-wise multiscale SSIM for each channel.\n\n    It allows to use either standard multiscale SSIM or its range-invariant version.\n\n    NOTE: images fed to this function should have channels dimension as the last one.\n    # TODO: do we want to allow this behavior? or we want the usual (N, C, H, W)?\n\n    Parameters\n    ----------\n    gt_ : Union[np.ndarray, torch.Tensor]\n        Ground truth image with shape (N, H, W, C).\n    pred_ : Union[np.ndarray, torch.Tensor]\n        Predicted image with shape (N, H, W, C).\n    range_invariant : bool\n        Whether to use standard or range invariant multiscale SSIM.\n\n    Returns\n    -------\n    list[float]\n        List of SSIM values for each channel.\n    \"\"\"\n    ms_ssim_values = {}\n    for ch_idx in range(gt_.shape[-1]):\n        tar_tmp = gt_[..., ch_idx]\n        pred_tmp = pred_[..., ch_idx]\n        if range_invariant:\n            ms_ssim_values[ch_idx] = _range_invariant_multiscale_ssim(\n                gt_=tar_tmp, pred_=pred_tmp\n            )\n        else:\n            ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(\n                data_range=tar_tmp.max() - tar_tmp.min()\n            )\n            ms_ssim_values[ch_idx] = ms_ssim(\n                torch.Tensor(pred_tmp[:, None]), torch.Tensor(tar_tmp[:, None])\n            ).item()\n\n    return [ms_ssim_values[i] for i in range(gt_.shape[-1])]  # type: ignore\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.psnr","title":"<code>psnr(gt, pred, data_range)</code>","text":"<p>Peak Signal to Noise Ratio.</p> <p>This method calls skimage.metrics.peak_signal_noise_ratio. See: https://scikit-image.org/docs/dev/api/skimage.metrics.html.</p> <p>NOTE: to avoid unwanted behaviors (e.g., data_range inferred from array dtype), the data_range parameter is mandatory.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>ndarray</code> <p>Ground truth array.</p> required <code>pred</code> <code>ndarray</code> <p>Predicted array.</p> required <code>data_range</code> <code>float</code> <p>The images pixel range.</p> required <p>Returns:</p> Type Description <code>float</code> <p>PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def psnr(gt: np.ndarray, pred: np.ndarray, data_range: float) -&gt; float:\n    \"\"\"\n    Peak Signal to Noise Ratio.\n\n    This method calls skimage.metrics.peak_signal_noise_ratio. See:\n    https://scikit-image.org/docs/dev/api/skimage.metrics.html.\n\n    NOTE: to avoid unwanted behaviors (e.g., data_range inferred from array dtype),\n    the data_range parameter is mandatory.\n\n    Parameters\n    ----------\n    gt : np.ndarray\n        Ground truth array.\n    pred : np.ndarray\n        Predicted array.\n    data_range : float\n        The images pixel range.\n\n    Returns\n    -------\n    float\n        PSNR value.\n    \"\"\"\n    return peak_signal_noise_ratio(gt, pred, data_range=data_range)\n</code></pre>"},{"location":"reference/careamics/utils/metrics/#careamics.utils.metrics.scale_invariant_psnr","title":"<code>scale_invariant_psnr(gt, pred)</code>","text":"<p>Scale invariant PSNR.</p> <p>Parameters:</p> Name Type Description Default <code>gt</code> <code>ndarray</code> <p>Ground truth image.</p> required <code>pred</code> <code>ndarray</code> <p>Predicted image.</p> required <p>Returns:</p> Type Description <code>Union[float, tensor]</code> <p>Scale invariant PSNR value.</p> Source code in <code>src/careamics/utils/metrics.py</code> <pre><code>def scale_invariant_psnr(\n    gt: np.ndarray, pred: np.ndarray\n) -&gt; Union[float, torch.tensor]:\n    \"\"\"\n    Scale invariant PSNR.\n\n    Parameters\n    ----------\n    gt : np.ndarray\n        Ground truth image.\n    pred : np.ndarray\n        Predicted image.\n\n    Returns\n    -------\n    Union[float, torch.tensor]\n        Scale invariant PSNR value.\n    \"\"\"\n    range_parameter = (np.max(gt) - np.min(gt)) / np.std(gt)\n    gt_ = _zero_mean(gt) / np.std(gt)\n    return psnr(_zero_mean(gt_), _fix(gt_, pred), range_parameter)\n</code></pre>"},{"location":"reference/careamics/utils/path_utils/","title":"path_utils","text":"<p>Utility functions for paths.</p>"},{"location":"reference/careamics/utils/path_utils/#careamics.utils.path_utils.check_path_exists","title":"<code>check_path_exists(path)</code>","text":"<p>Check if a path exists. If not, raise an error.</p> <p>Note that it returns <code>path</code> as a Path object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path as a Path object.</p> Source code in <code>src/careamics/utils/path_utils.py</code> <pre><code>def check_path_exists(path: Union[str, Path]) -&gt; Path:\n    \"\"\"Check if a path exists. If not, raise an error.\n\n    Note that it returns `path` as a Path object.\n\n    Parameters\n    ----------\n    path : Union[str, Path]\n        Path to check.\n\n    Returns\n    -------\n    Path\n        Path as a Path object.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Data path {path} is incorrect or does not exist.\")\n\n    return path\n</code></pre>"},{"location":"reference/careamics/utils/plotting/","title":"plotting","text":"<p>Plotting utilities.</p>"},{"location":"reference/careamics/utils/plotting/#careamics.utils.plotting.plot_noise_model_probability_distribution","title":"<code>plot_noise_model_probability_distribution(noise_model, signalBinIndex, histogram, channel=None, number_of_bins=100)</code>","text":"<p>Plot probability distribution P(x|s) for a certain ground truth signal.</p> <p>Predictions from both Histogram and GMM-based Noise models are displayed for comparison.</p> <p>Parameters:</p> Name Type Description Default <code>noise_model</code> <code>GaussianMixtureNoiseModel</code> <p>Trained GaussianMixtureNoiseModel.</p> required <code>signalBinIndex</code> <code>int</code> <p>Index of signal bin. Values go from 0 to number of bins (<code>n_bin</code>).</p> required <code>histogram</code> <code>NDArray</code> <p>Histogram based noise model.</p> required <code>channel</code> <code>Optional[str]</code> <p>Channel name used for plotting. Default is None.</p> <code>None</code> <code>number_of_bins</code> <code>int</code> <p>Number of bins in the resulting histogram. Default is 100.</p> <code>100</code> Source code in <code>src/careamics/utils/plotting.py</code> <pre><code>def plot_noise_model_probability_distribution(\n    noise_model: GaussianMixtureNoiseModel,\n    signalBinIndex: int,\n    histogram: NDArray,\n    channel: Optional[str] = None,\n    number_of_bins: int = 100,\n) -&gt; None:\n    \"\"\"Plot probability distribution P(x|s) for a certain ground truth signal.\n\n    Predictions from both Histogram and GMM-based\n    Noise models are displayed for comparison.\n\n    Parameters\n    ----------\n    noise_model : GaussianMixtureNoiseModel\n        Trained GaussianMixtureNoiseModel.\n    signalBinIndex : int\n        Index of signal bin. Values go from 0 to number of bins (`n_bin`).\n    histogram : NDArray\n        Histogram based noise model.\n    channel : Optional[str], optional\n        Channel name used for plotting. Default is None.\n    number_of_bins : int, optional\n        Number of bins in the resulting histogram. Default is 100.\n    \"\"\"\n    min_signal = noise_model.min_signal.item()\n    max_signal = noise_model.max_signal.item()\n    bin_size = (max_signal - min_signal) / number_of_bins\n\n    query_signal_normalized = signalBinIndex / number_of_bins\n    query_signal = query_signal_normalized * (max_signal - min_signal) + min_signal\n    query_signal += bin_size / 2\n    query_signal = torch.tensor(query_signal)\n\n    query_observations = torch.arange(min_signal, max_signal, bin_size)\n    query_observations += bin_size / 2\n\n    likelihoods = noise_model.likelihood(\n        observations=query_observations, signals=query_signal\n    ).numpy()\n\n    plt.figure(figsize=(12, 5))\n    if channel:\n        plt.suptitle(f\"Noise model for channel {channel}\")\n    else:\n        plt.suptitle(\"Noise model\")\n\n    plt.subplot(1, 2, 1)\n    plt.xlabel(\"Observation Bin\")\n    plt.ylabel(\"Signal Bin\")\n    plt.imshow(histogram**0.25, cmap=\"gray\")\n    plt.axhline(y=signalBinIndex + 0.5, linewidth=5, color=\"blue\", alpha=0.5)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(\n        query_observations,\n        likelihoods,\n        label=\"GMM : \" + \" signal = \" + str(np.round(query_signal, 2)),\n        marker=\".\",\n        color=\"red\",\n        linewidth=2,\n    )\n    plt.xlabel(\"Observations (x) for signal s = \" + str(query_signal))\n    plt.ylabel(\"Probability Density\")\n    plt.title(\"Probability Distribution P(x|s) at signal =\" + str(query_signal))\n    plt.legend()\n</code></pre>"},{"location":"reference/careamics/utils/ram/","title":"ram","text":"<p>Utility function to get RAM size.</p>"},{"location":"reference/careamics/utils/ram/#careamics.utils.ram.get_ram_size","title":"<code>get_ram_size()</code>","text":"<p>Get RAM size in mbytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>RAM size in mbytes.</p> Source code in <code>src/careamics/utils/ram.py</code> <pre><code>def get_ram_size() -&gt; int:\n    \"\"\"\n    Get RAM size in mbytes.\n\n    Returns\n    -------\n    int\n        RAM size in mbytes.\n    \"\"\"\n    return psutil.virtual_memory().available / 1024**2\n</code></pre>"},{"location":"reference/careamics/utils/receptive_field/","title":"receptive_field","text":"<p>Receptive field calculation for computing the tile overlap.</p>"},{"location":"reference/careamics/utils/serializers/","title":"serializers","text":"<p>A script for serializers in the careamics package.</p>"},{"location":"reference/careamics/utils/serializers/#careamics.utils.serializers._array_to_json","title":"<code>_array_to_json(arr)</code>","text":"<p>Convert an array to a list and then to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>Union[ndarray, Tensor]</code> <p>Array to be serialized.</p> required <p>Returns:</p> Type Description <code>str</code> <p>JSON string representing the array.</p> Source code in <code>src/careamics/utils/serializers.py</code> <pre><code>def _array_to_json(arr: Union[np.ndarray, torch.Tensor]) -&gt; str:\n    \"\"\"Convert an array to a list and then to a JSON string.\n\n    Parameters\n    ----------\n    arr : Union[np.ndarray, torch.Tensor]\n        Array to be serialized.\n\n    Returns\n    -------\n    str\n        JSON string representing the array.\n    \"\"\"\n    if isinstance(arr, str):\n        return arr\n    return json.dumps(arr.tolist())\n</code></pre>"},{"location":"reference/careamics/utils/serializers/#careamics.utils.serializers._to_numpy","title":"<code>_to_numpy(lst)</code>","text":"<p>Deserialize a list or string representing a list into <code>np.ndarray</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>List or string representing a list with the array content to be deserialized.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The deserialized array.</p> Source code in <code>src/careamics/utils/serializers.py</code> <pre><code>def _to_numpy(lst: Union[str, list]) -&gt; np.ndarray:\n    \"\"\"Deserialize a list or string representing a list into `np.ndarray`.\n\n    Parameters\n    ----------\n    lst : list\n        List or string representing a list with the array content to be deserialized.\n\n    Returns\n    -------\n    np.ndarray\n        The deserialized array.\n    \"\"\"\n    if isinstance(lst, str):\n        lst = ast.literal_eval(lst)\n    return np.asarray(lst)\n</code></pre>"},{"location":"reference/careamics/utils/serializers/#careamics.utils.serializers._to_torch","title":"<code>_to_torch(lst)</code>","text":"<p>Deserialize list or string representing a list into <code>torch.Tensor</code>.</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>Union[str, list]</code> <p>List or string representing a list swith the array content to be deserialized.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The deserialized tensor.</p> Source code in <code>src/careamics/utils/serializers.py</code> <pre><code>def _to_torch(lst: Union[str, list]) -&gt; torch.Tensor:\n    \"\"\"Deserialize list or string representing a list into `torch.Tensor`.\n\n    Parameters\n    ----------\n    lst : Union[str, list]\n        List or string representing a list swith the array content to be deserialized.\n\n    Returns\n    -------\n    torch.Tensor\n        The deserialized tensor.\n    \"\"\"\n    if isinstance(lst, str):\n        lst = ast.literal_eval(lst)\n    return torch.tensor(lst)\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/","title":"torch_utils","text":"<p>Convenience functions using torch.</p> <p>These functions are used to control certain aspects and behaviours of PyTorch.</p>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.filter_parameters","title":"<code>filter_parameters(func, user_params)</code>","text":"<p>Filter parameters according to the function signature.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>type</code> <p>Class object.</p> required <code>user_params</code> <code>dict</code> <p>User provided parameters.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Parameters matching <code>func</code>'s signature.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def filter_parameters(\n    func: type,\n    user_params: dict,\n) -&gt; dict:\n    \"\"\"\n    Filter parameters according to the function signature.\n\n    Parameters\n    ----------\n    func : type\n        Class object.\n    user_params : dict\n        User provided parameters.\n\n    Returns\n    -------\n    dict\n        Parameters matching `func`'s signature.\n    \"\"\"\n    # Get the list of all default parameters\n    default_params = list(inspect.signature(func).parameters.keys())\n\n    # Filter matching parameters\n    params_to_be_used = set(user_params.keys()) &amp; set(default_params)\n\n    return {key: user_params[key] for key in params_to_be_used}\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_optimizer","title":"<code>get_optimizer(name)</code>","text":"<p>Return the optimizer class given its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Optimizer name.</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>Optimizer class.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_optimizer(name: str) -&gt; torch.optim.Optimizer:\n    \"\"\"\n    Return the optimizer class given its name.\n\n    Parameters\n    ----------\n    name : str\n        Optimizer name.\n\n    Returns\n    -------\n    torch.nn.Optimizer\n        Optimizer class.\n    \"\"\"\n    if name not in SupportedOptimizer:\n        raise NotImplementedError(f\"Optimizer {name} is not yet supported.\")\n\n    return getattr(torch.optim, name)\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_optimizers","title":"<code>get_optimizers()</code>","text":"<p>Return the list of all optimizers available in torch.optim.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Optimizers available in torch.optim.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_optimizers() -&gt; dict[str, str]:\n    \"\"\"\n    Return the list of all optimizers available in torch.optim.\n\n    Returns\n    -------\n    dict\n        Optimizers available in torch.optim.\n    \"\"\"\n    optims = {}\n    for name, obj in inspect.getmembers(torch.optim):\n        if inspect.isclass(obj) and issubclass(obj, torch.optim.Optimizer):\n            if name != \"Optimizer\":\n                optims[name] = name\n    return optims\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_scheduler","title":"<code>get_scheduler(name)</code>","text":"<p>Return the scheduler class given its name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Scheduler name.</p> required <p>Returns:</p> Type Description <code>Union</code> <p>Scheduler class.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_scheduler(\n    name: str,\n) -&gt; Union[\n    # torch.optim.lr_scheduler.LRScheduler,\n    torch.optim.lr_scheduler.ReduceLROnPlateau,\n]:\n    \"\"\"\n    Return the scheduler class given its name.\n\n    Parameters\n    ----------\n    name : str\n        Scheduler name.\n\n    Returns\n    -------\n    Union\n        Scheduler class.\n    \"\"\"\n    if name not in SupportedScheduler:\n        raise NotImplementedError(f\"Scheduler {name} is not yet supported.\")\n\n    return getattr(torch.optim.lr_scheduler, name)\n</code></pre>"},{"location":"reference/careamics/utils/torch_utils/#careamics.utils.torch_utils.get_schedulers","title":"<code>get_schedulers()</code>","text":"<p>Return the list of all schedulers available in torch.optim.lr_scheduler.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Schedulers available in torch.optim.lr_scheduler.</p> Source code in <code>src/careamics/utils/torch_utils.py</code> <pre><code>def get_schedulers() -&gt; dict[str, str]:\n    \"\"\"\n    Return the list of all schedulers available in torch.optim.lr_scheduler.\n\n    Returns\n    -------\n    dict\n        Schedulers available in torch.optim.lr_scheduler.\n    \"\"\"\n    schedulers = {}\n    for name, obj in inspect.getmembers(torch.optim.lr_scheduler):\n        if inspect.isclass(obj) and issubclass(\n            obj, torch.optim.lr_scheduler.LRScheduler\n        ):\n            if \"LRScheduler\" not in name:\n                schedulers[name] = name\n        elif name == \"ReduceLROnPlateau\":  # somewhat not a subclass of LRScheduler\n            schedulers[name] = name\n    return schedulers\n</code></pre>"},{"location":"reference/careamics/utils/version/","title":"version","text":"<p>Version utility.</p>"},{"location":"reference/careamics/utils/version/#careamics.utils.version.get_careamics_version","title":"<code>get_careamics_version()</code>","text":"<p>Get clean CAREamics version.</p> <p>This method returns the latest <code>Major.Minor.Patch</code> version of CAREamics, removing any local version identifier.</p> <p>Returns:</p> Type Description <code>str</code> <p>Clean CAREamics version.</p> Source code in <code>src/careamics/utils/version.py</code> <pre><code>def get_careamics_version() -&gt; str:\n    \"\"\"Get clean CAREamics version.\n\n    This method returns the latest `Major.Minor.Patch` version of CAREamics, removing\n    any local version identifier.\n\n    Returns\n    -------\n    str\n        Clean CAREamics version.\n    \"\"\"\n    parts = __version__.split(\".\")\n\n    # for local installs that do not detect the latest versions via tags\n    # (typically our CI will install `0.1.devX&lt;hash&gt;` versions)\n    if \"dev\" in parts[-1]:\n        parts[-1] = \"*\"\n        clean_version = \".\".join(parts[:3])\n\n        logger.warning(\n            f\"Your CAREamics version seems to be a locally modified version \"\n            f\"({__version__}). The recorded version for loading models will be \"\n            f\"{clean_version}, which may not exist. If you want to ensure \"\n            f\"exporting the model with an existing version, please install the \"\n            f\"closest CAREamics version from PyPI or conda-forge.\"\n        )\n\n    # Remove any local version identifier)\n    return \".\".join(parts[:3])\n</code></pre>"},{"location":"reference/careamics_napari/","title":"CAREamics napari","text":"<p>Use the navigation index on the left to explore the documentation.</p>"},{"location":"reference/careamics_napari/_version/","title":"_version","text":""},{"location":"reference/careamics_napari/prediction_plugin/","title":"prediction_plugin","text":"<p>CAREamics prediction plugin.</p>"},{"location":"reference/careamics_napari/sample_data/","title":"sample_data","text":"<p>Sample data for the careamics napari plugin.</p>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data._load_sem_n2n","title":"<code>_load_sem_n2n()</code>","text":"<p>Download N2N SEM data.</p> <p>Returns:</p> Type Description <code>list of (numpy.ndarray, dict)</code> <p>List of data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def _load_sem_n2n() -&gt; list[tuple[NDArray, dict[str, str]]]:\n    \"\"\"Download N2N SEM data.\n\n    Returns\n    -------\n    list of (numpy.ndarray, dict)\n        List of data and layer name.\n    \"\"\"\n    download = PortfolioManager().denoising.N2N_SEM.download(path=get_careamics_home())\n    files = [f for f in download if f.endswith(\"tif\")]\n    files.sort()\n\n    imgs_train = imread(files[1])\n    img_train = imgs_train[2]\n    img_target = imgs_train[3]\n\n    return [(img_train, {\"name\": \"train\"}), (img_target, {\"name\": \"target\"})]\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data._load_sem_n2v","title":"<code>_load_sem_n2v()</code>","text":"<p>Download N2V SEM data.</p> <p>Returns:</p> Type Description <code>list of (numpy.ndarray, dict)</code> <p>List of data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def _load_sem_n2v() -&gt; list[tuple[NDArray, dict[str, str]]]:\n    \"\"\"Download N2V SEM data.\n\n    Returns\n    -------\n    list of (numpy.ndarray, dict)\n        List of data and layer name.\n    \"\"\"\n    files = PortfolioManager().denoising.N2V_SEM.download(path=get_careamics_home())\n\n    img_train = imread(files[0])\n    img_val = imread(files[1])\n\n    return [(img_train, {\"name\": \"train\"}), (img_val, {\"name\": \"val\"})]\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data._load_u2os_care","title":"<code>_load_u2os_care()</code>","text":"<p>Download CARE U2OS data.</p> <p>Returns:</p> Type Description <code>list of (numpy.ndarray, dict)</code> <p>List of data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def _load_u2os_care() -&gt; list[tuple[NDArray, dict[str, str]]]:\n    \"\"\"Download CARE U2OS data.\n\n    Returns\n    -------\n    list of (numpy.ndarray, dict)\n        List of data and layer name.\n    \"\"\"\n    root_path = get_careamics_home()\n    PortfolioManager().denoising.CARE_U2OS.download(root_path)\n\n    # path to the training data\n    root_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\"\n    train_path = root_path / \"train\" / \"low\"\n    target_path = root_path / \"train\" / \"GT\"\n\n    # load the training data\n    files_gt = list(target_path.glob(\"*.tif\"))\n    files_gt.sort()\n    img_target = np.array([imread(f) for f in files_gt])\n\n    files_low = list(train_path.glob(\"*.tif\"))\n    files_low.sort()\n    img_train = np.array([imread(f) for f in files_low])\n\n    return [(img_train, {\"name\": \"train\"}), (img_target, {\"name\": \"target\"})]\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.care_u2os_data","title":"<code>care_u2os_data()</code>","text":"<p>Load CARE U2OS data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def care_u2os_data() -&gt; LayerDataTuple:\n    \"\"\"Load CARE U2OS data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_u2os_care()\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.n2n_sem_data","title":"<code>n2n_sem_data()</code>","text":"<p>Load N2N SEM data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def n2n_sem_data() -&gt; LayerDataTuple:\n    \"\"\"Load N2N SEM data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_sem_n2n()\n</code></pre>"},{"location":"reference/careamics_napari/sample_data/#careamics_napari.sample_data.n2v_sem_data","title":"<code>n2v_sem_data()</code>","text":"<p>Load N2V SEM data.</p> <p>Returns:</p> Type Description <code>LayerDataTuple</code> <p>Data and layer name.</p> Source code in <code>src/careamics_napari/sample_data.py</code> <pre><code>def n2v_sem_data() -&gt; LayerDataTuple:\n    \"\"\"Load N2V SEM data.\n\n    Returns\n    -------\n    LayerDataTuple\n        Data and layer name.\n    \"\"\"\n    ntf.show_info(\"Downloading data might take a few minutes.\")\n    return _load_sem_n2v()\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/","title":"training_plugin","text":"<p>CAREamics training Qt widget.</p>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin","title":"<code>TrainPlugin</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>CAREamics training plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>class TrainPlugin(QWidget):\n    \"\"\"CAREamics training plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        napari_viewer: Optional[napari.Viewer] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__()\n        self.viewer = napari_viewer\n        self.careamist: Optional[CAREamist] = None\n\n        # create statuses, used to keep track of the threads statuses\n        self.train_status = TrainingStatus()  # type: ignore\n        self.pred_status = PredictionStatus()  # type: ignore\n        self.save_status = SavingStatus()  # type: ignore\n\n        # create signals, used to hold the various parameters modified by the UI\n        self.train_config_signal = TrainingSignal()  # type: ignore\n        self.pred_config_signal = PredictionSignal()\n        self.save_config_signal = SavingSignal()\n\n        self.train_config_signal.events.is_3d.connect(self._set_pred_3d)\n\n        # create queues, used to communicate between the threads and the UI\n        self._training_queue: Queue = Queue(10)\n        self._prediction_queue: Queue = Queue(10)\n\n        # set workdir\n        self.train_config_signal.work_dir = Path.cwd()\n\n        self._init_ui()\n\n    def _init_ui(self) -&gt; None:\n        \"\"\"Assemble the widgets.\"\"\"\n        # layout\n        self.setLayout(QVBoxLayout())\n        self.setMinimumWidth(200)\n\n        # add banner\n        self.layout().addWidget(\n            CAREamicsBanner(\n                title=\"CAREamics\",\n                short_desc=(\"CAREamics UI for training denoising models.\"),\n            )\n        )\n\n        # add GPU label and algorithm selection\n        algo_panel = QWidget()\n        algo_panel.setLayout(QHBoxLayout())\n\n        gpu_button = create_gpu_label()\n        gpu_button.setAlignment(Qt.AlignmentFlag.AlignRight)\n        gpu_button.setContentsMargins(0, 5, 0, 0)  # top margin\n\n        algo_choice = AlgorithmSelectionWidget(training_signal=self.train_config_signal)\n        gpu_button.setAlignment(Qt.AlignmentFlag.AlignLeft)\n\n        algo_panel.layout().addWidget(algo_choice)\n        algo_panel.layout().addWidget(gpu_button)\n\n        self.layout().addWidget(algo_panel)\n\n        # add data tabs\n        self.data_stck = QStackedWidget()\n        self.data_layers = [\n            TrainDataWidget(signal=self.train_config_signal),\n            TrainDataWidget(signal=self.train_config_signal, use_target=True),\n        ]\n        for layer in self.data_layers:\n            self.data_stck.addWidget(layer)\n        self.data_stck.setCurrentIndex(0)\n\n        self.layout().addWidget(self.data_stck)\n\n        # add configuration widget\n        self.config_widget = ConfigurationWidget(self.train_config_signal)\n        self.layout().addWidget(self.config_widget)\n\n        # add train widget\n        self.train_widget = TrainingWidget(self.train_status)\n        self.layout().addWidget(self.train_widget)\n\n        # add progress widget\n        self.progress_widget = TrainProgressWidget(\n            self.train_status, self.train_config_signal\n        )\n        self.layout().addWidget(self.progress_widget)\n\n        # add prediction\n        self.prediction_widget = PredictionWidget(\n            self.train_status,\n            self.pred_status,\n            self.train_config_signal,\n            self.pred_config_signal,\n        )\n        self.layout().addWidget(self.prediction_widget)\n\n        # add saving\n        self.saving_widget = SavingWidget(\n            train_status=self.train_status,\n            save_status=self.save_status,\n            save_signal=self.save_config_signal,\n        )\n        self.layout().addWidget(self.saving_widget)\n\n        # connect signals\n        # changes from the selected algorithm\n        self.train_config_signal.events.algorithm.connect(self._set_data_from_algorithm)\n        self._set_data_from_algorithm(\n            self.train_config_signal.algorithm\n        )  # force update\n\n        # changes from the training, prediction or saving state\n        self.train_status.events.state.connect(self._training_state_changed)\n        self.pred_status.events.state.connect(self._prediction_state_changed)\n        self.save_status.events.state.connect(self._saving_state_changed)\n\n    def _set_pred_3d(self, is_3d: bool) -&gt; None:\n        \"\"\"Set the 3D mode flag in the prediction signal.\n\n        Parameters\n        ----------\n        is_3d : bool\n            3D mode.\n        \"\"\"\n        self.pred_config_signal.is_3d = is_3d\n\n    def _training_state_changed(self, state: TrainingState) -&gt; None:\n        \"\"\"Handle training state changes.\n\n        This includes starting and stopping training.\n\n        Parameters\n        ----------\n        state : TrainingState\n            New state.\n        \"\"\"\n        if state == TrainingState.TRAINING:\n            self.train_worker = train_worker(\n                self.train_config_signal,\n                self._training_queue,\n                self._prediction_queue,\n                self.careamist,\n            )\n\n            self.train_worker.yielded.connect(self._update_from_training)\n            self.train_worker.start()\n\n        elif state == TrainingState.STOPPED:\n            if self.careamist is not None:\n                self.careamist.stop_training()\n\n        elif state == TrainingState.CRASHED or state == TrainingState.IDLE:\n            del self.careamist\n            self.careamist = None\n\n    def _prediction_state_changed(self, state: PredictionState) -&gt; None:\n        \"\"\"Handle prediction state changes.\n\n        Parameters\n        ----------\n        state : PredictionState\n            New state.\n        \"\"\"\n        if state == PredictionState.PREDICTING:\n            self.pred_worker = predict_worker(\n                self.careamist, self.pred_config_signal, self._prediction_queue\n            )\n\n            self.pred_worker.yielded.connect(self._update_from_prediction)\n            self.pred_worker.start()\n\n        elif state == PredictionState.STOPPED:\n            # self.careamist.stop_prediction()\n            # TODO not existing yet\n            pass\n\n    def _saving_state_changed(self, state: SavingState) -&gt; None:\n        \"\"\"Handle saving state changes.\n\n        Parameters\n        ----------\n        state : SavingState\n            New state.\n        \"\"\"\n        if state == SavingState.SAVING:\n            self.save_worker = save_worker(\n                self.careamist, self.train_config_signal, self.save_config_signal\n            )\n\n            self.save_worker.yielded.connect(self._update_from_saving)\n            self.save_worker.start()\n\n    def _update_from_training(self, update: TrainUpdate) -&gt; None:\n        \"\"\"Update the training status from the training worker.\n\n        This method receives the updates from the training worker.\n\n        Parameters\n        ----------\n        update : TrainUpdate\n            Update.\n        \"\"\"\n        if update.type == TrainUpdateType.CAREAMIST:\n            if isinstance(update.value, CAREamist):\n                self.careamist = update.value\n        elif update.type == TrainUpdateType.DEBUG:\n            print(update.value)\n        elif update.type == TrainUpdateType.EXCEPTION:\n            self.train_status.state = TrainingState.CRASHED\n\n            if isinstance(update.value, Exception):\n                raise update.value\n        else:\n            self.train_status.update(update)\n\n    def _update_from_prediction(self, update: PredictionUpdate) -&gt; None:\n        \"\"\"Update the signal from the prediction worker.\n\n        This method receives the updates from the prediction worker.\n\n        Parameters\n        ----------\n        update : PredictionUpdate\n            Update.\n        \"\"\"\n        if update.type == PredictionUpdateType.DEBUG:\n            print(update.value)\n        elif update.type == PredictionUpdateType.EXCEPTION:\n            self.pred_status.state = PredictionState.CRASHED\n\n            # print exception without raising it\n            print(f\"Error: {update.value}\")\n\n            if _has_napari:\n                ntf.show_error(\n                    f\"An error occurred during prediction: \\n {update.value} \\n\"\n                    f\"Note: if you get an error due to the sizes of \"\n                    f\"Tensors, try using tiling.\"\n                )\n\n        else:\n            if update.type == PredictionUpdateType.SAMPLE:\n                # add image to napari\n                # TODO keep scaling?\n                if self.viewer is not None:\n                    # value is eighter a numpy array or a list of numpy arrays with each sample/timepoint as an element\n                    if isinstance(update.value, list):\n                        # combine all samples\n                        samples = np.concatenate(update.value, axis=0)\n                    else:\n                        samples = update.value\n\n                    # reshape the prediction to match the input axes\n                    samples = reshape_prediction(samples, self.train_config_signal.axes, self.pred_config_signal.is_3d)\n\n                    self.viewer.add_image(samples, name=\"Prediction\")\n            else:\n                self.pred_status.update(update)\n\n    def _update_from_saving(self, update: SavingUpdate) -&gt; None:\n        \"\"\"Update the signal from the saving worker.\n\n        This method receives the updates from the saving worker.\n\n        Parameters\n        ----------\n        update : SavingUpdate\n            Update.\n        \"\"\"\n        if update.type == SavingUpdateType.DEBUG:\n            print(update.value)\n        elif update.type == SavingUpdateType.EXCEPTION:\n            self.save_status.state = SavingState.CRASHED\n\n            if _has_napari:\n                ntf.show_error(f\"An error occurred during saving: \\n {update.value}\")\n\n    def _set_data_from_algorithm(self, name: str) -&gt; None:\n        \"\"\"Update the data selection widget based on the algorithm.\n\n        Parameters\n        ----------\n        name : str\n            Algorithm name.\n        \"\"\"\n        if (\n            name == SupportedAlgorithm.CARE.value\n            or name == SupportedAlgorithm.N2N.value\n        ):\n            self.data_stck.setCurrentIndex(1)\n        else:\n            self.data_stck.setCurrentIndex(0)\n\n    def closeEvent(self, event) -&gt; None:\n        \"\"\"Close the plugin.\n\n        Parameters\n        ----------\n        event : QCloseEvent\n            Close event.\n        \"\"\"\n        super().closeEvent(event)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def __init__(\n    self: Self,\n    napari_viewer: Optional[napari.Viewer] = None,\n) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__()\n    self.viewer = napari_viewer\n    self.careamist: Optional[CAREamist] = None\n\n    # create statuses, used to keep track of the threads statuses\n    self.train_status = TrainingStatus()  # type: ignore\n    self.pred_status = PredictionStatus()  # type: ignore\n    self.save_status = SavingStatus()  # type: ignore\n\n    # create signals, used to hold the various parameters modified by the UI\n    self.train_config_signal = TrainingSignal()  # type: ignore\n    self.pred_config_signal = PredictionSignal()\n    self.save_config_signal = SavingSignal()\n\n    self.train_config_signal.events.is_3d.connect(self._set_pred_3d)\n\n    # create queues, used to communicate between the threads and the UI\n    self._training_queue: Queue = Queue(10)\n    self._prediction_queue: Queue = Queue(10)\n\n    # set workdir\n    self.train_config_signal.work_dir = Path.cwd()\n\n    self._init_ui()\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._init_ui","title":"<code>_init_ui()</code>","text":"<p>Assemble the widgets.</p> Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _init_ui(self) -&gt; None:\n    \"\"\"Assemble the widgets.\"\"\"\n    # layout\n    self.setLayout(QVBoxLayout())\n    self.setMinimumWidth(200)\n\n    # add banner\n    self.layout().addWidget(\n        CAREamicsBanner(\n            title=\"CAREamics\",\n            short_desc=(\"CAREamics UI for training denoising models.\"),\n        )\n    )\n\n    # add GPU label and algorithm selection\n    algo_panel = QWidget()\n    algo_panel.setLayout(QHBoxLayout())\n\n    gpu_button = create_gpu_label()\n    gpu_button.setAlignment(Qt.AlignmentFlag.AlignRight)\n    gpu_button.setContentsMargins(0, 5, 0, 0)  # top margin\n\n    algo_choice = AlgorithmSelectionWidget(training_signal=self.train_config_signal)\n    gpu_button.setAlignment(Qt.AlignmentFlag.AlignLeft)\n\n    algo_panel.layout().addWidget(algo_choice)\n    algo_panel.layout().addWidget(gpu_button)\n\n    self.layout().addWidget(algo_panel)\n\n    # add data tabs\n    self.data_stck = QStackedWidget()\n    self.data_layers = [\n        TrainDataWidget(signal=self.train_config_signal),\n        TrainDataWidget(signal=self.train_config_signal, use_target=True),\n    ]\n    for layer in self.data_layers:\n        self.data_stck.addWidget(layer)\n    self.data_stck.setCurrentIndex(0)\n\n    self.layout().addWidget(self.data_stck)\n\n    # add configuration widget\n    self.config_widget = ConfigurationWidget(self.train_config_signal)\n    self.layout().addWidget(self.config_widget)\n\n    # add train widget\n    self.train_widget = TrainingWidget(self.train_status)\n    self.layout().addWidget(self.train_widget)\n\n    # add progress widget\n    self.progress_widget = TrainProgressWidget(\n        self.train_status, self.train_config_signal\n    )\n    self.layout().addWidget(self.progress_widget)\n\n    # add prediction\n    self.prediction_widget = PredictionWidget(\n        self.train_status,\n        self.pred_status,\n        self.train_config_signal,\n        self.pred_config_signal,\n    )\n    self.layout().addWidget(self.prediction_widget)\n\n    # add saving\n    self.saving_widget = SavingWidget(\n        train_status=self.train_status,\n        save_status=self.save_status,\n        save_signal=self.save_config_signal,\n    )\n    self.layout().addWidget(self.saving_widget)\n\n    # connect signals\n    # changes from the selected algorithm\n    self.train_config_signal.events.algorithm.connect(self._set_data_from_algorithm)\n    self._set_data_from_algorithm(\n        self.train_config_signal.algorithm\n    )  # force update\n\n    # changes from the training, prediction or saving state\n    self.train_status.events.state.connect(self._training_state_changed)\n    self.pred_status.events.state.connect(self._prediction_state_changed)\n    self.save_status.events.state.connect(self._saving_state_changed)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._prediction_state_changed","title":"<code>_prediction_state_changed(state)</code>","text":"<p>Handle prediction state changes.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>PredictionState</code> <p>New state.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _prediction_state_changed(self, state: PredictionState) -&gt; None:\n    \"\"\"Handle prediction state changes.\n\n    Parameters\n    ----------\n    state : PredictionState\n        New state.\n    \"\"\"\n    if state == PredictionState.PREDICTING:\n        self.pred_worker = predict_worker(\n            self.careamist, self.pred_config_signal, self._prediction_queue\n        )\n\n        self.pred_worker.yielded.connect(self._update_from_prediction)\n        self.pred_worker.start()\n\n    elif state == PredictionState.STOPPED:\n        # self.careamist.stop_prediction()\n        # TODO not existing yet\n        pass\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._saving_state_changed","title":"<code>_saving_state_changed(state)</code>","text":"<p>Handle saving state changes.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>SavingState</code> <p>New state.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _saving_state_changed(self, state: SavingState) -&gt; None:\n    \"\"\"Handle saving state changes.\n\n    Parameters\n    ----------\n    state : SavingState\n        New state.\n    \"\"\"\n    if state == SavingState.SAVING:\n        self.save_worker = save_worker(\n            self.careamist, self.train_config_signal, self.save_config_signal\n        )\n\n        self.save_worker.yielded.connect(self._update_from_saving)\n        self.save_worker.start()\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._set_data_from_algorithm","title":"<code>_set_data_from_algorithm(name)</code>","text":"<p>Update the data selection widget based on the algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Algorithm name.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _set_data_from_algorithm(self, name: str) -&gt; None:\n    \"\"\"Update the data selection widget based on the algorithm.\n\n    Parameters\n    ----------\n    name : str\n        Algorithm name.\n    \"\"\"\n    if (\n        name == SupportedAlgorithm.CARE.value\n        or name == SupportedAlgorithm.N2N.value\n    ):\n        self.data_stck.setCurrentIndex(1)\n    else:\n        self.data_stck.setCurrentIndex(0)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._set_pred_3d","title":"<code>_set_pred_3d(is_3d)</code>","text":"<p>Set the 3D mode flag in the prediction signal.</p> <p>Parameters:</p> Name Type Description Default <code>is_3d</code> <code>bool</code> <p>3D mode.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _set_pred_3d(self, is_3d: bool) -&gt; None:\n    \"\"\"Set the 3D mode flag in the prediction signal.\n\n    Parameters\n    ----------\n    is_3d : bool\n        3D mode.\n    \"\"\"\n    self.pred_config_signal.is_3d = is_3d\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._training_state_changed","title":"<code>_training_state_changed(state)</code>","text":"<p>Handle training state changes.</p> <p>This includes starting and stopping training.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>New state.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _training_state_changed(self, state: TrainingState) -&gt; None:\n    \"\"\"Handle training state changes.\n\n    This includes starting and stopping training.\n\n    Parameters\n    ----------\n    state : TrainingState\n        New state.\n    \"\"\"\n    if state == TrainingState.TRAINING:\n        self.train_worker = train_worker(\n            self.train_config_signal,\n            self._training_queue,\n            self._prediction_queue,\n            self.careamist,\n        )\n\n        self.train_worker.yielded.connect(self._update_from_training)\n        self.train_worker.start()\n\n    elif state == TrainingState.STOPPED:\n        if self.careamist is not None:\n            self.careamist.stop_training()\n\n    elif state == TrainingState.CRASHED or state == TrainingState.IDLE:\n        del self.careamist\n        self.careamist = None\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._update_from_prediction","title":"<code>_update_from_prediction(update)</code>","text":"<p>Update the signal from the prediction worker.</p> <p>This method receives the updates from the prediction worker.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>PredictionUpdate</code> <p>Update.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _update_from_prediction(self, update: PredictionUpdate) -&gt; None:\n    \"\"\"Update the signal from the prediction worker.\n\n    This method receives the updates from the prediction worker.\n\n    Parameters\n    ----------\n    update : PredictionUpdate\n        Update.\n    \"\"\"\n    if update.type == PredictionUpdateType.DEBUG:\n        print(update.value)\n    elif update.type == PredictionUpdateType.EXCEPTION:\n        self.pred_status.state = PredictionState.CRASHED\n\n        # print exception without raising it\n        print(f\"Error: {update.value}\")\n\n        if _has_napari:\n            ntf.show_error(\n                f\"An error occurred during prediction: \\n {update.value} \\n\"\n                f\"Note: if you get an error due to the sizes of \"\n                f\"Tensors, try using tiling.\"\n            )\n\n    else:\n        if update.type == PredictionUpdateType.SAMPLE:\n            # add image to napari\n            # TODO keep scaling?\n            if self.viewer is not None:\n                # value is eighter a numpy array or a list of numpy arrays with each sample/timepoint as an element\n                if isinstance(update.value, list):\n                    # combine all samples\n                    samples = np.concatenate(update.value, axis=0)\n                else:\n                    samples = update.value\n\n                # reshape the prediction to match the input axes\n                samples = reshape_prediction(samples, self.train_config_signal.axes, self.pred_config_signal.is_3d)\n\n                self.viewer.add_image(samples, name=\"Prediction\")\n        else:\n            self.pred_status.update(update)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._update_from_saving","title":"<code>_update_from_saving(update)</code>","text":"<p>Update the signal from the saving worker.</p> <p>This method receives the updates from the saving worker.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>SavingUpdate</code> <p>Update.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _update_from_saving(self, update: SavingUpdate) -&gt; None:\n    \"\"\"Update the signal from the saving worker.\n\n    This method receives the updates from the saving worker.\n\n    Parameters\n    ----------\n    update : SavingUpdate\n        Update.\n    \"\"\"\n    if update.type == SavingUpdateType.DEBUG:\n        print(update.value)\n    elif update.type == SavingUpdateType.EXCEPTION:\n        self.save_status.state = SavingState.CRASHED\n\n        if _has_napari:\n            ntf.show_error(f\"An error occurred during saving: \\n {update.value}\")\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin._update_from_training","title":"<code>_update_from_training(update)</code>","text":"<p>Update the training status from the training worker.</p> <p>This method receives the updates from the training worker.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>TrainUpdate</code> <p>Update.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def _update_from_training(self, update: TrainUpdate) -&gt; None:\n    \"\"\"Update the training status from the training worker.\n\n    This method receives the updates from the training worker.\n\n    Parameters\n    ----------\n    update : TrainUpdate\n        Update.\n    \"\"\"\n    if update.type == TrainUpdateType.CAREAMIST:\n        if isinstance(update.value, CAREamist):\n            self.careamist = update.value\n    elif update.type == TrainUpdateType.DEBUG:\n        print(update.value)\n    elif update.type == TrainUpdateType.EXCEPTION:\n        self.train_status.state = TrainingState.CRASHED\n\n        if isinstance(update.value, Exception):\n            raise update.value\n    else:\n        self.train_status.update(update)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPlugin.closeEvent","title":"<code>closeEvent(event)</code>","text":"<p>Close the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>QCloseEvent</code> <p>Close event.</p> required Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def closeEvent(self, event) -&gt; None:\n    \"\"\"Close the plugin.\n\n    Parameters\n    ----------\n    event : QCloseEvent\n        Close event.\n    \"\"\"\n    super().closeEvent(event)\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPluginWrapper","title":"<code>TrainPluginWrapper</code>","text":"<p>               Bases: <code>ScrollWidgetWrapper</code></p> <p>Training plugin within a scrolling wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>class TrainPluginWrapper(ScrollWidgetWrapper):\n    \"\"\"Training plugin within a scrolling wrapper.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n\n    def __init__(self: Self, napari_viewer: Optional[napari.Viewer] = None) -&gt; None:\n        \"\"\"Initialize the plugin.\n\n        Parameters\n        ----------\n        napari_viewer : napari.Viewer or None, default=None\n            Napari viewer.\n        \"\"\"\n        super().__init__(TrainPlugin(napari_viewer))\n</code></pre>"},{"location":"reference/careamics_napari/training_plugin/#careamics_napari.training_plugin.TrainPluginWrapper.__init__","title":"<code>__init__(napari_viewer=None)</code>","text":"<p>Initialize the plugin.</p> <p>Parameters:</p> Name Type Description Default <code>napari_viewer</code> <code>Viewer or None</code> <p>Napari viewer.</p> <code>None</code> Source code in <code>src/careamics_napari/training_plugin.py</code> <pre><code>def __init__(self: Self, napari_viewer: Optional[napari.Viewer] = None) -&gt; None:\n    \"\"\"Initialize the plugin.\n\n    Parameters\n    ----------\n    napari_viewer : napari.Viewer or None, default=None\n        Napari viewer.\n    \"\"\"\n    super().__init__(TrainPlugin(napari_viewer))\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/","title":"algorithms","text":"<p>Utilities for handling algorithms shorthand and friendly names.</p>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.UNSUPPORTED","title":"<code>UNSUPPORTED = 'I would prefer not to.'</code>  <code>module-attribute</code>","text":"<p>Label for algorithm not currently supported in the napari plugin.</p>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_algorithm","title":"<code>get_algorithm(friendly_name)</code>","text":"<p>Return the algorithm corresponding to the friendly name.</p> <p>The string returned by this method can directly be used with CAREamics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>friendly_name</code> <code>str</code> <p>Friendly name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Algorithm.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_algorithm(friendly_name: str) -&gt; str:\n    \"\"\"Return the algorithm corresponding to the friendly name.\n\n    The string returned by this method can directly be used with CAREamics\n    configuration.\n\n    Parameters\n    ----------\n    friendly_name : str\n        Friendly name.\n\n    Returns\n    -------\n    str\n        Algorithm.\n    \"\"\"\n    if friendly_name == \"Noise2Void\":\n        return SupportedAlgorithm.N2V.value\n    elif friendly_name == \"CARE\":\n        return SupportedAlgorithm.CARE.value\n    elif friendly_name == \"Noise2Noise\":\n        return SupportedAlgorithm.N2N.value\n    else:\n        raise ValueError(f\"Unsupported algorithm: {friendly_name}\")\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_available_algorithms","title":"<code>get_available_algorithms()</code>","text":"<p>Return the available algorithms friendly names.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>A list of available algorithms.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_available_algorithms() -&gt; list[str]:\n    \"\"\"Return the available algorithms friendly names.\n\n    Returns\n    -------\n    list of str\n        A list of available algorithms.\n    \"\"\"\n    return [\n        get_friendly_name(algorithm)\n        for algorithm in SupportedAlgorithm\n        if get_friendly_name(algorithm) != UNSUPPORTED\n    ]\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/algorithms/#careamics_napari.careamics_utils.algorithms.get_friendly_name","title":"<code>get_friendly_name(algorithm)</code>","text":"<p>Return the friendly name of an algorithm.</p> <p>Friendly names are spelling out the algorithm names in a human-readable way.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>SupportedAlgorithm</code> <p>Algorithm.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Friendly name.</p> Source code in <code>src/careamics_napari/careamics_utils/algorithms.py</code> <pre><code>def get_friendly_name(algorithm: SupportedAlgorithm) -&gt; str:\n    \"\"\"Return the friendly name of an algorithm.\n\n    Friendly names are spelling out the algorithm names in a human-readable way.\n\n    Parameters\n    ----------\n    algorithm : SupportedAlgorithm\n        Algorithm.\n\n    Returns\n    -------\n    str\n        Friendly name.\n    \"\"\"\n    if algorithm == SupportedAlgorithm.N2V:\n        return \"Noise2Void\"\n    elif algorithm == SupportedAlgorithm.CARE:\n        return \"CARE\"\n    elif algorithm == SupportedAlgorithm.N2N:\n        return \"Noise2Noise\"\n    else:\n        return UNSUPPORTED\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/","title":"callback","text":"<p>PyTorch Lightning callback used to update GUI with progress.</p>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack","title":"<code>UpdaterCallBack</code>","text":"<p>               Bases: <code>Callback</code></p> <p>PyTorch Lightning callback for updating training and prediction UI states.</p> <p>Parameters:</p> Name Type Description Default <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> required <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> required <p>Attributes:</p> Name Type Description <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>class UpdaterCallBack(Callback):\n    \"\"\"PyTorch Lightning callback for updating training and prediction UI states.\n\n    Parameters\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n\n    Attributes\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n    \"\"\"\n\n    def __init__(self: Self, training_queue: Queue, prediction_queue: Queue) -&gt; None:\n        \"\"\"Initialize the callback.\n\n        Parameters\n        ----------\n        training_queue : Queue\n            Training queue used to pass updates between threads.\n        prediction_queue : Queue\n            Prediction queue used to pass updates between threads.\n        \"\"\"\n        self.training_queue = training_queue\n        self.prediction_queue = prediction_queue\n\n    def get_train_queue(self) -&gt; Queue:\n        \"\"\"Return the training queue.\n\n        Returns\n        -------\n        Queue\n            Training queue.\n        \"\"\"\n        return self.training_queue\n\n    def get_predict_queue(self) -&gt; Queue:\n        \"\"\"Return the prediction queue.\n\n        Returns\n        -------\n        Queue\n            Prediction queue.\n        \"\"\"\n        return self.prediction_queue\n\n    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the beginning of the training.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        # compute the number of batches\n        len_dataloader = len(trainer.train_dataloader)  # type: ignore\n\n        self.training_queue.put(\n            TrainUpdate(\n                TrainUpdateType.MAX_BATCH,\n                int(len_dataloader / trainer.accumulate_grad_batches),\n            )\n        )\n\n        # register number of epochs\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.MAX_EPOCH, trainer.max_epochs)\n        )\n\n    def on_train_epoch_start(\n        self, trainer: Trainer, pl_module: LightningModule\n    ) -&gt; None:\n        \"\"\"Method called at the beginning of each epoch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.EPOCH, trainer.current_epoch)\n        )\n\n    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the end of each epoch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        metrics = trainer.progress_bar_metrics\n\n        if \"train_loss_epoch\" in metrics:\n            self.training_queue.put(\n                TrainUpdate(TrainUpdateType.LOSS, metrics[\"train_loss_epoch\"])\n            )\n\n        if \"val_loss\" in metrics:\n            self.training_queue.put(\n                TrainUpdate(TrainUpdateType.VAL_LOSS, metrics[\"val_loss\"])\n            )\n\n    def on_train_batch_start(\n        self, trainer: Trainer, pl_module: LightningModule, batch: Any, batch_idx: int\n    ) -&gt; None:\n        \"\"\"Method called at the beginning of each batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        batch : Any\n            Batch.\n        batch_idx : int\n            Index of the batch.\n        \"\"\"\n        self.training_queue.put(TrainUpdate(TrainUpdateType.BATCH, batch_idx))\n\n    def on_predict_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n        \"\"\"Method called at the beginning of the prediction.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        \"\"\"\n        self.prediction_queue.put(\n            PredictionUpdate(\n                PredictionUpdateType.MAX_SAMPLES,\n                # lightning returns a number of batches per dataloader\n                trainer.num_predict_batches[0],\n            )\n        )\n\n    def on_predict_batch_start(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        batch: Any,\n        batch_idx: int,\n        dataloader_idx: int = 0,\n    ) -&gt; None:\n        \"\"\"Method called at the beginning of each prediction batch.\n\n        Parameters\n        ----------\n        trainer : Trainer\n            PyTorch Lightning trainer.\n        pl_module : LightningModule\n            PyTorch Lightning module.\n        batch : Any\n            Batch.\n        batch_idx : int\n            Index of the batch.\n        dataloader_idx : int, default=0\n            Index of the dataloader.\n        \"\"\"\n        self.prediction_queue.put(\n            PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, batch_idx)\n        )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.__init__","title":"<code>__init__(training_queue, prediction_queue)</code>","text":"<p>Initialize the callback.</p> <p>Parameters:</p> Name Type Description Default <code>training_queue</code> <code>Queue</code> <p>Training queue used to pass updates between threads.</p> required <code>prediction_queue</code> <code>Queue</code> <p>Prediction queue used to pass updates between threads.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def __init__(self: Self, training_queue: Queue, prediction_queue: Queue) -&gt; None:\n    \"\"\"Initialize the callback.\n\n    Parameters\n    ----------\n    training_queue : Queue\n        Training queue used to pass updates between threads.\n    prediction_queue : Queue\n        Prediction queue used to pass updates between threads.\n    \"\"\"\n    self.training_queue = training_queue\n    self.prediction_queue = prediction_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.get_predict_queue","title":"<code>get_predict_queue()</code>","text":"<p>Return the prediction queue.</p> <p>Returns:</p> Type Description <code>Queue</code> <p>Prediction queue.</p> Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def get_predict_queue(self) -&gt; Queue:\n    \"\"\"Return the prediction queue.\n\n    Returns\n    -------\n    Queue\n        Prediction queue.\n    \"\"\"\n    return self.prediction_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.get_train_queue","title":"<code>get_train_queue()</code>","text":"<p>Return the training queue.</p> <p>Returns:</p> Type Description <code>Queue</code> <p>Training queue.</p> Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def get_train_queue(self) -&gt; Queue:\n    \"\"\"Return the training queue.\n\n    Returns\n    -------\n    Queue\n        Training queue.\n    \"\"\"\n    return self.training_queue\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_predict_batch_start","title":"<code>on_predict_batch_start(trainer, pl_module, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Method called at the beginning of each prediction batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>batch</code> <code>Any</code> <p>Batch.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the dataloader.</p> <code>0</code> Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_predict_batch_start(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n    \"\"\"Method called at the beginning of each prediction batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    batch : Any\n        Batch.\n    batch_idx : int\n        Index of the batch.\n    dataloader_idx : int, default=0\n        Index of the dataloader.\n    \"\"\"\n    self.prediction_queue.put(\n        PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, batch_idx)\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_predict_start","title":"<code>on_predict_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of the prediction.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_predict_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the beginning of the prediction.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    self.prediction_queue.put(\n        PredictionUpdate(\n            PredictionUpdateType.MAX_SAMPLES,\n            # lightning returns a number of batches per dataloader\n            trainer.num_predict_batches[0],\n        )\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_train_batch_start","title":"<code>on_train_batch_start(trainer, pl_module, batch, batch_idx)</code>","text":"<p>Method called at the beginning of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required <code>batch</code> <code>Any</code> <p>Batch.</p> required <code>batch_idx</code> <code>int</code> <p>Index of the batch.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_train_batch_start(\n    self, trainer: Trainer, pl_module: LightningModule, batch: Any, batch_idx: int\n) -&gt; None:\n    \"\"\"Method called at the beginning of each batch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    batch : Any\n        Batch.\n    batch_idx : int\n        Index of the batch.\n    \"\"\"\n    self.training_queue.put(TrainUpdate(TrainUpdateType.BATCH, batch_idx))\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_train_epoch_end","title":"<code>on_train_epoch_end(trainer, pl_module)</code>","text":"<p>Method called at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the end of each epoch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    metrics = trainer.progress_bar_metrics\n\n    if \"train_loss_epoch\" in metrics:\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.LOSS, metrics[\"train_loss_epoch\"])\n        )\n\n    if \"val_loss\" in metrics:\n        self.training_queue.put(\n            TrainUpdate(TrainUpdateType.VAL_LOSS, metrics[\"val_loss\"])\n        )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_train_epoch_start(\n    self, trainer: Trainer, pl_module: LightningModule\n) -&gt; None:\n    \"\"\"Method called at the beginning of each epoch.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    self.training_queue.put(\n        TrainUpdate(TrainUpdateType.EPOCH, trainer.current_epoch)\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/callback/#careamics_napari.careamics_utils.callback.UpdaterCallBack.on_train_start","title":"<code>on_train_start(trainer, pl_module)</code>","text":"<p>Method called at the beginning of the training.</p> <p>Parameters:</p> Name Type Description Default <code>trainer</code> <code>Trainer</code> <p>PyTorch Lightning trainer.</p> required <code>pl_module</code> <code>LightningModule</code> <p>PyTorch Lightning module.</p> required Source code in <code>src/careamics_napari/careamics_utils/callback.py</code> <pre><code>def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -&gt; None:\n    \"\"\"Method called at the beginning of the training.\n\n    Parameters\n    ----------\n    trainer : Trainer\n        PyTorch Lightning trainer.\n    pl_module : LightningModule\n        PyTorch Lightning module.\n    \"\"\"\n    # compute the number of batches\n    len_dataloader = len(trainer.train_dataloader)  # type: ignore\n\n    self.training_queue.put(\n        TrainUpdate(\n            TrainUpdateType.MAX_BATCH,\n            int(len_dataloader / trainer.accumulate_grad_batches),\n        )\n    )\n\n    # register number of epochs\n    self.training_queue.put(\n        TrainUpdate(TrainUpdateType.MAX_EPOCH, trainer.max_epochs)\n    )\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/configuration/","title":"configuration","text":"<p>Utility to create CAREamics configurations from user-set settings.</p>"},{"location":"reference/careamics_napari/careamics_utils/configuration/#careamics_napari.careamics_utils.configuration.create_configuration","title":"<code>create_configuration(signal)</code>","text":"<p>Create a CAREamics configuration from a TrainingSignal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>TrainingSignal</code> <p>Signal containing user-set training parameters.</p> required <p>Returns:</p> Type Description <code>Configuration</code> <p>CAREamics configuration.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the algorithm is not supported.</p> Source code in <code>src/careamics_napari/careamics_utils/configuration.py</code> <pre><code>def create_configuration(signal: TrainingSignal) -&gt; Configuration:\n    \"\"\"Create a CAREamics configuration from a TrainingSignal.\n\n    Parameters\n    ----------\n    signal : TrainingSignal\n        Signal containing user-set training parameters.\n\n    Returns\n    -------\n    Configuration\n        CAREamics configuration.\n\n    Raises\n    ------\n    ValueError\n        If the algorithm is not supported.\n    \"\"\"\n    # experiment name\n    if signal.experiment_name == \"\":\n        experiment_name = f\"{signal.algorithm}_{signal.axes}\"\n    else:\n        experiment_name = signal.experiment_name\n\n    if signal.is_3d:\n        # order of patches is ZYX\n        patches: list[int] = [\n            signal.patch_size_z,\n            signal.patch_size_xy,\n            signal.patch_size_xy,\n        ]\n    else:\n        patches = [signal.patch_size_xy, signal.patch_size_xy]\n\n    # model params\n    model_params = {\n        \"depth\": signal.depth,\n        \"num_channels_init\": signal.num_conv_filters,\n    }\n\n    # augmentations\n    augs: list[Union[XYFlipModel, XYRandomRotate90Model]] = []\n    if signal.x_flip or signal.y_flip:\n        augs.append(XYFlipModel(flip_x=signal.x_flip, flip_y=signal.y_flip))\n\n    if signal.rotations:\n        augs.append(XYRandomRotate90Model())\n\n    # create configuration\n    if signal.algorithm == SupportedAlgorithm.N2V:\n        return create_n2v_configuration(\n            experiment_name=experiment_name,\n            data_type=\"tiff\" if signal.load_from_disk else \"array\",\n            axes=signal.axes,\n            patch_size=patches,\n            batch_size=signal.batch_size,\n            num_epochs=signal.n_epochs,\n            n_channels=signal.n_channels_n2v,\n            augmentations=augs,\n            independent_channels=signal.independent_channels,\n            use_n2v2=signal.use_n2v2,\n            logger=\"tensorboard\",\n            model_params=model_params,\n        )\n    elif signal.algorithm == SupportedAlgorithm.N2N:\n        return create_n2n_configuration(\n            experiment_name=experiment_name,\n            data_type=\"tiff\" if signal.load_from_disk else \"array\",\n            axes=signal.axes,\n            patch_size=patches,\n            batch_size=signal.batch_size,\n            num_epochs=signal.n_epochs,\n            n_channels_in=signal.n_channels_in_care,\n            n_channels_out=signal.n_channels_out_care,\n            augmentations=augs,\n            independent_channels=signal.independent_channels,\n            logger=\"tensorboard\",\n            model_params=model_params,\n        )\n    elif signal.algorithm == SupportedAlgorithm.CARE:\n        return create_care_configuration(\n            experiment_name=experiment_name,\n            data_type=\"tiff\" if signal.load_from_disk else \"array\",\n            axes=signal.axes,\n            patch_size=patches,\n            batch_size=signal.batch_size,\n            num_epochs=signal.n_epochs,\n            n_channels_in=signal.n_channels_in_care,\n            n_channels_out=signal.n_channels_out_care,\n            augmentations=augs,\n            independent_channels=signal.independent_channels,\n            logger=\"tensorboard\",\n            model_params=model_params,\n        )\n    else:\n        raise ValueError(f\"Unsupported algorithm: {signal.algorithm}\")\n</code></pre>"},{"location":"reference/careamics_napari/careamics_utils/free_memory/","title":"free_memory","text":"<p>Utility to free GPU memory.</p>"},{"location":"reference/careamics_napari/careamics_utils/free_memory/#careamics_napari.careamics_utils.free_memory.free_memory","title":"<code>free_memory(careamist)</code>","text":"<p>Free memory from CAREamics instance.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamics instance.</p> required Source code in <code>src/careamics_napari/careamics_utils/free_memory.py</code> <pre><code>def free_memory(careamist: CAREamist) -&gt; None:\n    \"\"\"Free memory from CAREamics instance.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamics instance.\n    \"\"\"\n    if (\n        careamist is not None\n        and careamist.trainer is not None\n        and careamist.trainer.model is not None\n    ):\n        careamist.trainer.model.cpu()\n        del careamist.trainer.model\n        del careamist.trainer\n        del careamist\n\n        gc.collect()\n        empty_cache()\n</code></pre>"},{"location":"reference/careamics_napari/resources/resources/","title":"resources","text":"<p>Logo and icons.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_CAREAMICS","title":"<code>ICON_CAREAMICS = str(Path(Path(__file__).parent, 'logo_careamics_v2_128.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the CAREamics logo.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_GEAR","title":"<code>ICON_GEAR = str(Path(Path(__file__).parent, 'gear_16.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the gear icon.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_GITHUB","title":"<code>ICON_GITHUB = str(Path(Path(__file__).parent, 'GitHub-Mark-Light-32px.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the GitHub icon.</p>"},{"location":"reference/careamics_napari/resources/resources/#careamics_napari.resources.resources.ICON_TF","title":"<code>ICON_TF = str(Path(Path(__file__).parent, 'TF_White_Icon.png').absolute())</code>  <code>module-attribute</code>","text":"<p>Path to the TensorFlow icon.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/","title":"prediction_signal","text":"<p>Prediction parameters set by the user.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal._has_napari","title":"<code>_has_napari = True</code>  <code>module-attribute</code>","text":"<p>Whether napari is installed.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal","title":"<code>PredictionSignal</code>  <code>dataclass</code>","text":"<p>Prediction signal class.</p> <p>This class holds the parameters required to run the prediction thread. These parameters should be set whenever the user interact with the corresponding UI elements. An instance of the class is then passed to the prediction worker.</p> Source code in <code>src/careamics_napari/signals/prediction_signal.py</code> <pre><code>@evented\n@dataclass\nclass PredictionSignal:\n    \"\"\"Prediction signal class.\n\n    This class holds the parameters required to run the prediction thread. These\n    parameters should be set whenever the user interact with the corresponding UI\n    elements. An instance of the class is then passed to the prediction worker.\n    \"\"\"\n\n    load_from_disk: bool = True\n    \"\"\"Whether to load the images from disk or from the viewer.\"\"\"\n\n    if _has_napari:\n        layer_pred: Image = None\n        \"\"\"Layer containing the data on which to predict.\"\"\"\n\n    path_pred: str = \"\"\n    \"\"\"Path to the data on which to predict.\"\"\"\n\n    is_3d: bool = False\n    \"\"\"Whether the data is 3D or 2D.\"\"\"\n\n    tiled: bool = False\n    \"\"\"Whether to predict the data in tiles.\"\"\"\n\n    tile_size_xy: int = 64\n    \"\"\"Size of the tiles along the X and Y dimensions.\"\"\"\n\n    tile_size_z: int = 8\n    \"\"\"Size of the tiles along the Z dimension.\"\"\"\n\n    tile_overlap_xy: int = 48  # TODO currently fixed\n    \"\"\"Overlap between the tiles along the X and Y dimensions.\"\"\"\n\n    tile_overlap_z: int = 4  # TODO currently fixed\n    \"\"\"Overlap between the tiles along the Z dimension.\"\"\"\n\n    batch_size: int = 1\n    \"\"\"Batch size.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.batch_size","title":"<code>batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.is_3d","title":"<code>is_3d = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the data is 3D or 2D.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.layer_pred","title":"<code>layer_pred = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Layer containing the data on which to predict.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.load_from_disk","title":"<code>load_from_disk = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to load the images from disk or from the viewer.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.path_pred","title":"<code>path_pred = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the data on which to predict.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.tile_overlap_xy","title":"<code>tile_overlap_xy = 48</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between the tiles along the X and Y dimensions.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.tile_overlap_z","title":"<code>tile_overlap_z = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Overlap between the tiles along the Z dimension.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.tile_size_xy","title":"<code>tile_size_xy = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the tiles along the X and Y dimensions.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.tile_size_z","title":"<code>tile_size_z = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the tiles along the Z dimension.</p>"},{"location":"reference/careamics_napari/signals/prediction_signal/#careamics_napari.signals.prediction_signal.PredictionSignal.tiled","title":"<code>tiled = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to predict the data in tiles.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/","title":"prediction_status","text":"<p>Status and updates generated by the prediction worker.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState","title":"<code>PredictionState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Prediction state.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionState(IntEnum):\n    \"\"\"Prediction state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Prediction is idle.\"\"\"\n\n    PREDICTING = 1\n    \"\"\"Prediction is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Prediction is done.\"\"\"\n\n    STOPPED = 3\n    \"\"\"Prediction was stopped.\"\"\"\n\n    CRASHED = 4\n    \"\"\"Prediction crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.CRASHED","title":"<code>CRASHED = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction crashed.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is done.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is idle.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.PREDICTING","title":"<code>PREDICTING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction is ongoing.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionState.STOPPED","title":"<code>STOPPED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction was stopped.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus","title":"<code>PredictionStatus</code>  <code>dataclass</code>","text":"<p>Status of the prediction thread.</p> <p>This dataclass is used to update the prediction UI with the current status and progress of the prediction. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>@evented\n@dataclass\nclass PredictionStatus:\n    \"\"\"Status of the prediction thread.\n\n    This dataclass is used to update the prediction UI with the current status and\n    progress of the prediction. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: PredictionStatusSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    max_samples: int = -1\n    \"\"\"Number of samples.\"\"\"\n\n    sample_idx: int = -1\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    state: PredictionState = PredictionState.IDLE\n    \"\"\"Current state of the prediction process.\"\"\"\n\n    def update(self, new_update: PredictionUpdate) -&gt; None:\n        \"\"\"Update the status with the new values.\n\n        Exceptions, debugging messages and samples are ignored.\n\n        Parameters\n        ----------\n        new_update : PredictionUpdate\n            New update to apply.\n        \"\"\"\n        if (\n            new_update.type != PredictionUpdateType.EXCEPTION\n            and new_update.type != PredictionUpdateType.DEBUG\n            and new_update.type != PredictionUpdateType.SAMPLE\n        ):\n            setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.max_samples","title":"<code>max_samples = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.sample_idx","title":"<code>sample_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.state","title":"<code>state = PredictionState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new values.</p> <p>Exceptions, debugging messages and samples are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>PredictionUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>def update(self, new_update: PredictionUpdate) -&gt; None:\n    \"\"\"Update the status with the new values.\n\n    Exceptions, debugging messages and samples are ignored.\n\n    Parameters\n    ----------\n    new_update : PredictionUpdate\n        New update to apply.\n    \"\"\"\n    if (\n        new_update.type != PredictionUpdateType.EXCEPTION\n        and new_update.type != PredictionUpdateType.DEBUG\n        and new_update.type != PredictionUpdateType.SAMPLE\n    ):\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup","title":"<code>PredictionStatusSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the prediction status dataclass.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionStatusSignalGroup(SignalGroup):\n    \"\"\"Signal group for the prediction status dataclass.\"\"\"\n\n    max_samples: SignalInstance\n    \"\"\"Number of samples.\"\"\"\n\n    sample_idx: SignalInstance\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the prediction process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.max_samples","title":"<code>max_samples</code>  <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.sample_idx","title":"<code>sample_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionStatusSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate","title":"<code>PredictionUpdate</code>  <code>dataclass</code>","text":"<p>Update from the prediction worker.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>@dataclass\nclass PredictionUpdate:\n    \"\"\"Update from the prediction worker.\"\"\"\n\n    type: PredictionUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    value: Optional[Union[int, float, str, NDArray, PredictionState, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType","title":"<code>PredictionUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of prediction update.</p> Source code in <code>src/careamics_napari/signals/prediction_status.py</code> <pre><code>class PredictionUpdateType(str, Enum):\n    \"\"\"Type of prediction update.\"\"\"\n\n    MAX_SAMPLES = \"max_samples\"\n    \"\"\"Number of samples.\"\"\"\n\n    SAMPLE_IDX = \"sample_idx\"\n    \"\"\"Index of the current sample being predicted.\"\"\"\n\n    SAMPLE = \"sample\"\n    \"\"\"Prediction result.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the prediction process.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the prediction process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the prediction process.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.MAX_SAMPLES","title":"<code>MAX_SAMPLES = 'max_samples'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of samples.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.SAMPLE","title":"<code>SAMPLE = 'sample'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prediction result.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.SAMPLE_IDX","title":"<code>SAMPLE_IDX = 'sample_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current sample being predicted.</p>"},{"location":"reference/careamics_napari/signals/prediction_status/#careamics_napari.signals.prediction_status.PredictionUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the prediction process.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/","title":"saving_signal","text":"<p>Saving parameters set by the user.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType","title":"<code>ExportType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Type of model export.</p> Source code in <code>src/careamics_napari/signals/saving_signal.py</code> <pre><code>class ExportType(Enum):\n    \"\"\"Type of model export.\"\"\"\n\n    BMZ = \"Bioimage.io\"\n    \"\"\"Bioimage.io model format.\"\"\"\n\n    CKPT = \"Checkpoint\"\n    \"\"\"PyTorch Lightning checkpoint.\"\"\"\n\n    @classmethod\n    def list(cls) -&gt; list[str]:\n        \"\"\"List of all available export types.\n\n        Returns\n        -------\n        list of str\n            List of all available export types.\n        \"\"\"\n        return [c.value for c in cls]\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.BMZ","title":"<code>BMZ = 'Bioimage.io'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bioimage.io model format.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.CKPT","title":"<code>CKPT = 'Checkpoint'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PyTorch Lightning checkpoint.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.ExportType.list","title":"<code>list()</code>  <code>classmethod</code>","text":"<p>List of all available export types.</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of all available export types.</p> Source code in <code>src/careamics_napari/signals/saving_signal.py</code> <pre><code>@classmethod\ndef list(cls) -&gt; list[str]:\n    \"\"\"List of all available export types.\n\n    Returns\n    -------\n    list of str\n        List of all available export types.\n    \"\"\"\n    return [c.value for c in cls]\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.SavingSignal","title":"<code>SavingSignal</code>  <code>dataclass</code>","text":"<p>Saving signal class.</p> <p>This class holds the parameters required to run the prediction thread. These parameters should be set whenever the user interact with the corresponding UI elements.</p> Source code in <code>src/careamics_napari/signals/saving_signal.py</code> <pre><code>@dataclass\nclass SavingSignal:\n    \"\"\"Saving signal class.\n\n    This class holds the parameters required to run the prediction thread. These\n    parameters should be set whenever the user interact with the corresponding UI\n    elements.\n    \"\"\"\n\n    path_model: Path = Path(\".\")\n    \"\"\"Path in which to save the model.\"\"\"\n\n    export_type: ExportType = ExportType.BMZ\n    \"\"\"Format of model export.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.SavingSignal.export_type","title":"<code>export_type = ExportType.BMZ</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format of model export.</p>"},{"location":"reference/careamics_napari/signals/saving_signal/#careamics_napari.signals.saving_signal.SavingSignal.path_model","title":"<code>path_model = Path('.')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path in which to save the model.</p>"},{"location":"reference/careamics_napari/signals/saving_status/","title":"saving_status","text":"<p>Status and updates generated by the saving worker.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingSignalGroup","title":"<code>SavingSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the saving status dataclass.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingSignalGroup(SignalGroup):\n    \"\"\"Signal group for the saving status dataclass.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the saving process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState","title":"<code>SavingState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Saving state.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingState(IntEnum):\n    \"\"\"Saving state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Saving is idle.\"\"\"\n\n    SAVING = 1\n    \"\"\"Saving is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Saving is done.\"\"\"\n\n    CRASHED = 3\n    \"\"\"Saving has crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.CRASHED","title":"<code>CRASHED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving has crashed.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is done.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is idle.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingState.SAVING","title":"<code>SAVING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Saving is ongoing.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus","title":"<code>SavingStatus</code>  <code>dataclass</code>","text":"<p>Status of the saving thread.</p> <p>This dataclass is used to update the saving UI with the current status and progress of the saving. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>@evented\n@dataclass\nclass SavingStatus:\n    \"\"\"Status of the saving thread.\n\n    This dataclass is used to update the saving UI with the current status and\n    progress of the saving. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: SavingSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    state: SavingState = SavingState.IDLE\n    \"\"\"Current state of the saving process.\"\"\"\n\n    def update(self, new_update: SavingUpdate) -&gt; None:\n        \"\"\"Update the status with the new update.\n\n        Parameters\n        ----------\n        new_update : SavingUpdate\n            New update to apply.\n        \"\"\"\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.state","title":"<code>state = SavingState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new update.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>SavingUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>def update(self, new_update: SavingUpdate) -&gt; None:\n    \"\"\"Update the status with the new update.\n\n    Parameters\n    ----------\n    new_update : SavingUpdate\n        New update to apply.\n    \"\"\"\n    setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate","title":"<code>SavingUpdate</code>  <code>dataclass</code>","text":"<p>Update from the saving worker.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>@dataclass\nclass SavingUpdate:\n    \"\"\"Update from the saving worker.\"\"\"\n\n    type: SavingUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    value: Optional[Union[str, SavingState, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType","title":"<code>SavingUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of saving update.</p> Source code in <code>src/careamics_napari/signals/saving_status.py</code> <pre><code>class SavingUpdateType(str, Enum):\n    \"\"\"Type of saving update.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the saving process.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the saving process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the saving process.</p>"},{"location":"reference/careamics_napari/signals/saving_status/#careamics_napari.signals.saving_status.SavingUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the saving process.</p>"},{"location":"reference/careamics_napari/signals/training_signal/","title":"training_signal","text":"<p>Training parameters set by the user.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal._has_napari","title":"<code>_has_napari = True</code>  <code>module-attribute</code>","text":"<p>Whether napari is installed.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal","title":"<code>TrainingSignal</code>  <code>dataclass</code>","text":"<p>Training signal class.</p> <p>This class holds the parameters required to run the training thread. These parameters should be set whenever the user interact with the corresponding UI elements. An instance of the class is then passed to the training worker.</p> Source code in <code>src/careamics_napari/signals/training_signal.py</code> <pre><code>@evented\n@dataclass\nclass TrainingSignal:\n    \"\"\"Training signal class.\n\n    This class holds the parameters required to run the training thread. These\n    parameters should be set whenever the user interact with the corresponding UI\n    elements. An instance of the class is then passed to the training worker.\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: TrainingSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    # signals used to change states across widgets\n    algorithm: str = \"n2v\"\n    \"\"\"Algorithm used for training.\"\"\"\n\n    use_channels: bool = False\n    \"\"\"Whether the data has channels.\"\"\"\n\n    is_3d: bool = False\n    \"\"\"Whether the data is 3D.\"\"\"\n\n    # parameters set by widgets for training\n    work_dir: Path = HOME\n    \"\"\"Directory where the checkpoints and logs are saved.\"\"\"\n\n    load_from_disk: bool = True\n    \"\"\"Whether to load the images from disk or from the viewer.\"\"\"\n\n    if _has_napari:\n        layer_train: Image = None\n        \"\"\"Layer containing the training data.\"\"\"\n\n        layer_train_target: Image = None\n        \"\"\"Layer containing the training target data.\"\"\"\n\n        layer_val: Image = None\n        \"\"\"Layer containing the validation data.\"\"\"\n\n        layer_val_target: Image = None\n        \"\"\"Layer containing the validation target data.\"\"\"\n\n    path_train: str = \"\"\n    \"\"\"Path to the training data.\"\"\"\n\n    path_train_target: str = \"\"\n    \"\"\"Path to the training target data.\"\"\"\n\n    path_val: str = \"\"\n    \"\"\"Path to the validation data.\"\"\"\n\n    path_val_target: str = \"\"\n    \"\"\"Path to the validation target.\"\"\"\n\n    axes: str = \"YX\"\n    \"\"\"Axes of the data.\"\"\"\n\n    patch_size_xy: int = 64\n    \"\"\"Size of the patches along the X and Y dimensions.\"\"\"\n\n    patch_size_z: int = 16\n    \"\"\"Size of the patches along the Z dimension.\"\"\"\n\n    n_epochs: int = 30\n    \"\"\"Number of epochs.\"\"\"\n\n    batch_size: int = 16\n    \"\"\"Batch size.\"\"\"\n\n    experiment_name = \"\"\n    \"\"\"Name of the experiment, used to export the model and save checkpoints.\"\"\"\n\n    x_flip: bool = True\n    \"\"\"Whether to apply flipping along the X dimension during augmentation.\"\"\"\n\n    y_flip: bool = True\n    \"\"\"Whether to apply flipping along the Y dimension during augmentation.\"\"\"\n\n    rotations: bool = True\n    \"\"\"Whether to apply rotations during augmentation.\"\"\"\n\n    independent_channels: bool = True\n    \"\"\"Whether to train the channels independently.\"\"\"\n\n    n_channels_n2v: int = 1\n    \"\"\"Number of channels when training Noise2Void.\"\"\"\n\n    n_channels_in_care: int = 1\n    \"\"\"Number of input channels when training CARE and Noise2Noise.\"\"\"\n\n    n_channels_out_care: int = 1\n    \"\"\"Number of output channels when training CARE and Noise2Noise.\"\"\"\n\n    use_n2v2: bool = False\n    \"\"\"Whether to use N2V2.\"\"\"\n\n    depth: int = 2\n    \"\"\"Depth of the U-Net.\"\"\"\n\n    num_conv_filters: int = 32\n    \"\"\"Number of convolutional filters in the first layer.\"\"\"\n\n    val_percentage: float = 0.1\n    \"\"\"Percentage of the training data used for validation.\"\"\"\n\n    val_minimum_split: int = 1\n    \"\"\"Minimum number of patches or images in the validation set.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.algorithm","title":"<code>algorithm = 'n2v'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Algorithm used for training.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.axes","title":"<code>axes = 'YX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Axes of the data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.batch_size","title":"<code>batch_size = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Batch size.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.depth","title":"<code>depth = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Depth of the U-Net.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.experiment_name","title":"<code>experiment_name = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the experiment, used to export the model and save checkpoints.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.independent_channels","title":"<code>independent_channels = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to train the channels independently.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.is_3d","title":"<code>is_3d = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the data is 3D.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.layer_train","title":"<code>layer_train = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Layer containing the training data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.layer_train_target","title":"<code>layer_train_target = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Layer containing the training target data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.layer_val","title":"<code>layer_val = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Layer containing the validation data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.layer_val_target","title":"<code>layer_val_target = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Layer containing the validation target data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.load_from_disk","title":"<code>load_from_disk = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to load the images from disk or from the viewer.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.n_channels_in_care","title":"<code>n_channels_in_care = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of input channels when training CARE and Noise2Noise.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.n_channels_n2v","title":"<code>n_channels_n2v = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of channels when training Noise2Void.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.n_channels_out_care","title":"<code>n_channels_out_care = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of output channels when training CARE and Noise2Noise.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.n_epochs","title":"<code>n_epochs = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.num_conv_filters","title":"<code>num_conv_filters = 32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of convolutional filters in the first layer.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.patch_size_xy","title":"<code>patch_size_xy = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the patches along the X and Y dimensions.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.patch_size_z","title":"<code>patch_size_z = 16</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Size of the patches along the Z dimension.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.path_train","title":"<code>path_train = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the training data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.path_train_target","title":"<code>path_train_target = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the training target data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.path_val","title":"<code>path_val = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the validation data.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.path_val_target","title":"<code>path_val_target = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the validation target.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.rotations","title":"<code>rotations = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply rotations during augmentation.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.use_channels","title":"<code>use_channels = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the data has channels.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.use_n2v2","title":"<code>use_n2v2 = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to use N2V2.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.val_minimum_split","title":"<code>val_minimum_split = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Minimum number of patches or images in the validation set.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.val_percentage","title":"<code>val_percentage = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Percentage of the training data used for validation.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.work_dir","title":"<code>work_dir = HOME</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Directory where the checkpoints and logs are saved.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.x_flip","title":"<code>x_flip = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply flipping along the X dimension during augmentation.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignal.y_flip","title":"<code>y_flip = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to apply flipping along the Y dimension during augmentation.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignalGroup","title":"<code>TrainingSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the training status dataclass.</p> Source code in <code>src/careamics_napari/signals/training_signal.py</code> <pre><code>class TrainingSignalGroup(SignalGroup):\n    \"\"\"Signal group for the training status dataclass.\"\"\"\n\n    # only parameters that have observers are listed here\n    algorithm: SignalInstance\n    \"\"\"Algorithm used for training.\"\"\"\n\n    use_channels: SignalInstance\n    \"\"\"Whether tthe data has channels.\"\"\"\n\n    is_3d: SignalInstance\n    \"\"\"Whether the data is 3D.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignalGroup.algorithm","title":"<code>algorithm</code>  <code>instance-attribute</code>","text":"<p>Algorithm used for training.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignalGroup.is_3d","title":"<code>is_3d</code>  <code>instance-attribute</code>","text":"<p>Whether the data is 3D.</p>"},{"location":"reference/careamics_napari/signals/training_signal/#careamics_napari.signals.training_signal.TrainingSignalGroup.use_channels","title":"<code>use_channels</code>  <code>instance-attribute</code>","text":"<p>Whether tthe data has channels.</p>"},{"location":"reference/careamics_napari/signals/training_status/","title":"training_status","text":"<p>Status and updates generated by the training worker.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate","title":"<code>TrainUpdate</code>  <code>dataclass</code>","text":"<p>Update from the training worker.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>@dataclass\nclass TrainUpdate:\n    \"\"\"Update from the training worker.\"\"\"\n\n    type: TrainUpdateType\n    \"\"\"Type of the update.\"\"\"\n\n    # TODO should we split into subclasses to make the value type more specific?\n    value: Optional[Union[int, float, str, TrainingState, CAREamist, Exception]] = None\n    \"\"\"Content of the update.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate.type","title":"<code>type</code>  <code>instance-attribute</code>","text":"<p>Type of the update.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdate.value","title":"<code>value = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Content of the update.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType","title":"<code>TrainUpdateType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of training update.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainUpdateType(str, Enum):\n    \"\"\"Type of training update.\"\"\"\n\n    MAX_EPOCH = \"max_epochs\"\n    \"\"\"Number of epochs.\"\"\"\n\n    EPOCH = \"epoch_idx\"\n    \"\"\"Index of the current epoch.\"\"\"\n\n    MAX_BATCH = \"max_batches\"\n    \"\"\"Number of batches.\"\"\"\n\n    BATCH = \"batch_idx\"\n    \"\"\"Index of the current batch.\"\"\"\n\n    LOSS = \"loss\"\n    \"\"\"Current loss value.\"\"\"\n\n    VAL_LOSS = \"val_loss\"\n    \"\"\"Current validation loss value.\"\"\"\n\n    STATE = \"state\"\n    \"\"\"Current state of the training process.\"\"\"\n\n    CAREAMIST = \"careamist\"\n    \"\"\"CAREamist instance.\"\"\"\n\n    DEBUG = \"debug message\"\n    \"\"\"Debug message.\"\"\"\n\n    EXCEPTION = \"exception\"\n    \"\"\"Exception raised during the training process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.BATCH","title":"<code>BATCH = 'batch_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.CAREAMIST","title":"<code>CAREAMIST = 'careamist'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>CAREamist instance.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.DEBUG","title":"<code>DEBUG = 'debug message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Debug message.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.EPOCH","title":"<code>EPOCH = 'epoch_idx'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.EXCEPTION","title":"<code>EXCEPTION = 'exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exception raised during the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.LOSS","title":"<code>LOSS = 'loss'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.MAX_BATCH","title":"<code>MAX_BATCH = 'max_batches'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.MAX_EPOCH","title":"<code>MAX_EPOCH = 'max_epochs'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.STATE","title":"<code>STATE = 'state'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainUpdateType.VAL_LOSS","title":"<code>VAL_LOSS = 'val_loss'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState","title":"<code>TrainingState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Training state.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainingState(IntEnum):\n    \"\"\"Training state.\"\"\"\n\n    IDLE = 0\n    \"\"\"Training is idle.\"\"\"\n\n    TRAINING = 1\n    \"\"\"Training is ongoing.\"\"\"\n\n    DONE = 2\n    \"\"\"Training is done.\"\"\"\n\n    STOPPED = 3\n    \"\"\"Training was stopped.\"\"\"\n\n    CRASHED = 4\n    \"\"\"Training crashed.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.CRASHED","title":"<code>CRASHED = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training crashed.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.DONE","title":"<code>DONE = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is done.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.IDLE","title":"<code>IDLE = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is idle.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.STOPPED","title":"<code>STOPPED = 3</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training was stopped.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingState.TRAINING","title":"<code>TRAINING = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Training is ongoing.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus","title":"<code>TrainingStatus</code>  <code>dataclass</code>","text":"<p>Status of the training thread.</p> <p>This dataclass is used to update the training UI with the current status and progress of the training. Listeners can be registered to the <code>events</code> attribute to be notified of changes in the value of the attributes (see <code>psygnal</code> documentation for more details).</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>@evented\n@dataclass\nclass TrainingStatus:\n    \"\"\"Status of the training thread.\n\n    This dataclass is used to update the training UI with the current status and\n    progress of the training. Listeners can be registered to the `events` attribute to\n    be notified of changes in the value of the attributes (see `psygnal` documentation\n    for more details).\n    \"\"\"\n\n    if TYPE_CHECKING:\n        events: TrainingStatusSignalGroup\n        \"\"\"Attribute allowing the registration of parameter-specific listeners.\"\"\"\n\n    max_epochs: int = -1\n    \"\"\"Number of epochs.\"\"\"\n\n    max_batches: int = -1\n    \"\"\"Number of batches.\"\"\"\n\n    epoch_idx: int = -1\n    \"\"\"Index of the current epoch.\"\"\"\n\n    batch_idx: int = -1\n    \"\"\"Index of the current batch.\"\"\"\n\n    loss: float = -1\n    \"\"\"Current loss value.\"\"\"\n\n    val_loss: float = -1\n    \"\"\"Current validation loss value.\"\"\"\n\n    state: TrainingState = TrainingState.IDLE\n    \"\"\"Current state of the training process.\"\"\"\n\n    def update(self, new_update: TrainUpdate) -&gt; None:\n        \"\"\"Update the status with the new values.\n\n        Exceptions, debugging messages and CAREamist instances are ignored.\n\n        Parameters\n        ----------\n        new_update : PredictionUpdate\n            New update to apply.\n        \"\"\"\n        if (\n            new_update.type != TrainUpdateType.CAREAMIST\n            and new_update.type != TrainUpdateType.EXCEPTION\n            and new_update.type != TrainUpdateType.DEBUG\n        ):\n            setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.batch_idx","title":"<code>batch_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.epoch_idx","title":"<code>epoch_idx = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>Attribute allowing the registration of parameter-specific listeners.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.loss","title":"<code>loss = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.max_batches","title":"<code>max_batches = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.max_epochs","title":"<code>max_epochs = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.state","title":"<code>state = TrainingState.IDLE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.val_loss","title":"<code>val_loss = -1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatus.update","title":"<code>update(new_update)</code>","text":"<p>Update the status with the new values.</p> <p>Exceptions, debugging messages and CAREamist instances are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>new_update</code> <code>PredictionUpdate</code> <p>New update to apply.</p> required Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>def update(self, new_update: TrainUpdate) -&gt; None:\n    \"\"\"Update the status with the new values.\n\n    Exceptions, debugging messages and CAREamist instances are ignored.\n\n    Parameters\n    ----------\n    new_update : PredictionUpdate\n        New update to apply.\n    \"\"\"\n    if (\n        new_update.type != TrainUpdateType.CAREAMIST\n        and new_update.type != TrainUpdateType.EXCEPTION\n        and new_update.type != TrainUpdateType.DEBUG\n    ):\n        setattr(self, new_update.type.value, new_update.value)\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup","title":"<code>TrainingStatusSignalGroup</code>","text":"<p>               Bases: <code>SignalGroup</code></p> <p>Signal group for the training status dataclass.</p> Source code in <code>src/careamics_napari/signals/training_status.py</code> <pre><code>class TrainingStatusSignalGroup(SignalGroup):\n    \"\"\"Signal group for the training status dataclass.\"\"\"\n\n    max_epochs: SignalInstance\n    \"\"\"Number of epochs.\"\"\"\n\n    max_batches: SignalInstance\n    \"\"\"Number of batches.\"\"\"\n\n    epoch_idx: SignalInstance\n    \"\"\"Index of the current epoch.\"\"\"\n\n    batch_idx: SignalInstance\n    \"\"\"Index of the current batch.\"\"\"\n\n    loss: SignalInstance\n    \"\"\"Current loss value.\"\"\"\n\n    val_loss: SignalInstance\n    \"\"\"Current validation loss value.\"\"\"\n\n    state: SignalInstance\n    \"\"\"Current state of the training process.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.batch_idx","title":"<code>batch_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current batch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.epoch_idx","title":"<code>epoch_idx</code>  <code>instance-attribute</code>","text":"<p>Index of the current epoch.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.loss","title":"<code>loss</code>  <code>instance-attribute</code>","text":"<p>Current loss value.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.max_batches","title":"<code>max_batches</code>  <code>instance-attribute</code>","text":"<p>Number of batches.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.max_epochs","title":"<code>max_epochs</code>  <code>instance-attribute</code>","text":"<p>Number of epochs.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.state","title":"<code>state</code>  <code>instance-attribute</code>","text":"<p>Current state of the training process.</p>"},{"location":"reference/careamics_napari/signals/training_status/#careamics_napari.signals.training_status.TrainingStatusSignalGroup.val_loss","title":"<code>val_loss</code>  <code>instance-attribute</code>","text":"<p>Current validation loss value.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/","title":"axes_utils","text":"<p>Utilities to check axes validity.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.NAPARI_AXES","title":"<code>NAPARI_AXES = 'TSZYXC'</code>  <code>module-attribute</code>","text":"<p>Axes used in Napari.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.REF_AXES","title":"<code>REF_AXES = 'STCZYX'</code>  <code>module-attribute</code>","text":"<p>References axes in CAREamics.</p>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.are_axes_valid","title":"<code>are_axes_valid(axes)</code>","text":"<p>Check if axes are valid.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether the axes are valid.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def are_axes_valid(axes: str) -&gt; bool:\n    \"\"\"Check if axes are valid.\n\n    Parameters\n    ----------\n    axes : str\n        Axes to check.\n\n    Returns\n    -------\n    bool\n        Whether the axes are valid.\n    \"\"\"\n    _axes = axes.upper()\n\n    # length 0 and &gt; 6\n    if 0 &gt; len(_axes) &gt; 6:\n        return False\n\n    # all characters must be in REF_AXES = 'STZYXC'\n    if not all(s in REF_AXES for s in _axes):\n        return False\n\n    # check for repeating characters\n    for i, s in enumerate(_axes):\n        if i != _axes.rfind(s):\n            return False\n\n    # prior: X and Y contiguous\n    return (\"XY\" in _axes) or (\"YX\" in _axes)\n</code></pre>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.filter_dimensions","title":"<code>filter_dimensions(shape_length, is_3D)</code>","text":"<p>Filter axes based on shape and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>shape_length</code> <code>int</code> <p>Number of dimensions.</p> required <code>is_3D</code> <code>bool</code> <p>Whether the dimensions include Z.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of valid axes.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def filter_dimensions(shape_length: int, is_3D: bool) -&gt; list[str]:\n    \"\"\"Filter axes based on shape and dimensions.\n\n    Parameters\n    ----------\n    shape_length : int\n        Number of dimensions.\n    is_3D : bool\n        Whether the dimensions include Z.\n\n    Returns\n    -------\n    list of str\n        List of valid axes.\n    \"\"\"\n    axes = list(REF_AXES)\n    n = shape_length\n\n    if not is_3D:  # if not 3D, remove it from the\n        axes.remove(\"Z\")\n\n    if n &gt; len(axes):\n        warnings.warn(\"Data shape length is too large.\", stacklevel=3)\n        return []\n    else:\n        all_permutations = [\"\".join(p) for p in permutations(axes, n)]\n\n        # X and Y must be in each permutation and contiguous (#FancyComments)\n        all_permutations = [p for p in all_permutations if (\"XY\" in p) or (\"YX\" in p)]\n\n        if is_3D:\n            all_permutations = [p for p in all_permutations if \"Z\" in p]\n\n        if len(all_permutations) == 0 and not is_3D:\n            all_permutations = [\"YX\"]\n\n        return all_permutations\n</code></pre>"},{"location":"reference/careamics_napari/utils/axes_utils/#careamics_napari.utils.axes_utils.reshape_prediction","title":"<code>reshape_prediction(prediction, axes, is_3d)</code>","text":"<p>Reshape the prediction to match the input axes. The default axes of the model prediction is SC(Z)YX.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>ndarray</code> <p>Prediction.</p> required <code>axes</code> <code>str</code> <p>Axes of the input data.</p> required <code>is_3d</code> <code>bool</code> <p>Whether the data is 3D.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Reshaped prediction.</p> Source code in <code>src/careamics_napari/utils/axes_utils.py</code> <pre><code>def reshape_prediction(prediction: np.ndarray, axes: str, is_3d: bool) -&gt; np.ndarray:\n    \"\"\"Reshape the prediction to match the input axes.\n    The default axes of the model prediction is SC(Z)YX.\n\n    Parameters\n    ----------\n    prediction : np.ndarray\n        Prediction.\n    axes : str\n        Axes of the input data.\n    is_3d : bool\n        Whether the data is 3D.\n\n    Returns\n    -------\n    np.ndarray\n        Reshaped prediction.\n    \"\"\"\n\n    # model outputs SC(Z)YX\n    pred_axes = \"SCZYX\" if is_3d else \"SCYX\"\n\n    # transpose the axes\n    # TODO: during prediction T and S are merged. Check how to handle this\n    input_axes = axes.replace(\"T\", \"S\")\n    remove_c, remove_s = False, False\n\n    if not \"C\" in input_axes:\n        # add C if missing\n        input_axes = \"C\" + input_axes\n        remove_c = True\n\n    if not \"S\" in input_axes:\n        # add S if missing\n        input_axes = \"S\" + input_axes\n        remove_s = True\n\n    # TODO: check if all axes are present\n    assert all([ax in input_axes for ax in pred_axes])\n\n    indices = [pred_axes.index(ax) for ax in input_axes]\n    prediction = np.transpose(prediction, indices)\n\n    # remove S if not present in the input axes\n    if remove_c:\n        prediction = prediction[0]\n\n    # remove C if not present in the input axes\n    if remove_s:\n        prediction = prediction[0]\n\n    return prediction\n</code></pre>"},{"location":"reference/careamics_napari/utils/gpu_utils/","title":"gpu_utils","text":"<p>Utilities to test GPU availability with torch.</p>"},{"location":"reference/careamics_napari/utils/gpu_utils/#careamics_napari.utils.gpu_utils.is_gpu_available","title":"<code>is_gpu_available()</code>","text":"<p>Check if GPU is available with torch.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if GPU is available, False otherwise.</p> Source code in <code>src/careamics_napari/utils/gpu_utils.py</code> <pre><code>def is_gpu_available() -&gt; bool:\n    \"\"\"Check if GPU is available with torch.\n\n    Returns\n    -------\n    bool\n        True if GPU is available, False otherwise.\n    \"\"\"\n    if platform.system() == \"Darwin\":\n        # adapted from Lightning\n        # pytorch-lightning/src/lightning/fabric/accelerators/mps.py\n        mps_disabled = os.getenv(\"DISABLE_MPS\", \"0\") == \"1\"\n        return (\n            not mps_disabled\n            and backends.mps.is_available()\n            and platform.processor() in (\"arm\", \"arm64\")\n        )\n    else:\n        return cuda.is_available()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/","title":"algorithm_choice","text":"<p>Algorithm selection widget.</p>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/#careamics_napari.widgets.algorithm_choice.AlgorithmSelectionWidget","title":"<code>AlgorithmSelectionWidget</code>","text":"<p>               Bases: <code>QComboBox</code></p> <p>Algorithm selection widget.</p> <p>Parameters:</p> Name Type Description Default <code>training_signal</code> <code>TrainingSignal or None</code> <p>Training signal holding all parameters to be set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/algorithm_choice.py</code> <pre><code>class AlgorithmSelectionWidget(QComboBox):\n    \"\"\"Algorithm selection widget.\n\n    Parameters\n    ----------\n    training_signal : TrainingSignal or None, default=None\n        Training signal holding all parameters to be set by the user.\n    \"\"\"\n\n    def __init__(self, training_signal: Optional[TrainingSignal] = None) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        training_signal : TrainingSignal or None, default=None\n            Training signal holding all parameters to be set by the user.\n        \"\"\"\n        super().__init__()\n\n        self.signal = training_signal\n\n        self.addItems(get_available_algorithms())\n        self.setToolTip(\"Select an algorithm.\")\n\n        # Connect the signal\n        self.currentIndexChanged.connect(self.algorithm_changed)\n        self.current_algorithm = self._get_current_algorithm()\n\n    def _get_current_algorithm(self) -&gt; str:\n        \"\"\"Return the current algorithm name.\n\n        Returns\n        -------\n        str\n            Current algorithm name.\n        \"\"\"\n        return get_algorithm(self.currentText())\n\n    def algorithm_changed(self, index: int) -&gt; None:\n        \"\"\"Emit the algorithm signal.\n\n        Parameters\n        ----------\n        index : int\n            Index of the selected algorithm.\n        \"\"\"\n        # save SupportedAlgorithm\n        self.current_algorithm = self._get_current_algorithm()\n\n        # emit the signal\n        if self.signal is not None:\n            self.signal.algorithm = self.current_algorithm\n</code></pre>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/#careamics_napari.widgets.algorithm_choice.AlgorithmSelectionWidget.__init__","title":"<code>__init__(training_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>training_signal</code> <code>TrainingSignal or None</code> <p>Training signal holding all parameters to be set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/algorithm_choice.py</code> <pre><code>def __init__(self, training_signal: Optional[TrainingSignal] = None) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    training_signal : TrainingSignal or None, default=None\n        Training signal holding all parameters to be set by the user.\n    \"\"\"\n    super().__init__()\n\n    self.signal = training_signal\n\n    self.addItems(get_available_algorithms())\n    self.setToolTip(\"Select an algorithm.\")\n\n    # Connect the signal\n    self.currentIndexChanged.connect(self.algorithm_changed)\n    self.current_algorithm = self._get_current_algorithm()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/#careamics_napari.widgets.algorithm_choice.AlgorithmSelectionWidget._get_current_algorithm","title":"<code>_get_current_algorithm()</code>","text":"<p>Return the current algorithm name.</p> <p>Returns:</p> Type Description <code>str</code> <p>Current algorithm name.</p> Source code in <code>src/careamics_napari/widgets/algorithm_choice.py</code> <pre><code>def _get_current_algorithm(self) -&gt; str:\n    \"\"\"Return the current algorithm name.\n\n    Returns\n    -------\n    str\n        Current algorithm name.\n    \"\"\"\n    return get_algorithm(self.currentText())\n</code></pre>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/#careamics_napari.widgets.algorithm_choice.AlgorithmSelectionWidget.algorithm_changed","title":"<code>algorithm_changed(index)</code>","text":"<p>Emit the algorithm signal.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the selected algorithm.</p> required Source code in <code>src/careamics_napari/widgets/algorithm_choice.py</code> <pre><code>def algorithm_changed(self, index: int) -&gt; None:\n    \"\"\"Emit the algorithm signal.\n\n    Parameters\n    ----------\n    index : int\n        Index of the selected algorithm.\n    \"\"\"\n    # save SupportedAlgorithm\n    self.current_algorithm = self._get_current_algorithm()\n\n    # emit the signal\n    if self.signal is not None:\n        self.signal.algorithm = self.current_algorithm\n</code></pre>"},{"location":"reference/careamics_napari/widgets/algorithm_choice/#careamics_napari.widgets.algorithm_choice.print_algorithm","title":"<code>print_algorithm(name)</code>","text":"<p>Print the selected algorithm.</p> Source code in <code>src/careamics_napari/widgets/algorithm_choice.py</code> <pre><code>@myalgo.events.algorithm.connect\ndef print_algorithm(name: str):\n    \"\"\"Print the selected algorithm.\"\"\"\n    print(f\"Selected algorithm: {name}\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/","title":"axes_widget","text":"<p>Widget for specifying axes order.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget","title":"<code>AxesWidget</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>A widget allowing users to specify axes.</p> <p>Parameters:</p> Name Type Description Default <code>n_axes</code> <code>int</code> <p>Number of axes.</p> <code>3</code> <code>is_3D</code> <code>bool</code> <p>Whether the data is 3D.</p> <code>False</code> <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal holding all training parameters to be set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class AxesWidget(QWidget):\n    \"\"\"A widget allowing users to specify axes.\n\n    Parameters\n    ----------\n    n_axes : int, default=3\n        Number of axes.\n    is_3D : bool, default=False\n        Whether the data is 3D.\n    training_signal : TrainingSignal or None, default=None\n        Signal holding all training parameters to be set by the user.\n    \"\"\"\n\n    # TODO unused parameters\n    def __init__(\n        self, n_axes=3, is_3D=False, training_signal: Optional[TrainingSignal] = None\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        n_axes : int, default=3\n            Number of axes.\n        is_3D : bool, default=False\n            Whether the data is 3D.\n        training_signal : TrainingSignal or None, default=None\n            Signal holding all training parameters to be set by the user.\n        \"\"\"\n        super().__init__()\n        self.configuration_signal = training_signal\n\n        # # max axes is 6\n        # assert 0 &lt; n_axes &lt;= 6\n\n        # self.n_axes = n_axes\n        # self.is_3D = is_3D\n        self.is_text_valid = True\n\n        # QtPy\n        self.setLayout(QHBoxLayout())\n        self.layout().setSpacing(0)\n        self.layout().setContentsMargins(0, 0, 0, 0)\n\n        # folder selection button\n        self.label = QLabel(\"Axes\")\n        self.layout().addWidget(self.label)\n\n        # text field\n        self.text_field = QLineEdit(self.get_default_text())\n        self.text_field.setMaxLength(6)\n        self.text_field.setValidator(LettersValidator(REF_AXES))\n\n        self.layout().addWidget(self.text_field)\n        self.text_field.textChanged.connect(self._validate_text)\n        self.text_field.setToolTip(\n            \"Enter the axes order as they are in your images, e.g. SZYX.\\n\"\n            \"Accepted axes are S(ample), T(ime), C(hannel), Z, Y, and X. Red\\n\"\n            \"color highlighting means that a character is not recognized,\\n\"\n            \"orange means that the axes order is not allowed. YX axes are\\n\"\n            \"mandatory.\"\n        )\n\n        # validate text\n        self._validate_text()\n\n        # set up signal handling when axes and 3D change\n        self.text_field.textChanged.connect(self._axes_changed)\n\n        # if self.configuration_signal is not None:\n        #     self.configuration_signal.events.is_3d.connect(self.update_is_3D)\n\n    def _axes_changed(self: Self) -&gt; None:\n        \"\"\"Update the axes in the configuration signal if valid.\"\"\"\n        if self.configuration_signal is not None and self.is_text_valid:\n            self.configuration_signal.use_channels = \"C\" in self.get_axes()\n            self.configuration_signal.axes = self.get_axes()\n\n    def _validate_text(self: Self) -&gt; None:\n        \"\"\"Validate the text in the text field.\"\"\"\n        axes = self.get_axes()\n\n        # change text color according to axes validation\n        if are_axes_valid(axes):\n            self._set_text_color(Highlight.VALID)\n            # if axes.upper() in filter_dimensions(self.n_axes, self.is_3D):\n            #     self._set_text_color(Highlight.VALID)\n            # else:\n            #     self._set_text_color(Highlight.NOT_ACCEPTED)\n        else:\n            self._set_text_color(Highlight.UNRECOGNIZED)\n\n    def _set_text_color(self: Self, highlight: Highlight) -&gt; None:\n        \"\"\"Set the text color according to the highlight type.\n\n        Parameters\n        ----------\n        highlight : Highlight\n            Highlight type.\n        \"\"\"\n        self.is_text_valid = highlight == Highlight.VALID\n\n        if highlight == Highlight.UNRECOGNIZED:\n            self.text_field.setStyleSheet(\"color: red;\")\n        elif highlight == Highlight.NOT_ACCEPTED:\n            self.text_field.setStyleSheet(\"color: orange;\")\n        else:  # VALID\n            self.text_field.setStyleSheet(\"color: white;\")\n\n    def get_default_text(self: Self) -&gt; str:\n        \"\"\"Return the default text.\n\n        Returns\n        -------\n        str\n            Default text.\n        \"\"\"\n        # if self.is_3D:\n        #     defaults = [\"YX\", \"ZYX\", \"SZYX\", \"STZYX\", \"STCZYX\"]\n        # else:\n        #     defaults = [\"YX\", \"SYX\", \"STYX\", \"STCYX\", \"STC?YX\"]\n\n        # return defaults[self.n_axes - 2]\n        return \"YX\"\n\n    # def update_axes_number(self, n):\n    #     self.n_axes = n\n    #     self._validate_text()  # force new validation\n\n    # def update_is_3D(self, is_3D):\n    #     self.is_3D = is_3D\n    #     self._validate_text()  # force new validation\n\n    def get_axes(self: Self) -&gt; str:\n        \"\"\"Return the axes order.\n\n        Returns\n        -------\n        str\n            Axes order.\n        \"\"\"\n        return self.text_field.text()\n\n    def is_valid(self: Self) -&gt; bool:\n        \"\"\"Return whether the axes are valid.\n\n        Returns\n        -------\n        bool\n            Whether the axes are valid.\n        \"\"\"\n        self._validate_text()  # probably unnecessary\n        return self.is_text_valid\n\n    def set_text_field(self: Self, text: str) -&gt; None:\n        \"\"\"Set the text field.\n\n        Parameters\n        ----------\n        text : str\n            Text to set.\n        \"\"\"\n        self.text_field.setText(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.__init__","title":"<code>__init__(n_axes=3, is_3D=False, training_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>n_axes</code> <code>int</code> <p>Number of axes.</p> <code>3</code> <code>is_3D</code> <code>bool</code> <p>Whether the data is 3D.</p> <code>False</code> <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal holding all training parameters to be set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def __init__(\n    self, n_axes=3, is_3D=False, training_signal: Optional[TrainingSignal] = None\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    n_axes : int, default=3\n        Number of axes.\n    is_3D : bool, default=False\n        Whether the data is 3D.\n    training_signal : TrainingSignal or None, default=None\n        Signal holding all training parameters to be set by the user.\n    \"\"\"\n    super().__init__()\n    self.configuration_signal = training_signal\n\n    # # max axes is 6\n    # assert 0 &lt; n_axes &lt;= 6\n\n    # self.n_axes = n_axes\n    # self.is_3D = is_3D\n    self.is_text_valid = True\n\n    # QtPy\n    self.setLayout(QHBoxLayout())\n    self.layout().setSpacing(0)\n    self.layout().setContentsMargins(0, 0, 0, 0)\n\n    # folder selection button\n    self.label = QLabel(\"Axes\")\n    self.layout().addWidget(self.label)\n\n    # text field\n    self.text_field = QLineEdit(self.get_default_text())\n    self.text_field.setMaxLength(6)\n    self.text_field.setValidator(LettersValidator(REF_AXES))\n\n    self.layout().addWidget(self.text_field)\n    self.text_field.textChanged.connect(self._validate_text)\n    self.text_field.setToolTip(\n        \"Enter the axes order as they are in your images, e.g. SZYX.\\n\"\n        \"Accepted axes are S(ample), T(ime), C(hannel), Z, Y, and X. Red\\n\"\n        \"color highlighting means that a character is not recognized,\\n\"\n        \"orange means that the axes order is not allowed. YX axes are\\n\"\n        \"mandatory.\"\n    )\n\n    # validate text\n    self._validate_text()\n\n    # set up signal handling when axes and 3D change\n    self.text_field.textChanged.connect(self._axes_changed)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget._axes_changed","title":"<code>_axes_changed()</code>","text":"<p>Update the axes in the configuration signal if valid.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def _axes_changed(self: Self) -&gt; None:\n    \"\"\"Update the axes in the configuration signal if valid.\"\"\"\n    if self.configuration_signal is not None and self.is_text_valid:\n        self.configuration_signal.use_channels = \"C\" in self.get_axes()\n        self.configuration_signal.axes = self.get_axes()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget._set_text_color","title":"<code>_set_text_color(highlight)</code>","text":"<p>Set the text color according to the highlight type.</p> <p>Parameters:</p> Name Type Description Default <code>highlight</code> <code>Highlight</code> <p>Highlight type.</p> required Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def _set_text_color(self: Self, highlight: Highlight) -&gt; None:\n    \"\"\"Set the text color according to the highlight type.\n\n    Parameters\n    ----------\n    highlight : Highlight\n        Highlight type.\n    \"\"\"\n    self.is_text_valid = highlight == Highlight.VALID\n\n    if highlight == Highlight.UNRECOGNIZED:\n        self.text_field.setStyleSheet(\"color: red;\")\n    elif highlight == Highlight.NOT_ACCEPTED:\n        self.text_field.setStyleSheet(\"color: orange;\")\n    else:  # VALID\n        self.text_field.setStyleSheet(\"color: white;\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget._validate_text","title":"<code>_validate_text()</code>","text":"<p>Validate the text in the text field.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def _validate_text(self: Self) -&gt; None:\n    \"\"\"Validate the text in the text field.\"\"\"\n    axes = self.get_axes()\n\n    # change text color according to axes validation\n    if are_axes_valid(axes):\n        self._set_text_color(Highlight.VALID)\n        # if axes.upper() in filter_dimensions(self.n_axes, self.is_3D):\n        #     self._set_text_color(Highlight.VALID)\n        # else:\n        #     self._set_text_color(Highlight.NOT_ACCEPTED)\n    else:\n        self._set_text_color(Highlight.UNRECOGNIZED)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.get_axes","title":"<code>get_axes()</code>","text":"<p>Return the axes order.</p> <p>Returns:</p> Type Description <code>str</code> <p>Axes order.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def get_axes(self: Self) -&gt; str:\n    \"\"\"Return the axes order.\n\n    Returns\n    -------\n    str\n        Axes order.\n    \"\"\"\n    return self.text_field.text()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.get_default_text","title":"<code>get_default_text()</code>","text":"<p>Return the default text.</p> <p>Returns:</p> Type Description <code>str</code> <p>Default text.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def get_default_text(self: Self) -&gt; str:\n    \"\"\"Return the default text.\n\n    Returns\n    -------\n    str\n        Default text.\n    \"\"\"\n    # if self.is_3D:\n    #     defaults = [\"YX\", \"ZYX\", \"SZYX\", \"STZYX\", \"STCZYX\"]\n    # else:\n    #     defaults = [\"YX\", \"SYX\", \"STYX\", \"STCYX\", \"STC?YX\"]\n\n    # return defaults[self.n_axes - 2]\n    return \"YX\"\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.is_valid","title":"<code>is_valid()</code>","text":"<p>Return whether the axes are valid.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the axes are valid.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def is_valid(self: Self) -&gt; bool:\n    \"\"\"Return whether the axes are valid.\n\n    Returns\n    -------\n    bool\n        Whether the axes are valid.\n    \"\"\"\n    self._validate_text()  # probably unnecessary\n    return self.is_text_valid\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.AxesWidget.set_text_field","title":"<code>set_text_field(text)</code>","text":"<p>Set the text field.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text to set.</p> required Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def set_text_field(self: Self, text: str) -&gt; None:\n    \"\"\"Set the text field.\n\n    Parameters\n    ----------\n    text : str\n        Text to set.\n    \"\"\"\n    self.text_field.setText(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight","title":"<code>Highlight</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Axes highlight types.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class Highlight(Enum):\n    \"\"\"Axes highlight types.\"\"\"\n\n    VALID = 0\n    \"\"\"Valid axes.\"\"\"\n\n    UNRECOGNIZED = 1\n    \"\"\"Unrecognized axes.\"\"\"\n\n    NOT_ACCEPTED = 2\n    \"\"\"Axes not accepted.\"\"\"\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.NOT_ACCEPTED","title":"<code>NOT_ACCEPTED = 2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Axes not accepted.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.UNRECOGNIZED","title":"<code>UNRECOGNIZED = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Unrecognized axes.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.Highlight.VALID","title":"<code>VALID = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Valid axes.</p>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator","title":"<code>LettersValidator</code>","text":"<p>               Bases: <code>QValidator</code></p> <p>Custom validator.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>str</code> <p>Allowed characters.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>class LettersValidator(QtGui.QValidator):\n    \"\"\"Custom validator.\n\n    Parameters\n    ----------\n    options : str\n        Allowed characters.\n    *args : Any\n        Variable length argument list.\n    **kwargs : Any\n        Arbitrary keyword arguments.\n    \"\"\"\n\n    def __init__(self: Self, options: str, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the validator.\n\n        Parameters\n        ----------\n        options : str\n            Allowed characters.\n        *args : Any\n            Variable length argument list.\n        **kwargs : Any\n            Arbitrary keyword arguments.\n        \"\"\"\n        QtGui.QValidator.__init__(self, *args, **kwargs)\n        self._options = options\n\n    def validate(\n        self: Self, value: str, pos: int\n    ) -&gt; tuple[QtGui.QValidator.State, str, int]:\n        \"\"\"Validate the input.\n\n        Parameters\n        ----------\n        value : str\n            Input value.\n        pos : int\n            Position of the cursor.\n\n        Returns\n        -------\n        (QtGui.QValidator.State, str, int)\n            Validation state, value, and position.\n        \"\"\"\n        if len(value) &gt; 0:\n            if value[-1] in self._options:\n                return QtGui.QValidator.Acceptable, value, pos\n        else:\n            if value == \"\":\n                return QtGui.QValidator.Intermediate, value, pos\n        return QtGui.QValidator.Invalid, value, pos\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator.__init__","title":"<code>__init__(options, *args, **kwargs)</code>","text":"<p>Initialize the validator.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>str</code> <p>Allowed characters.</p> required <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def __init__(self: Self, options: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the validator.\n\n    Parameters\n    ----------\n    options : str\n        Allowed characters.\n    *args : Any\n        Variable length argument list.\n    **kwargs : Any\n        Arbitrary keyword arguments.\n    \"\"\"\n    QtGui.QValidator.__init__(self, *args, **kwargs)\n    self._options = options\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.LettersValidator.validate","title":"<code>validate(value, pos)</code>","text":"<p>Validate the input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Input value.</p> required <code>pos</code> <code>int</code> <p>Position of the cursor.</p> required <p>Returns:</p> Type Description <code>(State, str, int)</code> <p>Validation state, value, and position.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>def validate(\n    self: Self, value: str, pos: int\n) -&gt; tuple[QtGui.QValidator.State, str, int]:\n    \"\"\"Validate the input.\n\n    Parameters\n    ----------\n    value : str\n        Input value.\n    pos : int\n        Position of the cursor.\n\n    Returns\n    -------\n    (QtGui.QValidator.State, str, int)\n        Validation state, value, and position.\n    \"\"\"\n    if len(value) &gt; 0:\n        if value[-1] in self._options:\n            return QtGui.QValidator.Acceptable, value, pos\n    else:\n        if value == \"\":\n            return QtGui.QValidator.Intermediate, value, pos\n    return QtGui.QValidator.Invalid, value, pos\n</code></pre>"},{"location":"reference/careamics_napari/widgets/axes_widget/#careamics_napari.widgets.axes_widget.print_axes","title":"<code>print_axes()</code>","text":"<p>Print axes.</p> Source code in <code>src/careamics_napari/widgets/axes_widget.py</code> <pre><code>@myalgo.events.use_channels.connect\ndef print_axes():\n    \"\"\"Print axes.\"\"\"\n    print(f\"Use channels: {myalgo.use_channels}\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/","title":"banner_widget","text":"<p>A banner widget with CAREamics logo, and links to Github and documentation.</p>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget.CAREamicsBanner","title":"<code>CAREamicsBanner</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>Banner widget with CAREamics logo, and links to Github and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the banner.</p> required <code>short_desc</code> <code>str</code> <p>Short description of the banner.</p> required Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>class CAREamicsBanner(QWidget):\n    \"\"\"Banner widget with CAREamics logo, and links to Github and documentation.\n\n    Parameters\n    ----------\n    title : str\n        Title of the banner.\n    short_desc : str\n        Short description of the banner.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        title: str,\n        short_desc: str,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        title : str\n            Title of the banner.\n        short_desc : str\n            Short description of the banner.\n        \"\"\"\n        super().__init__()\n\n        self.setMinimumSize(250, 200)\n\n        layout = QHBoxLayout()\n        # layout.setContentsMargins(0, 0, 0, 0)\n        self.setLayout(layout)\n\n        # bg_color = self.palette().color(QtGui.QPalette.ColorRole.Background).name()\n\n        # logo\n        icon = QPixmap(ICON_CAREAMICS)\n        img_widget = QLabel()\n        img_widget.setPixmap(icon)\n        img_widget.setFixedSize(94, 128)\n\n        # right panel\n        right_layout = QVBoxLayout()\n        right_widget = QWidget()\n        right_widget.setLayout(right_layout)\n\n        # title\n        title_label = QLabel(title)\n        title_label.setStyleSheet(\"font-weight: bold;\")\n\n        # description\n        description_widget = QPlainTextEdit()\n        description_widget.setReadOnly(True)\n        description_widget.setPlainText(short_desc)\n        description_widget.setFixedSize(200, 50)\n        # description_widget.setStyleSheet(\n        #     f\"background-color: {bg_color};\"\n        #     f\"border: 2px solid {bg_color};\"\n        # )\n\n        # bottom widget\n        bottom_widget = QWidget()\n        bottom_widget.setLayout(QHBoxLayout())\n\n        # github logo\n        gh_icon = QPixmap(ICON_GITHUB)\n        gh_widget = QLabel()\n        gh_widget.setPixmap(gh_icon)\n        gh_widget.mousePressEvent = _open_link(GH_LINK)\n        gh_widget.setCursor(QCursor(QtCore.Qt.CursorShape.PointingHandCursor))\n        gh_widget.setToolTip(\"Report issues\")\n\n        # add widgets\n        bottom_widget.layout().addWidget(_create_link(DOC_LINK, \"Documentation\"))\n        bottom_widget.layout().addWidget(gh_widget)\n\n        right_widget.layout().addWidget(title_label)\n        right_widget.layout().addWidget(description_widget)\n        right_widget.layout().addWidget(bottom_widget)\n\n        # add widgets\n        layout.addWidget(img_widget)\n        layout.addWidget(right_widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget.CAREamicsBanner.__init__","title":"<code>__init__(title, short_desc)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of the banner.</p> required <code>short_desc</code> <code>str</code> <p>Short description of the banner.</p> required Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    title: str,\n    short_desc: str,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    title : str\n        Title of the banner.\n    short_desc : str\n        Short description of the banner.\n    \"\"\"\n    super().__init__()\n\n    self.setMinimumSize(250, 200)\n\n    layout = QHBoxLayout()\n    # layout.setContentsMargins(0, 0, 0, 0)\n    self.setLayout(layout)\n\n    # bg_color = self.palette().color(QtGui.QPalette.ColorRole.Background).name()\n\n    # logo\n    icon = QPixmap(ICON_CAREAMICS)\n    img_widget = QLabel()\n    img_widget.setPixmap(icon)\n    img_widget.setFixedSize(94, 128)\n\n    # right panel\n    right_layout = QVBoxLayout()\n    right_widget = QWidget()\n    right_widget.setLayout(right_layout)\n\n    # title\n    title_label = QLabel(title)\n    title_label.setStyleSheet(\"font-weight: bold;\")\n\n    # description\n    description_widget = QPlainTextEdit()\n    description_widget.setReadOnly(True)\n    description_widget.setPlainText(short_desc)\n    description_widget.setFixedSize(200, 50)\n    # description_widget.setStyleSheet(\n    #     f\"background-color: {bg_color};\"\n    #     f\"border: 2px solid {bg_color};\"\n    # )\n\n    # bottom widget\n    bottom_widget = QWidget()\n    bottom_widget.setLayout(QHBoxLayout())\n\n    # github logo\n    gh_icon = QPixmap(ICON_GITHUB)\n    gh_widget = QLabel()\n    gh_widget.setPixmap(gh_icon)\n    gh_widget.mousePressEvent = _open_link(GH_LINK)\n    gh_widget.setCursor(QCursor(QtCore.Qt.CursorShape.PointingHandCursor))\n    gh_widget.setToolTip(\"Report issues\")\n\n    # add widgets\n    bottom_widget.layout().addWidget(_create_link(DOC_LINK, \"Documentation\"))\n    bottom_widget.layout().addWidget(gh_widget)\n\n    right_widget.layout().addWidget(title_label)\n    right_widget.layout().addWidget(description_widget)\n    right_widget.layout().addWidget(bottom_widget)\n\n    # add widgets\n    layout.addWidget(img_widget)\n    layout.addWidget(right_widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget._create_link","title":"<code>_create_link(link, text)</code>","text":"<p>Create link label.</p> <p>Parameters:</p> Name Type Description Default <code>link</code> <code>str</code> <p>Link.</p> required <code>text</code> <code>str</code> <p>Text to display.</p> required <p>Returns:</p> Type Description <code>QLabel</code> <p>Link label.</p> Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>def _create_link(link: str, text: str) -&gt; QLabel:\n    \"\"\"Create link label.\n\n    Parameters\n    ----------\n    link : str\n        Link.\n    text : str\n        Text to display.\n\n    Returns\n    -------\n    QLabel\n        Link label.\n    \"\"\"\n    label = QLabel()\n    label.setContentsMargins(0, 5, 0, 5)\n\n    label.setText(f\"&lt;a href='{link}' style='color:white'&gt;{text}&lt;/a&gt;\")\n    label.setToolTip(\"Visit the documentation for how to use this plugin.\")\n\n    font = QFont()\n    font.setPointSize(11)\n    label.setFont(font)\n\n    label.setOpenExternalLinks(True)\n\n    return label\n</code></pre>"},{"location":"reference/careamics_napari/widgets/banner_widget/#careamics_napari.widgets.banner_widget._open_link","title":"<code>_open_link(link)</code>","text":"<p>Open link in browser.</p> <p>Parameters:</p> Name Type Description Default <code>link</code> <code>str</code> <p>Link to open.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Link opener.</p> Source code in <code>src/careamics_napari/widgets/banner_widget.py</code> <pre><code>def _open_link(link: str) -&gt; Callable:\n    \"\"\"Open link in browser.\n\n    Parameters\n    ----------\n    link : str\n        Link to open.\n\n    Returns\n    -------\n    Callable\n        Link opener.\n    \"\"\"\n\n    def link_opener(_: Any) -&gt; None:\n        \"\"\"Open link in browser.\n\n        Parameters\n        ----------\n        _ : Any\n            Unused parameter.\n        \"\"\"\n        webbrowser.open(link)\n\n    return link_opener\n</code></pre>"},{"location":"reference/careamics_napari/widgets/configuration_window/","title":"configuration_window","text":"<p>A dialog widget allowing modifying advanced settings.</p>"},{"location":"reference/careamics_napari/widgets/configuration_window/#careamics_napari.widgets.configuration_window.AdvancedConfigurationWindow","title":"<code>AdvancedConfigurationWindow</code>","text":"<p>               Bases: <code>QDialog</code></p> <p>A dialog widget allowing modifying advanced settings.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>Parent widget.</p> required <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal used to update the parameters set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/configuration_window.py</code> <pre><code>class AdvancedConfigurationWindow(QDialog):\n    \"\"\"A dialog widget allowing modifying advanced settings.\n\n    Parameters\n    ----------\n    parent : QWidget\n        Parent widget.\n    training_signal : TrainingSignal or None, default=None\n        Signal used to update the parameters set by the user.\n    \"\"\"\n\n    def __init__(\n        self, parent: QWidget, training_signal: Optional[TrainingSignal] = None\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        parent : QWidget\n            Parent widget.\n        training_signal : TrainingSignal or None, default=None\n            Signal used to update the parameters set by the user.\n        \"\"\"\n        super().__init__(parent)\n\n        self.configuration_signal = (\n            TrainingSignal()  # type: ignore\n            if training_signal is None\n            else training_signal\n        )\n\n        self.setLayout(QVBoxLayout())\n\n        ##################\n        # experiment name text box\n        experiment_widget = QWidget()\n        experiment_layout = QFormLayout()\n        experiment_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        experiment_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.experiment_name = QLineEdit()\n        self.experiment_name.setToolTip(\n            \"Name of the experiment. It will be used to save the model\\n\"\n            \"and the training history.\"\n        )\n\n        experiment_layout.addRow(\"Experiment name\", self.experiment_name)\n        experiment_widget.setLayout(experiment_layout)\n        self.layout().addWidget(experiment_widget)\n\n        ##################\n        # validation\n        validation = QGroupBox(\"Validation\")\n        validation_layout = QFormLayout()\n        validation_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        validation_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.validation_perc = create_double_spinbox(\n            0.01, 1, self.configuration_signal.val_percentage, 0.01, n_decimal=2\n        )\n        self.validation_perc.setToolTip(\n            \"Percentage of the training data used for validation.\"\n        )\n\n        self.validation_split = create_int_spinbox(\n            1, 100, self.configuration_signal.val_minimum_split, 1\n        )\n        self.validation_perc.setToolTip(\n            \"Minimum number of patches or images in the validation set.\"\n        )\n\n        validation_layout.addRow(\"Percentage\", self.validation_perc)\n        validation_layout.addRow(\"Minimum split\", self.validation_split)\n        validation.setLayout(validation_layout)\n        self.layout().addWidget(validation)\n\n        ##################\n        # augmentations group box, with x_flip, y_flip and rotations\n        augmentations = QGroupBox(\"Augmentations\")\n        augmentations_layout = QFormLayout()\n        augmentations_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        augmentations_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.x_flip = QCheckBox(\"X Flip\")\n        self.x_flip.setToolTip(\n            \"Check to add augmentation that flips the image\\n\" \"along the x-axis\"\n        )\n        self.x_flip.setChecked(self.configuration_signal.x_flip)\n\n        self.y_flip = QCheckBox(\"Y Flip\")\n        self.y_flip.setToolTip(\n            \"Check to add augmentation that flips the image\\n\" \"along the y-axis\"\n        )\n        self.y_flip.setChecked(self.configuration_signal.y_flip)\n\n        self.rotations = QCheckBox(\"90 Rotations\")\n        self.rotations.setToolTip(\n            \"Check to add augmentation that rotates the image\\n\"\n            \"in 90 degree increments in XY\"\n        )\n        self.rotations.setChecked(self.configuration_signal.rotations)\n\n        augmentations_layout.addRow(self.x_flip)\n        augmentations_layout.addRow(self.y_flip)\n        augmentations_layout.addRow(self.rotations)\n        augmentations.setLayout(augmentations_layout)\n        self.layout().addWidget(augmentations)\n\n        ##################\n        # channels\n        self.channels = QGroupBox(\"Channels\")\n        channels_layout = QVBoxLayout()\n\n        ind_channels_widget = QWidget()\n        ind_channels_layout = QFormLayout()\n        ind_channels_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        ind_channels_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.independent_channels = QCheckBox(\"Independent\")\n        self.independent_channels.setToolTip(\n            \"Check to treat the channels independently during\\n\" \"training.\"\n        )\n        self.independent_channels.setChecked(\n            self.configuration_signal.independent_channels\n        )\n\n        ind_channels_layout.addRow(self.independent_channels)\n        ind_channels_widget.setLayout(ind_channels_layout)\n\n        # n2v\n        n2v_channels_widget = QWidget()\n        n2v_channels_widget_layout = QFormLayout()\n        n2v_channels_widget_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        n2v_channels_widget_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.n_channels = create_int_spinbox(\n            1, 10, self.configuration_signal.n_channels_n2v, 1\n        )\n        self.n_channels.setToolTip(\"Number of channels in the input images\")\n\n        n2v_channels_widget_layout.addRow(\"Channels\", self.n_channels)\n        n2v_channels_widget.setLayout(n2v_channels_widget_layout)\n\n        # care/n2n\n        care_channels_widget = QWidget()\n        care_channels_widget_layout = QFormLayout()\n        care_channels_widget_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        care_channels_widget_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.n_channels_in = create_int_spinbox(\n            1, 10, self.configuration_signal.n_channels_in_care, 1\n        )\n        self.n_channels_out = create_int_spinbox(\n            1, 10, self.configuration_signal.n_channels_out_care, 1\n        )\n\n        care_channels_widget_layout.addRow(\"Channels in\", self.n_channels_in)\n        care_channels_widget_layout.addRow(\"Channels out\", self.n_channels_out)\n        care_channels_widget.setLayout(care_channels_widget_layout)\n\n        # stack n2v and care\n        self.channels_stack = QStackedWidget()\n        self.channels_stack.addWidget(n2v_channels_widget)\n        self.channels_stack.addWidget(care_channels_widget)\n\n        channels_layout.addWidget(ind_channels_widget)\n        channels_layout.addWidget(self.channels_stack)\n        self.channels.setLayout(channels_layout)\n        self.layout().addWidget(self.channels)\n\n        ##################\n        # n2v2\n        self.n2v2_widget = QGroupBox(\"N2V2\")\n        n2v2_layout = QFormLayout()\n        n2v2_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        n2v2_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.use_n2v2 = QCheckBox(\"Use N2V2\")\n        self.use_n2v2.setToolTip(\"Check to use N2V2 for training.\")\n        self.use_n2v2.setChecked(self.configuration_signal.use_n2v2)\n\n        n2v2_layout.addRow(self.use_n2v2)\n        self.n2v2_widget.setLayout(n2v2_layout)\n        self.layout().addWidget(self.n2v2_widget)\n\n        ##################\n        # model params\n        model_params = QGroupBox(\"UNet parameters\")\n        model_params_layout = QFormLayout()\n        model_params_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        model_params_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.model_depth = create_int_spinbox(2, 5, self.configuration_signal.depth, 1)\n        self.model_depth.setToolTip(\"Depth of the U-Net model.\")\n        self.size_conv_filters = create_int_spinbox(\n            8, 1024, self.configuration_signal.num_conv_filters, 8\n        )\n        self.size_conv_filters.setToolTip(\n            \"Number of convolutional filters in the first layer.\"\n        )\n\n        model_params_layout.addRow(\"Depth\", self.model_depth)\n        model_params_layout.addRow(\"N filters\", self.size_conv_filters)\n        model_params.setLayout(model_params_layout)\n        self.layout().addWidget(model_params)\n\n        ##################\n        # save button\n        button_widget = QWidget()\n        button_widget.setLayout(QVBoxLayout())\n        self.save_button = QPushButton(\"Save\")\n        self.save_button.setMinimumWidth(120)\n        button_widget.layout().addWidget(self.save_button)\n        button_widget.layout().setAlignment(Qt.AlignmentFlag.AlignCenter)\n        self.layout().addWidget(button_widget)\n\n        ##################\n        # actions and set defaults\n        self.save_button.clicked.connect(self._save)\n\n        if self.configuration_signal is not None:\n            self.configuration_signal.events.use_channels.connect(\n                self._update_to_channels\n            )\n            self._update_to_channels(self.configuration_signal.use_channels)\n\n            self.configuration_signal.events.algorithm.connect(\n                self._update_to_algorithm\n            )\n            self._update_to_algorithm(self.configuration_signal.algorithm)\n\n    def _update_to_algorithm(self: Self, name: str) -&gt; None:\n        \"\"\"Update the widget to the selected algorithm.\n\n        If Noise2Void is selected, the widget will show the N2V2 parameters.\n\n        Parameters\n        ----------\n        name : str\n            Name of the selected algorithm, as defined in SupportedAlgorithm.\n        \"\"\"\n        if name == SupportedAlgorithm.N2V.value:\n            self.n2v2_widget.setVisible(True)\n            self.channels_stack.setCurrentIndex(0)\n        else:\n            self.n2v2_widget.setVisible(False)\n            self.channels_stack.setCurrentIndex(1)\n\n    def _update_to_channels(self: Self, use_channels: bool) -&gt; None:\n        \"\"\"Update the widget to show the channels parameters.\n\n        Parameters\n        ----------\n        use_channels : bool\n            Whether to show the channels parameters.\n        \"\"\"\n        self.channels.setVisible(use_channels)\n\n    def _save(self: Self) -&gt; None:\n        \"\"\"Save the parameters and close the dialog.\"\"\"\n        # Update the parameters\n        if self.configuration_signal is not None:\n            self.configuration_signal.experiment_name = self.experiment_name.text()\n            self.configuration_signal.val_percentage = self.validation_perc.value()\n            self.configuration_signal.val_minimum_split = self.validation_split.value()\n            self.configuration_signal.x_flip = self.x_flip.isChecked()\n            self.configuration_signal.y_flip = self.y_flip.isChecked()\n            self.configuration_signal.rotations = self.rotations.isChecked()\n            self.configuration_signal.independent_channels = (\n                self.independent_channels.isChecked()\n            )\n            self.configuration_signal.n_channels_n2v = self.n_channels.value()\n            self.configuration_signal.n_channels_in_care = self.n_channels_in.value()\n            self.configuration_signal.n_channels_out_care = self.n_channels_out.value()\n            self.configuration_signal.use_n2v2 = self.use_n2v2.isChecked()\n            self.configuration_signal.depth = self.model_depth.value()\n            self.configuration_signal.num_conv_filters = self.size_conv_filters.value()\n\n        self.close()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/configuration_window/#careamics_napari.widgets.configuration_window.AdvancedConfigurationWindow.__init__","title":"<code>__init__(parent, training_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>parent</code> <code>QWidget</code> <p>Parent widget.</p> required <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal used to update the parameters set by the user.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/configuration_window.py</code> <pre><code>def __init__(\n    self, parent: QWidget, training_signal: Optional[TrainingSignal] = None\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    parent : QWidget\n        Parent widget.\n    training_signal : TrainingSignal or None, default=None\n        Signal used to update the parameters set by the user.\n    \"\"\"\n    super().__init__(parent)\n\n    self.configuration_signal = (\n        TrainingSignal()  # type: ignore\n        if training_signal is None\n        else training_signal\n    )\n\n    self.setLayout(QVBoxLayout())\n\n    ##################\n    # experiment name text box\n    experiment_widget = QWidget()\n    experiment_layout = QFormLayout()\n    experiment_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    experiment_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.experiment_name = QLineEdit()\n    self.experiment_name.setToolTip(\n        \"Name of the experiment. It will be used to save the model\\n\"\n        \"and the training history.\"\n    )\n\n    experiment_layout.addRow(\"Experiment name\", self.experiment_name)\n    experiment_widget.setLayout(experiment_layout)\n    self.layout().addWidget(experiment_widget)\n\n    ##################\n    # validation\n    validation = QGroupBox(\"Validation\")\n    validation_layout = QFormLayout()\n    validation_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    validation_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.validation_perc = create_double_spinbox(\n        0.01, 1, self.configuration_signal.val_percentage, 0.01, n_decimal=2\n    )\n    self.validation_perc.setToolTip(\n        \"Percentage of the training data used for validation.\"\n    )\n\n    self.validation_split = create_int_spinbox(\n        1, 100, self.configuration_signal.val_minimum_split, 1\n    )\n    self.validation_perc.setToolTip(\n        \"Minimum number of patches or images in the validation set.\"\n    )\n\n    validation_layout.addRow(\"Percentage\", self.validation_perc)\n    validation_layout.addRow(\"Minimum split\", self.validation_split)\n    validation.setLayout(validation_layout)\n    self.layout().addWidget(validation)\n\n    ##################\n    # augmentations group box, with x_flip, y_flip and rotations\n    augmentations = QGroupBox(\"Augmentations\")\n    augmentations_layout = QFormLayout()\n    augmentations_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    augmentations_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.x_flip = QCheckBox(\"X Flip\")\n    self.x_flip.setToolTip(\n        \"Check to add augmentation that flips the image\\n\" \"along the x-axis\"\n    )\n    self.x_flip.setChecked(self.configuration_signal.x_flip)\n\n    self.y_flip = QCheckBox(\"Y Flip\")\n    self.y_flip.setToolTip(\n        \"Check to add augmentation that flips the image\\n\" \"along the y-axis\"\n    )\n    self.y_flip.setChecked(self.configuration_signal.y_flip)\n\n    self.rotations = QCheckBox(\"90 Rotations\")\n    self.rotations.setToolTip(\n        \"Check to add augmentation that rotates the image\\n\"\n        \"in 90 degree increments in XY\"\n    )\n    self.rotations.setChecked(self.configuration_signal.rotations)\n\n    augmentations_layout.addRow(self.x_flip)\n    augmentations_layout.addRow(self.y_flip)\n    augmentations_layout.addRow(self.rotations)\n    augmentations.setLayout(augmentations_layout)\n    self.layout().addWidget(augmentations)\n\n    ##################\n    # channels\n    self.channels = QGroupBox(\"Channels\")\n    channels_layout = QVBoxLayout()\n\n    ind_channels_widget = QWidget()\n    ind_channels_layout = QFormLayout()\n    ind_channels_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    ind_channels_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.independent_channels = QCheckBox(\"Independent\")\n    self.independent_channels.setToolTip(\n        \"Check to treat the channels independently during\\n\" \"training.\"\n    )\n    self.independent_channels.setChecked(\n        self.configuration_signal.independent_channels\n    )\n\n    ind_channels_layout.addRow(self.independent_channels)\n    ind_channels_widget.setLayout(ind_channels_layout)\n\n    # n2v\n    n2v_channels_widget = QWidget()\n    n2v_channels_widget_layout = QFormLayout()\n    n2v_channels_widget_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    n2v_channels_widget_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.n_channels = create_int_spinbox(\n        1, 10, self.configuration_signal.n_channels_n2v, 1\n    )\n    self.n_channels.setToolTip(\"Number of channels in the input images\")\n\n    n2v_channels_widget_layout.addRow(\"Channels\", self.n_channels)\n    n2v_channels_widget.setLayout(n2v_channels_widget_layout)\n\n    # care/n2n\n    care_channels_widget = QWidget()\n    care_channels_widget_layout = QFormLayout()\n    care_channels_widget_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    care_channels_widget_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.n_channels_in = create_int_spinbox(\n        1, 10, self.configuration_signal.n_channels_in_care, 1\n    )\n    self.n_channels_out = create_int_spinbox(\n        1, 10, self.configuration_signal.n_channels_out_care, 1\n    )\n\n    care_channels_widget_layout.addRow(\"Channels in\", self.n_channels_in)\n    care_channels_widget_layout.addRow(\"Channels out\", self.n_channels_out)\n    care_channels_widget.setLayout(care_channels_widget_layout)\n\n    # stack n2v and care\n    self.channels_stack = QStackedWidget()\n    self.channels_stack.addWidget(n2v_channels_widget)\n    self.channels_stack.addWidget(care_channels_widget)\n\n    channels_layout.addWidget(ind_channels_widget)\n    channels_layout.addWidget(self.channels_stack)\n    self.channels.setLayout(channels_layout)\n    self.layout().addWidget(self.channels)\n\n    ##################\n    # n2v2\n    self.n2v2_widget = QGroupBox(\"N2V2\")\n    n2v2_layout = QFormLayout()\n    n2v2_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    n2v2_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.use_n2v2 = QCheckBox(\"Use N2V2\")\n    self.use_n2v2.setToolTip(\"Check to use N2V2 for training.\")\n    self.use_n2v2.setChecked(self.configuration_signal.use_n2v2)\n\n    n2v2_layout.addRow(self.use_n2v2)\n    self.n2v2_widget.setLayout(n2v2_layout)\n    self.layout().addWidget(self.n2v2_widget)\n\n    ##################\n    # model params\n    model_params = QGroupBox(\"UNet parameters\")\n    model_params_layout = QFormLayout()\n    model_params_layout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    model_params_layout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.model_depth = create_int_spinbox(2, 5, self.configuration_signal.depth, 1)\n    self.model_depth.setToolTip(\"Depth of the U-Net model.\")\n    self.size_conv_filters = create_int_spinbox(\n        8, 1024, self.configuration_signal.num_conv_filters, 8\n    )\n    self.size_conv_filters.setToolTip(\n        \"Number of convolutional filters in the first layer.\"\n    )\n\n    model_params_layout.addRow(\"Depth\", self.model_depth)\n    model_params_layout.addRow(\"N filters\", self.size_conv_filters)\n    model_params.setLayout(model_params_layout)\n    self.layout().addWidget(model_params)\n\n    ##################\n    # save button\n    button_widget = QWidget()\n    button_widget.setLayout(QVBoxLayout())\n    self.save_button = QPushButton(\"Save\")\n    self.save_button.setMinimumWidth(120)\n    button_widget.layout().addWidget(self.save_button)\n    button_widget.layout().setAlignment(Qt.AlignmentFlag.AlignCenter)\n    self.layout().addWidget(button_widget)\n\n    ##################\n    # actions and set defaults\n    self.save_button.clicked.connect(self._save)\n\n    if self.configuration_signal is not None:\n        self.configuration_signal.events.use_channels.connect(\n            self._update_to_channels\n        )\n        self._update_to_channels(self.configuration_signal.use_channels)\n\n        self.configuration_signal.events.algorithm.connect(\n            self._update_to_algorithm\n        )\n        self._update_to_algorithm(self.configuration_signal.algorithm)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/configuration_window/#careamics_napari.widgets.configuration_window.AdvancedConfigurationWindow._save","title":"<code>_save()</code>","text":"<p>Save the parameters and close the dialog.</p> Source code in <code>src/careamics_napari/widgets/configuration_window.py</code> <pre><code>def _save(self: Self) -&gt; None:\n    \"\"\"Save the parameters and close the dialog.\"\"\"\n    # Update the parameters\n    if self.configuration_signal is not None:\n        self.configuration_signal.experiment_name = self.experiment_name.text()\n        self.configuration_signal.val_percentage = self.validation_perc.value()\n        self.configuration_signal.val_minimum_split = self.validation_split.value()\n        self.configuration_signal.x_flip = self.x_flip.isChecked()\n        self.configuration_signal.y_flip = self.y_flip.isChecked()\n        self.configuration_signal.rotations = self.rotations.isChecked()\n        self.configuration_signal.independent_channels = (\n            self.independent_channels.isChecked()\n        )\n        self.configuration_signal.n_channels_n2v = self.n_channels.value()\n        self.configuration_signal.n_channels_in_care = self.n_channels_in.value()\n        self.configuration_signal.n_channels_out_care = self.n_channels_out.value()\n        self.configuration_signal.use_n2v2 = self.use_n2v2.isChecked()\n        self.configuration_signal.depth = self.model_depth.value()\n        self.configuration_signal.num_conv_filters = self.size_conv_filters.value()\n\n    self.close()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/configuration_window/#careamics_napari.widgets.configuration_window.AdvancedConfigurationWindow._update_to_algorithm","title":"<code>_update_to_algorithm(name)</code>","text":"<p>Update the widget to the selected algorithm.</p> <p>If Noise2Void is selected, the widget will show the N2V2 parameters.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the selected algorithm, as defined in SupportedAlgorithm.</p> required Source code in <code>src/careamics_napari/widgets/configuration_window.py</code> <pre><code>def _update_to_algorithm(self: Self, name: str) -&gt; None:\n    \"\"\"Update the widget to the selected algorithm.\n\n    If Noise2Void is selected, the widget will show the N2V2 parameters.\n\n    Parameters\n    ----------\n    name : str\n        Name of the selected algorithm, as defined in SupportedAlgorithm.\n    \"\"\"\n    if name == SupportedAlgorithm.N2V.value:\n        self.n2v2_widget.setVisible(True)\n        self.channels_stack.setCurrentIndex(0)\n    else:\n        self.n2v2_widget.setVisible(False)\n        self.channels_stack.setCurrentIndex(1)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/configuration_window/#careamics_napari.widgets.configuration_window.AdvancedConfigurationWindow._update_to_channels","title":"<code>_update_to_channels(use_channels)</code>","text":"<p>Update the widget to show the channels parameters.</p> <p>Parameters:</p> Name Type Description Default <code>use_channels</code> <code>bool</code> <p>Whether to show the channels parameters.</p> required Source code in <code>src/careamics_napari/widgets/configuration_window.py</code> <pre><code>def _update_to_channels(self: Self, use_channels: bool) -&gt; None:\n    \"\"\"Update the widget to show the channels parameters.\n\n    Parameters\n    ----------\n    use_channels : bool\n        Whether to show the channels parameters.\n    \"\"\"\n    self.channels.setVisible(use_channels)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/","title":"folder_widget","text":"<p>A widget used for selecting an existing folder.</p>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget","title":"<code>FolderWidget</code>","text":"<p>               Bases: <code>QWidget</code></p> <p>A widget used for selecting an existing folder.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text displayed on the button.</p> required Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>class FolderWidget(QWidget):\n    \"\"\"A widget used for selecting an existing folder.\n\n    Parameters\n    ----------\n    text : str\n        Text displayed on the button.\n    \"\"\"\n\n    def __init__(self: Self, text: str) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        text : str\n            Text displayed on the button.\n        \"\"\"\n        super().__init__()\n\n        self.setLayout(QHBoxLayout())\n        self.layout().setSpacing(0)\n        self.layout().setContentsMargins(0, 0, 0, 0)\n\n        # text field\n        self.text_field = QLineEdit(\"\")\n        self.layout().addWidget(self.text_field)\n\n        # folder selection button\n        self.button = QPushButton(text)\n        self.layout().addWidget(self.button)\n        self.button.clicked.connect(self._open_dialog)\n\n    def _open_dialog(self: Self) -&gt; None:\n        \"\"\"Open a dialog to select a folder.\"\"\"\n        path = QFileDialog.getExistingDirectory(self, \"Select Folder\")\n        print(path)\n\n        # set text in the text field\n        self.text_field.setText(path)\n\n    def get_folder(self: Self) -&gt; str:\n        \"\"\"Get the selected folder.\n\n        Returns\n        -------\n        str\n            The selected folder as read out from the text field.\n        \"\"\"\n        return self.text_field.text()\n\n    def get_text_widget(self: Self) -&gt; QLineEdit:\n        \"\"\"Get the text widget.\n\n        Returns\n        -------\n        QLineEdit\n            The text widget.\n        \"\"\"\n        return self.text_field\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.__init__","title":"<code>__init__(text)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text displayed on the button.</p> required Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def __init__(self: Self, text: str) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    text : str\n        Text displayed on the button.\n    \"\"\"\n    super().__init__()\n\n    self.setLayout(QHBoxLayout())\n    self.layout().setSpacing(0)\n    self.layout().setContentsMargins(0, 0, 0, 0)\n\n    # text field\n    self.text_field = QLineEdit(\"\")\n    self.layout().addWidget(self.text_field)\n\n    # folder selection button\n    self.button = QPushButton(text)\n    self.layout().addWidget(self.button)\n    self.button.clicked.connect(self._open_dialog)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget._open_dialog","title":"<code>_open_dialog()</code>","text":"<p>Open a dialog to select a folder.</p> Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def _open_dialog(self: Self) -&gt; None:\n    \"\"\"Open a dialog to select a folder.\"\"\"\n    path = QFileDialog.getExistingDirectory(self, \"Select Folder\")\n    print(path)\n\n    # set text in the text field\n    self.text_field.setText(path)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.get_folder","title":"<code>get_folder()</code>","text":"<p>Get the selected folder.</p> <p>Returns:</p> Type Description <code>str</code> <p>The selected folder as read out from the text field.</p> Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def get_folder(self: Self) -&gt; str:\n    \"\"\"Get the selected folder.\n\n    Returns\n    -------\n    str\n        The selected folder as read out from the text field.\n    \"\"\"\n    return self.text_field.text()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/folder_widget/#careamics_napari.widgets.folder_widget.FolderWidget.get_text_widget","title":"<code>get_text_widget()</code>","text":"<p>Get the text widget.</p> <p>Returns:</p> Type Description <code>QLineEdit</code> <p>The text widget.</p> Source code in <code>src/careamics_napari/widgets/folder_widget.py</code> <pre><code>def get_text_widget(self: Self) -&gt; QLineEdit:\n    \"\"\"Get the text widget.\n\n    Returns\n    -------\n    QLineEdit\n        The text widget.\n    \"\"\"\n    return self.text_field\n</code></pre>"},{"location":"reference/careamics_napari/widgets/gpu_widget/","title":"gpu_widget","text":"<p>A label indicating whether GPU is available to torch.</p>"},{"location":"reference/careamics_napari/widgets/gpu_widget/#careamics_napari.widgets.gpu_widget.create_gpu_label","title":"<code>create_gpu_label()</code>","text":"<p>A label widget indicating whether GPU or CPU is available with torch.</p> <p>Returns:</p> Type Description <code>QLabel</code> <p>GPU label widget.</p> Source code in <code>src/careamics_napari/widgets/gpu_widget.py</code> <pre><code>def create_gpu_label() -&gt; QLabel:\n    \"\"\"A label widget indicating whether GPU or CPU is available with torch.\n\n    Returns\n    -------\n    QLabel\n        GPU label widget.\n    \"\"\"\n    if is_gpu_available():\n        text = \"GPU\"\n        color = \"ADC2A9\"  # green\n    else:\n        text = \"CPU\"\n        color = \"FFDBA4\"  # yellow\n\n    gpu_label = QLabel(text)\n    font_color = gpu_label.palette().color(gpu_label.foregroundRole()).name()[1:]\n    gpu_label.setStyleSheet(\n        f\"\"\"\n            QLabel {{\n                font-weight: bold; color: #{color};\n            }}\n            QToolTip {{\n                color: {font_color};\n            }}\n        \"\"\"\n    )\n    gpu_label.setToolTip(\n        \"Indicates whether PyTorch has access to a GPU.\\n\"\n        \"If your machine has GPU and this label indicates\\n\"\n        \"CPU, please check your PyTorch installation.\"\n    )\n\n    return gpu_label\n</code></pre>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/","title":"magicgui_widgets","text":"<p>Magicgui widgets.</p>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/#careamics_napari.widgets.magicgui_widgets.layer_choice","title":"<code>layer_choice(annotation=Image, **kwargs)</code>","text":"<p>Create a widget to select a layer from the napari viewer.</p> <p>Parameters:</p> Name Type Description Default <code>annotation</code> <code>Any or None</code> <p>The annotation type to filter the layers.</p> <code>Image</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the widget.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Widget</code> <p>The widget to select a layer from the napari viewer.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If napari is not installed.</p> Source code in <code>src/careamics_napari/widgets/magicgui_widgets.py</code> <pre><code>def layer_choice(annotation: Optional[Any] = Image, **kwargs: Any) -&gt; Widget:\n    \"\"\"Create a widget to select a layer from the napari viewer.\n\n    Parameters\n    ----------\n    annotation : Any or None, default=Image\n        The annotation type to filter the layers.\n    **kwargs : Any\n        Additional keyword arguments to pass to the widget.\n\n    Returns\n    -------\n    Widget\n        The widget to select a layer from the napari viewer.\n\n    Raises\n    ------\n    ImportError\n        If napari is not installed.\n    \"\"\"\n    if not _has_napari:\n        raise ImportError(\"napari is not installed.\")\n\n    widget: Widget = create_widget(annotation=annotation, **kwargs)\n    widget.reset_choices()\n    viewer = current_viewer()\n\n    # connect to napari events\n    viewer.layers.events.inserted.connect(widget.reset_choices)\n    viewer.layers.events.removed.connect(widget.reset_choices)\n    viewer.layers.events.changed.connect(widget.reset_choices)\n\n    return widget\n</code></pre>"},{"location":"reference/careamics_napari/widgets/magicgui_widgets/#careamics_napari.widgets.magicgui_widgets.load_button","title":"<code>load_button(Model)</code>","text":"<p>A button to load model files.</p> <p>Parameters:</p> Name Type Description Default <code>Model</code> <code>Path</code> <p>The path to the model file.</p> required Source code in <code>src/careamics_napari/widgets/magicgui_widgets.py</code> <pre><code>@magic_factory(auto_call=True, Model={\"mode\": \"r\", \"filter\": \"*.ckpt *.zip\"})\ndef load_button(Model: Path):\n    \"\"\"A button to load model files.\n\n    Parameters\n    ----------\n    Model : pathlib.Path\n        The path to the model file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/","title":"predict_data_widget","text":"<p>A widget used to select a path or layer for prediction.</p>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget","title":"<code>PredictDataWidget</code>","text":"<p>               Bases: <code>QTabWidget</code></p> <p>A widget offering to select a layer from napari or a path from disk.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_signal</code> <code>PredConfigurationSignal</code> <p>Signal to be updated with changed in widgets values.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>class PredictDataWidget(QTabWidget):\n    \"\"\"A widget offering to select a layer from napari or a path from disk.\n\n    Parameters\n    ----------\n    prediction_signal : PredConfigurationSignal, default=None\n        Signal to be updated with changed in widgets values.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        prediction_signal: Optional[PredictionSignal] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        prediction_signal : PredConfigurationSignal, default=None\n            Signal to be updated with changed in widgets values.\n        \"\"\"\n        super().__init__()\n\n        self.config_signal = (\n            PredictionSignal()  # type: ignore\n            if prediction_signal is None\n            else prediction_signal\n        )\n\n        # QTabs\n        layer_tab = QWidget()\n        layer_tab.setLayout(QVBoxLayout())\n        disk_tab = QWidget()\n        disk_tab.setLayout(QVBoxLayout())\n\n        # add tabs\n        self.addTab(layer_tab, \"From layers\")\n        self.addTab(disk_tab, \"From disk\")\n        self.setTabToolTip(0, \"Use images from napari layers\")\n        self.setTabToolTip(1, \"Use iamges saved on the disk\")\n\n        # set tabs\n        self._set_layer_tab(layer_tab)\n        self._set_disk_tab(disk_tab)\n\n        # set actions\n        if self.config_signal is not None:\n            self.currentChanged.connect(self._set_data_source)\n            self._set_data_source(self.currentIndex())\n\n    def _set_layer_tab(\n        self: Self,\n        layer_tab: QWidget,\n    ) -&gt; None:\n        \"\"\"Set up the layer tab.\n\n        Parameters\n        ----------\n        layer_tab : QWidget\n            The layer tab.\n        \"\"\"\n        if _has_napari and napari.current_viewer() is not None:\n            form = QFormLayout()\n            form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n            form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n            form.setContentsMargins(12, 12, 0, 0)\n            widget_layers = QWidget()\n            widget_layers.setLayout(form)\n\n            self.img_pred = layer_choice()\n            form.addRow(\"Predict\", self.img_pred.native)\n\n            layer_tab.layout().addWidget(widget_layers)\n\n            # connection actions for images\n            self.img_pred.changed.connect(self._update_pred_layer)\n            # to cover the case when image was loaded before the plugin\n            if self.img_pred.value is not None:\n                self._update_pred_layer(self.img_pred.value)\n\n        else:\n            # simply remove the tab\n            self.removeTab(0)\n\n    def _set_disk_tab(self: Self, disk_tab: QWidget) -&gt; None:\n        \"\"\"Set up the disk tab.\n\n        Parameters\n        ----------\n        disk_tab : QWidget\n            The disk tab.\n        \"\"\"\n        # disk tab\n        buttons = QWidget()\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n        self.pred_images_folder = FolderWidget(\"Choose\")\n        form.addRow(\"Predict\", self.pred_images_folder)\n\n        self.pred_images_folder.setToolTip(\"Select a folder containing images.\")\n\n        # add actions\n        self.pred_images_folder.get_text_widget().textChanged.connect(\n            self._update_pred_folder\n        )\n\n        buttons.setLayout(form)\n        disk_tab.layout().addWidget(buttons)\n\n    def _set_data_source(self: Self, index: int) -&gt; None:\n        \"\"\"Set the load_from_disk attribute of the signal based on the selected tab.\n\n        Parameters\n        ----------\n        index : int\n            Index of the selected tab.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.load_from_disk = index == self.count() - 1\n\n    def _update_pred_layer(self: Self, layer: Image) -&gt; None:\n        \"\"\"Update the layer attribute of the signal.\n\n        Parameters\n        ----------\n        layer : Image\n            The selected layer.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.layer_pred = layer\n\n    def _update_pred_folder(self: Self, folder: str) -&gt; None:\n        \"\"\"Update the path attribute of the signal.\n\n        Parameters\n        ----------\n        folder : str\n            The selected folder.\n        \"\"\"\n        if self.config_signal.path_pred is not None:\n            self.config_signal.path_pred = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget.__init__","title":"<code>__init__(prediction_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>prediction_signal</code> <code>PredConfigurationSignal</code> <p>Signal to be updated with changed in widgets values.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    prediction_signal: Optional[PredictionSignal] = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    prediction_signal : PredConfigurationSignal, default=None\n        Signal to be updated with changed in widgets values.\n    \"\"\"\n    super().__init__()\n\n    self.config_signal = (\n        PredictionSignal()  # type: ignore\n        if prediction_signal is None\n        else prediction_signal\n    )\n\n    # QTabs\n    layer_tab = QWidget()\n    layer_tab.setLayout(QVBoxLayout())\n    disk_tab = QWidget()\n    disk_tab.setLayout(QVBoxLayout())\n\n    # add tabs\n    self.addTab(layer_tab, \"From layers\")\n    self.addTab(disk_tab, \"From disk\")\n    self.setTabToolTip(0, \"Use images from napari layers\")\n    self.setTabToolTip(1, \"Use iamges saved on the disk\")\n\n    # set tabs\n    self._set_layer_tab(layer_tab)\n    self._set_disk_tab(disk_tab)\n\n    # set actions\n    if self.config_signal is not None:\n        self.currentChanged.connect(self._set_data_source)\n        self._set_data_source(self.currentIndex())\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget._set_data_source","title":"<code>_set_data_source(index)</code>","text":"<p>Set the load_from_disk attribute of the signal based on the selected tab.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the selected tab.</p> required Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def _set_data_source(self: Self, index: int) -&gt; None:\n    \"\"\"Set the load_from_disk attribute of the signal based on the selected tab.\n\n    Parameters\n    ----------\n    index : int\n        Index of the selected tab.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.load_from_disk = index == self.count() - 1\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget._set_disk_tab","title":"<code>_set_disk_tab(disk_tab)</code>","text":"<p>Set up the disk tab.</p> <p>Parameters:</p> Name Type Description Default <code>disk_tab</code> <code>QWidget</code> <p>The disk tab.</p> required Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def _set_disk_tab(self: Self, disk_tab: QWidget) -&gt; None:\n    \"\"\"Set up the disk tab.\n\n    Parameters\n    ----------\n    disk_tab : QWidget\n        The disk tab.\n    \"\"\"\n    # disk tab\n    buttons = QWidget()\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n\n    self.pred_images_folder = FolderWidget(\"Choose\")\n    form.addRow(\"Predict\", self.pred_images_folder)\n\n    self.pred_images_folder.setToolTip(\"Select a folder containing images.\")\n\n    # add actions\n    self.pred_images_folder.get_text_widget().textChanged.connect(\n        self._update_pred_folder\n    )\n\n    buttons.setLayout(form)\n    disk_tab.layout().addWidget(buttons)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget._set_layer_tab","title":"<code>_set_layer_tab(layer_tab)</code>","text":"<p>Set up the layer tab.</p> <p>Parameters:</p> Name Type Description Default <code>layer_tab</code> <code>QWidget</code> <p>The layer tab.</p> required Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def _set_layer_tab(\n    self: Self,\n    layer_tab: QWidget,\n) -&gt; None:\n    \"\"\"Set up the layer tab.\n\n    Parameters\n    ----------\n    layer_tab : QWidget\n        The layer tab.\n    \"\"\"\n    if _has_napari and napari.current_viewer() is not None:\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n        form.setContentsMargins(12, 12, 0, 0)\n        widget_layers = QWidget()\n        widget_layers.setLayout(form)\n\n        self.img_pred = layer_choice()\n        form.addRow(\"Predict\", self.img_pred.native)\n\n        layer_tab.layout().addWidget(widget_layers)\n\n        # connection actions for images\n        self.img_pred.changed.connect(self._update_pred_layer)\n        # to cover the case when image was loaded before the plugin\n        if self.img_pred.value is not None:\n            self._update_pred_layer(self.img_pred.value)\n\n    else:\n        # simply remove the tab\n        self.removeTab(0)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget._update_pred_folder","title":"<code>_update_pred_folder(folder)</code>","text":"<p>Update the path attribute of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The selected folder.</p> required Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def _update_pred_folder(self: Self, folder: str) -&gt; None:\n    \"\"\"Update the path attribute of the signal.\n\n    Parameters\n    ----------\n    folder : str\n        The selected folder.\n    \"\"\"\n    if self.config_signal.path_pred is not None:\n        self.config_signal.path_pred = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/predict_data_widget/#careamics_napari.widgets.predict_data_widget.PredictDataWidget._update_pred_layer","title":"<code>_update_pred_layer(layer)</code>","text":"<p>Update the layer attribute of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Image</code> <p>The selected layer.</p> required Source code in <code>src/careamics_napari/widgets/predict_data_widget.py</code> <pre><code>def _update_pred_layer(self: Self, layer: Image) -&gt; None:\n    \"\"\"Update the layer attribute of the signal.\n\n    Parameters\n    ----------\n    layer : Image\n        The selected layer.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.layer_pred = layer\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/","title":"prediction_widget","text":"<p>Widget used to run prediction from the Training plugin.</p>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget","title":"<code>PredictionWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget to run prediction on images from within the Training plugin.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>The training status signal.</p> <code>None</code> <code>pred_status</code> <code>PredictionStatus or None</code> <p>The prediction status signal.</p> <code>None</code> <code>train_signal</code> <code>TrainingSignal or None</code> <p>The training configuration signal.</p> <code>None</code> <code>pred_signal</code> <code>PredictionSignal or None</code> <p>The prediction configuration signal.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>class PredictionWidget(QGroupBox):\n    \"\"\"A widget to run prediction on images from within the Training plugin.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        The training status signal.\n    pred_status : PredictionStatus or None, default=None\n        The prediction status signal.\n    train_signal : TrainingSignal or None, default=None\n        The training configuration signal.\n    pred_signal : PredictionSignal or None, default=None\n        The prediction configuration signal.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        train_status: Optional[TrainingStatus] = None,\n        pred_status: Optional[PredictionStatus] = None,\n        train_signal: Optional[TrainingSignal] = None,\n        pred_signal: Optional[PredictionSignal] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        train_status : TrainingStatus or None, default=None\n            The training status signal.\n        pred_status : PredictionStatus or None, default=None\n            The prediction status signal.\n        train_signal : TrainingSignal or None, default=None\n            The training configuration signal.\n        pred_signal : PredictionSignal or None, default=None\n            The prediction configuration signal.\n        \"\"\"\n        super().__init__()\n\n        self.train_status = (\n            TrainingStatus() if train_status is None else train_status  # type: ignore\n        )\n        self.pred_status = (\n            PredictionStatus() if pred_status is None else pred_status  # type: ignore\n        )\n        self.train_signal = (\n            TrainingSignal() if train_signal is None else train_signal  # type: ignore\n        )\n        self.pred_signal = PredictionSignal() if pred_signal is None else pred_signal\n\n        self.setTitle(\"Prediction\")\n        self.setLayout(QVBoxLayout())\n\n        # data selection\n        predict_data_widget = PredictDataWidget(self.pred_signal)\n        self.layout().addWidget(predict_data_widget)\n\n        # checkbox\n        self.tiling_cbox = QCheckBox(\"Tile prediction\")\n        self.tiling_cbox.setToolTip(\n            \"Select to predict the image by tiles, allowing \"\n            \"to predict on large images.\"\n        )\n        self.layout().addWidget(self.tiling_cbox)\n\n        # tiling spinboxes\n        self.tile_size_xy = PowerOfTwoSpinBox(64, 1024, self.pred_signal.tile_size_xy)\n        self.tile_size_xy.setToolTip(\"Tile size in the xy dimension.\")\n        self.tile_size_xy.setEnabled(False)\n\n        self.tile_size_z = PowerOfTwoSpinBox(4, 32, self.pred_signal.tile_size_z)\n        self.tile_size_z.setToolTip(\"Tile size in the z dimension.\")\n        self.tile_size_z.setEnabled(False)\n\n        self.batch_size_spin = create_int_spinbox(1, 512, 1, 1)\n        self.batch_size_spin.setToolTip(\n            \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n        )\n        self.batch_size_spin.setEnabled(False)\n\n        tiling_form = QFormLayout()\n        tiling_form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        tiling_form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n        tiling_form.addRow(\"XY tile size\", self.tile_size_xy)\n        tiling_form.addRow(\"Z tile size\", self.tile_size_z)\n        tiling_form.addRow(\"Batch size\", self.batch_size_spin)\n        tiling_widget = QWidget()\n        tiling_widget.setLayout(tiling_form)\n        self.layout().addWidget(tiling_widget)\n\n        # prediction progress bar\n        self.pb_prediction = create_progressbar(\n            max_value=20, text_format=\"Prediction ?/?\"\n        )\n        self.pb_prediction.setToolTip(\"Show the progress of the prediction\")\n\n        # predict button\n        predictions = QWidget()\n        predictions.setLayout(QHBoxLayout())\n        self.predict_button = QPushButton(\"Predict\", self)\n        self.predict_button.setMinimumWidth(120)\n        self.predict_button.setEnabled(False)\n        self.predict_button.setToolTip(\"Run the trained model on the images\")\n\n        predictions.layout().addWidget(self.predict_button, alignment=Qt.AlignLeft)\n\n        # add to the group\n        self.layout().addWidget(self.pb_prediction)\n        self.layout().addWidget(predictions)\n\n        # actions\n        self.tiling_cbox.stateChanged.connect(self._update_tiles)\n\n        if self.pred_status is not None and self.train_status is not None:\n            # what to do when the buttons are clicked\n            self.predict_button.clicked.connect(self._predict_button_clicked)\n\n            self.tile_size_xy.valueChanged.connect(self._set_xy_tile_size)\n            self.tile_size_z.valueChanged.connect(self._set_z_tile_size)\n            self.batch_size_spin.valueChanged.connect(self._set_batch_size)\n\n            # listening to the signals\n            self.train_signal.events.is_3d.connect(self._set_3d)\n            self.train_status.events.state.connect(self._update_button_from_train)\n            self.pred_status.events.state.connect(self._update_button_from_pred)\n\n            self.pred_status.events.sample_idx.connect(self._update_sample_idx)\n            self.pred_status.events.max_samples.connect(self._update_max_sample)\n\n    def _set_xy_tile_size(self: Self, size: int) -&gt; None:\n        \"\"\"Update the signal tile size in the xy dimension.\n\n        Parameters\n        ----------\n        size : int\n            The new tile size in the xy dimension.\n        \"\"\"\n        if self.pred_signal is not None:\n            self.pred_signal.tile_size_xy = size\n\n    def _set_z_tile_size(self: Self, size: int) -&gt; None:\n        \"\"\"Update the signal tile size in the z dimension.\n\n        Parameters\n        ----------\n        size : int\n            The new tile size in the z dimension.\n        \"\"\"\n        if self.pred_signal is not None:\n            self.pred_signal.tile_size_z = size\n\n    def _set_batch_size(self: Self, size: int) -&gt; None:\n        \"\"\"Update the signal batch size.\n\n        Parameters\n        ----------\n        size : int\n            The new batch size.\n        \"\"\"\n        if self.pred_signal is not None:\n            self.pred_signal.batch_size = size\n\n    def _set_3d(self: Self, state: bool) -&gt; None:\n        \"\"\"Enable the z tile size spinbox if the data is 3D.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the 3D checkbox.\n        \"\"\"\n        if self.pred_signal.tiled:\n            self.tile_size_z.setEnabled(state)\n\n    def _update_tiles(self: Self, state: bool) -&gt; None:\n        \"\"\"Update the weidgets and the signal tiling parameter.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the tiling checkbox.\n        \"\"\"\n        self.pred_signal.tiled = state\n        self.tile_size_xy.setEnabled(state)\n        self.batch_size_spin.setEnabled(state)\n\n        if self.train_signal.is_3d:\n            self.tile_size_z.setEnabled(state)\n\n    def _update_3d_tiles(self: Self, state: bool) -&gt; None:\n        \"\"\"Enable the z tile size spinbox if the data is 3D and tiled.\n\n        Parameters\n        ----------\n        state : bool\n            The new state of the 3D checkbox.\n        \"\"\"\n        if self.pred_signal.tiled:\n            self.tile_size_z.setEnabled(state)\n\n    def _update_max_sample(self: Self, max_sample: int) -&gt; None:\n        \"\"\"Update the maximum value of the progress bar.\n\n        Parameters\n        ----------\n        max_sample : int\n            The new maximum value of the progress bar.\n        \"\"\"\n        self.pb_prediction.setMaximum(max_sample)\n\n    def _update_sample_idx(self: Self, sample: int) -&gt; None:\n        \"\"\"Update the value of the progress bar.\n\n        Parameters\n        ----------\n        sample : int\n            The new value of the progress bar.\n        \"\"\"\n        self.pb_prediction.setValue(sample + 1)\n        self.pb_prediction.setFormat(\n            f\"Sample {sample+1}/{self.pred_status.max_samples}\"\n        )\n\n    def _predict_button_clicked(self: Self) -&gt; None:\n        \"\"\"Run the prediction on the images.\"\"\"\n        if self.pred_status is not None:\n            if (\n                self.pred_status.state == PredictionState.IDLE\n                or self.train_status.state == TrainingState.DONE\n                or self.pred_status.state == PredictionState.CRASHED\n            ):\n                self.pred_status.state = PredictionState.PREDICTING\n                self.predict_button.setEnabled(False)\n\n    def _update_button_from_train(self: Self, state: TrainingState) -&gt; None:\n        \"\"\"Update the predict button based on the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            The new state of the training plugin.\n        \"\"\"\n        if state == TrainingState.DONE:\n            self.predict_button.setEnabled(True)\n        else:\n            self.predict_button.setEnabled(False)\n\n    def _update_button_from_pred(self: Self, state: PredictionState) -&gt; None:\n        \"\"\"Update the predict button based on the prediction state.\n\n        Parameters\n        ----------\n        state : PredictionState\n            The new state of the prediction plugin.\n        \"\"\"\n        if state == PredictionState.DONE or state == PredictionState.CRASHED:\n            self.predict_button.setEnabled(True)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget.__init__","title":"<code>__init__(train_status=None, pred_status=None, train_signal=None, pred_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>The training status signal.</p> <code>None</code> <code>pred_status</code> <code>PredictionStatus or None</code> <p>The prediction status signal.</p> <code>None</code> <code>train_signal</code> <code>TrainingSignal or None</code> <p>The training configuration signal.</p> <code>None</code> <code>pred_signal</code> <code>PredictionSignal or None</code> <p>The prediction configuration signal.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    train_status: Optional[TrainingStatus] = None,\n    pred_status: Optional[PredictionStatus] = None,\n    train_signal: Optional[TrainingSignal] = None,\n    pred_signal: Optional[PredictionSignal] = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        The training status signal.\n    pred_status : PredictionStatus or None, default=None\n        The prediction status signal.\n    train_signal : TrainingSignal or None, default=None\n        The training configuration signal.\n    pred_signal : PredictionSignal or None, default=None\n        The prediction configuration signal.\n    \"\"\"\n    super().__init__()\n\n    self.train_status = (\n        TrainingStatus() if train_status is None else train_status  # type: ignore\n    )\n    self.pred_status = (\n        PredictionStatus() if pred_status is None else pred_status  # type: ignore\n    )\n    self.train_signal = (\n        TrainingSignal() if train_signal is None else train_signal  # type: ignore\n    )\n    self.pred_signal = PredictionSignal() if pred_signal is None else pred_signal\n\n    self.setTitle(\"Prediction\")\n    self.setLayout(QVBoxLayout())\n\n    # data selection\n    predict_data_widget = PredictDataWidget(self.pred_signal)\n    self.layout().addWidget(predict_data_widget)\n\n    # checkbox\n    self.tiling_cbox = QCheckBox(\"Tile prediction\")\n    self.tiling_cbox.setToolTip(\n        \"Select to predict the image by tiles, allowing \"\n        \"to predict on large images.\"\n    )\n    self.layout().addWidget(self.tiling_cbox)\n\n    # tiling spinboxes\n    self.tile_size_xy = PowerOfTwoSpinBox(64, 1024, self.pred_signal.tile_size_xy)\n    self.tile_size_xy.setToolTip(\"Tile size in the xy dimension.\")\n    self.tile_size_xy.setEnabled(False)\n\n    self.tile_size_z = PowerOfTwoSpinBox(4, 32, self.pred_signal.tile_size_z)\n    self.tile_size_z.setToolTip(\"Tile size in the z dimension.\")\n    self.tile_size_z.setEnabled(False)\n\n    self.batch_size_spin = create_int_spinbox(1, 512, 1, 1)\n    self.batch_size_spin.setToolTip(\n        \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n    )\n    self.batch_size_spin.setEnabled(False)\n\n    tiling_form = QFormLayout()\n    tiling_form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    tiling_form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n    tiling_form.addRow(\"XY tile size\", self.tile_size_xy)\n    tiling_form.addRow(\"Z tile size\", self.tile_size_z)\n    tiling_form.addRow(\"Batch size\", self.batch_size_spin)\n    tiling_widget = QWidget()\n    tiling_widget.setLayout(tiling_form)\n    self.layout().addWidget(tiling_widget)\n\n    # prediction progress bar\n    self.pb_prediction = create_progressbar(\n        max_value=20, text_format=\"Prediction ?/?\"\n    )\n    self.pb_prediction.setToolTip(\"Show the progress of the prediction\")\n\n    # predict button\n    predictions = QWidget()\n    predictions.setLayout(QHBoxLayout())\n    self.predict_button = QPushButton(\"Predict\", self)\n    self.predict_button.setMinimumWidth(120)\n    self.predict_button.setEnabled(False)\n    self.predict_button.setToolTip(\"Run the trained model on the images\")\n\n    predictions.layout().addWidget(self.predict_button, alignment=Qt.AlignLeft)\n\n    # add to the group\n    self.layout().addWidget(self.pb_prediction)\n    self.layout().addWidget(predictions)\n\n    # actions\n    self.tiling_cbox.stateChanged.connect(self._update_tiles)\n\n    if self.pred_status is not None and self.train_status is not None:\n        # what to do when the buttons are clicked\n        self.predict_button.clicked.connect(self._predict_button_clicked)\n\n        self.tile_size_xy.valueChanged.connect(self._set_xy_tile_size)\n        self.tile_size_z.valueChanged.connect(self._set_z_tile_size)\n        self.batch_size_spin.valueChanged.connect(self._set_batch_size)\n\n        # listening to the signals\n        self.train_signal.events.is_3d.connect(self._set_3d)\n        self.train_status.events.state.connect(self._update_button_from_train)\n        self.pred_status.events.state.connect(self._update_button_from_pred)\n\n        self.pred_status.events.sample_idx.connect(self._update_sample_idx)\n        self.pred_status.events.max_samples.connect(self._update_max_sample)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._predict_button_clicked","title":"<code>_predict_button_clicked()</code>","text":"<p>Run the prediction on the images.</p> Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _predict_button_clicked(self: Self) -&gt; None:\n    \"\"\"Run the prediction on the images.\"\"\"\n    if self.pred_status is not None:\n        if (\n            self.pred_status.state == PredictionState.IDLE\n            or self.train_status.state == TrainingState.DONE\n            or self.pred_status.state == PredictionState.CRASHED\n        ):\n            self.pred_status.state = PredictionState.PREDICTING\n            self.predict_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._set_3d","title":"<code>_set_3d(state)</code>","text":"<p>Enable the z tile size spinbox if the data is 3D.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>bool</code> <p>The new state of the 3D checkbox.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _set_3d(self: Self, state: bool) -&gt; None:\n    \"\"\"Enable the z tile size spinbox if the data is 3D.\n\n    Parameters\n    ----------\n    state : bool\n        The new state of the 3D checkbox.\n    \"\"\"\n    if self.pred_signal.tiled:\n        self.tile_size_z.setEnabled(state)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._set_batch_size","title":"<code>_set_batch_size(size)</code>","text":"<p>Update the signal batch size.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The new batch size.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _set_batch_size(self: Self, size: int) -&gt; None:\n    \"\"\"Update the signal batch size.\n\n    Parameters\n    ----------\n    size : int\n        The new batch size.\n    \"\"\"\n    if self.pred_signal is not None:\n        self.pred_signal.batch_size = size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._set_xy_tile_size","title":"<code>_set_xy_tile_size(size)</code>","text":"<p>Update the signal tile size in the xy dimension.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The new tile size in the xy dimension.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _set_xy_tile_size(self: Self, size: int) -&gt; None:\n    \"\"\"Update the signal tile size in the xy dimension.\n\n    Parameters\n    ----------\n    size : int\n        The new tile size in the xy dimension.\n    \"\"\"\n    if self.pred_signal is not None:\n        self.pred_signal.tile_size_xy = size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._set_z_tile_size","title":"<code>_set_z_tile_size(size)</code>","text":"<p>Update the signal tile size in the z dimension.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>The new tile size in the z dimension.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _set_z_tile_size(self: Self, size: int) -&gt; None:\n    \"\"\"Update the signal tile size in the z dimension.\n\n    Parameters\n    ----------\n    size : int\n        The new tile size in the z dimension.\n    \"\"\"\n    if self.pred_signal is not None:\n        self.pred_signal.tile_size_z = size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_3d_tiles","title":"<code>_update_3d_tiles(state)</code>","text":"<p>Enable the z tile size spinbox if the data is 3D and tiled.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>bool</code> <p>The new state of the 3D checkbox.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_3d_tiles(self: Self, state: bool) -&gt; None:\n    \"\"\"Enable the z tile size spinbox if the data is 3D and tiled.\n\n    Parameters\n    ----------\n    state : bool\n        The new state of the 3D checkbox.\n    \"\"\"\n    if self.pred_signal.tiled:\n        self.tile_size_z.setEnabled(state)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_button_from_pred","title":"<code>_update_button_from_pred(state)</code>","text":"<p>Update the predict button based on the prediction state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>PredictionState</code> <p>The new state of the prediction plugin.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_button_from_pred(self: Self, state: PredictionState) -&gt; None:\n    \"\"\"Update the predict button based on the prediction state.\n\n    Parameters\n    ----------\n    state : PredictionState\n        The new state of the prediction plugin.\n    \"\"\"\n    if state == PredictionState.DONE or state == PredictionState.CRASHED:\n        self.predict_button.setEnabled(True)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_button_from_train","title":"<code>_update_button_from_train(state)</code>","text":"<p>Update the predict button based on the training state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>The new state of the training plugin.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_button_from_train(self: Self, state: TrainingState) -&gt; None:\n    \"\"\"Update the predict button based on the training state.\n\n    Parameters\n    ----------\n    state : TrainingState\n        The new state of the training plugin.\n    \"\"\"\n    if state == TrainingState.DONE:\n        self.predict_button.setEnabled(True)\n    else:\n        self.predict_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_max_sample","title":"<code>_update_max_sample(max_sample)</code>","text":"<p>Update the maximum value of the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>max_sample</code> <code>int</code> <p>The new maximum value of the progress bar.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_max_sample(self: Self, max_sample: int) -&gt; None:\n    \"\"\"Update the maximum value of the progress bar.\n\n    Parameters\n    ----------\n    max_sample : int\n        The new maximum value of the progress bar.\n    \"\"\"\n    self.pb_prediction.setMaximum(max_sample)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_sample_idx","title":"<code>_update_sample_idx(sample)</code>","text":"<p>Update the value of the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>int</code> <p>The new value of the progress bar.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_sample_idx(self: Self, sample: int) -&gt; None:\n    \"\"\"Update the value of the progress bar.\n\n    Parameters\n    ----------\n    sample : int\n        The new value of the progress bar.\n    \"\"\"\n    self.pb_prediction.setValue(sample + 1)\n    self.pb_prediction.setFormat(\n        f\"Sample {sample+1}/{self.pred_status.max_samples}\"\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/prediction_widget/#careamics_napari.widgets.prediction_widget.PredictionWidget._update_tiles","title":"<code>_update_tiles(state)</code>","text":"<p>Update the weidgets and the signal tiling parameter.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>bool</code> <p>The new state of the tiling checkbox.</p> required Source code in <code>src/careamics_napari/widgets/prediction_widget.py</code> <pre><code>def _update_tiles(self: Self, state: bool) -&gt; None:\n    \"\"\"Update the weidgets and the signal tiling parameter.\n\n    Parameters\n    ----------\n    state : bool\n        The new state of the tiling checkbox.\n    \"\"\"\n    self.pred_signal.tiled = state\n    self.tile_size_xy.setEnabled(state)\n    self.batch_size_spin.setEnabled(state)\n\n    if self.train_signal.is_3d:\n        self.tile_size_z.setEnabled(state)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/","title":"qt_widgets","text":"<p>Various pure Qt widgets.</p>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.DoubleSpinBox","title":"<code>DoubleSpinBox</code>","text":"<p>               Bases: <code>QDoubleSpinBox</code></p> <p>A double spin box that ignores wheel events.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class DoubleSpinBox(QDoubleSpinBox):\n    \"\"\"A double spin box that ignores wheel events.\"\"\"\n\n    def wheelEvent(self: Self, event: Any) -&gt; None:\n        \"\"\"Ignore wheel events.\n\n        Parameters\n        ----------\n        event : Any\n            The wheel event.\n        \"\"\"\n        event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.DoubleSpinBox.wheelEvent","title":"<code>wheelEvent(event)</code>","text":"<p>Ignore wheel events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The wheel event.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def wheelEvent(self: Self, event: Any) -&gt; None:\n    \"\"\"Ignore wheel events.\n\n    Parameters\n    ----------\n    event : Any\n        The wheel event.\n    \"\"\"\n    event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox","title":"<code>PowerOfTwoSpinBox</code>","text":"<p>               Bases: <code>QSpinBox</code></p> <p>A spin box that only accepts power of two values.</p> <p>Parameters:</p> Name Type Description Default <code>min_val</code> <code>int</code> <p>Minimum value.</p> required <code>max_val</code> <code>int</code> <p>Maximum value.</p> required <code>default</code> <code>int</code> <p>Default value.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class PowerOfTwoSpinBox(QSpinBox):\n    \"\"\"A spin box that only accepts power of two values.\n\n    Parameters\n    ----------\n    min_val : int\n        Minimum value.\n    max_val : int\n        Maximum value.\n    default : int\n        Default value.\n    *args : Any\n        Additional arguments.\n    **kwargs : Any\n        Additional keyword arguments.\n    \"\"\"\n\n    def __init__(\n        self: Self, min_val: int, max_val: int, default: int, *args: Any, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        min_val : int\n            Minimum value.\n        max_val : int\n            Maximum value.\n        default : int\n            Default value.\n        *args : Any\n            Additional arguments.\n        **kwargs : Any\n            Additional keyword arguments.\n\n        Raises\n        ------\n        ValueError\n            If min or max are not power of 2.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # min or max are not power of 2\n        if min_val &amp; (min_val - 1) != 0:\n            raise ValueError(f\"Minimum value must be a power of 2, got {min_val}.\")\n\n        if max_val &amp; (max_val - 1) != 0:\n            raise ValueError(f\"Maximum value must be a power of 2, got {max_val}.\")\n\n        self.setRange(min_val, max_val)\n        self.setSingleStep(1)\n        self.setValue(default)\n\n    def stepBy(self: Self, steps: int) -&gt; None:\n        \"\"\"Step the value by a given number of steps.\n\n        Parameters\n        ----------\n        steps : int\n            Number of steps to step the value by.\n        \"\"\"\n        current_value = self.value()\n        current_power = self._get_base_2_log(current_value)\n\n        # Step up or down by adjusting the power of two\n        new_power = current_power + steps\n        new_value = 2**new_power\n        self.setValue(new_value)\n\n    def _get_base_2_log(self: Self, value: int) -&gt; int:\n        \"\"\"Get base-2 logarithm.\n\n        Parameters\n        ----------\n        value : int\n            The value to get the power of two for.\n\n        Returns\n        -------\n        int\n            The power of two of the given value.\n        \"\"\"\n        return int(math.log2(value))\n\n    def textFromValue(self: Self, value: int) -&gt; str:\n        \"\"\"Get the text representation of the value.\n\n        Parameters\n        ----------\n        value : int\n            The value.\n\n        Returns\n        -------\n        str\n            The text representation of the value.\n        \"\"\"\n        return str(value)\n\n    def valueFromText(self: Self, text: str) -&gt; int:\n        \"\"\"Get the value from the text representation.\n\n        Parameters\n        ----------\n        text : str\n            The text representation.\n\n        Returns\n        -------\n        int\n            The value.\n        \"\"\"\n        return int(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.__init__","title":"<code>__init__(min_val, max_val, default, *args, **kwargs)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>min_val</code> <code>int</code> <p>Minimum value.</p> required <code>max_val</code> <code>int</code> <p>Maximum value.</p> required <code>default</code> <code>int</code> <p>Default value.</p> required <code>*args</code> <code>Any</code> <p>Additional arguments.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If min or max are not power of 2.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def __init__(\n    self: Self, min_val: int, max_val: int, default: int, *args: Any, **kwargs: Any\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    min_val : int\n        Minimum value.\n    max_val : int\n        Maximum value.\n    default : int\n        Default value.\n    *args : Any\n        Additional arguments.\n    **kwargs : Any\n        Additional keyword arguments.\n\n    Raises\n    ------\n    ValueError\n        If min or max are not power of 2.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n\n    # min or max are not power of 2\n    if min_val &amp; (min_val - 1) != 0:\n        raise ValueError(f\"Minimum value must be a power of 2, got {min_val}.\")\n\n    if max_val &amp; (max_val - 1) != 0:\n        raise ValueError(f\"Maximum value must be a power of 2, got {max_val}.\")\n\n    self.setRange(min_val, max_val)\n    self.setSingleStep(1)\n    self.setValue(default)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox._get_base_2_log","title":"<code>_get_base_2_log(value)</code>","text":"<p>Get base-2 logarithm.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The value to get the power of two for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The power of two of the given value.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def _get_base_2_log(self: Self, value: int) -&gt; int:\n    \"\"\"Get base-2 logarithm.\n\n    Parameters\n    ----------\n    value : int\n        The value to get the power of two for.\n\n    Returns\n    -------\n    int\n        The power of two of the given value.\n    \"\"\"\n    return int(math.log2(value))\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.stepBy","title":"<code>stepBy(steps)</code>","text":"<p>Step the value by a given number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps to step the value by.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def stepBy(self: Self, steps: int) -&gt; None:\n    \"\"\"Step the value by a given number of steps.\n\n    Parameters\n    ----------\n    steps : int\n        Number of steps to step the value by.\n    \"\"\"\n    current_value = self.value()\n    current_power = self._get_base_2_log(current_value)\n\n    # Step up or down by adjusting the power of two\n    new_power = current_power + steps\n    new_value = 2**new_power\n    self.setValue(new_value)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.textFromValue","title":"<code>textFromValue(value)</code>","text":"<p>Get the text representation of the value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>The value.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The text representation of the value.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def textFromValue(self: Self, value: int) -&gt; str:\n    \"\"\"Get the text representation of the value.\n\n    Parameters\n    ----------\n    value : int\n        The value.\n\n    Returns\n    -------\n    str\n        The text representation of the value.\n    \"\"\"\n    return str(value)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.PowerOfTwoSpinBox.valueFromText","title":"<code>valueFromText(text)</code>","text":"<p>Get the value from the text representation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text representation.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The value.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def valueFromText(self: Self, text: str) -&gt; int:\n    \"\"\"Get the value from the text representation.\n\n    Parameters\n    ----------\n    text : str\n        The text representation.\n\n    Returns\n    -------\n    int\n        The value.\n    \"\"\"\n    return int(text)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.SpinBox","title":"<code>SpinBox</code>","text":"<p>               Bases: <code>QSpinBox</code></p> <p>A spin box that ignores wheel events.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>class SpinBox(QSpinBox):\n    \"\"\"A spin box that ignores wheel events.\"\"\"\n\n    def wheelEvent(self: Self, event: Any) -&gt; None:\n        \"\"\"Ignore wheel events.\n\n        Parameters\n        ----------\n        event : Any\n            The wheel event.\n        \"\"\"\n        event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.SpinBox.wheelEvent","title":"<code>wheelEvent(event)</code>","text":"<p>Ignore wheel events.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Any</code> <p>The wheel event.</p> required Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def wheelEvent(self: Self, event: Any) -&gt; None:\n    \"\"\"Ignore wheel events.\n\n    Parameters\n    ----------\n    event : Any\n        The wheel event.\n    \"\"\"\n    event.ignore()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_double_spinbox","title":"<code>create_double_spinbox(min_value=0, max_value=1, value=0.5, step=0.1, visible=True, tooltip=None, n_decimal=1)</code>","text":"<p>Create a double-typed spin box.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum value.</p> <code>0</code> <code>max_value</code> <code>float</code> <p>Maximum value.</p> <code>1</code> <code>value</code> <code>float</code> <p>Default value.</p> <code>0.5</code> <code>step</code> <code>float</code> <p>Step value.</p> <code>0.1</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <code>n_decimal</code> <code>int</code> <p>Number of decimal places.</p> <code>1</code> <p>Returns:</p> Type Description <code>DoubleSpinBox</code> <p>The double spin box.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_double_spinbox(\n    min_value: float = 0,\n    max_value: float = 1,\n    value: float = 0.5,\n    step: float = 0.1,\n    visible: bool = True,\n    tooltip: Optional[str] = None,\n    n_decimal: int = 1,\n) -&gt; DoubleSpinBox:\n    \"\"\"Create a double-typed spin box.\n\n    Parameters\n    ----------\n    min_value : float, default=0\n        Minimum value.\n    max_value : float, default=1\n        Maximum value.\n    value : float, default=0.5\n        Default value.\n    step : float, default=0.1\n        Step value.\n    visible : bool, default=True\n        Visibility.\n    tooltip : str or None, default=None\n        Tooltip text.\n    n_decimal : int, default=1\n        Number of decimal places.\n\n    Returns\n    -------\n    DoubleSpinBox\n        The double spin box.\n    \"\"\"\n    spin_box = DoubleSpinBox()\n    spin_box.setDecimals(n_decimal)\n    spin_box.setMinimum(min_value)\n    spin_box.setMaximum(max_value)\n    spin_box.setSingleStep(step)\n    spin_box.setValue(value)\n    spin_box.setVisible(visible)\n    spin_box.setToolTip(tooltip)\n    spin_box.setMinimumHeight(50)\n    spin_box.setContentsMargins(0, 3, 0, 3)\n    return spin_box\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_int_spinbox","title":"<code>create_int_spinbox(min_value=1, max_value=1000, value=2, step=1, visible=True, tooltip=None)</code>","text":"<p>Create an integer-typed spin box.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>int</code> <p>Minimum value.</p> <code>1</code> <code>max_value</code> <code>int</code> <p>Maximum value.</p> <code>1000</code> <code>value</code> <code>int</code> <p>Default value.</p> <code>2</code> <code>step</code> <code>int</code> <p>Step value.</p> <code>1</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <p>Returns:</p> Type Description <code>SpinBox</code> <p>The integer spin box.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_int_spinbox(\n    min_value: int = 1,\n    max_value: int = 1000,\n    value: int = 2,\n    step: int = 1,\n    visible: bool = True,\n    tooltip: Optional[str] = None,\n) -&gt; SpinBox:\n    \"\"\"Create an integer-typed spin box.\n\n    Parameters\n    ----------\n    min_value : int, default=1\n        Minimum value.\n    max_value : int, default=1000\n        Maximum value.\n    value : int, default=2\n        Default value.\n    step : int, default=1\n        Step value.\n    visible : bool, default=True\n        Visibility.\n    tooltip : str or None, default=None\n        Tooltip text.\n\n    Returns\n    -------\n    SpinBox\n        The integer spin box.\n    \"\"\"\n    spin_box = SpinBox()\n    spin_box.setMinimum(min_value)\n    spin_box.setMaximum(max_value)\n    spin_box.setSingleStep(step)\n    spin_box.setValue(value)\n    spin_box.setVisible(visible)\n    spin_box.setToolTip(tooltip)\n    spin_box.setMinimumHeight(50)\n    spin_box.setContentsMargins(0, 3, 0, 3)\n\n    return spin_box\n</code></pre>"},{"location":"reference/careamics_napari/widgets/qt_widgets/#careamics_napari.widgets.qt_widgets.create_progressbar","title":"<code>create_progressbar(min_value=0, max_value=100, value=0, text_visible=True, visible=True, text_format=f'Epoch ?/{100}', tooltip=None)</code>","text":"<p>Create a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>int</code> <p>Minimum value.</p> <code>0</code> <code>max_value</code> <code>int</code> <p>Maximum value.</p> <code>100</code> <code>value</code> <code>int</code> <p>Default value.</p> <code>0</code> <code>text_visible</code> <code>bool</code> <p>Visibility of the text.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Visibility.</p> <code>True</code> <code>text_format</code> <code>str</code> <p>Text format.</p> <code>\"Epoch ?/{100}\"</code> <code>tooltip</code> <code>str or None</code> <p>Tooltip text.</p> <code>None</code> <p>Returns:</p> Type Description <code>QProgressBar</code> <p>The progress bar.</p> Source code in <code>src/careamics_napari/widgets/qt_widgets.py</code> <pre><code>def create_progressbar(\n    min_value: int = 0,\n    max_value: int = 100,\n    value: int = 0,\n    text_visible: bool = True,\n    visible: bool = True,\n    text_format: str = f\"Epoch ?/{100}\",\n    tooltip: Optional[str] = None,\n) -&gt; QProgressBar:\n    \"\"\"Create a progress bar.\n\n    Parameters\n    ----------\n    min_value : int, default=0\n        Minimum value.\n    max_value : int, default=100\n        Maximum value.\n    value : int, default=0\n        Default value.\n    text_visible : bool, default=True\n        Visibility of the text.\n    visible : bool, default=True\n        Visibility.\n    text_format : str, default=\"Epoch ?/{100}\"\n        Text format.\n    tooltip : str or None, default=None\n        Tooltip text.\n\n    Returns\n    -------\n    QProgressBar\n        The progress bar.\n    \"\"\"\n    progress_bar = QProgressBar()\n    progress_bar.setMinimum(min_value)\n    progress_bar.setMaximum(max_value)\n    progress_bar.setValue(value)\n    progress_bar.setVisible(visible)\n    progress_bar.setTextVisible(text_visible)\n    progress_bar.setFormat(text_format)\n    progress_bar.setToolTip(tooltip)\n    progress_bar.setMinimumHeight(30)\n\n    return progress_bar\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/","title":"saving_widget","text":"<p>A widget allowing users to select a model type and a path.</p>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget","title":"<code>SavingWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget allowing users to select a model type and a path.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal containing training parameters.</p> <code>None</code> <code>save_status</code> <code>SavingStatus or None</code> <p>Signal containing saving parameters.</p> <code>None</code> <code>save_signal</code> <code>SavingSignal or None</code> <p>Signal to trigger saving.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>class SavingWidget(QGroupBox):\n    \"\"\"A widget allowing users to select a model type and a path.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Signal containing training parameters.\n    save_status : SavingStatus or None, default=None\n        Signal containing saving parameters.\n    save_signal : SavingSignal or None, default=None\n        Signal to trigger saving.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        train_status: Optional[TrainingStatus] = None,\n        save_status: Optional[SavingStatus] = None,\n        save_signal: Optional[SavingSignal] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        train_status : TrainingStatus or None, default=None\n            Signal containing training parameters.\n        save_status : SavingStatus or None, default=None\n            Signal containing saving parameters.\n        save_signal : SavingSignal or None, default=None\n            Signal to trigger saving.\n        \"\"\"\n        super().__init__()\n\n        self.train_status = train_status\n        self.save_status = save_status\n        self.save_signal = save_signal\n\n        self.setTitle(\"Save\")\n        self.setLayout(QVBoxLayout())\n\n        # Save button\n        save_widget = QWidget()\n        save_widget.setLayout(QHBoxLayout())\n        self.save_choice = QComboBox()\n        self.save_choice.addItems(ExportType.list())\n        self.save_choice.setToolTip(\"Output format\")\n\n        self.save_button = QPushButton(\"Save model\", self)\n        self.save_button.setMinimumWidth(120)\n        self.save_button.setEnabled(False)\n        self.save_choice.setToolTip(\"Save the model weights and configuration.\")\n\n        save_widget.layout().addWidget(self.save_choice)\n        save_widget.layout().addWidget(self.save_button, alignment=Qt.AlignLeft)\n        self.layout().addWidget(save_widget)\n\n        # actions\n        if self.train_status is not None:\n            # updates from signals\n            self.train_status.events.state.connect(self._update_training_state)\n\n            # when clicking the save button\n            self.save_button.clicked.connect(self._save_model)\n\n            # when changing the format\n            self.save_choice.currentIndexChanged.connect(self._update_export_type)\n\n    def _update_export_type(self: Self, index: int) -&gt; None:\n        \"\"\"Set the signal export type to the selected format.\n\n        Parameters\n        ----------\n        index : int\n            Index of the selected format.\n        \"\"\"\n        if self.save_signal is not None:\n            self.save_signal.export_type = self.save_choice.currentText()\n\n    def _update_training_state(self: Self, state: TrainingState) -&gt; None:\n        \"\"\"Update the widget state based on the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            Current training state.\n        \"\"\"\n        if state == TrainingState.DONE or state == TrainingState.STOPPED:\n            self.save_button.setEnabled(True)\n        elif state == TrainingState.IDLE:\n            self.save_button.setEnabled(False)\n\n    def _save_model(self: Self) -&gt; None:\n        \"\"\"Prompt users with a path selection dialog and update the saving state.\"\"\"\n        if self.save_status is not None:\n            if self.save_signal is not None and (\n                self.save_status.state == SavingState.IDLE\n                or self.save_status.state == SavingState.DONE\n                or self.save_status.state == SavingState.CRASHED\n            ):\n                # destination = Path(QFileDialog.getSaveFileName(caption='Save model'))\n                destination = Path(\n                    QFileDialog.getExistingDirectory(caption=\"Save model\")\n                )\n                self.save_signal.path_model = destination\n\n                # trigger saving\n                self.save_status.state = SavingState.SAVING\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget.__init__","title":"<code>__init__(train_status=None, save_status=None, save_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal containing training parameters.</p> <code>None</code> <code>save_status</code> <code>SavingStatus or None</code> <p>Signal containing saving parameters.</p> <code>None</code> <code>save_signal</code> <code>SavingSignal or None</code> <p>Signal to trigger saving.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    train_status: Optional[TrainingStatus] = None,\n    save_status: Optional[SavingStatus] = None,\n    save_signal: Optional[SavingSignal] = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Signal containing training parameters.\n    save_status : SavingStatus or None, default=None\n        Signal containing saving parameters.\n    save_signal : SavingSignal or None, default=None\n        Signal to trigger saving.\n    \"\"\"\n    super().__init__()\n\n    self.train_status = train_status\n    self.save_status = save_status\n    self.save_signal = save_signal\n\n    self.setTitle(\"Save\")\n    self.setLayout(QVBoxLayout())\n\n    # Save button\n    save_widget = QWidget()\n    save_widget.setLayout(QHBoxLayout())\n    self.save_choice = QComboBox()\n    self.save_choice.addItems(ExportType.list())\n    self.save_choice.setToolTip(\"Output format\")\n\n    self.save_button = QPushButton(\"Save model\", self)\n    self.save_button.setMinimumWidth(120)\n    self.save_button.setEnabled(False)\n    self.save_choice.setToolTip(\"Save the model weights and configuration.\")\n\n    save_widget.layout().addWidget(self.save_choice)\n    save_widget.layout().addWidget(self.save_button, alignment=Qt.AlignLeft)\n    self.layout().addWidget(save_widget)\n\n    # actions\n    if self.train_status is not None:\n        # updates from signals\n        self.train_status.events.state.connect(self._update_training_state)\n\n        # when clicking the save button\n        self.save_button.clicked.connect(self._save_model)\n\n        # when changing the format\n        self.save_choice.currentIndexChanged.connect(self._update_export_type)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget._save_model","title":"<code>_save_model()</code>","text":"<p>Prompt users with a path selection dialog and update the saving state.</p> Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>def _save_model(self: Self) -&gt; None:\n    \"\"\"Prompt users with a path selection dialog and update the saving state.\"\"\"\n    if self.save_status is not None:\n        if self.save_signal is not None and (\n            self.save_status.state == SavingState.IDLE\n            or self.save_status.state == SavingState.DONE\n            or self.save_status.state == SavingState.CRASHED\n        ):\n            # destination = Path(QFileDialog.getSaveFileName(caption='Save model'))\n            destination = Path(\n                QFileDialog.getExistingDirectory(caption=\"Save model\")\n            )\n            self.save_signal.path_model = destination\n\n            # trigger saving\n            self.save_status.state = SavingState.SAVING\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget._update_export_type","title":"<code>_update_export_type(index)</code>","text":"<p>Set the signal export type to the selected format.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the selected format.</p> required Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>def _update_export_type(self: Self, index: int) -&gt; None:\n    \"\"\"Set the signal export type to the selected format.\n\n    Parameters\n    ----------\n    index : int\n        Index of the selected format.\n    \"\"\"\n    if self.save_signal is not None:\n        self.save_signal.export_type = self.save_choice.currentText()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/saving_widget/#careamics_napari.widgets.saving_widget.SavingWidget._update_training_state","title":"<code>_update_training_state(state)</code>","text":"<p>Update the widget state based on the training state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>Current training state.</p> required Source code in <code>src/careamics_napari/widgets/saving_widget.py</code> <pre><code>def _update_training_state(self: Self, state: TrainingState) -&gt; None:\n    \"\"\"Update the widget state based on the training state.\n\n    Parameters\n    ----------\n    state : TrainingState\n        Current training state.\n    \"\"\"\n    if state == TrainingState.DONE or state == TrainingState.STOPPED:\n        self.save_button.setEnabled(True)\n    elif state == TrainingState.IDLE:\n        self.save_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/","title":"scroll_wrapper","text":"<p>Wrap a widget in a scroll area.</p>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/#careamics_napari.widgets.scroll_wrapper.ScrollWidgetWrapper","title":"<code>ScrollWidgetWrapper</code>","text":"<p>               Bases: <code>QScrollArea</code></p> <p>Wrap a widget in a scroll area.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>QWidget</code> <p>Widget to wrap.</p> required Source code in <code>src/careamics_napari/widgets/scroll_wrapper.py</code> <pre><code>class ScrollWidgetWrapper(QScrollArea):\n    \"\"\"Wrap a widget in a scroll area.\n\n    Parameters\n    ----------\n    widget : QWidget\n        Widget to wrap.\n    \"\"\"\n\n    def __init__(self, widget: QWidget) -&gt; None:\n        \"\"\"Wrap a widget in a scroll area.\n\n        Parameters\n        ----------\n        widget : QWidget\n            Widget to wrap.\n        \"\"\"\n        super().__init__()\n        self.setVerticalScrollBarPolicy(\n            Qt.ScrollBarPolicy.ScrollBarAlwaysOn\n        )  # ScrollBarAsNeeded\n        self.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)\n        self.setWidgetResizable(True)\n        self.setWidget(widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/scroll_wrapper/#careamics_napari.widgets.scroll_wrapper.ScrollWidgetWrapper.__init__","title":"<code>__init__(widget)</code>","text":"<p>Wrap a widget in a scroll area.</p> <p>Parameters:</p> Name Type Description Default <code>widget</code> <code>QWidget</code> <p>Widget to wrap.</p> required Source code in <code>src/careamics_napari/widgets/scroll_wrapper.py</code> <pre><code>def __init__(self, widget: QWidget) -&gt; None:\n    \"\"\"Wrap a widget in a scroll area.\n\n    Parameters\n    ----------\n    widget : QWidget\n        Widget to wrap.\n    \"\"\"\n    super().__init__()\n    self.setVerticalScrollBarPolicy(\n        Qt.ScrollBarPolicy.ScrollBarAlwaysOn\n    )  # ScrollBarAsNeeded\n    self.setHorizontalScrollBarPolicy(Qt.ScrollBarPolicy.ScrollBarAlwaysOff)\n    self.setWidgetResizable(True)\n    self.setWidget(widget)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/","title":"tbplot_widget","text":"<p>A widget displaying losses and a button to open TensorBoard in the browser.</p>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget","title":"<code>TBPlotWidget</code>","text":"<p>               Bases: <code>Container</code></p> <p>A widget displaying losses and a button to open TensorBoard in the browser.</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int or None</code> <p>Minimum width of the widget.</p> <code>None</code> <code>min_height</code> <code>int or None</code> <p>Minimum height of the widget.</p> <code>None</code> <code>max_width</code> <code>int or None</code> <p>Maximum width of the widget.</p> <code>None</code> <code>max_height</code> <code>int or None</code> <p>Maximum height of the widget.</p> <code>None</code> <code>train_signal</code> <code>TrainingSignal or None</code> <p>Signal containing training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>class TBPlotWidget(Container):\n    \"\"\"A widget displaying losses and a button to open TensorBoard in the browser.\n\n    Parameters\n    ----------\n    min_width : int or None, default=None\n        Minimum width of the widget.\n    min_height : int or None, default=None\n        Minimum height of the widget.\n    max_width : int or None, default=None\n        Maximum width of the widget.\n    max_height : int or None, default=None\n        Maximum height of the widget.\n    train_signal : TrainingSignal or None, default=None\n        Signal containing training parameters.\n    \"\"\"\n\n    # TODO what is this method used for?\n    def __setitem__(self: Self, key: Any, value: Any) -&gt; None:\n        \"\"\"Ignore set item.\n\n        Parameters\n        ----------\n        key : Any\n            Ignored.\n        value : Any\n            Ignored.\n        \"\"\"\n        pass\n\n    def __init__(\n        self: Self,\n        min_width: Optional[int] = None,\n        min_height: Optional[int] = None,\n        max_width: Optional[int] = None,\n        max_height: Optional[int] = None,\n        train_signal: Optional[TrainingSignal] = None,\n    ):\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        min_width : int or None, default=None\n            Minimum width of the widget.\n        min_height : int or None, default=None\n            Minimum height of the widget.\n        max_width : int or None, default=None\n            Maximum width of the widget.\n        max_height : int or None, default=None\n            Maximum height of the widget.\n        train_signal : TrainingSignal or None, default=None\n            Signal containing training parameters.\n        \"\"\"\n        super().__init__()\n\n        self.train_signal = train_signal\n\n        if max_width:\n            self.native.setMaximumWidth(max_width)\n        if max_height:\n            self.native.setMaximumHeight(max_height)\n        if min_width:\n            self.native.setMinimumWidth(min_width)\n        if min_height:\n            self.native.setMinimumHeight(min_height)\n\n        self.graphics_widget = pg.GraphicsLayoutWidget()\n        self.graphics_widget.setBackground(None)\n        self.native.layout().addWidget(self.graphics_widget)\n\n        # plot widget\n        self.plot = self.graphics_widget.addPlot()\n        self.plot.setLabel(\"bottom\", \"epoch\")\n        self.plot.setLabel(\"left\", \"loss\")\n        self.plot.addLegend(offset=(125, -50))\n\n        # tensorboard button\n        tb_button = QPushButton(\"Open in TensorBoard\")\n        tb_button.setToolTip(\"Open TensorBoard in your browser\")\n        tb_button.setIcon(QIcon(QPixmap(ICON_TF)))\n        tb_button.setLayoutDirection(Qt.LeftToRight)\n        tb_button.setIconSize(QSize(32, 29))\n        tb_button.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))\n        tb_button.clicked.connect(self.open_tb)\n\n        # add to layout on the bottom left\n        button_widget = QWidget()\n        button_widget.setLayout(QHBoxLayout())\n        button_widget.layout().addWidget(tb_button)\n        button_widget.layout().addWidget(QLabel(\"\"))\n        self.native.layout().addWidget(button_widget)\n\n        # set empty references\n        self.epochs: list[int] = []\n        self.train_loss: list[float] = []\n        self.val_loss: list[float] = []\n        self.url: Optional[str] = None\n        self.tb = None\n\n    def stop_tb(self: Self) -&gt; None:\n        \"\"\"Stop the TensorBoard process.\n\n        Currently not implemented.\n        \"\"\"\n        # haven't found any good way to stop the tb process, there's currently no API\n        # for it\n        pass\n\n    def open_tb(self: Self) -&gt; None:\n        \"\"\"Open TensorBoard in the browser.\"\"\"\n        if self.tb is None and self.train_signal is not None:\n            from tensorboard import program\n\n            self.tb = program.TensorBoard()\n\n            path = str(self.train_signal.work_dir / \"logs\" / \"lightning_logs\")\n            self.tb.configure(argv=[None, \"--logdir\", path])  # type: ignore\n            self.url = self.tb.launch()  # type: ignore\n\n            if self.url is not None:\n                webbrowser.open(self.url)\n        else:\n            if self.url is not None:\n                webbrowser.open(self.url)\n\n    def update_plot(self: Self, epoch: int, train_loss: float, val_loss: float) -&gt; None:\n        \"\"\"Update the plot with new data.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number.\n        train_loss : float\n            Training loss.\n        val_loss : float\n            Validation loss.\n        \"\"\"\n        # clear the plot\n        self.plot.clear()\n\n        # add the new points\n        self.epochs.append(epoch)\n        self.train_loss.append(train_loss)\n        self.val_loss.append(val_loss)\n\n        # replot\n        self.plot.plot(\n            self.epochs,\n            self.train_loss,\n            pen=pg.mkPen(color=(204, 221, 255)),\n            symbol=\"o\",\n            symbolSize=2,\n            name=\"Train\",\n        )\n        self.plot.plot(\n            self.epochs,\n            self.val_loss,\n            pen=pg.mkPen(color=(244, 173, 173)),\n            symbol=\"o\",\n            symbolSize=2,\n            name=\"Val\",\n        )\n\n    def clear_plot(self: Self) -&gt; None:\n        \"\"\"Clear the plot.\"\"\"\n        self.plot.clear()\n        self.epochs = []\n        self.train_loss = []\n        self.val_loss = []\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.__init__","title":"<code>__init__(min_width=None, min_height=None, max_width=None, max_height=None, train_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int or None</code> <p>Minimum width of the widget.</p> <code>None</code> <code>min_height</code> <code>int or None</code> <p>Minimum height of the widget.</p> <code>None</code> <code>max_width</code> <code>int or None</code> <p>Maximum width of the widget.</p> <code>None</code> <code>max_height</code> <code>int or None</code> <p>Maximum height of the widget.</p> <code>None</code> <code>train_signal</code> <code>TrainingSignal or None</code> <p>Signal containing training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    min_width: Optional[int] = None,\n    min_height: Optional[int] = None,\n    max_width: Optional[int] = None,\n    max_height: Optional[int] = None,\n    train_signal: Optional[TrainingSignal] = None,\n):\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    min_width : int or None, default=None\n        Minimum width of the widget.\n    min_height : int or None, default=None\n        Minimum height of the widget.\n    max_width : int or None, default=None\n        Maximum width of the widget.\n    max_height : int or None, default=None\n        Maximum height of the widget.\n    train_signal : TrainingSignal or None, default=None\n        Signal containing training parameters.\n    \"\"\"\n    super().__init__()\n\n    self.train_signal = train_signal\n\n    if max_width:\n        self.native.setMaximumWidth(max_width)\n    if max_height:\n        self.native.setMaximumHeight(max_height)\n    if min_width:\n        self.native.setMinimumWidth(min_width)\n    if min_height:\n        self.native.setMinimumHeight(min_height)\n\n    self.graphics_widget = pg.GraphicsLayoutWidget()\n    self.graphics_widget.setBackground(None)\n    self.native.layout().addWidget(self.graphics_widget)\n\n    # plot widget\n    self.plot = self.graphics_widget.addPlot()\n    self.plot.setLabel(\"bottom\", \"epoch\")\n    self.plot.setLabel(\"left\", \"loss\")\n    self.plot.addLegend(offset=(125, -50))\n\n    # tensorboard button\n    tb_button = QPushButton(\"Open in TensorBoard\")\n    tb_button.setToolTip(\"Open TensorBoard in your browser\")\n    tb_button.setIcon(QIcon(QPixmap(ICON_TF)))\n    tb_button.setLayoutDirection(Qt.LeftToRight)\n    tb_button.setIconSize(QSize(32, 29))\n    tb_button.setCursor(QCursor(Qt.CursorShape.PointingHandCursor))\n    tb_button.clicked.connect(self.open_tb)\n\n    # add to layout on the bottom left\n    button_widget = QWidget()\n    button_widget.setLayout(QHBoxLayout())\n    button_widget.layout().addWidget(tb_button)\n    button_widget.layout().addWidget(QLabel(\"\"))\n    self.native.layout().addWidget(button_widget)\n\n    # set empty references\n    self.epochs: list[int] = []\n    self.train_loss: list[float] = []\n    self.val_loss: list[float] = []\n    self.url: Optional[str] = None\n    self.tb = None\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Ignore set item.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>Ignored.</p> required <code>value</code> <code>Any</code> <p>Ignored.</p> required Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def __setitem__(self: Self, key: Any, value: Any) -&gt; None:\n    \"\"\"Ignore set item.\n\n    Parameters\n    ----------\n    key : Any\n        Ignored.\n    value : Any\n        Ignored.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.clear_plot","title":"<code>clear_plot()</code>","text":"<p>Clear the plot.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def clear_plot(self: Self) -&gt; None:\n    \"\"\"Clear the plot.\"\"\"\n    self.plot.clear()\n    self.epochs = []\n    self.train_loss = []\n    self.val_loss = []\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.open_tb","title":"<code>open_tb()</code>","text":"<p>Open TensorBoard in the browser.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def open_tb(self: Self) -&gt; None:\n    \"\"\"Open TensorBoard in the browser.\"\"\"\n    if self.tb is None and self.train_signal is not None:\n        from tensorboard import program\n\n        self.tb = program.TensorBoard()\n\n        path = str(self.train_signal.work_dir / \"logs\" / \"lightning_logs\")\n        self.tb.configure(argv=[None, \"--logdir\", path])  # type: ignore\n        self.url = self.tb.launch()  # type: ignore\n\n        if self.url is not None:\n            webbrowser.open(self.url)\n    else:\n        if self.url is not None:\n            webbrowser.open(self.url)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.stop_tb","title":"<code>stop_tb()</code>","text":"<p>Stop the TensorBoard process.</p> <p>Currently not implemented.</p> Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def stop_tb(self: Self) -&gt; None:\n    \"\"\"Stop the TensorBoard process.\n\n    Currently not implemented.\n    \"\"\"\n    # haven't found any good way to stop the tb process, there's currently no API\n    # for it\n    pass\n</code></pre>"},{"location":"reference/careamics_napari/widgets/tbplot_widget/#careamics_napari.widgets.tbplot_widget.TBPlotWidget.update_plot","title":"<code>update_plot(epoch, train_loss, val_loss)</code>","text":"<p>Update the plot with new data.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number.</p> required <code>train_loss</code> <code>float</code> <p>Training loss.</p> required <code>val_loss</code> <code>float</code> <p>Validation loss.</p> required Source code in <code>src/careamics_napari/widgets/tbplot_widget.py</code> <pre><code>def update_plot(self: Self, epoch: int, train_loss: float, val_loss: float) -&gt; None:\n    \"\"\"Update the plot with new data.\n\n    Parameters\n    ----------\n    epoch : int\n        Epoch number.\n    train_loss : float\n        Training loss.\n    val_loss : float\n        Validation loss.\n    \"\"\"\n    # clear the plot\n    self.plot.clear()\n\n    # add the new points\n    self.epochs.append(epoch)\n    self.train_loss.append(train_loss)\n    self.val_loss.append(val_loss)\n\n    # replot\n    self.plot.plot(\n        self.epochs,\n        self.train_loss,\n        pen=pg.mkPen(color=(204, 221, 255)),\n        symbol=\"o\",\n        symbolSize=2,\n        name=\"Train\",\n    )\n    self.plot.plot(\n        self.epochs,\n        self.val_loss,\n        pen=pg.mkPen(color=(244, 173, 173)),\n        symbol=\"o\",\n        symbolSize=2,\n        name=\"Val\",\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/","title":"train_data_widget","text":"<p>A widget allowing users to select data source for the training.</p>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget","title":"<code>TrainDataWidget</code>","text":"<p>               Bases: <code>QTabWidget</code></p> <p>A widget offering to select layers from napari or paths from disk.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>TrainConfigurationSignal or None</code> <p>Signal representing the training parameters.</p> <code>None</code> <code>use_target</code> <code>bool</code> <p>Whether to target fields.</p> <code>False</code> Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>class TrainDataWidget(QTabWidget):\n    \"\"\"A widget offering to select layers from napari or paths from disk.\n\n    Parameters\n    ----------\n    signal : TrainConfigurationSignal or None, default=None\n        Signal representing the training parameters.\n    use_target : bool, default=False\n        Whether to target fields.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        signal: Optional[TrainingSignal] = None,\n        use_target: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        signal : TrainConfigurationSignal or None, default=None\n            Signal representing the training parameters.\n        use_target : bool, default=False\n            Whether to target fields.\n        \"\"\"\n        super().__init__()\n        self.config_signal = signal\n        self.use_target = use_target\n\n        # QTabs\n        layer_tab = QWidget()\n        layer_tab.setLayout(QVBoxLayout())\n        disk_tab = QWidget()\n        disk_tab.setLayout(QVBoxLayout())\n\n        # add tabs\n        self.addTab(layer_tab, \"From layers\")\n        self.addTab(disk_tab, \"From disk\")\n        self.setTabToolTip(0, \"Use images from napari layers\")\n        self.setTabToolTip(1, \"Use patches saved on the disk\")\n\n        # set tabs\n        self._set_layer_tab(layer_tab)\n        self._set_disk_tab(disk_tab)\n\n        # self.setMaximumHeight(400 if self.use_target else 200)\n\n        # set actions\n        if self.config_signal is not None:\n            self.currentChanged.connect(self._set_data_source)\n            self._set_data_source(self.currentIndex())\n\n    def _set_layer_tab(\n        self: Self,\n        layer_tab: QWidget,\n    ) -&gt; None:\n        \"\"\"Set up the layer tab.\n\n        Parameters\n        ----------\n        layer_tab : QWidget\n            Layer tab widget.\n        \"\"\"\n        if _has_napari and napari.current_viewer() is not None:\n            form = QFormLayout()\n            form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n            form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n            form.setContentsMargins(12, 12, 0, 0)\n\n            self.img_train = layer_choice()\n            self.img_train.native.setToolTip(\"Select a training layer.\")\n\n            self.img_val = layer_choice()\n            self.img_train.native.setToolTip(\"Select a validation layer.\")\n\n            # connection actions for images\n            self.img_train.changed.connect(self._update_train_layer)\n            self.img_val.changed.connect(self._update_val_layer)\n\n            # to cover the case when image was loaded before the plugin\n            if self.img_train.value is not None:\n                self._update_train_layer(self.img_train.value)\n            if self.img_val.value is not None:\n                self._update_val_layer(self.img_val.value)\n\n            if self.use_target:\n                # get the target layers\n                self.target_train = layer_choice()\n                self.target_val = layer_choice()\n\n                # tool tips\n                self.target_train.native.setToolTip(\"Select a training target layer.\")\n                self.target_val.native.setToolTip(\"Select a validation target layer.\")\n\n                # connection actions for targets\n                self.target_train.changed.connect(self._update_train_target_layer)\n                self.target_val.changed.connect(self._update_val_target_layer)\n\n                # to cover the case when image was loaded before the plugin\n                if self.target_train.value is not None:\n                    self._update_train_target_layer(self.target_train.value)\n                if self.target_val.value is not None:\n                    self._update_val_target_layer(self.target_val.value)\n\n                form.addRow(\"Train\", self.img_train.native)\n                form.addRow(\"Val\", self.img_val.native)\n                form.addRow(\"Train target\", self.target_train.native)\n                form.addRow(\"Val target\", self.target_val.native)\n\n            else:\n                form.addRow(\"Train\", self.img_train.native)\n                form.addRow(\"Val\", self.img_val.native)\n\n            # layer_tab.layout().addWidget(widget_layers)\n            layer_tab.layout().addLayout(form)\n\n        else:\n            # simply remove the tab\n            self.removeTab(0)\n\n    def _set_disk_tab(self: Self, disk_tab: QWidget) -&gt; None:\n        \"\"\"Set up the disk tab.\n\n        Parameters\n        ----------\n        disk_tab : QWidget\n            Disk tab widget.\n        \"\"\"\n        # disk tab\n        buttons = QWidget()\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n        # form.setSpacing(0)\n\n        self.train_images_folder = FolderWidget(\"Choose\")\n        self.val_images_folder = FolderWidget(\"Choose\")\n        form.addRow(\"Train\", self.train_images_folder)\n        form.addRow(\"Val\", self.val_images_folder)\n\n        if self.use_target:\n            self.train_target_folder = FolderWidget(\"Choose\")\n            self.val_target_folder = FolderWidget(\"Choose\")\n\n            form.addRow(\"Train target\", self.train_target_folder)\n            form.addRow(\"Val target\", self.val_target_folder)\n\n            self.train_target_folder.setToolTip(\n                \"Select a folder containing the training\\n\" \"target.\"\n            )\n            self.val_target_folder.setToolTip(\n                \"Select a folder containing the validation\\n\" \"target.\"\n            )\n            self.train_images_folder.setToolTip(\n                \"Select a folder containing the training\\n\" \"images.\"\n            )\n            self.val_images_folder.setToolTip(\n                \"Select a folder containing the validation\\n\" \"images.\"\n            )\n\n            # add actions to target\n            self.train_target_folder.get_text_widget().textChanged.connect(\n                self._update_train_target_folder\n            )\n            self.val_target_folder.get_text_widget().textChanged.connect(\n                self._update_val_target_folder\n            )\n\n        else:\n            self.train_images_folder.setToolTip(\n                \"Select a folder containing the training\\n\" \"images.\"\n            )\n            self.val_images_folder.setToolTip(\n                \"Select a folder containing the validation\\n\"\n                \"images, if you select the same folder as\\n\"\n                \"for training, the validation patches will\\n\"\n                \"be extracted from the training data.\"\n            )\n\n        # add actions\n        self.train_images_folder.get_text_widget().textChanged.connect(\n            self._update_train_folder\n        )\n        self.val_images_folder.get_text_widget().textChanged.connect(\n            self._update_val_folder\n        )\n\n        buttons.setLayout(form)\n        disk_tab.layout().addWidget(buttons)\n\n    def _set_data_source(self: Self, index: int) -&gt; None:\n        \"\"\"Set the signal data source to the selected tab.\n\n        Parameters\n        ----------\n        index : int\n            Index of the selected tab.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.load_from_disk = index == self.count() - 1\n\n    def _update_train_layer(self: Self, layer: Image) -&gt; None:\n        \"\"\"Update the training layer.\n\n        Parameters\n        ----------\n        layer : Image\n            Training layer.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.layer_train = layer\n\n    def _update_val_layer(self: Self, layer: Image) -&gt; None:\n        \"\"\"Update the validation layer.\n\n        Parameters\n        ----------\n        layer : Image\n            Validation layer.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.layer_val = layer\n\n    def _update_train_target_layer(self: Self, layer: Image) -&gt; None:\n        \"\"\"Update the training target layer.\n\n        Parameters\n        ----------\n        layer : Image\n            Training target layer.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.layer_train_target = layer\n\n    def _update_val_target_layer(self: Self, layer: Image) -&gt; None:\n        \"\"\"Update the validation target layer.\n\n        Parameters\n        ----------\n        layer : Image\n            Validation target layer.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.layer_val_target = layer\n\n    def _update_train_folder(self: Self, folder: str) -&gt; None:\n        \"\"\"Update the training folder.\n\n        Parameters\n        ----------\n        folder : str\n            Training folder.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.path_train = folder\n\n    def _update_val_folder(self: Self, folder: str) -&gt; None:\n        \"\"\"Update the validation folder.\n\n        Parameters\n        ----------\n        folder : str\n            Validation folder.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.path_val = folder\n\n    def _update_train_target_folder(self: Self, folder: str) -&gt; None:\n        \"\"\"Update the training target folder.\n\n        Parameters\n        ----------\n        folder : str\n            Training target folder.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.path_train_target = folder\n\n    def _update_val_target_folder(self: Self, folder: str) -&gt; None:\n        \"\"\"Update the validation target folder.\n\n        Parameters\n        ----------\n        folder : str\n            Validation target folder.\n        \"\"\"\n        if self.config_signal is not None:\n            self.config_signal.path_val_target = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget.__init__","title":"<code>__init__(signal=None, use_target=False)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>TrainConfigurationSignal or None</code> <p>Signal representing the training parameters.</p> <code>None</code> <code>use_target</code> <code>bool</code> <p>Whether to target fields.</p> <code>False</code> Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    signal: Optional[TrainingSignal] = None,\n    use_target: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    signal : TrainConfigurationSignal or None, default=None\n        Signal representing the training parameters.\n    use_target : bool, default=False\n        Whether to target fields.\n    \"\"\"\n    super().__init__()\n    self.config_signal = signal\n    self.use_target = use_target\n\n    # QTabs\n    layer_tab = QWidget()\n    layer_tab.setLayout(QVBoxLayout())\n    disk_tab = QWidget()\n    disk_tab.setLayout(QVBoxLayout())\n\n    # add tabs\n    self.addTab(layer_tab, \"From layers\")\n    self.addTab(disk_tab, \"From disk\")\n    self.setTabToolTip(0, \"Use images from napari layers\")\n    self.setTabToolTip(1, \"Use patches saved on the disk\")\n\n    # set tabs\n    self._set_layer_tab(layer_tab)\n    self._set_disk_tab(disk_tab)\n\n    # self.setMaximumHeight(400 if self.use_target else 200)\n\n    # set actions\n    if self.config_signal is not None:\n        self.currentChanged.connect(self._set_data_source)\n        self._set_data_source(self.currentIndex())\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._set_data_source","title":"<code>_set_data_source(index)</code>","text":"<p>Set the signal data source to the selected tab.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the selected tab.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _set_data_source(self: Self, index: int) -&gt; None:\n    \"\"\"Set the signal data source to the selected tab.\n\n    Parameters\n    ----------\n    index : int\n        Index of the selected tab.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.load_from_disk = index == self.count() - 1\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._set_disk_tab","title":"<code>_set_disk_tab(disk_tab)</code>","text":"<p>Set up the disk tab.</p> <p>Parameters:</p> Name Type Description Default <code>disk_tab</code> <code>QWidget</code> <p>Disk tab widget.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _set_disk_tab(self: Self, disk_tab: QWidget) -&gt; None:\n    \"\"\"Set up the disk tab.\n\n    Parameters\n    ----------\n    disk_tab : QWidget\n        Disk tab widget.\n    \"\"\"\n    # disk tab\n    buttons = QWidget()\n    form = QFormLayout()\n    form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n    # form.setSpacing(0)\n\n    self.train_images_folder = FolderWidget(\"Choose\")\n    self.val_images_folder = FolderWidget(\"Choose\")\n    form.addRow(\"Train\", self.train_images_folder)\n    form.addRow(\"Val\", self.val_images_folder)\n\n    if self.use_target:\n        self.train_target_folder = FolderWidget(\"Choose\")\n        self.val_target_folder = FolderWidget(\"Choose\")\n\n        form.addRow(\"Train target\", self.train_target_folder)\n        form.addRow(\"Val target\", self.val_target_folder)\n\n        self.train_target_folder.setToolTip(\n            \"Select a folder containing the training\\n\" \"target.\"\n        )\n        self.val_target_folder.setToolTip(\n            \"Select a folder containing the validation\\n\" \"target.\"\n        )\n        self.train_images_folder.setToolTip(\n            \"Select a folder containing the training\\n\" \"images.\"\n        )\n        self.val_images_folder.setToolTip(\n            \"Select a folder containing the validation\\n\" \"images.\"\n        )\n\n        # add actions to target\n        self.train_target_folder.get_text_widget().textChanged.connect(\n            self._update_train_target_folder\n        )\n        self.val_target_folder.get_text_widget().textChanged.connect(\n            self._update_val_target_folder\n        )\n\n    else:\n        self.train_images_folder.setToolTip(\n            \"Select a folder containing the training\\n\" \"images.\"\n        )\n        self.val_images_folder.setToolTip(\n            \"Select a folder containing the validation\\n\"\n            \"images, if you select the same folder as\\n\"\n            \"for training, the validation patches will\\n\"\n            \"be extracted from the training data.\"\n        )\n\n    # add actions\n    self.train_images_folder.get_text_widget().textChanged.connect(\n        self._update_train_folder\n    )\n    self.val_images_folder.get_text_widget().textChanged.connect(\n        self._update_val_folder\n    )\n\n    buttons.setLayout(form)\n    disk_tab.layout().addWidget(buttons)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._set_layer_tab","title":"<code>_set_layer_tab(layer_tab)</code>","text":"<p>Set up the layer tab.</p> <p>Parameters:</p> Name Type Description Default <code>layer_tab</code> <code>QWidget</code> <p>Layer tab widget.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _set_layer_tab(\n    self: Self,\n    layer_tab: QWidget,\n) -&gt; None:\n    \"\"\"Set up the layer tab.\n\n    Parameters\n    ----------\n    layer_tab : QWidget\n        Layer tab widget.\n    \"\"\"\n    if _has_napari and napari.current_viewer() is not None:\n        form = QFormLayout()\n        form.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        form.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n        form.setContentsMargins(12, 12, 0, 0)\n\n        self.img_train = layer_choice()\n        self.img_train.native.setToolTip(\"Select a training layer.\")\n\n        self.img_val = layer_choice()\n        self.img_train.native.setToolTip(\"Select a validation layer.\")\n\n        # connection actions for images\n        self.img_train.changed.connect(self._update_train_layer)\n        self.img_val.changed.connect(self._update_val_layer)\n\n        # to cover the case when image was loaded before the plugin\n        if self.img_train.value is not None:\n            self._update_train_layer(self.img_train.value)\n        if self.img_val.value is not None:\n            self._update_val_layer(self.img_val.value)\n\n        if self.use_target:\n            # get the target layers\n            self.target_train = layer_choice()\n            self.target_val = layer_choice()\n\n            # tool tips\n            self.target_train.native.setToolTip(\"Select a training target layer.\")\n            self.target_val.native.setToolTip(\"Select a validation target layer.\")\n\n            # connection actions for targets\n            self.target_train.changed.connect(self._update_train_target_layer)\n            self.target_val.changed.connect(self._update_val_target_layer)\n\n            # to cover the case when image was loaded before the plugin\n            if self.target_train.value is not None:\n                self._update_train_target_layer(self.target_train.value)\n            if self.target_val.value is not None:\n                self._update_val_target_layer(self.target_val.value)\n\n            form.addRow(\"Train\", self.img_train.native)\n            form.addRow(\"Val\", self.img_val.native)\n            form.addRow(\"Train target\", self.target_train.native)\n            form.addRow(\"Val target\", self.target_val.native)\n\n        else:\n            form.addRow(\"Train\", self.img_train.native)\n            form.addRow(\"Val\", self.img_val.native)\n\n        # layer_tab.layout().addWidget(widget_layers)\n        layer_tab.layout().addLayout(form)\n\n    else:\n        # simply remove the tab\n        self.removeTab(0)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_train_folder","title":"<code>_update_train_folder(folder)</code>","text":"<p>Update the training folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Training folder.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_train_folder(self: Self, folder: str) -&gt; None:\n    \"\"\"Update the training folder.\n\n    Parameters\n    ----------\n    folder : str\n        Training folder.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.path_train = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_train_layer","title":"<code>_update_train_layer(layer)</code>","text":"<p>Update the training layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Image</code> <p>Training layer.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_train_layer(self: Self, layer: Image) -&gt; None:\n    \"\"\"Update the training layer.\n\n    Parameters\n    ----------\n    layer : Image\n        Training layer.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.layer_train = layer\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_train_target_folder","title":"<code>_update_train_target_folder(folder)</code>","text":"<p>Update the training target folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Training target folder.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_train_target_folder(self: Self, folder: str) -&gt; None:\n    \"\"\"Update the training target folder.\n\n    Parameters\n    ----------\n    folder : str\n        Training target folder.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.path_train_target = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_train_target_layer","title":"<code>_update_train_target_layer(layer)</code>","text":"<p>Update the training target layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Image</code> <p>Training target layer.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_train_target_layer(self: Self, layer: Image) -&gt; None:\n    \"\"\"Update the training target layer.\n\n    Parameters\n    ----------\n    layer : Image\n        Training target layer.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.layer_train_target = layer\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_val_folder","title":"<code>_update_val_folder(folder)</code>","text":"<p>Update the validation folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Validation folder.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_val_folder(self: Self, folder: str) -&gt; None:\n    \"\"\"Update the validation folder.\n\n    Parameters\n    ----------\n    folder : str\n        Validation folder.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.path_val = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_val_layer","title":"<code>_update_val_layer(layer)</code>","text":"<p>Update the validation layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Image</code> <p>Validation layer.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_val_layer(self: Self, layer: Image) -&gt; None:\n    \"\"\"Update the validation layer.\n\n    Parameters\n    ----------\n    layer : Image\n        Validation layer.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.layer_val = layer\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_val_target_folder","title":"<code>_update_val_target_folder(folder)</code>","text":"<p>Update the validation target folder.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>Validation target folder.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_val_target_folder(self: Self, folder: str) -&gt; None:\n    \"\"\"Update the validation target folder.\n\n    Parameters\n    ----------\n    folder : str\n        Validation target folder.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.path_val_target = folder\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_data_widget/#careamics_napari.widgets.train_data_widget.TrainDataWidget._update_val_target_layer","title":"<code>_update_val_target_layer(layer)</code>","text":"<p>Update the validation target layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Image</code> <p>Validation target layer.</p> required Source code in <code>src/careamics_napari/widgets/train_data_widget.py</code> <pre><code>def _update_val_target_layer(self: Self, layer: Image) -&gt; None:\n    \"\"\"Update the validation target layer.\n\n    Parameters\n    ----------\n    layer : Image\n        Validation target layer.\n    \"\"\"\n    if self.config_signal is not None:\n        self.config_signal.layer_val_target = layer\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/","title":"train_progress_widget","text":"<p>A widget displaying the training progress using two progress bars.</p>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget","title":"<code>TrainProgressWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget displaying the training progress using two progress bars.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal representing the training status.</p> <code>None</code> <code>train_config</code> <code>TrainingSignal or None</code> <p>Signal representing the training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>class TrainProgressWidget(QGroupBox):\n    \"\"\"A widget displaying the training progress using two progress bars.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Signal representing the training status.\n    train_config : TrainingSignal or None, default=None\n        Signal representing the training parameters.\n    \"\"\"\n\n    def __init__(\n        self: Self,\n        train_status: Optional[TrainingStatus] = None,\n        train_config: Optional[TrainingSignal] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        train_status : TrainingStatus or None, default=None\n            Signal representing the training status.\n        train_config : TrainingSignal or None, default=None\n            Signal representing the training parameters.\n        \"\"\"\n        super().__init__()\n\n        self.train_status = (\n            train_status\n            if train_status is not None  # for typing purposes\n            else TrainingStatus()  # type: ignore\n        )\n\n        self.setTitle(\"Training progress\")\n        self.setLayout(QVBoxLayout())\n\n        # progress bars\n        self.layout().setContentsMargins(20, 20, 20, 0)\n\n        self.pb_epochs = create_progressbar(\n            max_value=self.train_status.max_epochs,\n            text_format=f\"Epoch ?/{self.train_status.max_epochs}\",\n            value=0,\n        )\n\n        self.pb_batch = create_progressbar(\n            max_value=self.train_status.max_batches,\n            text_format=f\"Batch ?/{self.train_status.max_batches}\",\n            value=0,\n        )\n\n        self.layout().addWidget(self.pb_epochs)\n        self.layout().addWidget(self.pb_batch)\n\n        # plot widget\n        self.plot = TBPlotWidget(\n            max_width=300, max_height=300, min_height=250, train_signal=train_config\n        )\n        self.layout().addWidget(self.plot.native)\n\n        # actions\n        if self.train_status is not None:\n            self.train_status.events.state.connect(self._update_training_state)\n\n            self.train_status.events.epoch_idx.connect(self._update_epoch)\n            self.train_status.events.max_epochs.connect(self._update_max_epoch)\n            self.train_status.events.batch_idx.connect(self._update_batch)\n            self.train_status.events.max_batches.connect(self._update_max_batch)\n            self.train_status.events.val_loss.connect(self._update_loss)\n\n    def _update_training_state(self: Self, state: TrainingState) -&gt; None:\n        \"\"\"Update the widget according to the training state.\n\n        Parameters\n        ----------\n        state : TrainingState\n            Training state.\n        \"\"\"\n        if state == TrainingState.IDLE or state == TrainingState.TRAINING:\n            self.plot.clear_plot()\n\n    def _update_max_epoch(self: Self, max_epoch: int):\n        \"\"\"Update the maximum number of epochs in the progress bar.\n\n        Parameters\n        ----------\n        max_epoch : int\n            Maximum number of epochs.\n        \"\"\"\n        self.pb_epochs.setMaximum(max_epoch)\n\n    def _update_epoch(self: Self, epoch: int) -&gt; None:\n        \"\"\"Update the epoch progress bar.\n\n        Parameters\n        ----------\n        epoch : int\n            Current epoch.\n        \"\"\"\n        self.pb_epochs.setValue(epoch + 1)\n        self.pb_epochs.setFormat(f\"Epoch {epoch+1}/{self.train_status.max_epochs}\")\n\n    def _update_max_batch(self: Self, max_batches: int) -&gt; None:\n        \"\"\"Update the maximum number of batches in the progress bar.\n\n        Parameters\n        ----------\n        max_batches : int\n            Maximum number of batches.\n        \"\"\"\n        self.pb_batch.setMaximum(max_batches)\n\n    def _update_batch(self: Self) -&gt; None:\n        \"\"\"Update the batch progress bar.\"\"\"\n        self.pb_batch.setValue(self.train_status.batch_idx + 1)\n        self.pb_batch.setFormat(\n            f\"Batch {self.train_status.batch_idx+1}/{self.train_status.max_batches}\"\n        )\n\n    def _update_loss(self: Self) -&gt; None:\n        \"\"\"Update the loss plot.\"\"\"\n        self.plot.update_plot(\n            epoch=self.train_status.epoch_idx,\n            train_loss=self.train_status.loss,\n            val_loss=self.train_status.val_loss,\n        )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget.__init__","title":"<code>__init__(train_status=None, train_config=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Signal representing the training status.</p> <code>None</code> <code>train_config</code> <code>TrainingSignal or None</code> <p>Signal representing the training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def __init__(\n    self: Self,\n    train_status: Optional[TrainingStatus] = None,\n    train_config: Optional[TrainingSignal] = None,\n) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Signal representing the training status.\n    train_config : TrainingSignal or None, default=None\n        Signal representing the training parameters.\n    \"\"\"\n    super().__init__()\n\n    self.train_status = (\n        train_status\n        if train_status is not None  # for typing purposes\n        else TrainingStatus()  # type: ignore\n    )\n\n    self.setTitle(\"Training progress\")\n    self.setLayout(QVBoxLayout())\n\n    # progress bars\n    self.layout().setContentsMargins(20, 20, 20, 0)\n\n    self.pb_epochs = create_progressbar(\n        max_value=self.train_status.max_epochs,\n        text_format=f\"Epoch ?/{self.train_status.max_epochs}\",\n        value=0,\n    )\n\n    self.pb_batch = create_progressbar(\n        max_value=self.train_status.max_batches,\n        text_format=f\"Batch ?/{self.train_status.max_batches}\",\n        value=0,\n    )\n\n    self.layout().addWidget(self.pb_epochs)\n    self.layout().addWidget(self.pb_batch)\n\n    # plot widget\n    self.plot = TBPlotWidget(\n        max_width=300, max_height=300, min_height=250, train_signal=train_config\n    )\n    self.layout().addWidget(self.plot.native)\n\n    # actions\n    if self.train_status is not None:\n        self.train_status.events.state.connect(self._update_training_state)\n\n        self.train_status.events.epoch_idx.connect(self._update_epoch)\n        self.train_status.events.max_epochs.connect(self._update_max_epoch)\n        self.train_status.events.batch_idx.connect(self._update_batch)\n        self.train_status.events.max_batches.connect(self._update_max_batch)\n        self.train_status.events.val_loss.connect(self._update_loss)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_batch","title":"<code>_update_batch()</code>","text":"<p>Update the batch progress bar.</p> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_batch(self: Self) -&gt; None:\n    \"\"\"Update the batch progress bar.\"\"\"\n    self.pb_batch.setValue(self.train_status.batch_idx + 1)\n    self.pb_batch.setFormat(\n        f\"Batch {self.train_status.batch_idx+1}/{self.train_status.max_batches}\"\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_epoch","title":"<code>_update_epoch(epoch)</code>","text":"<p>Update the epoch progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch.</p> required Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_epoch(self: Self, epoch: int) -&gt; None:\n    \"\"\"Update the epoch progress bar.\n\n    Parameters\n    ----------\n    epoch : int\n        Current epoch.\n    \"\"\"\n    self.pb_epochs.setValue(epoch + 1)\n    self.pb_epochs.setFormat(f\"Epoch {epoch+1}/{self.train_status.max_epochs}\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_loss","title":"<code>_update_loss()</code>","text":"<p>Update the loss plot.</p> Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_loss(self: Self) -&gt; None:\n    \"\"\"Update the loss plot.\"\"\"\n    self.plot.update_plot(\n        epoch=self.train_status.epoch_idx,\n        train_loss=self.train_status.loss,\n        val_loss=self.train_status.val_loss,\n    )\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_max_batch","title":"<code>_update_max_batch(max_batches)</code>","text":"<p>Update the maximum number of batches in the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>max_batches</code> <code>int</code> <p>Maximum number of batches.</p> required Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_max_batch(self: Self, max_batches: int) -&gt; None:\n    \"\"\"Update the maximum number of batches in the progress bar.\n\n    Parameters\n    ----------\n    max_batches : int\n        Maximum number of batches.\n    \"\"\"\n    self.pb_batch.setMaximum(max_batches)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_max_epoch","title":"<code>_update_max_epoch(max_epoch)</code>","text":"<p>Update the maximum number of epochs in the progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>max_epoch</code> <code>int</code> <p>Maximum number of epochs.</p> required Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_max_epoch(self: Self, max_epoch: int):\n    \"\"\"Update the maximum number of epochs in the progress bar.\n\n    Parameters\n    ----------\n    max_epoch : int\n        Maximum number of epochs.\n    \"\"\"\n    self.pb_epochs.setMaximum(max_epoch)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/train_progress_widget/#careamics_napari.widgets.train_progress_widget.TrainProgressWidget._update_training_state","title":"<code>_update_training_state(state)</code>","text":"<p>Update the widget according to the training state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>Training state.</p> required Source code in <code>src/careamics_napari/widgets/train_progress_widget.py</code> <pre><code>def _update_training_state(self: Self, state: TrainingState) -&gt; None:\n    \"\"\"Update the widget according to the training state.\n\n    Parameters\n    ----------\n    state : TrainingState\n        Training state.\n    \"\"\"\n    if state == TrainingState.IDLE or state == TrainingState.TRAINING:\n        self.plot.clear_plot()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/","title":"training_configuration_widget","text":"<p>A widget allowing the creation of a CAREamics configuration.</p>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget","title":"<code>ConfigurationWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>A widget allowing the creation of a CAREamics configuration.</p> <p>Parameters:</p> Name Type Description Default <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal containing the training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>class ConfigurationWidget(QGroupBox):\n    \"\"\"A widget allowing the creation of a CAREamics configuration.\n\n    Parameters\n    ----------\n    training_signal : TrainingSignal or None, default=None\n        Signal containing the training parameters.\n    \"\"\"\n\n    def __init__(self: Self, training_signal: Optional[TrainingSignal] = None) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        training_signal : TrainingSignal or None, default=None\n            Signal containing the training parameters.\n        \"\"\"\n        super().__init__()\n\n        self.configuration_signal = training_signal\n        self.config_window: Optional[AdvancedConfigurationWindow] = None\n\n        self.setTitle(\"Training parameters\")\n        self.setMinimumWidth(200)\n\n        # expert settings\n        icon = QtGui.QIcon(ICON_GEAR)\n        self.training_expert_btn = QPushButton(icon, \"\")\n        self.training_expert_btn.setFixedSize(35, 35)\n        self.training_expert_btn.setToolTip(\"Open the expert settings menu.\")\n\n        # 3D checkbox\n        self.enable_3d = QCheckBox()\n        self.enable_3d.setToolTip(\"Use a 3D network\")\n\n        # axes\n        self.axes_widget = AxesWidget(training_signal=self.configuration_signal)\n\n        # others\n        self.n_epochs_spin = create_int_spinbox(1, 1000, 30, tooltip=\"Number of epochs\")\n        self.n_epochs = self.n_epochs_spin.value()\n\n        # batch size\n        self.batch_size_spin = create_int_spinbox(1, 512, 16, 1)\n        self.batch_size_spin.setToolTip(\n            \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n        )\n\n        # patch size\n        self.patch_XY_spin = PowerOfTwoSpinBox(16, 512, 64)\n        self.patch_XY_spin.setToolTip(\"Dimension of the patches in XY.\")\n\n        self.patch_Z_spin = PowerOfTwoSpinBox(8, 512, 8)\n        self.patch_Z_spin.setToolTip(\"Dimension of the patches in Z.\")\n\n        # TODO: is this necessary?\n        if self.configuration_signal is not None:\n            self.patch_Z_spin.setEnabled(self.configuration_signal.is_3d)\n\n        formLayout = QFormLayout()\n        formLayout.setContentsMargins(0, 0, 0, 0)\n        formLayout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n        formLayout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n        formLayout.addRow(\"Enable 3D\", self.enable_3d)\n        formLayout.addRow(self.axes_widget.label.text(), self.axes_widget.text_field)\n        formLayout.addRow(\"N epochs\", self.n_epochs_spin)\n        formLayout.addRow(\"Batch size\", self.batch_size_spin)\n        formLayout.addRow(\"Patch XY\", self.patch_XY_spin)\n        formLayout.addRow(\"Patch Z\", self.patch_Z_spin)\n        formLayout.minimumSize()\n\n        hlayout = QVBoxLayout()\n        hlayout.addWidget(\n            self.training_expert_btn, alignment=Qt.AlignRight | Qt.AlignVCenter\n        )\n        hlayout.addLayout(formLayout)\n\n        self.setLayout(hlayout)\n        self.layout().setContentsMargins(5, 20, 5, 10)\n\n        # set actions\n        self.training_expert_btn.clicked.connect(self._show_configuration_window)\n        self.enable_3d.clicked.connect(self._enable_3d_changed)\n        self.axes_widget.text_field.textChanged.connect(self._update_axes)\n        self.n_epochs_spin.valueChanged.connect(self._update_n_epochs)\n        self.batch_size_spin.valueChanged.connect(self._update_batch_size)\n        self.patch_XY_spin.valueChanged.connect(self._update_patch_size_XY)\n        self.patch_Z_spin.valueChanged.connect(self._update_patch_size_Z)\n\n    def _show_configuration_window(self: Self) -&gt; None:\n        \"\"\"Show the advanced configuration window.\"\"\"\n        if self.config_window is None or self.config_window.isHidden():\n            self.config_window = AdvancedConfigurationWindow(\n                self, self.configuration_signal\n            )\n\n            self.config_window.show()\n\n    def _enable_3d_changed(self: Self, state: bool) -&gt; None:\n        \"\"\"Update the signal 3D state.\n\n        Parameters\n        ----------\n        state : bool\n            3D state.\n        \"\"\"\n        self.patch_Z_spin.setVisible(state)\n        self.patch_Z_spin.setEnabled(state)\n\n        if self.configuration_signal is not None:\n            self.configuration_signal.is_3d = state\n\n    def _update_axes(self: Self, axes: str) -&gt; None:\n        \"\"\"Update the signal axes.\n\n        Parameters\n        ----------\n        axes : str\n            Axes.\n        \"\"\"\n        if self.configuration_signal is not None:\n            self.configuration_signal.axes = axes\n\n    def _update_n_epochs(self: Self, n_epochs: int) -&gt; None:\n        \"\"\"Update the signal number of epochs.\n\n        Parameters\n        ----------\n        n_epochs : int\n            Number of epochs.\n        \"\"\"\n        if self.configuration_signal is not None:\n            self.configuration_signal.n_epochs = n_epochs\n\n    def _update_batch_size(self: Self, batch_size: int) -&gt; None:\n        \"\"\"Update the signal batch size.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size.\n        \"\"\"\n        if self.configuration_signal is not None:\n            self.configuration_signal.batch_size = batch_size\n\n    def _update_patch_size_XY(self: Self, patch_size: int) -&gt; None:\n        \"\"\"Update the signal patch size in XY.\n\n        Parameters\n        ----------\n        patch_size : int\n            Patch size.\n        \"\"\"\n        if self.configuration_signal is not None:\n            self.configuration_signal.patch_size_xy = patch_size\n\n    def _update_patch_size_Z(self: Self, patch_size: int) -&gt; None:\n        \"\"\"Update the signal patch size in Z.\n\n        Parameters\n        ----------\n        patch_size : int\n            Patch size.\n        \"\"\"\n        if self.configuration_signal is not None:\n            self.configuration_signal.patch_size_z = patch_size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget.__init__","title":"<code>__init__(training_signal=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>training_signal</code> <code>TrainingSignal or None</code> <p>Signal containing the training parameters.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def __init__(self: Self, training_signal: Optional[TrainingSignal] = None) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    training_signal : TrainingSignal or None, default=None\n        Signal containing the training parameters.\n    \"\"\"\n    super().__init__()\n\n    self.configuration_signal = training_signal\n    self.config_window: Optional[AdvancedConfigurationWindow] = None\n\n    self.setTitle(\"Training parameters\")\n    self.setMinimumWidth(200)\n\n    # expert settings\n    icon = QtGui.QIcon(ICON_GEAR)\n    self.training_expert_btn = QPushButton(icon, \"\")\n    self.training_expert_btn.setFixedSize(35, 35)\n    self.training_expert_btn.setToolTip(\"Open the expert settings menu.\")\n\n    # 3D checkbox\n    self.enable_3d = QCheckBox()\n    self.enable_3d.setToolTip(\"Use a 3D network\")\n\n    # axes\n    self.axes_widget = AxesWidget(training_signal=self.configuration_signal)\n\n    # others\n    self.n_epochs_spin = create_int_spinbox(1, 1000, 30, tooltip=\"Number of epochs\")\n    self.n_epochs = self.n_epochs_spin.value()\n\n    # batch size\n    self.batch_size_spin = create_int_spinbox(1, 512, 16, 1)\n    self.batch_size_spin.setToolTip(\n        \"Number of patches per batch (decrease if GPU memory is insufficient)\"\n    )\n\n    # patch size\n    self.patch_XY_spin = PowerOfTwoSpinBox(16, 512, 64)\n    self.patch_XY_spin.setToolTip(\"Dimension of the patches in XY.\")\n\n    self.patch_Z_spin = PowerOfTwoSpinBox(8, 512, 8)\n    self.patch_Z_spin.setToolTip(\"Dimension of the patches in Z.\")\n\n    # TODO: is this necessary?\n    if self.configuration_signal is not None:\n        self.patch_Z_spin.setEnabled(self.configuration_signal.is_3d)\n\n    formLayout = QFormLayout()\n    formLayout.setContentsMargins(0, 0, 0, 0)\n    formLayout.setFormAlignment(Qt.AlignLeft | Qt.AlignTop)\n    formLayout.setFieldGrowthPolicy(QFormLayout.AllNonFixedFieldsGrow)\n    formLayout.addRow(\"Enable 3D\", self.enable_3d)\n    formLayout.addRow(self.axes_widget.label.text(), self.axes_widget.text_field)\n    formLayout.addRow(\"N epochs\", self.n_epochs_spin)\n    formLayout.addRow(\"Batch size\", self.batch_size_spin)\n    formLayout.addRow(\"Patch XY\", self.patch_XY_spin)\n    formLayout.addRow(\"Patch Z\", self.patch_Z_spin)\n    formLayout.minimumSize()\n\n    hlayout = QVBoxLayout()\n    hlayout.addWidget(\n        self.training_expert_btn, alignment=Qt.AlignRight | Qt.AlignVCenter\n    )\n    hlayout.addLayout(formLayout)\n\n    self.setLayout(hlayout)\n    self.layout().setContentsMargins(5, 20, 5, 10)\n\n    # set actions\n    self.training_expert_btn.clicked.connect(self._show_configuration_window)\n    self.enable_3d.clicked.connect(self._enable_3d_changed)\n    self.axes_widget.text_field.textChanged.connect(self._update_axes)\n    self.n_epochs_spin.valueChanged.connect(self._update_n_epochs)\n    self.batch_size_spin.valueChanged.connect(self._update_batch_size)\n    self.patch_XY_spin.valueChanged.connect(self._update_patch_size_XY)\n    self.patch_Z_spin.valueChanged.connect(self._update_patch_size_Z)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._enable_3d_changed","title":"<code>_enable_3d_changed(state)</code>","text":"<p>Update the signal 3D state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>bool</code> <p>3D state.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _enable_3d_changed(self: Self, state: bool) -&gt; None:\n    \"\"\"Update the signal 3D state.\n\n    Parameters\n    ----------\n    state : bool\n        3D state.\n    \"\"\"\n    self.patch_Z_spin.setVisible(state)\n    self.patch_Z_spin.setEnabled(state)\n\n    if self.configuration_signal is not None:\n        self.configuration_signal.is_3d = state\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._show_configuration_window","title":"<code>_show_configuration_window()</code>","text":"<p>Show the advanced configuration window.</p> Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _show_configuration_window(self: Self) -&gt; None:\n    \"\"\"Show the advanced configuration window.\"\"\"\n    if self.config_window is None or self.config_window.isHidden():\n        self.config_window = AdvancedConfigurationWindow(\n            self, self.configuration_signal\n        )\n\n        self.config_window.show()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._update_axes","title":"<code>_update_axes(axes)</code>","text":"<p>Update the signal axes.</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>str</code> <p>Axes.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _update_axes(self: Self, axes: str) -&gt; None:\n    \"\"\"Update the signal axes.\n\n    Parameters\n    ----------\n    axes : str\n        Axes.\n    \"\"\"\n    if self.configuration_signal is not None:\n        self.configuration_signal.axes = axes\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._update_batch_size","title":"<code>_update_batch_size(batch_size)</code>","text":"<p>Update the signal batch size.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _update_batch_size(self: Self, batch_size: int) -&gt; None:\n    \"\"\"Update the signal batch size.\n\n    Parameters\n    ----------\n    batch_size : int\n        Batch size.\n    \"\"\"\n    if self.configuration_signal is not None:\n        self.configuration_signal.batch_size = batch_size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._update_n_epochs","title":"<code>_update_n_epochs(n_epochs)</code>","text":"<p>Update the signal number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>n_epochs</code> <code>int</code> <p>Number of epochs.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _update_n_epochs(self: Self, n_epochs: int) -&gt; None:\n    \"\"\"Update the signal number of epochs.\n\n    Parameters\n    ----------\n    n_epochs : int\n        Number of epochs.\n    \"\"\"\n    if self.configuration_signal is not None:\n        self.configuration_signal.n_epochs = n_epochs\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._update_patch_size_XY","title":"<code>_update_patch_size_XY(patch_size)</code>","text":"<p>Update the signal patch size in XY.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Patch size.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _update_patch_size_XY(self: Self, patch_size: int) -&gt; None:\n    \"\"\"Update the signal patch size in XY.\n\n    Parameters\n    ----------\n    patch_size : int\n        Patch size.\n    \"\"\"\n    if self.configuration_signal is not None:\n        self.configuration_signal.patch_size_xy = patch_size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_configuration_widget/#careamics_napari.widgets.training_configuration_widget.ConfigurationWidget._update_patch_size_Z","title":"<code>_update_patch_size_Z(patch_size)</code>","text":"<p>Update the signal patch size in Z.</p> <p>Parameters:</p> Name Type Description Default <code>patch_size</code> <code>int</code> <p>Patch size.</p> required Source code in <code>src/careamics_napari/widgets/training_configuration_widget.py</code> <pre><code>def _update_patch_size_Z(self: Self, patch_size: int) -&gt; None:\n    \"\"\"Update the signal patch size in Z.\n\n    Parameters\n    ----------\n    patch_size : int\n        Patch size.\n    \"\"\"\n    if self.configuration_signal is not None:\n        self.configuration_signal.patch_size_z = patch_size\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/","title":"training_widget","text":"<p>Training widget.</p>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget","title":"<code>TrainingWidget</code>","text":"<p>               Bases: <code>QGroupBox</code></p> <p>Training widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>class TrainingWidget(QGroupBox):\n    \"\"\"Training widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Training status.\n    \"\"\"\n\n    def __init__(self: Self, train_status: Optional[TrainingStatus] = None) -&gt; None:\n        \"\"\"Initialize the widget.\n\n        Parameters\n        ----------\n        train_status : TrainingStatus or None, default=None\n            Training status.\n        \"\"\"\n        super().__init__()  # TODO needed? and in the other classes? to pass parent?\n\n        self.train_status = (\n            TrainingStatus() if train_status is None else train_status  # type: ignore\n        )\n\n        self.setTitle(\"Train\")\n        self.setLayout(QVBoxLayout())\n\n        # train buttons\n        train_buttons = QWidget()\n        train_buttons.setLayout(QHBoxLayout())\n\n        self.train_button = QPushButton(\"Train\", self)\n        self.train_button.setMinimumWidth(120)\n\n        self.reset_model_button = QPushButton(\"Reset\", self)\n        self.reset_model_button.setMinimumWidth(120)\n        self.reset_model_button.setEnabled(False)\n        self.reset_model_button.setToolTip(\n            \"Reset the weights of the model (forget the training)\"\n        )\n\n        train_buttons.layout().addWidget(self.train_button, alignment=Qt.AlignLeft)\n        train_buttons.layout().addWidget(self.reset_model_button, alignment=Qt.AlignLeft)\n        self.layout().addWidget(train_buttons)\n\n        # actions\n        if self.train_status is not None:\n            # what to do when the buttons are clicked\n            self.train_button.clicked.connect(self._train_stop_clicked)\n            self.reset_model_button.clicked.connect(self._reset_clicked)\n\n            # listening to the signal\n            self.train_status.events.state.connect(self._update_button)\n\n    def _train_stop_clicked(self) -&gt; None:\n        \"\"\"Update the UI and training status when the train button is clicked.\"\"\"\n        if self.train_status is not None:\n            if (\n                self.train_status.state == TrainingState.IDLE\n                or self.train_status.state == TrainingState.DONE\n            ):\n                self.train_status.state = TrainingState.TRAINING\n                self.reset_model_button.setEnabled(False)\n                self.reset_model_button.setText(\"\")\n                self.train_button.setText(\"Stop\")\n\n            elif self.train_status.state == TrainingState.TRAINING:\n                self.train_status.state = TrainingState.STOPPED\n                self.train_button.setText(\"Train\")\n                self.reset_model_button.setEnabled(True)\n                self.reset_model_button.setText(\"Reset\")\n\n            elif self.train_status.state == TrainingState.STOPPED:\n                self.train_status.state = TrainingState.TRAINING\n                self.train_button.setText(\"Stop\")\n\n    def _reset_clicked(self) -&gt; None:\n        \"\"\"Update the UI and training status when the reset button is clicked.\"\"\"\n        if self.train_status is not None:\n            if self.train_status.state != TrainingState.TRAINING:\n                self.train_status.state = TrainingState.IDLE\n                self.train_button.setText(\"Train\")\n                self.reset_model_button.setEnabled(False)\n\n    def _update_button(self, new_state: TrainingState) -&gt; None:\n        \"\"\"Update the button text based on the training state.\n\n        Parameters\n        ----------\n        new_state : TrainingState\n            New training state.\n        \"\"\"\n        if new_state == TrainingState.DONE or new_state == TrainingState.STOPPED:\n            self.train_button.setText(\"Train\")\n            self.reset_model_button.setEnabled(True)\n            self.reset_model_button.setText(\"Reset\")\n        elif new_state == TrainingState.CRASHED:\n            self._reset_clicked()\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget.__init__","title":"<code>__init__(train_status=None)</code>","text":"<p>Initialize the widget.</p> <p>Parameters:</p> Name Type Description Default <code>train_status</code> <code>TrainingStatus or None</code> <p>Training status.</p> <code>None</code> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>def __init__(self: Self, train_status: Optional[TrainingStatus] = None) -&gt; None:\n    \"\"\"Initialize the widget.\n\n    Parameters\n    ----------\n    train_status : TrainingStatus or None, default=None\n        Training status.\n    \"\"\"\n    super().__init__()  # TODO needed? and in the other classes? to pass parent?\n\n    self.train_status = (\n        TrainingStatus() if train_status is None else train_status  # type: ignore\n    )\n\n    self.setTitle(\"Train\")\n    self.setLayout(QVBoxLayout())\n\n    # train buttons\n    train_buttons = QWidget()\n    train_buttons.setLayout(QHBoxLayout())\n\n    self.train_button = QPushButton(\"Train\", self)\n    self.train_button.setMinimumWidth(120)\n\n    self.reset_model_button = QPushButton(\"Reset\", self)\n    self.reset_model_button.setMinimumWidth(120)\n    self.reset_model_button.setEnabled(False)\n    self.reset_model_button.setToolTip(\n        \"Reset the weights of the model (forget the training)\"\n    )\n\n    train_buttons.layout().addWidget(self.train_button, alignment=Qt.AlignLeft)\n    train_buttons.layout().addWidget(self.reset_model_button, alignment=Qt.AlignLeft)\n    self.layout().addWidget(train_buttons)\n\n    # actions\n    if self.train_status is not None:\n        # what to do when the buttons are clicked\n        self.train_button.clicked.connect(self._train_stop_clicked)\n        self.reset_model_button.clicked.connect(self._reset_clicked)\n\n        # listening to the signal\n        self.train_status.events.state.connect(self._update_button)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget._reset_clicked","title":"<code>_reset_clicked()</code>","text":"<p>Update the UI and training status when the reset button is clicked.</p> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>def _reset_clicked(self) -&gt; None:\n    \"\"\"Update the UI and training status when the reset button is clicked.\"\"\"\n    if self.train_status is not None:\n        if self.train_status.state != TrainingState.TRAINING:\n            self.train_status.state = TrainingState.IDLE\n            self.train_button.setText(\"Train\")\n            self.reset_model_button.setEnabled(False)\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget._train_stop_clicked","title":"<code>_train_stop_clicked()</code>","text":"<p>Update the UI and training status when the train button is clicked.</p> Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>def _train_stop_clicked(self) -&gt; None:\n    \"\"\"Update the UI and training status when the train button is clicked.\"\"\"\n    if self.train_status is not None:\n        if (\n            self.train_status.state == TrainingState.IDLE\n            or self.train_status.state == TrainingState.DONE\n        ):\n            self.train_status.state = TrainingState.TRAINING\n            self.reset_model_button.setEnabled(False)\n            self.reset_model_button.setText(\"\")\n            self.train_button.setText(\"Stop\")\n\n        elif self.train_status.state == TrainingState.TRAINING:\n            self.train_status.state = TrainingState.STOPPED\n            self.train_button.setText(\"Train\")\n            self.reset_model_button.setEnabled(True)\n            self.reset_model_button.setText(\"Reset\")\n\n        elif self.train_status.state == TrainingState.STOPPED:\n            self.train_status.state = TrainingState.TRAINING\n            self.train_button.setText(\"Stop\")\n</code></pre>"},{"location":"reference/careamics_napari/widgets/training_widget/#careamics_napari.widgets.training_widget.TrainingWidget._update_button","title":"<code>_update_button(new_state)</code>","text":"<p>Update the button text based on the training state.</p> <p>Parameters:</p> Name Type Description Default <code>new_state</code> <code>TrainingState</code> <p>New training state.</p> required Source code in <code>src/careamics_napari/widgets/training_widget.py</code> <pre><code>def _update_button(self, new_state: TrainingState) -&gt; None:\n    \"\"\"Update the button text based on the training state.\n\n    Parameters\n    ----------\n    new_state : TrainingState\n        New training state.\n    \"\"\"\n    if new_state == TrainingState.DONE or new_state == TrainingState.STOPPED:\n        self.train_button.setText(\"Train\")\n        self.reset_model_button.setEnabled(True)\n        self.reset_model_button.setText(\"Reset\")\n    elif new_state == TrainingState.CRASHED:\n        self._reset_clicked()\n</code></pre>"},{"location":"reference/careamics_napari/workers/prediction_worker/","title":"prediction_worker","text":"<p>A thread worker function running CAREamics prediction.</p>"},{"location":"reference/careamics_napari/workers/prediction_worker/#careamics_napari.workers.prediction_worker._predict","title":"<code>_predict(careamist, config_signal, update_queue)</code>","text":"<p>Run the prediction.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamist instance.</p> required <code>config_signal</code> <code>PredictionSignal</code> <p>Prediction signal.</p> required <code>update_queue</code> <code>Queue</code> <p>Queue used to send updates to the UI.</p> required Source code in <code>src/careamics_napari/workers/prediction_worker.py</code> <pre><code>def _predict(\n    careamist: CAREamist,\n    config_signal: PredictionSignal,\n    update_queue: Queue,\n) -&gt; None:\n    \"\"\"Run the prediction.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamist instance.\n    config_signal : PredictionSignal\n        Prediction signal.\n    update_queue : Queue\n        Queue used to send updates to the UI.\n    \"\"\"\n    # Format data\n    if config_signal.load_from_disk:\n\n        if config_signal.path_pred == \"\":\n            _push_exception(update_queue, ValueError(\"Prediction data path is empty.\"))\n            return\n\n        pred_data = config_signal.path_pred\n\n    else:\n        if config_signal.layer_pred is None:\n            _push_exception(\n                update_queue, ValueError(\"Prediction layer has not been selected.\")\n            )\n            return\n\n        elif config_signal.layer_pred.data is None:\n            _push_exception(\n                update_queue,\n                ValueError(\n                    f\"Prediction layer {config_signal.layer_pred.name} is empty.\"\n                ),\n            )\n            return\n        else:\n            pred_data = config_signal.layer_pred.data\n\n    # tiling\n    if config_signal.tiled:\n        if config_signal.is_3d:\n            tile_size: Optional[Union[tuple[int, int, int], tuple[int, int]]] = (\n                config_signal.tile_size_z,\n                config_signal.tile_size_xy,\n                config_signal.tile_size_xy,\n            )\n            tile_overlap: Optional[Union[tuple[int, int, int], tuple[int, int]]] = (\n                config_signal.tile_overlap_z,\n                config_signal.tile_overlap_xy,\n                config_signal.tile_overlap_xy,\n            )\n        else:\n            tile_size = (config_signal.tile_size_xy, config_signal.tile_size_xy)\n            tile_overlap = (\n                config_signal.tile_overlap_xy,\n                config_signal.tile_overlap_xy,\n            )\n        batch_size = config_signal.batch_size\n    else:\n        tile_size = None\n        tile_overlap = None\n        batch_size = 1\n\n    # Predict with CAREamist\n    try:\n        result = careamist.predict(  # type: ignore\n            pred_data,\n            data_type=\"tiff\" if config_signal.load_from_disk else \"array\",\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            batch_size=batch_size,\n        )\n\n        update_queue.put(PredictionUpdate(PredictionUpdateType.SAMPLE, result))\n\n        # # TODO can we use this to monkey patch the training process?\n        # import time\n        # update_queue.put(\n        #   PredictionUpdate(PredictionUpdateType.MAX_SAMPLES, 1_000 // 10)\n        # )\n        # for i in range(1_000):\n\n        #     # if stopper.stop:\n        #     #     update_queue.put(Update(UpdateType.STATE, TrainingState.STOPPED))\n        #     #     break\n\n        #     if i % 10 == 0:\n        #         update_queue.put(\n        #              PredictionUpdate(PredictionUpdateType.SAMPLE_IDX, i // 10)\n        #         )\n        #         print(i)\n\n        #     time.sleep(0.2)\n\n    except Exception as e:\n        traceback.print_exc()\n\n        update_queue.put(PredictionUpdate(PredictionUpdateType.EXCEPTION, e))\n        return\n\n    # signify end of prediction\n    update_queue.put(PredictionUpdate(PredictionUpdateType.STATE, PredictionState.DONE))\n</code></pre>"},{"location":"reference/careamics_napari/workers/prediction_worker/#careamics_napari.workers.prediction_worker._push_exception","title":"<code>_push_exception(queue, e)</code>","text":"<p>Push an exception to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue</code> <p>Queue.</p> required <code>e</code> <code>Exception</code> <p>Exception.</p> required Source code in <code>src/careamics_napari/workers/prediction_worker.py</code> <pre><code>def _push_exception(queue: Queue, e: Exception) -&gt; None:\n    \"\"\"Push an exception to the queue.\n\n    Parameters\n    ----------\n    queue : Queue\n        Queue.\n    e : Exception\n        Exception.\n    \"\"\"\n    try:\n        raise e\n    except Exception as _:\n        traceback.print_exc()\n\n    queue.put(PredictionUpdate(PredictionUpdateType.EXCEPTION, e))\n</code></pre>"},{"location":"reference/careamics_napari/workers/prediction_worker/#careamics_napari.workers.prediction_worker.predict_worker","title":"<code>predict_worker(careamist, config_signal, update_queue)</code>","text":"<p>Model prediction worker.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamist instance.</p> required <code>config_signal</code> <code>PredictionSignal</code> <p>Prediction signal.</p> required <code>update_queue</code> <code>Queue</code> <p>Queue used to send updates to the UI.</p> required <p>Yields:</p> Type Description <code>Generator[PredictionUpdate, None, None]</code> <p>Updates.</p> Source code in <code>src/careamics_napari/workers/prediction_worker.py</code> <pre><code>@thread_worker\ndef predict_worker(\n    careamist: CAREamist,\n    config_signal: PredictionSignal,\n    update_queue: Queue,\n) -&gt; Generator[PredictionUpdate, None, None]:\n    \"\"\"Model prediction worker.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamist instance.\n    config_signal : PredictionSignal\n        Prediction signal.\n    update_queue : Queue\n        Queue used to send updates to the UI.\n\n    Yields\n    ------\n    Generator[PredictionUpdate, None, None]\n        Updates.\n    \"\"\"\n    # start training thread\n    training = Thread(\n        target=_predict,\n        args=(\n            careamist,\n            config_signal,\n            update_queue,\n        ),\n    )\n    training.start()\n\n    # look for updates\n    while True:\n        update: PredictionUpdate = update_queue.get(block=True)\n\n        yield update\n\n        if (\n            update.type == PredictionUpdateType.STATE\n            or update.type == PredictionUpdateType.EXCEPTION\n        ):\n            break\n</code></pre>"},{"location":"reference/careamics_napari/workers/saving_worker/","title":"saving_worker","text":"<p>A thread worker function running CAREamics prediction.</p>"},{"location":"reference/careamics_napari/workers/saving_worker/#careamics_napari.workers.saving_worker.save_worker","title":"<code>save_worker(careamist, training_signal, config_signal)</code>","text":"<p>Model saving worker.</p> <p>Parameters:</p> Name Type Description Default <code>careamist</code> <code>CAREamist</code> <p>CAREamist instance.</p> required <code>training_signal</code> <code>TrainingSignal</code> <p>Training signal.</p> required <code>config_signal</code> <code>SavingSignal</code> <p>Saving signal.</p> required <p>Yields:</p> Type Description <code>Generator[SavingUpdate, None, None]</code> <p>Updates.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Export to BMZ not implemented yet.</p> Source code in <code>src/careamics_napari/workers/saving_worker.py</code> <pre><code>@thread_worker\ndef save_worker(\n    careamist: CAREamist,\n    training_signal: TrainingSignal,\n    config_signal: SavingSignal,\n) -&gt; Generator[SavingUpdate, None, None]:\n    \"\"\"Model saving worker.\n\n    Parameters\n    ----------\n    careamist : CAREamist\n        CAREamist instance.\n    training_signal : TrainingSignal\n        Training signal.\n    config_signal : SavingSignal\n        Saving signal.\n\n    Yields\n    ------\n    Generator[SavingUpdate, None, None]\n        Updates.\n\n    Raises\n    ------\n    NotImplementedError\n        Export to BMZ not implemented yet.\n    \"\"\"\n    dims = \"3D\" if training_signal.is_3d else \"2D\"\n    name = f\"{training_signal.algorithm}_{dims}_{training_signal.experiment_name}\"\n\n    # save model\n    try:\n        if config_signal.export_type == ExportType.BMZ:\n\n            raise NotImplementedError(\"Export to BMZ not implemented yet (but soon).\")\n\n        else:\n            name = name + \".ckpt\"\n            # TODO: should we reexport the model every time?\n            careamist.trainer.save_checkpoint(\n                config_signal.path_model / name,\n            )\n\n    except Exception as e:\n        traceback.print_exc()\n\n        yield SavingUpdate(SavingUpdateType.EXCEPTION, e)\n\n    yield SavingUpdate(SavingUpdateType.STATE, SavingState.DONE)\n</code></pre>"},{"location":"reference/careamics_napari/workers/training_worker/","title":"training_worker","text":"<p>A thread worker function running CAREamics training.</p>"},{"location":"reference/careamics_napari/workers/training_worker/#careamics_napari.workers.training_worker._push_exception","title":"<code>_push_exception(queue, e)</code>","text":"<p>Push an exception to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>Queue</code> <p>Queue.</p> required <code>e</code> <code>Exception</code> <p>Exception.</p> required Source code in <code>src/careamics_napari/workers/training_worker.py</code> <pre><code>def _push_exception(queue: Queue, e: Exception) -&gt; None:\n    \"\"\"Push an exception to the queue.\n\n    Parameters\n    ----------\n    queue : Queue\n        Queue.\n    e : Exception\n        Exception.\n    \"\"\"\n    queue.put(TrainUpdate(TrainUpdateType.EXCEPTION, e))\n</code></pre>"},{"location":"reference/careamics_napari/workers/training_worker/#careamics_napari.workers.training_worker._train","title":"<code>_train(config_signal, training_queue, predict_queue, careamist=None)</code>","text":"<p>Run the training.</p> <p>Parameters:</p> Name Type Description Default <code>config_signal</code> <code>TrainingSignal</code> <p>Training signal.</p> required <code>training_queue</code> <code>Queue</code> <p>Training update queue.</p> required <code>predict_queue</code> <code>Queue</code> <p>Prediction update queue.</p> required <code>careamist</code> <code>CAREamist or None</code> <p>CAREamist instance.</p> <code>None</code> Source code in <code>src/careamics_napari/workers/training_worker.py</code> <pre><code>def _train(\n    config_signal: TrainingSignal,\n    training_queue: Queue,\n    predict_queue: Queue,\n    careamist: Optional[CAREamist] = None,\n) -&gt; None:\n    \"\"\"Run the training.\n\n    Parameters\n    ----------\n    config_signal : TrainingSignal\n        Training signal.\n    training_queue : Queue\n        Training update queue.\n    predict_queue : Queue\n        Prediction update queue.\n    careamist : CAREamist or None, default=None\n        CAREamist instance.\n    \"\"\"\n    # get configuration and queue\n    try:\n        # create_configuration can raise an exception\n        config = create_configuration(config_signal)\n\n        # Create CAREamist\n        if careamist is None:\n            careamist = CAREamist(\n                config, callbacks=[UpdaterCallBack(training_queue, predict_queue)]\n            )\n\n        else:\n            # only update the number of epochs\n            careamist.cfg.training_config.num_epochs = config.training_config.num_epochs\n\n            if config_signal.layer_val == \"\" and config_signal.path_val == \"\":\n                ntf.show_error(\n                    \"Continuing training is currently not supported without explicitely \"\n                    \"passing validation. The reason is that otherwise, the data used for \"\n                    \"validation will be different and there will be data leakage in the \"\n                    \"training set.\"\n                )\n    except Exception as e:\n        traceback.print_exc()\n\n        training_queue.put(TrainUpdate(TrainUpdateType.EXCEPTION, e))\n\n    # Register CAREamist\n    training_queue.put(TrainUpdate(TrainUpdateType.CAREAMIST, careamist))\n\n    # Format data\n    train_data_target = None\n    val_data_target = None\n\n    if config_signal.load_from_disk:\n\n        if config_signal.path_train == \"\":\n            _push_exception(training_queue, ValueError(\"Training data path is empty.\"))\n            return\n\n        train_data = config_signal.path_train\n        val_data = config_signal.path_val if config_signal.path_val != \"\" else None\n\n        if train_data == val_data:\n            val_data = None\n\n        if config_signal.algorithm != SupportedAlgorithm.N2V:\n            if config_signal.path_train_target == \"\":\n                _push_exception(\n                    training_queue, ValueError(\"Training target data path is empty.\")\n                )\n                return\n\n            train_data_target = config_signal.path_train_target\n\n            if val_data is not None:\n                val_data_target = (\n                    config_signal.path_val_target\n                    if config_signal.path_val_target != \"\"\n                    else None\n                )\n\n    else:\n        if config_signal.layer_train is None:\n            _push_exception(\n                training_queue, ValueError(\"Training layer has not been selected.\")\n            )\n            return\n\n        elif config_signal.layer_train.data is None:\n            _push_exception(\n                training_queue,\n                ValueError(\n                    f\"Training layer {config_signal.layer_train.name} is empty.\"\n                ),\n            )\n            return\n        else:\n            train_data = config_signal.layer_train.data\n\n        val_data = (\n            config_signal.layer_val.data\n            if config_signal.layer_val is not None\n            and config_signal.layer_val.data is not None\n            else None\n        )\n\n        if (\n            config_signal.layer_train is not None\n            and config_signal.layer_val is not None\n            and (config_signal.layer_train.name == config_signal.layer_val.name)\n        ):\n            val_data = None\n\n        if config_signal.algorithm != SupportedAlgorithm.N2V:\n\n            if config_signal.layer_train_target is None:\n                _push_exception(\n                    training_queue,\n                    ValueError(\"Training target layer has not been selected.\"),\n                )\n                return\n            elif config_signal.layer_train_target.data is None:\n                _push_exception(\n                    training_queue,\n                    ValueError(\n                        f\"Training target layer {config_signal.layer_train_target.name}\"\n                        f\" is empty.\"\n                    ),\n                )\n                return\n            else:\n                train_data_target = config_signal.layer_train_target.data\n\n            if val_data is not None:\n                val_data_target = (\n                    config_signal.layer_val_target.data\n                    if config_signal.layer_val_target is not None\n                    and config_signal.layer_val_target.data is not None\n                    else None\n                )\n            else:\n                val_data_target = None\n\n    # TODO add val percentage and val minimum\n    # Train CAREamist\n    try:\n        careamist.train(\n            train_source=train_data,\n            val_source=val_data,\n            train_target=train_data_target,\n            val_target=val_data_target,\n            val_minimum_split=config_signal.val_minimum_split,\n            val_percentage=config_signal.val_percentage,\n        )\n\n        # # TODO can we use this to monkey patch the training process?\n        # update_queue.put(Update(UpdateType.MAX_EPOCH, 10_000 // 10))\n        # update_queue.put(Update(UpdateType.MAX_BATCH, 10_000))\n        # for i in range(10_000):\n\n        #     # if stopper.stop:\n        #     #     update_queue.put(Update(UpdateType.STATE, TrainingState.STOPPED))\n        #     #     break\n\n        #     if i % 10 == 0:\n        #         update_queue.put(Update(UpdateType.EPOCH, i // 10))\n        #         print(i)\n\n        #     update_queue.put(Update(UpdateType.BATCH, i))\n\n        #     time.sleep(0.2)\n\n    except Exception as e:\n        traceback.print_exc()\n\n        training_queue.put(TrainUpdate(TrainUpdateType.EXCEPTION, e))\n\n    training_queue.put(TrainUpdate(TrainUpdateType.STATE, TrainingState.DONE))\n</code></pre>"},{"location":"reference/careamics_napari/workers/training_worker/#careamics_napari.workers.training_worker.train_worker","title":"<code>train_worker(train_config_signal, training_queue, predict_queue, careamist=None)</code>","text":"<p>Model training worker.</p> <p>Parameters:</p> Name Type Description Default <code>train_config_signal</code> <code>TrainingSignal</code> <p>Training signal.</p> required <code>training_queue</code> <code>Queue</code> <p>Training update queue.</p> required <code>predict_queue</code> <code>Queue</code> <p>Prediction update queue.</p> required <code>careamist</code> <code>CAREamist or None</code> <p>CAREamist instance.</p> <code>None</code> <p>Yields:</p> Type Description <code>Generator[TrainUpdate, None, None]</code> <p>Updates.</p> Source code in <code>src/careamics_napari/workers/training_worker.py</code> <pre><code>@thread_worker\ndef train_worker(\n    train_config_signal: TrainingSignal,\n    training_queue: Queue,\n    predict_queue: Queue,\n    careamist: Optional[CAREamist] = None,\n) -&gt; Generator[TrainUpdate, None, None]:\n    \"\"\"Model training worker.\n\n    Parameters\n    ----------\n    train_config_signal : TrainingSignal\n        Training signal.\n    training_queue : Queue\n        Training update queue.\n    predict_queue : Queue\n        Prediction update queue.\n    careamist : CAREamist or None, default=None\n        CAREamist instance.\n\n    Yields\n    ------\n    Generator[TrainUpdate, None, None]\n        Updates.\n    \"\"\"\n    # start training thread\n    training = Thread(\n        target=_train,\n        args=(\n            train_config_signal,\n            training_queue,\n            predict_queue,\n            careamist,\n        ),\n    )\n    training.start()\n\n    # look for updates\n    while True:\n        update: TrainUpdate = training_queue.get(block=True)\n\n        yield update\n\n        if (\n            update.type == TrainUpdateType.STATE and update.value == TrainingState.DONE\n        ) or (update.type == TrainUpdateType.EXCEPTION):\n            break\n\n    # wait for the other thread to finish\n    training.join()\n</code></pre>"},{"location":"reference/careamics_portfolio/","title":"CAREamics portfolio","text":"<p>Use the navigation index on the left to explore the documentation.</p>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/","title":"denoiseg_datasets","text":""},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.DSB2018","title":"<code>DSB2018</code>","text":"<p>               Bases: <code>PortfolioEntry</code>, <code>NoisyObject</code></p> <p>The 2018 Data Science Bowl dataset used by DenoiSeg.</p> <p>The dataset is available in three different noise levels: N0, N10 and N20.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>noise_level (NoiseLevel): Noise level of the dataset. name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>class DSB2018(PortfolioEntry, NoisyObject):\n    \"\"\"The 2018 Data Science Bowl dataset used by DenoiSeg.\n\n    The dataset is available in three different noise levels: N0, N10 and N20.\n\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        noise_level (NoiseLevel): Noise level of the dataset.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n        \"\"\"Initialize a DSB2018 instance.\n\n        Parameters\n        ----------\n        noise_level : NoiseLevel, optional\n            Noise level of the dataset, by default NoiseLevel.N0\n        \"\"\"\n        super().__init__(\n            portfolio=DENOISEG,\n            noise_level=noise_level,\n            name=f\"DSB2018_n{noise_level.value}\",\n            url=self._get_url(noise_level),\n            file_name=f\"DSB2018_n{noise_level.value}.zip\",\n            sha256=self._get_hash(noise_level),\n            description=\"From the Kaggle 2018 Data Science Bowl challenge, the \"\n            \"training and validation sets consist of 3800 and 670 patches \"\n            \"respectively, while the test set counts 50 images.\\n\"\n            \"Original data: \"\n            \"https://www.kaggle.com/competitions/data-science-bowl-2018/data\",\n            license=\"GPL-3.0\",\n            citation=\"Caicedo, J.C., Goodman, A., Karhohs, K.W. et al. Nucleus \"\n            \"segmentation across imaging experiments: the 2018 Data Science \"\n            \"Bowl. Nat Methods 16, 1247-1253 (2019). \"\n            \"https://doi.org/10.1038/s41592-019-0612-7\",\n            size=self._get_size(noise_level),\n            tags=[\"denoising\", \"segmentation\", \"nuclei\", \"fluorescence\"],\n        )\n\n    @staticmethod\n    def _get_url(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"https://zenodo.org/record/5156969/files/DSB2018_n0.zip?download=1\"\n        elif noise == NoiseLevel.N10:\n            return \"https://zenodo.org/record/5156977/files/DSB2018_n10.zip?download=1\"\n        else:\n            return \"https://zenodo.org/record/5156983/files/DSB2018_n20.zip?download=1\"\n\n    @staticmethod\n    def _get_hash(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"729d7683ccfa1ad437f666256b23e73b3b3b3da6a8e47bb37303f0c64376a299\"\n        elif noise == NoiseLevel.N10:\n            return \"a4cf731aa0652f8198275f8ce29fb98e0c76c391a96b6092d0792fe447e4103a\"\n        else:\n            return \"6a732a12bf18fecc590230b1cd4df5e32acfa1b35ef2fca42db811cb8277c67c\"\n\n    @staticmethod\n    def _get_size(noise: NoiseLevel) -&gt; float:\n        if noise == NoiseLevel.N0:\n            return 40.2\n        elif noise == NoiseLevel.N10:\n            return 366.0\n        else:\n            return 368.0\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.DSB2018.__init__","title":"<code>__init__(noise_level=NoiseLevel.N0)</code>","text":"<p>Initialize a DSB2018 instance.</p> <p>Parameters:</p> Name Type Description Default <code>noise_level</code> <code>NoiseLevel</code> <p>Noise level of the dataset, by default NoiseLevel.N0</p> <code>N0</code> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n    \"\"\"Initialize a DSB2018 instance.\n\n    Parameters\n    ----------\n    noise_level : NoiseLevel, optional\n        Noise level of the dataset, by default NoiseLevel.N0\n    \"\"\"\n    super().__init__(\n        portfolio=DENOISEG,\n        noise_level=noise_level,\n        name=f\"DSB2018_n{noise_level.value}\",\n        url=self._get_url(noise_level),\n        file_name=f\"DSB2018_n{noise_level.value}.zip\",\n        sha256=self._get_hash(noise_level),\n        description=\"From the Kaggle 2018 Data Science Bowl challenge, the \"\n        \"training and validation sets consist of 3800 and 670 patches \"\n        \"respectively, while the test set counts 50 images.\\n\"\n        \"Original data: \"\n        \"https://www.kaggle.com/competitions/data-science-bowl-2018/data\",\n        license=\"GPL-3.0\",\n        citation=\"Caicedo, J.C., Goodman, A., Karhohs, K.W. et al. Nucleus \"\n        \"segmentation across imaging experiments: the 2018 Data Science \"\n        \"Bowl. Nat Methods 16, 1247-1253 (2019). \"\n        \"https://doi.org/10.1038/s41592-019-0612-7\",\n        size=self._get_size(noise_level),\n        tags=[\"denoising\", \"segmentation\", \"nuclei\", \"fluorescence\"],\n    )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.MouseNuclei","title":"<code>MouseNuclei</code>","text":"<p>               Bases: <code>PortfolioEntry</code>, <code>NoisyObject</code></p> <p>Mouse nuclei dataset used by DenoiSeg.</p> <p>The dataset is available in three different noise levels: N0, N10 and N20.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>noise_level (NoiseLevel): Noise level of the dataset. name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>class MouseNuclei(PortfolioEntry, NoisyObject):\n    \"\"\"Mouse nuclei dataset used by DenoiSeg.\n\n    The dataset is available in three different noise levels: N0, N10 and N20.\n\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        noise_level (NoiseLevel): Noise level of the dataset.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n        \"\"\"Initialize a MouseNuclei instance.\n\n        Parameters\n        ----------\n        noise_level : NoiseLevel, optional\n            Noise level of the dataset, by default NoiseLevel.N0\n        \"\"\"\n        super().__init__(\n            portfolio=DENOISEG,\n            noise_level=noise_level,\n            name=f\"MouseNuclei_n{noise_level.value}\",\n            url=self._get_url(noise_level),\n            file_name=f\"MouseNuclei_n{noise_level.value}.zip\",\n            sha256=self._get_hash(noise_level),\n            description=\"A dataset depicting diverse and non-uniformly \"\n            \"clustered nuclei in the mouse skull, consisting of 908 training \"\n            \"and 160 validation patches. The test set counts 67 additional images\",\n            license=\"CC BY-SA 4.0\",\n            citation=\"Buchholz, T.O., Prakash, M., Schmidt, D., Krull, A., Jug, \"\n            \"F.: Denoiseg: joint denoising and segmentation. In: European \"\n            \"Conference on Computer Vision (ECCV). pp. 324-337. Springer (2020) 8, 9\",\n            size=self._get_size(noise_level),\n            tags=[\"denoising\", \"segmentation\", \"nuclei\", \"fluorescence\"],\n        )\n\n    @staticmethod\n    def _get_url(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"https://zenodo.org/record/5157001/files/Mouse_n0.zip?download=1\"\n        elif noise == NoiseLevel.N10:\n            return \"https://zenodo.org/record/5157003/files/Mouse_n10.zip?download=1\"\n        else:\n            return \"https://zenodo.org/record/5157008/files/Mouse_n20.zip?download=1\"\n\n    @staticmethod\n    def _get_hash(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"5d6fd2fc23ab991a8fde4bd0ec5e9fc9299f9a9ddc2a8acb7095f9b02ff3c9d7\"\n        elif noise == NoiseLevel.N10:\n            return \"de634496e3e46a4887907b713fe6f575e410c3006046054bce67ef9398523c2c\"\n        else:\n            return \"d3d1bf8c89bb97a673a0791874e5b75a6a516ccaaeece0244b4e1e0afe7ab3ec\"\n\n    @staticmethod\n    def _get_size(noise: NoiseLevel) -&gt; float:\n        if noise == NoiseLevel.N0:\n            return 12.4\n        elif noise == NoiseLevel.N10:\n            return 161.0\n        else:\n            return 160.0\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.MouseNuclei.__init__","title":"<code>__init__(noise_level=NoiseLevel.N0)</code>","text":"<p>Initialize a MouseNuclei instance.</p> <p>Parameters:</p> Name Type Description Default <code>noise_level</code> <code>NoiseLevel</code> <p>Noise level of the dataset, by default NoiseLevel.N0</p> <code>N0</code> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n    \"\"\"Initialize a MouseNuclei instance.\n\n    Parameters\n    ----------\n    noise_level : NoiseLevel, optional\n        Noise level of the dataset, by default NoiseLevel.N0\n    \"\"\"\n    super().__init__(\n        portfolio=DENOISEG,\n        noise_level=noise_level,\n        name=f\"MouseNuclei_n{noise_level.value}\",\n        url=self._get_url(noise_level),\n        file_name=f\"MouseNuclei_n{noise_level.value}.zip\",\n        sha256=self._get_hash(noise_level),\n        description=\"A dataset depicting diverse and non-uniformly \"\n        \"clustered nuclei in the mouse skull, consisting of 908 training \"\n        \"and 160 validation patches. The test set counts 67 additional images\",\n        license=\"CC BY-SA 4.0\",\n        citation=\"Buchholz, T.O., Prakash, M., Schmidt, D., Krull, A., Jug, \"\n        \"F.: Denoiseg: joint denoising and segmentation. In: European \"\n        \"Conference on Computer Vision (ECCV). pp. 324-337. Springer (2020) 8, 9\",\n        size=self._get_size(noise_level),\n        tags=[\"denoising\", \"segmentation\", \"nuclei\", \"fluorescence\"],\n    )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.NoiseLevel","title":"<code>NoiseLevel</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>An IntEnum representing the noise level of a dataset.</p> <p>N0 corresponds to the noise-free version of the dataset, N10 and N20 to images corrupted with Gaussian noise with zero-mean and standard deviations of 10 and 20, respectively.</p> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>class NoiseLevel(str, Enum):\n    \"\"\"An IntEnum representing the noise level of a dataset.\n\n    N0 corresponds to the noise-free version of the dataset, N10 and N20 to\n    images corrupted with Gaussian noise with zero-mean and standard deviations\n    of 10 and 20, respectively.\n    \"\"\"\n\n    N0 = \"0\"\n    N10 = \"10\"\n    N20 = \"20\"\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.NoisyObject","title":"<code>NoisyObject</code>","text":"<p>A mixin class for datasets with different noise levels.</p> <p>Attributes:</p> Name Type Description <code>noise_level (NoiseLevel)</code> <code>Noise level of the dataset.</code> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>class NoisyObject:\n    \"\"\"A mixin class for datasets with different noise levels.\n\n    Attributes\n    ----------\n    noise_level (NoiseLevel): Noise level of the dataset.\n    \"\"\"\n\n    def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0, **kwargs: str) -&gt; None:\n        self._noise_level = noise_level\n\n    @property\n    def noise_level(self) -&gt; NoiseLevel:\n        \"\"\"Noise level of the dataset.\"\"\"\n        return self._noise_level\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.NoisyObject.noise_level","title":"<code>noise_level</code>  <code>property</code>","text":"<p>Noise level of the dataset.</p>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.SegFlywing","title":"<code>SegFlywing</code>","text":"<p>               Bases: <code>PortfolioEntry</code>, <code>NoisyObject</code></p> <p>Flywing dataset used by DenoiSeg.</p> <p>The dataset is available in three different noise levels: N0, N10 and N20.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>noise_level (NoiseLevel): Noise level of the dataset. name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>class SegFlywing(PortfolioEntry, NoisyObject):\n    \"\"\"Flywing dataset used by DenoiSeg.\n\n    The dataset is available in three different noise levels: N0, N10 and N20.\n\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        noise_level (NoiseLevel): Noise level of the dataset.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n        \"\"\"Initialize a Flywing instance.\n\n        Parameters\n        ----------\n        noise_level : NoiseLevel, optional\n            Noise level of the dataset, by default NoiseLevel.N0\n        \"\"\"\n        super().__init__(\n            portfolio=DENOISEG,\n            noise_level=noise_level,\n            name=f\"Flywing_n{noise_level.value}\",\n            url=self._get_url(noise_level),\n            file_name=f\"Flywing_n{noise_level.value}.zip\",\n            sha256=self._get_hash(noise_level),\n            description=\"This dataset consist of 1428 training and 252 \"\n            \"validation patches of a membrane labeled fly wing. The test set \"\n            \"is comprised of 50 additional images.\",\n            license=\"CC BY-SA 4.0\",\n            citation=\"Buchholz, T.O., Prakash, M., Schmidt, D., Krull, A., Jug, \"\n            \"F.: Denoiseg: joint denoising and segmentation. In: European \"\n            \"Conference on Computer Vision (ECCV). pp. 324-337. Springer (2020) 8, 9\",\n            size=self._get_size(noise_level),\n            tags=[\"denoising\", \"segmentation\", \"membrane\", \"fluorescence\"],\n        )\n\n    @staticmethod\n    def _get_url(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"https://zenodo.org/record/5156991/files/Flywing_n0.zip?download=1\"\n        elif noise == NoiseLevel.N10:\n            return \"https://zenodo.org/record/5156993/files/Flywing_n10.zip?download=1\"\n        else:\n            return \"https://zenodo.org/record/5156995/files/Flywing_n20.zip?download=1\"\n\n    @staticmethod\n    def _get_hash(noise: NoiseLevel) -&gt; str:\n        if noise == NoiseLevel.N0:\n            return \"3fb49ba44e7e3e20b4fc3c77754f1bbff7184af7f343f23653f258d50e5d5aca\"\n        elif noise == NoiseLevel.N10:\n            return \"c599981b0900e6b43f0a742f84a5fde664373600dc5334f537b61a76a7be2a3c\"\n        else:\n            return \"604b3a3a081eaa57ee25d708bc9b76b85d05235ba09d7c2b25b171e201ea966f\"\n\n    @staticmethod\n    def _get_size(noise: NoiseLevel) -&gt; float:\n        if noise == NoiseLevel.N0:\n            return 47.0\n        elif noise == NoiseLevel.N10:\n            return 282.0\n        else:\n            return 293.0\n</code></pre>"},{"location":"reference/careamics_portfolio/denoiseg_datasets/#careamics_portfolio.denoiseg_datasets.SegFlywing.__init__","title":"<code>__init__(noise_level=NoiseLevel.N0)</code>","text":"<p>Initialize a Flywing instance.</p> <p>Parameters:</p> Name Type Description Default <code>noise_level</code> <code>NoiseLevel</code> <p>Noise level of the dataset, by default NoiseLevel.N0</p> <code>N0</code> Source code in <code>src/careamics_portfolio/denoiseg_datasets.py</code> <pre><code>def __init__(self, noise_level: NoiseLevel = NoiseLevel.N0) -&gt; None:\n    \"\"\"Initialize a Flywing instance.\n\n    Parameters\n    ----------\n    noise_level : NoiseLevel, optional\n        Noise level of the dataset, by default NoiseLevel.N0\n    \"\"\"\n    super().__init__(\n        portfolio=DENOISEG,\n        noise_level=noise_level,\n        name=f\"Flywing_n{noise_level.value}\",\n        url=self._get_url(noise_level),\n        file_name=f\"Flywing_n{noise_level.value}.zip\",\n        sha256=self._get_hash(noise_level),\n        description=\"This dataset consist of 1428 training and 252 \"\n        \"validation patches of a membrane labeled fly wing. The test set \"\n        \"is comprised of 50 additional images.\",\n        license=\"CC BY-SA 4.0\",\n        citation=\"Buchholz, T.O., Prakash, M., Schmidt, D., Krull, A., Jug, \"\n        \"F.: Denoiseg: joint denoising and segmentation. In: European \"\n        \"Conference on Computer Vision (ECCV). pp. 324-337. Springer (2020) 8, 9\",\n        size=self._get_size(noise_level),\n        tags=[\"denoising\", \"segmentation\", \"membrane\", \"fluorescence\"],\n    )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/","title":"denoising_datasets","text":""},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.CARE_U2OS","title":"<code>CARE_U2OS</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>U2OS cells with artificial noise dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class CARE_U2OS(PortfolioEntry):\n    \"\"\"U2OS cells with artificial noise dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"CARE_U2OS\",\n            url=\"https://dl-at-mbl-2023-data.s3.us-east-2.amazonaws.com/\"\n            \"image_restoration_data.zip\",\n            file_name=\"image_restoration_data.zip\",\n            sha256=\"4112d3666a4f419bbd51ab0b7853c12e16c904f89481cbe7f1a90e48f3241f72\",\n            description=\"CARE dataset used during the MBL course. Original data from\"\n            \"the image set BBBC006v1 of the Broad Bioimage Benchmark Collection \"\n            \"(Ljosa et al., Nature Methods, 2012). The iamges were corrupted with \"\n            \"artificial noise.\",\n            license=\"CC0-1.0\",\n            citation=\"We used the image set BBBC006v1 from the Broad Bioimage \"\n            \"Benchmark Collection [Ljosa et al., Nature Methods, 2012].\",\n            size=760.5,\n            tags=[\"denoising\", \"nuclei\", \"fluorescence\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.Convallaria","title":"<code>Convallaria</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>Convallaria dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class Convallaria(PortfolioEntry):\n    \"\"\"Convallaria dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"Convallaria\",\n            url=\"https://cloud.mpi-cbg.de/index.php/s/BE8raMtHQlgLDF3/download\",\n            file_name=\"Convallaria_diaphragm.zip\",\n            sha256=\"8a2ac3e2792334c833ee8a3ca449fc14eada18145f9d56fa2cb40f462c2e8909\",\n            description=\"Image of a convallaria flower (35x692x520 pixels).\\n\"\n            \"The image also comes with a defocused image in order to allow \\n\"\n            \"estimating the noise distribution.\",\n            license=\"CC-BY-4.0\",\n            citation=\"Krull, A., Vi\u010dar, T., Prakash, M., Lalit, M., &amp; Jug, F. (2020). \"\n            \"Probabilistic noise2void: Unsupervised content-aware denoising. Frontiers\"\n            \" in Computer Science, 2, 5.\",\n            size=344.0,\n            tags=[\"denoising\", \"membrane\", \"fluorescence\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.Flywing","title":"<code>Flywing</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>Flywing dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class Flywing(PortfolioEntry):\n    \"\"\"Flywing dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"Flywing\",\n            url=\"https://download.fht.org/jug/n2v/flywing-data.zip\",\n            file_name=\"flywing-data.zip\",\n            sha256=\"01106b6dc096c423babfca47ef27059a01c2ca053769da06e8649381089a559f\",\n            description=\"Image of a membrane-labeled fly wing (35x692x520 pixels).\",\n            license=\"CC-BY-4.0\",\n            citation=\"Buchholz, T.O., Prakash, M., Schmidt, D., Krull, A., Jug, \"\n            \"F.: Denoiseg: joint denoising and segmentation. In: European \"\n            \"Conference on Computer Vision (ECCV). pp. 324-337. Springer (2020) 8, 9\",\n            size=10.2,\n            tags=[\"denoising\", \"membrane\", \"fluorescence\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.N2N_SEM","title":"<code>N2N_SEM</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>SEM dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class N2N_SEM(PortfolioEntry):\n    \"\"\"SEM dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"N2N_SEM\",\n            url=\"https://download.fht.org/jug/n2n/SEM.zip\",\n            file_name=\"SEM.zip\",\n            sha256=\"03aca31eac4d00a8381577579de2d48b98c77bab91e2f8f925999ec3252d0dac\",\n            description=\"SEM dataset from T.-O. Buchholz et al \"\n            \"(Methods Cell Biol, 2020).\",\n            license=\"CC-BY-4.0\",\n            citation=\"T.-O. Buchholz, A. Krull, R. Shahidi, G. Pigino, G. J\u00e9kely, \"\n            'F. Jug, \"Content-aware image restoration for electron '\n            'microscopy\", Methods Cell Biol 152, 277-289',\n            size=172.7,\n            tags=[\"denoising\", \"electron microscopy\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.N2V_BSD68","title":"<code>N2V_BSD68</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>BSD68 dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class N2V_BSD68(PortfolioEntry):\n    \"\"\"BSD68 dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"N2V_BSD68\",\n            url=\"https://download.fht.org/jug/n2v/BSD68_reproducibility_data.zip\",\n            file_name=\"BSD68_reproducibility_data.zip\",\n            sha256=\"32c66d41196c9cafff465f3c7c42730f851c24766f70383672e18b8832ea8e55\",\n            description=\"This dataset is taken from K. Zhang et al (TIP, 2017). \\n\"\n            \"It consists of 400 gray-scale 180x180 images (cropped from the \"\n            \"BSD dataset) and splitted between training and validation, and \"\n            \"68 gray-scale test images (BSD68).\\n\"\n            \"All images were corrupted with Gaussian noise with standard \"\n            \"deviation of 25 pixels. The test dataset contains the uncorrupted \"\n            \"images as well.\\n\"\n            \"Original dataset: \"\n            \"https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/\",\n            license=\"Unknown\",\n            citation='D. Martin, C. Fowlkes, D. Tal and J. Malik, \"A database of '\n            \"human segmented natural images and its application to \"\n            \"evaluating segmentation algorithms and measuring ecological \"\n            'statistics,\" Proceedings Eighth IEEE International '\n            \"Conference on Computer Vision. ICCV 2001, Vancouver, BC, \"\n            \"Canada, 2001, pp. 416-423 vol.2, doi: \"\n            \"10.1109/ICCV.2001.937655.\",\n            size=395.0,\n            tags=[\"denoising\", \"natural images\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.N2V_RGB","title":"<code>N2V_RGB</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>RGB dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class N2V_RGB(PortfolioEntry):\n    \"\"\"RGB dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"N2V_RGB\",\n            url=\"https://download.fht.org/jug/n2v/RGB.zip\",\n            file_name=\"RGB.zip\",\n            sha256=\"4c2010c6b5c253d3a580afe744cbff969d387617c9dde29fea4463636d285657\",\n            description=\"Banner of the CVPR 2019 conference with extra noise.\",\n            license=\"CC-BY-4.0\",\n            citation='A. Krull, T.-O. Buchholz and F. Jug, \"Noise2Void - Learning '\n            'Denoising From Single Noisy Images,\" 2019 IEEE/CVF '\n            \"Conference on Computer Vision and Pattern Recognition (CVPR),\"\n            \" 2019, pp. 2124-2132\",\n            size=10.4,\n            tags=[\"denoising\", \"natural images\", \"RGB\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.N2V_SEM","title":"<code>N2V_SEM</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>SEM dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class N2V_SEM(PortfolioEntry):\n    \"\"\"SEM dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"N2V_SEM\",\n            url=\"https://download.fht.org/jug/n2v/SEM.zip\",\n            file_name=\"SEM.zip\",\n            sha256=\"e1999b5d10abb1714b7663463f83d0bfb73990f5e0705b6cd212c4d3e824b96c\",\n            description=\"Cropped images from a SEM dataset from T.-O. Buchholz et al \"\n            \"(Methods Cell Biol, 2020).\",\n            license=\"CC-BY-4.0\",\n            citation=\"T.-O. Buchholz, A. Krull, R. Shahidi, G. Pigino, G. J\u00e9kely, \"\n            'F. Jug, \"Content-aware image restoration for electron '\n            'microscopy\", Methods Cell Biol 152, 277-289',\n            size=13.0,\n            tags=[\"denoising\", \"electron microscopy\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/denoising_datasets/#careamics_portfolio.denoising_datasets.Tribolium","title":"<code>Tribolium</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>Tribolium dataset.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/denoising_datasets.py</code> <pre><code>class Tribolium(PortfolioEntry):\n    \"\"\"Tribolium dataset.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=DENOISING,\n            name=\"Tribolium\",\n            url=\"https://edmond.mpg.de/file.xhtml?fileId=264091&amp;version=1.0\",\n            file_name=\"Denoising_Tribolium.tar.gz\",\n            sha256=\"d6ae165eb94c68fdc4af16796fb12c4c36ad3c23afb3dd791e725069874b2e97\",\n            description=(\n                \"Confocal microscopy recordings of developing Tribolium castaneum \"\n                \"with 4 laser-power imaging conditions: GT and C1-C3 (700x700x50)\"\n            ),\n            license=\"CC0 1.0\",\n            citation=(\n                \"M. Weigert, U. Schmidt, T. Boothe, A. M\u00fcller, A. Dibrov, A. Jain, \"\n                \"B. Wilhelm, D. Schmidt, C. Broaddus, S. Culley, M. Rocha-Martins, \"\n                \"F. Segovia-Miranda, C. Norden, R. Henriques, M. Zerial, M. Solimena, \"\n                \"J. Rink, P. Tomancak, L. A. Royer, F. Jug, and E. Myers \"\n                \"Content Aware Image Restoration: Pushing the Limits of Fluorescence \"\n                \"Microscopy Data, Edmond, vol. 1, 2025. https://doi.org/10.17617/3.FDFZOF.\"\n            ),\n            size=4812.8,\n            tags=[\"denoising\", \"nuclei\", \"fluorescence\"],\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/","title":"portfolio","text":""},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg","title":"<code>DenoiSeg</code>","text":"<p>               Bases: <code>IterablePortfolio</code></p> <p>An IterablePortfolio of DenoiSeg datasets.</p> <p>Attributes:</p> Name Type Description <code>DSB2018_n0 (DSB2018)</code> <code>DSB2018 dataset with noise level 0.</code> <p>DSB2018_n10 (DSB2018): DSB2018 dataset with noise level 10. DSB2018_n20 (DSB2018): DSB2018 dataset with noise level 20. Flywing_n0 (SegFlywing): Flywing dataset with noise level 0. Flywing_n10 (SegFlywing): Flywing dataset with noise level 10. Flywing_n20 (SegFlywing): Flywing dataset with noise level 20. MouseNuclei_n0 (MouseNuclei): MouseNuclei dataset with noise level 0. MouseNuclei_n10 (MouseNuclei): MouseNuclei dataset with noise level 10. MouseNuclei_n20 (MouseNuclei): MouseNuclei dataset with noise level 20.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>class DenoiSeg(IterablePortfolio):\n    \"\"\"An IterablePortfolio of DenoiSeg datasets.\n\n    Attributes\n    ----------\n        DSB2018_n0 (DSB2018): DSB2018 dataset with noise level 0.\n        DSB2018_n10 (DSB2018): DSB2018 dataset with noise level 10.\n        DSB2018_n20 (DSB2018): DSB2018 dataset with noise level 20.\n        Flywing_n0 (SegFlywing): Flywing dataset with noise level 0.\n        Flywing_n10 (SegFlywing): Flywing dataset with noise level 10.\n        Flywing_n20 (SegFlywing): Flywing dataset with noise level 20.\n        MouseNuclei_n0 (MouseNuclei): MouseNuclei dataset with noise level 0.\n        MouseNuclei_n10 (MouseNuclei): MouseNuclei dataset with noise level 10.\n        MouseNuclei_n20 (MouseNuclei): MouseNuclei dataset with noise level 20.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._DSB2018_n0 = DSB2018(NoiseLevel.N0)\n        self._DSB2018_n10 = DSB2018(NoiseLevel.N10)\n        self._DSB2018_n20 = DSB2018(NoiseLevel.N20)\n        self._SegFlywing_n0 = SegFlywing(NoiseLevel.N0)\n        self._SegFlywing_n10 = SegFlywing(NoiseLevel.N10)\n        self._SegFlywing_n20 = SegFlywing(NoiseLevel.N20)\n        self._MouseNuclei_n0 = MouseNuclei(NoiseLevel.N0)\n        self._MouseNuclei_n10 = MouseNuclei(NoiseLevel.N10)\n        self._MouseNuclei_n20 = MouseNuclei(NoiseLevel.N20)\n\n        super().__init__(DENOISEG)\n\n    @property\n    def DSB2018_n0(self) -&gt; DSB2018:\n        \"\"\"DSB2018 dataset with noise level 0.\n\n        Returns\n        -------\n        DSB2018\n            DSB2018 dataset with noise level 0.\n        \"\"\"\n        return self._DSB2018_n0\n\n    @property\n    def DSB2018_n10(self) -&gt; DSB2018:\n        \"\"\"DSB2018 dataset with noise level 10.\n\n        Returns\n        -------\n        DSB2018\n            DSB2018 dataset with noise level 10.\n        \"\"\"\n        return self._DSB2018_n10\n\n    @property\n    def DSB2018_n20(self) -&gt; DSB2018:\n        \"\"\"DSB2018 dataset with noise level 20.\n\n        Returns\n        -------\n        DSB2018\n            DSB2018 dataset with noise level 20.\n        \"\"\"\n        return self._DSB2018_n20\n\n    @property\n    def Flywing_n0(self) -&gt; SegFlywing:\n        \"\"\"Flywing dataset with noise level 0.\n\n        Returns\n        -------\n        SegFlywing\n            Flywing dataset with noise level 0.\n        \"\"\"\n        return self._SegFlywing_n0\n\n    @property\n    def Flywing_n10(self) -&gt; SegFlywing:\n        \"\"\"Flywing dataset with noise level 10.\n\n        Returns\n        -------\n        SegFlywing\n            Flywing dataset with noise level 10.\n        \"\"\"\n        return self._SegFlywing_n10\n\n    @property\n    def Flywing_n20(self) -&gt; SegFlywing:\n        \"\"\"Flywing dataset with noise level 20.\n\n        Returns\n        -------\n        SegFlywing\n            Flywing dataset with noise level 20.\n        \"\"\"\n        return self._SegFlywing_n20\n\n    @property\n    def MouseNuclei_n0(self) -&gt; MouseNuclei:\n        \"\"\"MouseNuclei dataset with noise level 0.\n\n        Returns\n        -------\n        MouseNuclei\n            MouseNuclei dataset with noise level 0.\n        \"\"\"\n        return self._MouseNuclei_n0\n\n    @property\n    def MouseNuclei_n10(self) -&gt; MouseNuclei:\n        \"\"\"MouseNuclei dataset with noise level 10.\n\n        Returns\n        -------\n        MouseNuclei\n            MouseNuclei dataset with noise level 10.\n        \"\"\"\n        return self._MouseNuclei_n10\n\n    @property\n    def MouseNuclei_n20(self) -&gt; MouseNuclei:\n        \"\"\"MouseNuclei dataset with noise level 20.\n\n        Returns\n        -------\n        MouseNuclei\n            MouseNuclei dataset with noise level 20.\n        \"\"\"\n        return self._MouseNuclei_n20\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.DSB2018_n0","title":"<code>DSB2018_n0</code>  <code>property</code>","text":"<p>DSB2018 dataset with noise level 0.</p> <p>Returns:</p> Type Description <code>DSB2018</code> <p>DSB2018 dataset with noise level 0.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.DSB2018_n10","title":"<code>DSB2018_n10</code>  <code>property</code>","text":"<p>DSB2018 dataset with noise level 10.</p> <p>Returns:</p> Type Description <code>DSB2018</code> <p>DSB2018 dataset with noise level 10.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.DSB2018_n20","title":"<code>DSB2018_n20</code>  <code>property</code>","text":"<p>DSB2018 dataset with noise level 20.</p> <p>Returns:</p> Type Description <code>DSB2018</code> <p>DSB2018 dataset with noise level 20.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.Flywing_n0","title":"<code>Flywing_n0</code>  <code>property</code>","text":"<p>Flywing dataset with noise level 0.</p> <p>Returns:</p> Type Description <code>SegFlywing</code> <p>Flywing dataset with noise level 0.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.Flywing_n10","title":"<code>Flywing_n10</code>  <code>property</code>","text":"<p>Flywing dataset with noise level 10.</p> <p>Returns:</p> Type Description <code>SegFlywing</code> <p>Flywing dataset with noise level 10.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.Flywing_n20","title":"<code>Flywing_n20</code>  <code>property</code>","text":"<p>Flywing dataset with noise level 20.</p> <p>Returns:</p> Type Description <code>SegFlywing</code> <p>Flywing dataset with noise level 20.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.MouseNuclei_n0","title":"<code>MouseNuclei_n0</code>  <code>property</code>","text":"<p>MouseNuclei dataset with noise level 0.</p> <p>Returns:</p> Type Description <code>MouseNuclei</code> <p>MouseNuclei dataset with noise level 0.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.MouseNuclei_n10","title":"<code>MouseNuclei_n10</code>  <code>property</code>","text":"<p>MouseNuclei dataset with noise level 10.</p> <p>Returns:</p> Type Description <code>MouseNuclei</code> <p>MouseNuclei dataset with noise level 10.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.DenoiSeg.MouseNuclei_n20","title":"<code>MouseNuclei_n20</code>  <code>property</code>","text":"<p>MouseNuclei dataset with noise level 20.</p> <p>Returns:</p> Type Description <code>MouseNuclei</code> <p>MouseNuclei dataset with noise level 20.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising","title":"<code>Denoising</code>","text":"<p>               Bases: <code>IterablePortfolio</code></p> <p>An IterablePortfolio of denoising datasets.</p> <p>Attributes:</p> Name Type Description <code>N2V_BSD68 (N2V_BSD68)</code> <code>BSD68 dataset.</code> <code>N2V_SEM (N2V_SEM)</code> <code>SEM dataset.</code> <code>N2V_RGB (N2V_RGB)</code> <code>RGB dataset.</code> <code>flywing (Flywing)</code> <code>Flywing dataset.</code> <code>Convallaria (Convallaria)</code> <code>Convallaria dataset.</code> <code>CARE_U2OS (CARE_U2OS)</code> <code>CARE_U2OS dataset.</code> <code>Tribolium (Tribolium)</code> <code>Tribolium dataset.</code> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>class Denoising(IterablePortfolio):\n    \"\"\"An IterablePortfolio of denoising datasets.\n\n    Attributes\n    ----------\n    N2V_BSD68 (N2V_BSD68): BSD68 dataset.\n    N2V_SEM (N2V_SEM): SEM dataset.\n    N2V_RGB (N2V_RGB): RGB dataset.\n    flywing (Flywing): Flywing dataset.\n    Convallaria (Convallaria): Convallaria dataset.\n    CARE_U2OS (CARE_U2OS): CARE_U2OS dataset.\n    Tribolium (Tribolium): Tribolium dataset.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._N2N_SEM = N2N_SEM()\n        self._N2V_BSD68 = N2V_BSD68()\n        self._N2V_SEM = N2V_SEM()\n        self._N2V_RGB = N2V_RGB()\n        self._flywing = Flywing()\n        self._Convallaria = Convallaria()\n        self._CARE_U2OS = CARE_U2OS()\n        self._Tribolium = Tribolium()\n\n        super().__init__(DENOISING)\n\n    @property\n    def N2N_SEM(self) -&gt; N2N_SEM:\n        \"\"\"SEM dataset.\n\n        Returns\n        -------\n        N2N_SEM\n            SEM dataset.\n        \"\"\"\n        return self._N2N_SEM\n\n    @property\n    def N2V_BSD68(self) -&gt; N2V_BSD68:\n        \"\"\"BSD68 dataset.\n\n        Returns\n        -------\n        N2V_BSD68\n            BSD68 dataset.\n        \"\"\"\n        return self._N2V_BSD68\n\n    @property\n    def N2V_SEM(self) -&gt; N2V_SEM:\n        \"\"\"SEM dataset.\n\n        Returns\n        -------\n        N2V_SEM\n            SEM dataset.\n        \"\"\"\n        return self._N2V_SEM\n\n    @property\n    def N2V_RGB(self) -&gt; N2V_RGB:\n        \"\"\"RGB dataset.\n\n        Returns\n        -------\n        N2V_RGB\n            RGB dataset.\n        \"\"\"\n        return self._N2V_RGB\n\n    @property\n    def Flywing(self) -&gt; Flywing:\n        \"\"\"Flywing dataset.\n\n        Returns\n        -------\n        Flywing\n            Flywing dataset.\n        \"\"\"\n        return self._flywing\n\n    @property\n    def Convallaria(self) -&gt; Convallaria:\n        \"\"\"Convallaria dataset.\n\n        Returns\n        -------\n        Convallaria\n            Convallaria dataset.\n        \"\"\"\n        return self._Convallaria\n\n    @property\n    def CARE_U2OS(self) -&gt; CARE_U2OS:\n        \"\"\"CARE_U2OS dataset.\n\n        Returns\n        -------\n        CARE_U2OS\n            CARE_U2OS dataset.\n        \"\"\"\n        return self._CARE_U2OS\n\n    @property\n    def Tribolium(self) -&gt; Tribolium:\n        \"\"\"Tribolium dataset.\n\n        Returns\n        -------\n        Tribolium\n            Tribolium dataset.\n        \"\"\"\n        return self._Tribolium\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.CARE_U2OS","title":"<code>CARE_U2OS</code>  <code>property</code>","text":"<p>CARE_U2OS dataset.</p> <p>Returns:</p> Type Description <code>CARE_U2OS</code> <p>CARE_U2OS dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.Convallaria","title":"<code>Convallaria</code>  <code>property</code>","text":"<p>Convallaria dataset.</p> <p>Returns:</p> Type Description <code>Convallaria</code> <p>Convallaria dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.Flywing","title":"<code>Flywing</code>  <code>property</code>","text":"<p>Flywing dataset.</p> <p>Returns:</p> Type Description <code>Flywing</code> <p>Flywing dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.N2N_SEM","title":"<code>N2N_SEM</code>  <code>property</code>","text":"<p>SEM dataset.</p> <p>Returns:</p> Type Description <code>N2N_SEM</code> <p>SEM dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.N2V_BSD68","title":"<code>N2V_BSD68</code>  <code>property</code>","text":"<p>BSD68 dataset.</p> <p>Returns:</p> Type Description <code>N2V_BSD68</code> <p>BSD68 dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.N2V_RGB","title":"<code>N2V_RGB</code>  <code>property</code>","text":"<p>RGB dataset.</p> <p>Returns:</p> Type Description <code>N2V_RGB</code> <p>RGB dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.N2V_SEM","title":"<code>N2V_SEM</code>  <code>property</code>","text":"<p>SEM dataset.</p> <p>Returns:</p> Type Description <code>N2V_SEM</code> <p>SEM dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.Denoising.Tribolium","title":"<code>Tribolium</code>  <code>property</code>","text":"<p>Tribolium dataset.</p> <p>Returns:</p> Type Description <code>Tribolium</code> <p>Tribolium dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.ItarablePortfolioEncoder","title":"<code>ItarablePortfolioEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>Portfolio encoder class.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>class ItarablePortfolioEncoder(JSONEncoder):\n    \"\"\"Portfolio encoder class.\"\"\"\n\n    def default(self, o: IterablePortfolio) -&gt; dict[str, dict[str, str]]:\n        \"\"\"Default method for json export.\n\n        Parameters\n        ----------\n        o : IterablePortfolio\n            Portfolio to export.\n\n        Returns\n        -------\n        dict[str, str]\n            Dictionary representation of the portfolio.\n        \"\"\"\n        return o.as_dict()\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.ItarablePortfolioEncoder.default","title":"<code>default(o)</code>","text":"<p>Default method for json export.</p> <p>Parameters:</p> Name Type Description Default <code>o</code> <code>IterablePortfolio</code> <p>Portfolio to export.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary representation of the portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def default(self, o: IterablePortfolio) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Default method for json export.\n\n    Parameters\n    ----------\n    o : IterablePortfolio\n        Portfolio to export.\n\n    Returns\n    -------\n    dict[str, str]\n        Dictionary representation of the portfolio.\n    \"\"\"\n    return o.as_dict()\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio","title":"<code>IterablePortfolio</code>","text":"<p>Iterable portfolio class.</p> <p>Subclass this class and add PortfolioEntry objects as attributes.</p> <p>Attributes:</p> Name Type Description <code>_name</code> <code>str</code> <p>Name of the portfolio.</p> <code>_datasets</code> <code>List[PortfolioEntry]</code> <p>List of datasets in the portfolio.</p> <code>_current_index</code> <code>int</code> <code>Current index of the iterator.</code> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>class IterablePortfolio:\n    \"\"\"Iterable portfolio class.\n\n    Subclass this class and add PortfolioEntry objects as attributes.\n\n\n    Attributes\n    ----------\n    _name : str\n        Name of the portfolio.\n    _datasets : List[PortfolioEntry]\n        List of datasets in the portfolio.\n    _current_index : int\n    Current index of the iterator.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        self._name = name\n\n        # create list of datasets\n        datasets = []\n        for dataset in vars(self).values():\n            if isinstance(dataset, PortfolioEntry):\n                datasets.append(dataset)\n\n        # record datasets\n        self._datasets = datasets\n        self._current_index = 0\n\n    def __iter__(self) -&gt; IterablePortfolio:\n        \"\"\"Iterator method.\n\n        Returns\n        -------\n        IterablePortfolio\n            Iterator over the portfolio.\n        \"\"\"\n        self._current_index = 0\n        return self\n\n    def __next__(self) -&gt; PortfolioEntry:\n        \"\"\"Next method.\n\n        Returns\n        -------\n        PortfolioEntry\n            Next dataset in the portfolio.\n        \"\"\"\n        if self._current_index &lt; len(self._datasets):\n            next_dataset = self._datasets[self._current_index]\n            self._current_index += 1\n            return next_dataset\n        raise StopIteration(\"The iterator does not have any more elements.\")\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the portfolio.\n\n        Returns\n        -------\n        str\n            Name of the portfolio.\n        \"\"\"\n        return self._name\n\n    def list_datasets(self) -&gt; list[str]:\n        \"\"\"List datasets in the portfolio using friendly names.\n\n        The friendly names are the names of the portfolio entries, rather\n        than that of the IterablePortfolio attributes.\n\n        Returns\n        -------\n        list[str]\n            List of datasets in the portfolio.\n        \"\"\"\n        attributes = []\n\n        # for each attribute\n        for attribute in vars(self).values():\n            if isinstance(attribute, PortfolioEntry):\n                attributes.append(attribute.name)\n\n        return attributes\n\n    def as_dict(self) -&gt; dict:\n        \"\"\"Dictionary representation of a portfolio.\n\n        Used to serialize the class to json, with friendly names as entries.\n\n        Returns\n        -------\n        dict[str]\n            Dictionary representation of the DenoiSeg portfolio.\n        \"\"\"\n        entries = {}\n\n        # for each attribute\n        for attribute in vars(self).values():\n            # if the attribute is a PortfolioEntry\n            if isinstance(attribute, PortfolioEntry):\n                # add the attribute to the entries dictionary\n                entries[attribute.name] = {\n                    \"URL\": attribute.url,\n                    \"Description\": attribute.description,\n                    \"Citation\": attribute.citation,\n                    \"License\": attribute.license,\n                    \"Hash\": attribute.hash,\n                    \"File size\": f\"{attribute.size} MB\",\n                    \"Tags\": attribute.tags,\n                }\n        return entries\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of a portfolio.\n\n        Returns\n        -------\n        str\n        String representation of a portfolio.\n        \"\"\"\n        return f\"{self.name} datasets: {self.list_datasets()}\"\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the portfolio.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the portfolio.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator method.</p> <p>Returns:</p> Type Description <code>IterablePortfolio</code> <p>Iterator over the portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def __iter__(self) -&gt; IterablePortfolio:\n    \"\"\"Iterator method.\n\n    Returns\n    -------\n    IterablePortfolio\n        Iterator over the portfolio.\n    \"\"\"\n    self._current_index = 0\n    return self\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.__next__","title":"<code>__next__()</code>","text":"<p>Next method.</p> <p>Returns:</p> Type Description <code>PortfolioEntry</code> <p>Next dataset in the portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def __next__(self) -&gt; PortfolioEntry:\n    \"\"\"Next method.\n\n    Returns\n    -------\n    PortfolioEntry\n        Next dataset in the portfolio.\n    \"\"\"\n    if self._current_index &lt; len(self._datasets):\n        next_dataset = self._datasets[self._current_index]\n        self._current_index += 1\n        return next_dataset\n    raise StopIteration(\"The iterator does not have any more elements.\")\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.__str__","title":"<code>__str__()</code>","text":"<p>String representation of a portfolio.</p> <p>Returns:</p> Type Description <code>str</code> <code>String representation of a portfolio.</code> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of a portfolio.\n\n    Returns\n    -------\n    str\n    String representation of a portfolio.\n    \"\"\"\n    return f\"{self.name} datasets: {self.list_datasets()}\"\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.as_dict","title":"<code>as_dict()</code>","text":"<p>Dictionary representation of a portfolio.</p> <p>Used to serialize the class to json, with friendly names as entries.</p> <p>Returns:</p> Type Description <code>dict[str]</code> <p>Dictionary representation of the DenoiSeg portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def as_dict(self) -&gt; dict:\n    \"\"\"Dictionary representation of a portfolio.\n\n    Used to serialize the class to json, with friendly names as entries.\n\n    Returns\n    -------\n    dict[str]\n        Dictionary representation of the DenoiSeg portfolio.\n    \"\"\"\n    entries = {}\n\n    # for each attribute\n    for attribute in vars(self).values():\n        # if the attribute is a PortfolioEntry\n        if isinstance(attribute, PortfolioEntry):\n            # add the attribute to the entries dictionary\n            entries[attribute.name] = {\n                \"URL\": attribute.url,\n                \"Description\": attribute.description,\n                \"Citation\": attribute.citation,\n                \"License\": attribute.license,\n                \"Hash\": attribute.hash,\n                \"File size\": f\"{attribute.size} MB\",\n                \"Tags\": attribute.tags,\n            }\n    return entries\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.IterablePortfolio.list_datasets","title":"<code>list_datasets()</code>","text":"<p>List datasets in the portfolio using friendly names.</p> <p>The friendly names are the names of the portfolio entries, rather than that of the IterablePortfolio attributes.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of datasets in the portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def list_datasets(self) -&gt; list[str]:\n    \"\"\"List datasets in the portfolio using friendly names.\n\n    The friendly names are the names of the portfolio entries, rather\n    than that of the IterablePortfolio attributes.\n\n    Returns\n    -------\n    list[str]\n        List of datasets in the portfolio.\n    \"\"\"\n    attributes = []\n\n    # for each attribute\n    for attribute in vars(self).values():\n        if isinstance(attribute, PortfolioEntry):\n            attributes.append(attribute.name)\n\n    return attributes\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager","title":"<code>PortfolioManager</code>  <code>dataclass</code>","text":"<p>Portfolio of datasets.</p> <p>Attributes:</p> Name Type Description <code>denoising (Denoising)</code> <code>Denoising datasets.</code> <code>denoiseg (DenoiSeg)</code> <code>DenoiSeg datasets.</code> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>@dataclass\nclass PortfolioManager:\n    \"\"\"Portfolio of datasets.\n\n    Attributes\n    ----------\n    denoising (Denoising): Denoising datasets.\n    denoiseg (DenoiSeg): DenoiSeg datasets.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._denoising = Denoising()\n        self._denoiseg = DenoiSeg()\n        # self._segmentation = Segmentation()\n\n    @property\n    def denoising(self) -&gt; Denoising:\n        \"\"\"Denoising datasets.\n\n        Returns\n        -------\n        Denoising\n            Denoising datasets.\n        \"\"\"\n        return self._denoising\n\n    @property\n    def denoiseg(self) -&gt; DenoiSeg:\n        \"\"\"DenoiSeg datasets.\n\n        Returns\n        -------\n        DenoiSeg\n            DenoiSeg datasets.\n        \"\"\"\n        return self._denoiseg\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of the portfolio.\n\n        This method allows having a frendly representation of the portfolio as string.\n\n        Returns\n        -------\n        str\n            String representation of the portfolio.\n        \"\"\"\n        return (\n            f\"Portfolio:\\n\"\n            f\"Denoising datasets: {self.denoising.list_datasets()}\\n\"\n            f\"DenoiSeg datasets: {self.denoiseg.list_datasets()}\"\n        )\n\n    def as_dict(self) -&gt; dict[str, IterablePortfolio]:\n        \"\"\"Portfolio as dictionary.\n\n        This method is used during json serialization to maintain human readable\n        keys.\n\n        Returns\n        -------\n        dict[str, IterablePortfolio]\n            Portfolio as dictionary.\n        \"\"\"\n        attributes = {}\n\n        for attribute in vars(self).values():\n            if isinstance(attribute, IterablePortfolio):\n                attributes[attribute.name] = attribute\n\n        return attributes\n\n    def to_json(self, path: str | Path) -&gt; None:\n        \"\"\"Save portfolio to json file using the `as_dict` method.\n\n        Parameters\n        ----------\n        path : str or Path\n            Path to json file.\n        \"\"\"\n        with open(path, \"w\") as f:\n            json.dump(self.as_dict(), f, indent=4, cls=ItarablePortfolioEncoder)\n\n    def to_registry(self, path: str | Path) -&gt; None:\n        \"\"\"Save portfolio as registry (Pooch).\n\n        See: https://www.fatiando.org/pooch/latest/registry-files.html\n\n        Parameters\n        ----------\n        path : str or Path\n            Path to json file.\n        \"\"\"\n        portfolios = self.as_dict()\n        with open(path, \"w\") as file:\n            file.write(\"# Portfolio datasets - pooch registry\\n\")\n            file.write(\"# Generated by running \" \"scripts/update_registry.py\\n\\n\")\n\n            # write each portfolio\n            for key in portfolios.keys():\n                file.write(f\"# {key} \\n\")\n                for entry in portfolios[key]:\n                    file.write(\n                        f\"{entry.get_registry_name()} {entry.hash} {entry.url}\\n\"\n                    )\n                file.write(\"\\n\")\n\n            # add pale blue dot for testing purposes\n            file.write(\"# Test sample\\n\")\n            pale_blue_dot = PaleBlueDot()\n            file.write(\n                f\"{pale_blue_dot.get_registry_name()} \"\n                f\"{pale_blue_dot.hash} {pale_blue_dot.url}\\n\"\n            )\n            pale_blue_dot_zip = PaleBlueDotZip()\n            file.write(\n                f\"{pale_blue_dot_zip.get_registry_name()} \"\n                f\"{pale_blue_dot_zip.hash} {pale_blue_dot_zip.url}\\n\"\n            )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.denoiseg","title":"<code>denoiseg</code>  <code>property</code>","text":"<p>DenoiSeg datasets.</p> <p>Returns:</p> Type Description <code>DenoiSeg</code> <p>DenoiSeg datasets.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.denoising","title":"<code>denoising</code>  <code>property</code>","text":"<p>Denoising datasets.</p> <p>Returns:</p> Type Description <code>Denoising</code> <p>Denoising datasets.</p>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the portfolio.</p> <p>This method allows having a frendly representation of the portfolio as string.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the portfolio.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of the portfolio.\n\n    This method allows having a frendly representation of the portfolio as string.\n\n    Returns\n    -------\n    str\n        String representation of the portfolio.\n    \"\"\"\n    return (\n        f\"Portfolio:\\n\"\n        f\"Denoising datasets: {self.denoising.list_datasets()}\\n\"\n        f\"DenoiSeg datasets: {self.denoiseg.list_datasets()}\"\n    )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.as_dict","title":"<code>as_dict()</code>","text":"<p>Portfolio as dictionary.</p> <p>This method is used during json serialization to maintain human readable keys.</p> <p>Returns:</p> Type Description <code>dict[str, IterablePortfolio]</code> <p>Portfolio as dictionary.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def as_dict(self) -&gt; dict[str, IterablePortfolio]:\n    \"\"\"Portfolio as dictionary.\n\n    This method is used during json serialization to maintain human readable\n    keys.\n\n    Returns\n    -------\n    dict[str, IterablePortfolio]\n        Portfolio as dictionary.\n    \"\"\"\n    attributes = {}\n\n    for attribute in vars(self).values():\n        if isinstance(attribute, IterablePortfolio):\n            attributes[attribute.name] = attribute\n\n    return attributes\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.to_json","title":"<code>to_json(path)</code>","text":"<p>Save portfolio to json file using the <code>as_dict</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to json file.</p> required Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def to_json(self, path: str | Path) -&gt; None:\n    \"\"\"Save portfolio to json file using the `as_dict` method.\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to json file.\n    \"\"\"\n    with open(path, \"w\") as f:\n        json.dump(self.as_dict(), f, indent=4, cls=ItarablePortfolioEncoder)\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.PortfolioManager.to_registry","title":"<code>to_registry(path)</code>","text":"<p>Save portfolio as registry (Pooch).</p> <p>See: https://www.fatiando.org/pooch/latest/registry-files.html</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to json file.</p> required Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def to_registry(self, path: str | Path) -&gt; None:\n    \"\"\"Save portfolio as registry (Pooch).\n\n    See: https://www.fatiando.org/pooch/latest/registry-files.html\n\n    Parameters\n    ----------\n    path : str or Path\n        Path to json file.\n    \"\"\"\n    portfolios = self.as_dict()\n    with open(path, \"w\") as file:\n        file.write(\"# Portfolio datasets - pooch registry\\n\")\n        file.write(\"# Generated by running \" \"scripts/update_registry.py\\n\\n\")\n\n        # write each portfolio\n        for key in portfolios.keys():\n            file.write(f\"# {key} \\n\")\n            for entry in portfolios[key]:\n                file.write(\n                    f\"{entry.get_registry_name()} {entry.hash} {entry.url}\\n\"\n                )\n            file.write(\"\\n\")\n\n        # add pale blue dot for testing purposes\n        file.write(\"# Test sample\\n\")\n        pale_blue_dot = PaleBlueDot()\n        file.write(\n            f\"{pale_blue_dot.get_registry_name()} \"\n            f\"{pale_blue_dot.hash} {pale_blue_dot.url}\\n\"\n        )\n        pale_blue_dot_zip = PaleBlueDotZip()\n        file.write(\n            f\"{pale_blue_dot_zip.get_registry_name()} \"\n            f\"{pale_blue_dot_zip.hash} {pale_blue_dot_zip.url}\\n\"\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio/#careamics_portfolio.portfolio.update_registry","title":"<code>update_registry(path=None)</code>","text":"<p>Update the registry.txt file.</p> Source code in <code>src/careamics_portfolio/portfolio.py</code> <pre><code>def update_registry(path: str | Path | None = None) -&gt; None:\n    \"\"\"Update the registry.txt file.\"\"\"\n    if path is None:\n        path = get_registry_path()\n\n    portfolio = PortfolioManager()\n    portfolio.to_registry(path)\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio_entry/","title":"portfolio_entry","text":""},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry","title":"<code>PortfolioEntry</code>","text":"<p>Base class for portfolio entries.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/portfolio_entry.py</code> <pre><code>class PortfolioEntry:\n    \"\"\"Base class for portfolio entries.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(\n        self,\n        portfolio: str,\n        name: str,\n        url: str,\n        description: str,\n        license: str,\n        citation: str,\n        file_name: str,\n        sha256: str,\n        size: float,\n        tags: List[str],\n        is_zip: bool = True,\n        **kwargs: str,\n    ) -&gt; None:\n        self._portfolio = portfolio\n\n        if \" \" in name:\n            raise ValueError(\"Dataset name cannot contain spaces.\")\n        self._name = name\n\n        self._url = url\n        self._description = description\n        self._license = license\n        self._citation = citation\n        self._file_name = file_name\n        self._hash = sha256\n        self._size = size\n        self._tags = tags\n        self._is_zip = is_zip\n\n    @property\n    def portfolio(self) -&gt; str:\n        \"\"\"Name of the portfolio the dataset belong to.\n\n        Returns\n        -------\n        str\n            Name of the portfolio the dataset belong to.\n        \"\"\"\n        return self._portfolio\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the dataset.\n\n        Returns\n        -------\n        str\n            Name of the dataset.\n        \"\"\"\n        return self._name\n\n    @property\n    def url(self) -&gt; str:\n        \"\"\"URL of the dataset.\n\n        Returns\n        -------\n        str\n            URL of the dataset.\n        \"\"\"\n        return self._url\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Description of the dataset.\n\n        Returns\n        -------\n        str\n            Description of the dataset.\n        \"\"\"\n        return self._description\n\n    @property\n    def license(self) -&gt; str:\n        \"\"\"License of the dataset.\n\n        Returns\n        -------\n        str\n            License of the dataset.\n        \"\"\"\n        return self._license\n\n    @property\n    def citation(self) -&gt; str:\n        \"\"\"Citation to use when referring to the dataset.\n\n        Returns\n        -------\n        str\n            Citation to use when referring to the dataset.\n        \"\"\"\n        return self._citation\n\n    @property\n    def file_name(self) -&gt; str:\n        \"\"\"Name of the downloaded file.\n\n        Returns\n        -------\n        str\n            Name of the downloaded file.\n        \"\"\"\n        return self._file_name\n\n    @property\n    def hash(self) -&gt; str:\n        \"\"\"SHA256 hash of the downloaded file.\n\n        Returns\n        -------\n        str\n            SHA256 hash of the downloaded file.\n        \"\"\"\n        return self._hash\n\n    @property\n    def size(self) -&gt; float:\n        \"\"\"Size of the dataset in MB.\n\n        Returns\n        -------\n        float\n            Size of the dataset in MB.\n        \"\"\"\n        return self._size\n\n    @property\n    def tags(self) -&gt; List[str]:\n        \"\"\"List of tags associated to the dataset.\n\n        Returns\n        -------\n        List[str]\n            List of tags associated to the dataset.\n        \"\"\"\n        return self._tags\n\n    @property\n    def is_zip(self) -&gt; bool:\n        \"\"\"Whether the dataset is a zip file.\n\n        Returns\n        -------\n        bool\n            Whether the dataset is a zip file.\n        \"\"\"\n        return self._is_zip\n\n    def __str__(self) -&gt; str:\n        \"\"\"Convert PortfolioEntry to a string.\n\n        Returns\n        -------\n        str: A string containing the PortfolioEntry attributes.\n        \"\"\"\n        return str(self.to_dict())\n\n    def get_registry_name(self) -&gt; str:\n        \"\"\"Return the name of the entry in the global registry.\n\n        Returns\n        -------\n        str\n            Name of the entry.\n        \"\"\"\n        return self.portfolio + \"-\" + self.name\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert PortfolioEntry to a dictionary.\n\n        Returns\n        -------\n            dict: A dictionary containing the PortfolioEntry attributes.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"url\": self.url,\n            \"description\": self.description,\n            \"license\": self.license,\n            \"citation\": self.citation,\n            \"file_name\": self.file_name,\n            \"hash\": self.hash,\n            \"size\": self.size,\n            \"tags\": self.tags,\n        }\n\n    def download(\n        self,\n        path: Optional[Union[str, Path]] = None,\n    ) -&gt; Union[List[str], Any]:\n        \"\"\"Download dataset in the specified path.\n\n        By default the files will be downloaded in the system's cache folder,\n        and can be retrieved using this function without downloading the file\n        anew (thanks pooch!).\n\n        Parameters\n        ----------\n        path : str | Path\n            Path to the folder in which to download the dataset. Defaults to\n            None.\n\n        Returns\n        -------\n        List[str]\n            List of path(s) to the downloaded file(s).\n        \"\"\"\n        poochfolio = get_poochfolio(path)\n\n        # download data\n        if self.is_zip:\n            return poochfolio.fetch(\n                fname=self.get_registry_name(),\n                processor=Unzip(),\n                progressbar=True,\n            )\n        else:\n            return poochfolio.fetch(\n                fname=self.get_registry_name(),\n                progressbar=True,\n            )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.citation","title":"<code>citation</code>  <code>property</code>","text":"<p>Citation to use when referring to the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>Citation to use when referring to the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.description","title":"<code>description</code>  <code>property</code>","text":"<p>Description of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>Description of the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.file_name","title":"<code>file_name</code>  <code>property</code>","text":"<p>Name of the downloaded file.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the downloaded file.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.hash","title":"<code>hash</code>  <code>property</code>","text":"<p>SHA256 hash of the downloaded file.</p> <p>Returns:</p> Type Description <code>str</code> <p>SHA256 hash of the downloaded file.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.is_zip","title":"<code>is_zip</code>  <code>property</code>","text":"<p>Whether the dataset is a zip file.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the dataset is a zip file.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.license","title":"<code>license</code>  <code>property</code>","text":"<p>License of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>License of the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.portfolio","title":"<code>portfolio</code>  <code>property</code>","text":"<p>Name of the portfolio the dataset belong to.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the portfolio the dataset belong to.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.size","title":"<code>size</code>  <code>property</code>","text":"<p>Size of the dataset in MB.</p> <p>Returns:</p> Type Description <code>float</code> <p>Size of the dataset in MB.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.tags","title":"<code>tags</code>  <code>property</code>","text":"<p>List of tags associated to the dataset.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of tags associated to the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.url","title":"<code>url</code>  <code>property</code>","text":"<p>URL of the dataset.</p> <p>Returns:</p> Type Description <code>str</code> <p>URL of the dataset.</p>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.__str__","title":"<code>__str__()</code>","text":"<p>Convert PortfolioEntry to a string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>A string containing the PortfolioEntry attributes.</code> Source code in <code>src/careamics_portfolio/portfolio_entry.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Convert PortfolioEntry to a string.\n\n    Returns\n    -------\n    str: A string containing the PortfolioEntry attributes.\n    \"\"\"\n    return str(self.to_dict())\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.download","title":"<code>download(path=None)</code>","text":"<p>Download dataset in the specified path.</p> <p>By default the files will be downloaded in the system's cache folder, and can be retrieved using this function without downloading the file anew (thanks pooch!).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the folder in which to download the dataset. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of path(s) to the downloaded file(s).</p> Source code in <code>src/careamics_portfolio/portfolio_entry.py</code> <pre><code>def download(\n    self,\n    path: Optional[Union[str, Path]] = None,\n) -&gt; Union[List[str], Any]:\n    \"\"\"Download dataset in the specified path.\n\n    By default the files will be downloaded in the system's cache folder,\n    and can be retrieved using this function without downloading the file\n    anew (thanks pooch!).\n\n    Parameters\n    ----------\n    path : str | Path\n        Path to the folder in which to download the dataset. Defaults to\n        None.\n\n    Returns\n    -------\n    List[str]\n        List of path(s) to the downloaded file(s).\n    \"\"\"\n    poochfolio = get_poochfolio(path)\n\n    # download data\n    if self.is_zip:\n        return poochfolio.fetch(\n            fname=self.get_registry_name(),\n            processor=Unzip(),\n            progressbar=True,\n        )\n    else:\n        return poochfolio.fetch(\n            fname=self.get_registry_name(),\n            progressbar=True,\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.get_registry_name","title":"<code>get_registry_name()</code>","text":"<p>Return the name of the entry in the global registry.</p> <p>Returns:</p> Type Description <code>str</code> <p>Name of the entry.</p> Source code in <code>src/careamics_portfolio/portfolio_entry.py</code> <pre><code>def get_registry_name(self) -&gt; str:\n    \"\"\"Return the name of the entry in the global registry.\n\n    Returns\n    -------\n    str\n        Name of the entry.\n    \"\"\"\n    return self.portfolio + \"-\" + self.name\n</code></pre>"},{"location":"reference/careamics_portfolio/portfolio_entry/#careamics_portfolio.portfolio_entry.PortfolioEntry.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert PortfolioEntry to a dictionary.</p> <p>Returns:</p> Type Description <code>    dict: A dictionary containing the PortfolioEntry attributes.</code> Source code in <code>src/careamics_portfolio/portfolio_entry.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert PortfolioEntry to a dictionary.\n\n    Returns\n    -------\n        dict: A dictionary containing the PortfolioEntry attributes.\n    \"\"\"\n    return {\n        \"name\": self.name,\n        \"url\": self.url,\n        \"description\": self.description,\n        \"license\": self.license,\n        \"citation\": self.citation,\n        \"file_name\": self.file_name,\n        \"hash\": self.hash,\n        \"size\": self.size,\n        \"tags\": self.tags,\n    }\n</code></pre>"},{"location":"reference/careamics_portfolio/utils/download_utils/","title":"download_utils","text":""},{"location":"reference/careamics_portfolio/utils/download_utils/#careamics_portfolio.utils.download_utils.get_poochfolio","title":"<code>get_poochfolio(path=None)</code>","text":"<p>Create the pooch object for the whole portfolio.</p> <p>By default the files will be downloaded and cached in the user's cache folder and can be retrieved automatically without downloading the file anew (thanks pooch!).</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>Path</code> <p>Path to the folder in which to download the dataset. Defaults to None.</p> <p>Returns:</p> Type Description <code>Pooch</code> <p>Pooch object for the whole portfolio.</p> Source code in <code>src/careamics_portfolio/utils/download_utils.py</code> <pre><code>def get_poochfolio(path: Optional[Union[str, Path]] = None) -&gt; Pooch:\n    \"\"\"Create the pooch object for the whole portfolio.\n\n    By default the files will be downloaded and cached in the user's\n    cache folder and can be retrieved automatically without downloading\n    the file anew (thanks pooch!).\n\n\n    Attributes\n    ----------\n    path : Path\n        Path to the folder in which to download the dataset. Defaults to None.\n\n    Returns\n    -------\n    Pooch\n        Pooch object for the whole portfolio.\n\n    \"\"\"\n    if path is None:\n        path = pooch.os_cache(\"portfolio\")\n\n    poochfolio = pooch.create(\n        path=path,\n        base_url=\"\",\n    )\n\n    # Path to the registry.txt file\n    poochfolio.load_registry(get_registry_path())\n\n    return poochfolio\n</code></pre>"},{"location":"reference/careamics_portfolio/utils/download_utils/#careamics_portfolio.utils.download_utils.get_registry_path","title":"<code>get_registry_path()</code>","text":"<p>Get the path to the registry.txt file.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the registry.txt file.</p> Source code in <code>src/careamics_portfolio/utils/download_utils.py</code> <pre><code>def get_registry_path() -&gt; Path:\n    \"\"\"Get the path to the registry.txt file.\n\n    Returns\n    -------\n    Path\n        Path to the registry.txt file.\n    \"\"\"\n    return Path(__file__).parent / \"../registry/registry.txt\"\n</code></pre>"},{"location":"reference/careamics_portfolio/utils/pale_blue_dot/","title":"pale_blue_dot","text":""},{"location":"reference/careamics_portfolio/utils/pale_blue_dot/#careamics_portfolio.utils.pale_blue_dot.PaleBlueDot","title":"<code>PaleBlueDot</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>The original Pale Blue Dot image.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/utils/pale_blue_dot.py</code> <pre><code>class PaleBlueDot(PortfolioEntry):\n    \"\"\"The original Pale Blue Dot image.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=\"test\",\n            name=\"PaleBlueDot\",\n            url=\"https://download.fht.org/jug/careamics/P36254.jpg\",\n            file_name=\"P36254.jpg\",\n            sha256=\"68d0f037a448dc099e893b8cbf4d303ffa4b4289903c764f737101d6ad7555dd\",\n            description=\"Pale Blue Dot, credit NASA/JPL-Caltech.\"\n            \"Original caption: This narrow-angle color image of the\"\n            \" Earth, dubbed 'Pale Blue Dot', is a part of the first\"\n            \" ever 'portrait' of the solar system taken by Voyager \"\n            \"1. The spacecraft acquired a total of 60 frames for a \"\n            \"mosaic of the solar system from a distance of more \"\n            \"than 4 billion miles from Earth and about 32 degrees \"\n            \"above the ecliptic. From Voyager's great distance \"\n            \"Earth is a mere point of light, less than the size of \"\n            \"a picture element even in the narrow-angle camera. \"\n            \"Earth was a crescent only 0.12 pixel in size. \"\n            \"Coincidentally, Earth lies right in the center of one \"\n            \"of the scattered light rays resulting from taking the \"\n            \"image so close to the sun. This blown-up image of the \"\n            \"Earth was taken through three color filters - violet, \"\n            \"blue and green - and recombined to produce the color \"\n            \"image. The background features in the image are \"\n            \"artifacts resulting from the magnification.\",\n            citation=\"NASA/JPL-Caltech\",\n            license=\"Public domain\",\n            size=0.4,\n            tags=[\"pale blue dot\", \"voyager\", \"nasa\", \"jpl\"],\n            is_zip=False,\n        )\n</code></pre>"},{"location":"reference/careamics_portfolio/utils/pale_blue_dot_zip/","title":"pale_blue_dot_zip","text":""},{"location":"reference/careamics_portfolio/utils/pale_blue_dot_zip/#careamics_portfolio.utils.pale_blue_dot_zip.PaleBlueDotZip","title":"<code>PaleBlueDotZip</code>","text":"<p>               Bases: <code>PortfolioEntry</code></p> <p>The original Pale Blue Dot image.</p> <p>Attributes:</p> Name Type Description <code>portfolio (str)</code> <code>Name of the portfolio to which the dataset belong.</code> <p>name (str): Name of the dataset. url (str): URL of the dataset. description (str): Description of the dataset. license (str): License of the dataset. citation (str): Citation to use when referring to the dataset. file_name (str): Name of the downloaded file. hash (str): SHA256 hash of the downloaded file. size (int): Size of the dataset in MB. tags (list[str]): List of tags associated to the dataset. is_zip (bool): Whether the dataset is a zip file.</p> Source code in <code>src/careamics_portfolio/utils/pale_blue_dot_zip.py</code> <pre><code>class PaleBlueDotZip(PortfolioEntry):\n    \"\"\"The original Pale Blue Dot image.\n\n    Attributes\n    ----------\n        portfolio (str): Name of the portfolio to which the dataset belong.\n        name (str): Name of the dataset.\n        url (str): URL of the dataset.\n        description (str): Description of the dataset.\n        license (str): License of the dataset.\n        citation (str): Citation to use when referring to the dataset.\n        file_name (str): Name of the downloaded file.\n        hash (str): SHA256 hash of the downloaded file.\n        size (int): Size of the dataset in MB.\n        tags (list[str]): List of tags associated to the dataset.\n        is_zip (bool): Whether the dataset is a zip file.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\n            portfolio=\"test\",\n            name=\"PaleBlueDotZip\",\n            url=\"https://download.fht.org/jug/careamics/pale_blue_dot.zip\",\n            file_name=\"pale_blue_dot.zip\",\n            sha256=\"90b03ec7a9e1980fd112a40c2c935015bb349cdf89fbf3db78c715dd2a49db47\",\n            description=\"Pale Blue Dot, credit NASA/JPL-Caltech.\"\n            \"Original caption: This narrow-angle color image of the\"\n            \" Earth, dubbed 'Pale Blue Dot', is a part of the first\"\n            \" ever 'portrait' of the solar system taken by Voyager \"\n            \"1. The spacecraft acquired a total of 60 frames for a \"\n            \"mosaic of the solar system from a distance of more \"\n            \"than 4 billion miles from Earth and about 32 degrees \"\n            \"above the ecliptic. From Voyager's great distance \"\n            \"Earth is a mere point of light, less than the size of \"\n            \"a picture element even in the narrow-angle camera. \"\n            \"Earth was a crescent only 0.12 pixel in size. \"\n            \"Coincidentally, Earth lies right in the center of one \"\n            \"of the scattered light rays resulting from taking the \"\n            \"image so close to the sun. This blown-up image of the \"\n            \"Earth was taken through three color filters - violet, \"\n            \"blue and green - and recombined to produce the color \"\n            \"image. The background features in the image are \"\n            \"artifacts resulting from the magnification.\",\n            citation=\"NASA/JPL-Caltech\",\n            license=\"Public domain\",\n            size=0.4,\n            tags=[\"pale blue dot\", \"voyager\", \"nasa\", \"jpl\"],\n            is_zip=True,\n        )\n</code></pre>"}]}