<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://careamics.github.io/0.1/reference/careamics/models/lvae/layers/ rel=canonical><link href=../../layers/ rel=prev><link href=../likelihoods/ rel=next><link rel=icon href=../../../../../assets/icon_careamics.png><meta name=generator content="mkdocs-1.6.0, mkdocs-material-9.5.28"><title>layers -</title><link rel=stylesheet href=../../../../../assets/stylesheets/main.6543a935.min.css><link rel=stylesheet href=../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=stylesheet href=../../../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../../../stylesheets/extra.css><link rel=stylesheet href=../../../../../stylesheets/grid_menu.css><link rel=stylesheet href=../../../../../stylesheets/termynal.css><link rel=stylesheet href=https://unpkg.com/katex@0/dist/katex.min.css><script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#careamics.models.lvae.layers class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../.. title class="md-header__button md-logo" aria-label data-md-component=logo> <img src=../../../../../assets/banner_careamics_large.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> layers </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.768 3.96v.001l-.002-.005a9.08 9.08 0 0 0-.218-.779c-.13-.394.21-.8.602-.67.29.096.575.205.855.328l.01.005A10.002 10.002 0 0 1 12 22a10.002 10.002 0 0 1-9.162-5.985l-.004-.01a9.722 9.722 0 0 1-.329-.855c-.13-.392.277-.732.67-.602.257.084.517.157.78.218l.004.002A9 9 0 0 0 14.999 6a9.09 9.09 0 0 0-.231-2.04ZM16.5 6c0 5.799-4.701 10.5-10.5 10.5-.426 0-.847-.026-1.26-.075A8.5 8.5 0 1 0 16.425 4.74c.05.413.075.833.075 1.259Z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=amber aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 19a7 7 0 1 1 0-14 7 7 0 0 1 0 14Zm0-1.5a5.5 5.5 0 1 0 0-11 5.5 5.5 0 1 0 0 11Zm-5.657.157a.75.75 0 0 1 0 1.06l-1.768 1.768a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734l1.767-1.768a.75.75 0 0 1 1.061 0ZM3.515 3.515a.75.75 0 0 1 1.06 0l1.768 1.768a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L3.515 4.575a.75.75 0 0 1 0-1.06ZM12 0a.75.75 0 0 1 .75.75v2.5a.75.75 0 0 1-1.5 0V.75A.75.75 0 0 1 12 0ZM4 12a.75.75 0 0 1-.75.75H.75a.75.75 0 0 1 0-1.5h2.5A.75.75 0 0 1 4 12Zm8 8a.75.75 0 0 1 .75.75v2.5a.75.75 0 0 1-1.5 0v-2.5A.75.75 0 0 1 12 20Zm12-8a.75.75 0 0 1-.75.75h-2.5a.75.75 0 0 1 0-1.5h2.5A.75.75 0 0 1 24 12Zm-6.343 5.657a.75.75 0 0 1 1.06 0l1.768 1.768a.751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018l-1.768-1.767a.75.75 0 0 1 0-1.061Zm2.828-14.142a.75.75 0 0 1 0 1.06l-1.768 1.768a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042l1.767-1.768a.75.75 0 0 1 1.061 0Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/CAREamics/careamics title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> CAREamics </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../.. title class="md-nav__button md-logo" aria-label data-md-component=logo> <img src=../../../../../assets/banner_careamics_large.png alt=logo> </a> </label> <div class=md-nav__source> <a href=https://github.com/CAREamics/careamics title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </div> <div class=md-source__repository> CAREamics </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M11.03 2.59a1.501 1.501 0 0 1 1.94 0l7.5 6.363a1.5 1.5 0 0 1 .53 1.144V19.5a1.5 1.5 0 0 1-1.5 1.5h-5.75a.75.75 0 0 1-.75-.75V14h-2v6.25a.75.75 0 0 1-.75.75H4.5A1.5 1.5 0 0 1 3 19.5v-9.403c0-.44.194-.859.53-1.144ZM12 3.734l-7.5 6.363V19.5h5v-6.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v6.25h5v-9.403Z"/></svg> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../../../../../installation/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M11.25 9.331V.75a.75.75 0 0 1 1.5 0v8.58l1.949-2.11A.75.75 0 1 1 15.8 8.237l-3.25 3.52a.75.75 0 0 1-1.102 0l-3.25-3.52A.75.75 0 1 1 9.3 7.22l1.949 2.111Z"/><path d="M2.5 3.75v11.5c0 .138.112.25.25.25h18.5a.25.25 0 0 0 .25-.25V3.75a.25.25 0 0 0-.25-.25h-5.5a.75.75 0 0 1 0-1.5h5.5c.966 0 1.75.784 1.75 1.75v11.5A1.75 1.75 0 0 1 21.25 17h-6.204c.171 1.375.805 2.652 1.769 3.757A.752.752 0 0 1 16.25 22h-8.5a.75.75 0 0 1-.566-1.243c.965-1.105 1.599-2.382 1.77-3.757H2.75A1.75 1.75 0 0 1 1 15.25V3.75C1 2.784 1.784 2 2.75 2h5.5a.75.75 0 0 1 0 1.5h-5.5a.25.25 0 0 0-.25.25ZM10.463 17c-.126 1.266-.564 2.445-1.223 3.5h5.52c-.66-1.055-1.098-2.234-1.223-3.5Z"/></svg> <span class=md-ellipsis> Installation </span> </a> </li> <li class=md-nav__item> <a href=../../../../../current_state/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M11.75 1a.75.75 0 0 1 .75.75V4h6.532c.42 0 .826.15 1.143.425l3.187 2.75a1.75 1.75 0 0 1 0 2.65l-3.187 2.75a1.75 1.75 0 0 1-1.143.425H12.5v9.25a.75.75 0 0 1-1.5 0V13H3.75A1.75 1.75 0 0 1 2 11.25v-5.5C2 4.783 2.784 4 3.75 4H11V1.75a.75.75 0 0 1 .75-.75Zm7.282 4.5H3.75a.25.25 0 0 0-.25.25v5.5c0 .138.112.25.25.25h15.282c.06 0 .118-.021.163-.06l3.188-2.75a.248.248 0 0 0 0-.38l-3.188-2.75a.249.249 0 0 0-.163-.06Z"/></svg> <span class=md-ellipsis> Current State </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 2.75A2.75 2.75 0 0 1 5.75 0h14.5a.75.75 0 0 1 .75.75v20.5a.75.75 0 0 1-.75.75h-6a.75.75 0 0 1 0-1.5h5.25v-4H6A1.5 1.5 0 0 0 4.5 18v.75c0 .716.43 1.334 1.05 1.605a.75.75 0 0 1-.6 1.374A3.251 3.251 0 0 1 3 18.75ZM19.5 1.5H5.75c-.69 0-1.25.56-1.25 1.25v12.651A2.989 2.989 0 0 1 6 15h13.5Z"/><path d="M7 18.25a.25.25 0 0 1 .25-.25h5a.25.25 0 0 1 .25.25v5.01a.25.25 0 0 1-.397.201l-2.206-1.604a.25.25 0 0 0-.294 0L7.397 23.46a.25.25 0 0 1-.397-.2v-5.01Z"/></svg> <span class=md-ellipsis> Guides </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Guides </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/careamist_api/ class="md-nav__link "> <span class=md-ellipsis> CAREamist API </span> </a> <label class="md-nav__link " for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> CAREamist API </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_1> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/careamist_api/configuration/ class="md-nav__link "> <span class=md-ellipsis> Configuration </span> </a> <label class="md-nav__link " for=__nav_4_2_1 id=__nav_4_2_1_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_1_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_1> <span class="md-nav__icon md-icon"></span> Configuration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/convenience_functions/ class=md-nav__link> <span class=md-ellipsis> Convenience functions </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/save_load/ class=md-nav__link> <span class=md-ellipsis> Save and load </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/build_configuration/ class=md-nav__link> <span class=md-ellipsis> Build the configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/full_spec/ class=md-nav__link> <span class=md-ellipsis> Full specification </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/algorithm_requirements/ class=md-nav__link> <span class=md-ellipsis> Algorithm requirements </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/advanced_configuration/ class=md-nav__link> <span class=md-ellipsis> Advanced configuration </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/configuration/understanding_errors/ class=md-nav__link> <span class=md-ellipsis> Configuration errors </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2_2> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/careamist_api/usage/ class="md-nav__link "> <span class=md-ellipsis> Using CAREamics </span> </a> <label class="md-nav__link " for=__nav_4_2_2 id=__nav_4_2_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_4_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2_2> <span class="md-nav__icon md-icon"></span> Using CAREamics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/usage/careamist/ class=md-nav__link> <span class=md-ellipsis> CAREamist </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/usage/training/ class=md-nav__link> <span class=md-ellipsis> Training </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/usage/datasets/ class=md-nav__link> <span class=md-ellipsis> (Intermediate) Datasets </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/usage/prediction/ class=md-nav__link> <span class=md-ellipsis> Prediction </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/usage/model_export/ class=md-nav__link> <span class=md-ellipsis> Export to BMZ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../guides/careamist_api/faq/ class=md-nav__link> <span class=md-ellipsis> FAQ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/lightning_api/ class="md-nav__link "> <span class=md-ellipsis> Lightning API </span> </a> <label class="md-nav__link " for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Lightning API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../guides/lightning_api/lightning_module/ class=md-nav__link> <span class=md-ellipsis> Lightning module </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/lightning_api/train_data_module/ class=md-nav__link> <span class=md-ellipsis> Train data module </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/lightning_api/prediction/ class=md-nav__link> <span class=md-ellipsis> Prediction </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../../../guides/cli/ class=md-nav__link> <span class=md-ellipsis> Command-line interface </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_5> <div class="md-nav__link md-nav__container"> <a href=../../../../../guides/dev_resources/ class="md-nav__link "> <span class=md-ellipsis> Developer resources </span> </a> <label class="md-nav__link " for=__nav_4_5 id=__nav_4_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=false> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> Developer resources </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../guides/dev_resources/contribute/ class=md-nav__link> <span class=md-ellipsis> Contribute to CAREamics </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/dev_resources/website/ class=md-nav__link> <span class=md-ellipsis> Github pages </span> </a> </li> <li class=md-nav__item> <a href=../../../../../guides/dev_resources/docstring/ class=md-nav__link> <span class=md-ellipsis> Docstring conventions </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../../../../applications/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21.75 21.5H2.25A1.75 1.75 0 0 1 .5 19.75V4.25c0-.966.784-1.75 1.75-1.75h19.5c.966 0 1.75.784 1.75 1.75v15.5a1.75 1.75 0 0 1-1.75 1.75ZM2.25 4a.25.25 0 0 0-.25.25v15.5c0 .138.112.25.25.25h3.178L14 10.977a1.749 1.749 0 0 1 2.506-.032L22 16.44V4.25a.25.25 0 0 0-.25-.25ZM22 19.75v-1.19l-6.555-6.554a.248.248 0 0 0-.18-.073.247.247 0 0 0-.178.077L7.497 20H21.75a.25.25 0 0 0 .25-.25ZM10.5 9.25a3.25 3.25 0 1 1-6.5 0 3.25 3.25 0 0 1 6.5 0Zm-1.5 0a1.75 1.75 0 1 0-3.501.001A1.75 1.75 0 0 0 9 9.25Z"/></svg> <span class=md-ellipsis> Applications </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Applications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_2> <label class=md-nav__link for=__nav_5_2 id=__nav_5_2_label tabindex=0> <span class=md-ellipsis> Lightning_API </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_2_label aria-expanded=false> <label class=md-nav__title for=__nav_5_2> <span class="md-nav__icon md-icon"></span> Lightning_API </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../applications/Lightning_API/2D_BSD68/ class=md-nav__link> <span class=md-ellipsis> 2D BSD68 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_3> <label class=md-nav__link for=__nav_5_3 id=__nav_5_3_label tabindex=0> <span class=md-ellipsis> Noise2Noise </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_3_label aria-expanded=false> <label class=md-nav__title for=__nav_5_3> <span class="md-nav__icon md-icon"></span> Noise2Noise </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../applications/Noise2Noise/2D_SEM/ class=md-nav__link> <span class=md-ellipsis> 2D SEM </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_4> <label class=md-nav__link for=__nav_5_4 id=__nav_5_4_label tabindex=0> <span class=md-ellipsis> CARE </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_4_label aria-expanded=false> <label class=md-nav__title for=__nav_5_4> <span class="md-nav__icon md-icon"></span> CARE </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../applications/CARE/2D_denoising_U2OS/ class=md-nav__link> <span class=md-ellipsis> 2D denoising U2OS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_5> <label class=md-nav__link for=__nav_5_5 id=__nav_5_5_label tabindex=0> <span class=md-ellipsis> N2V2 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5_5> <span class="md-nav__icon md-icon"></span> N2V2 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../applications/N2V2/2D_BSD68/ class=md-nav__link> <span class=md-ellipsis> 2D BSD68 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../applications/N2V2/2D_SEM/ class=md-nav__link> <span class=md-ellipsis> 2D SEM </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5_6> <label class=md-nav__link for=__nav_5_6 id=__nav_5_6_label tabindex=0> <span class=md-ellipsis> Noise2Void </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_5_6_label aria-expanded=false> <label class=md-nav__title for=__nav_5_6> <span class="md-nav__icon md-icon"></span> Noise2Void </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../applications/Noise2Void/2D_BSD68/ class=md-nav__link> <span class=md-ellipsis> 2D BSD68 </span> </a> </li> <li class=md-nav__item> <a href=../../../../../applications/Noise2Void/2D_SEM/ class=md-nav__link> <span class=md-ellipsis> 2D SEM </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <div class="md-nav__link md-nav__container"> <a href=../../../../../algorithms/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M8.75 8h6.5a.75.75 0 0 1 .75.75v6.5a.75.75 0 0 1-.75.75h-6.5a.75.75 0 0 1-.75-.75v-6.5A.75.75 0 0 1 8.75 8Zm.75 6.5h5v-5h-5Z"/><path d="M15.25 1a.75.75 0 0 1 .75.75V4h2.25c.966 0 1.75.784 1.75 1.75V8h2.25a.75.75 0 0 1 0 1.5H20v5h2.25a.75.75 0 0 1 0 1.5H20v2.25A1.75 1.75 0 0 1 18.25 20H16v2.25a.75.75 0 0 1-1.5 0V20h-5v2.25a.75.75 0 0 1-1.5 0V20H5.75A1.75 1.75 0 0 1 4 18.25V16H1.75a.75.75 0 0 1 0-1.5H4v-5H1.75a.75.75 0 0 1 0-1.5H4V5.75C4 4.784 4.784 4 5.75 4H8V1.75a.75.75 0 0 1 1.5 0V4h5V1.75a.75.75 0 0 1 .75-.75Zm3 17.5a.25.25 0 0 0 .25-.25V5.75a.25.25 0 0 0-.25-.25H5.75a.25.25 0 0 0-.25.25v12.5c0 .138.112.25.25.25Z"/></svg> <span class=md-ellipsis> Algorithms </span> </a> <label class="md-nav__link " for=__nav_6 id=__nav_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> Algorithms </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../../algorithms/n2v/Noise2Void/ class=md-nav__link> <span class=md-ellipsis> Noise2Void </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7 checked> <div class="md-nav__link md-nav__container"> <a href=../../../../ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M15.22 4.97a.75.75 0 0 1 1.06 0l6.5 6.5a.75.75 0 0 1 0 1.06l-6.5 6.5a.749.749 0 0 1-1.275-.326.749.749 0 0 1 .215-.734L21.19 12l-5.97-5.97a.75.75 0 0 1 0-1.06Zm-6.44 0a.75.75 0 0 1 0 1.06L2.81 12l5.97 5.97a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215l-6.5-6.5a.75.75 0 0 1 0-1.06l6.5-6.5a.75.75 0 0 1 1.06 0Z"/></svg> <span class=md-ellipsis> Code Reference </span> </a> <label class="md-nav__link " for=__nav_7 id=__nav_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=true> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Code Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../../ class="md-nav__link "> <span class=md-ellipsis> careamics </span> </a> <label class="md-nav__link " for=__nav_7_2 id=__nav_7_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_2_label aria-expanded=true> <label class=md-nav__title for=__nav_7_2> <span class="md-nav__icon md-icon"></span> careamics </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../careamist/ class=md-nav__link> <span class=md-ellipsis> careamist </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3> <label class=md-nav__link for=__nav_7_2_3 id=__nav_7_2_3_label tabindex=0> <span class=md-ellipsis> config </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3> <span class="md-nav__icon md-icon"></span> config </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/algorithm_model/ class=md-nav__link> <span class=md-ellipsis> algorithm_model </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3_2> <label class=md-nav__link for=__nav_7_2_3_2 id=__nav_7_2_3_2_label tabindex=0> <span class=md-ellipsis> architectures </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3_2> <span class="md-nav__icon md-icon"></span> architectures </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/architectures/architecture_model/ class=md-nav__link> <span class=md-ellipsis> architecture_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/architectures/custom_model/ class=md-nav__link> <span class=md-ellipsis> custom_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/architectures/register_model/ class=md-nav__link> <span class=md-ellipsis> register_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/architectures/unet_model/ class=md-nav__link> <span class=md-ellipsis> unet_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/architectures/vae_model/ class=md-nav__link> <span class=md-ellipsis> vae_model </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../config/callback_model/ class=md-nav__link> <span class=md-ellipsis> callback_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/configuration_factory/ class=md-nav__link> <span class=md-ellipsis> configuration_factory </span> </a> </li> <li class=md-nav__item> <a href=../../../config/configuration_model/ class=md-nav__link> <span class=md-ellipsis> configuration_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/data_model/ class=md-nav__link> <span class=md-ellipsis> data_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/inference_model/ class=md-nav__link> <span class=md-ellipsis> inference_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/optimizer_models/ class=md-nav__link> <span class=md-ellipsis> optimizer_models </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3_9> <label class=md-nav__link for=__nav_7_2_3_9 id=__nav_7_2_3_9_label tabindex=0> <span class=md-ellipsis> references </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_3_9_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3_9> <span class="md-nav__icon md-icon"></span> references </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/references/algorithm_descriptions/ class=md-nav__link> <span class=md-ellipsis> algorithm_descriptions </span> </a> </li> <li class=md-nav__item> <a href=../../../config/references/references/ class=md-nav__link> <span class=md-ellipsis> references </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3_10> <label class=md-nav__link for=__nav_7_2_3_10 id=__nav_7_2_3_10_label tabindex=0> <span class=md-ellipsis> support </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_3_10_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3_10> <span class="md-nav__icon md-icon"></span> support </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/support/supported_activations/ class=md-nav__link> <span class=md-ellipsis> supported_activations </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_algorithms/ class=md-nav__link> <span class=md-ellipsis> supported_algorithms </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_architectures/ class=md-nav__link> <span class=md-ellipsis> supported_architectures </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_data/ class=md-nav__link> <span class=md-ellipsis> supported_data </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_loggers/ class=md-nav__link> <span class=md-ellipsis> supported_loggers </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_losses/ class=md-nav__link> <span class=md-ellipsis> supported_losses </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_optimizers/ class=md-nav__link> <span class=md-ellipsis> supported_optimizers </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_pixel_manipulations/ class=md-nav__link> <span class=md-ellipsis> supported_pixel_manipulations </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_struct_axis/ class=md-nav__link> <span class=md-ellipsis> supported_struct_axis </span> </a> </li> <li class=md-nav__item> <a href=../../../config/support/supported_transforms/ class=md-nav__link> <span class=md-ellipsis> supported_transforms </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../config/tile_information/ class=md-nav__link> <span class=md-ellipsis> tile_information </span> </a> </li> <li class=md-nav__item> <a href=../../../config/training_model/ class=md-nav__link> <span class=md-ellipsis> training_model </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3_13> <label class=md-nav__link for=__nav_7_2_3_13 id=__nav_7_2_3_13_label tabindex=0> <span class=md-ellipsis> transformations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_3_13_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3_13> <span class="md-nav__icon md-icon"></span> transformations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/transformations/n2v_manipulate_model/ class=md-nav__link> <span class=md-ellipsis> n2v_manipulate_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/transformations/normalize_model/ class=md-nav__link> <span class=md-ellipsis> normalize_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/transformations/transform_model/ class=md-nav__link> <span class=md-ellipsis> transform_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/transformations/xy_flip_model/ class=md-nav__link> <span class=md-ellipsis> xy_flip_model </span> </a> </li> <li class=md-nav__item> <a href=../../../config/transformations/xy_random_rotate90_model/ class=md-nav__link> <span class=md-ellipsis> xy_random_rotate90_model </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_3_14> <label class=md-nav__link for=__nav_7_2_3_14 id=__nav_7_2_3_14_label tabindex=0> <span class=md-ellipsis> validators </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_3_14_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_3_14> <span class="md-nav__icon md-icon"></span> validators </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../config/validators/validator_utils/ class=md-nav__link> <span class=md-ellipsis> validator_utils </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../conftest/ class=md-nav__link> <span class=md-ellipsis> conftest </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_5> <label class=md-nav__link for=__nav_7_2_5 id=__nav_7_2_5_label tabindex=0> <span class=md-ellipsis> dataset </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_5> <span class="md-nav__icon md-icon"></span> dataset </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_5_1> <label class=md-nav__link for=__nav_7_2_5_1 id=__nav_7_2_5_1_label tabindex=0> <span class=md-ellipsis> dataset_utils </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_5_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_5_1> <span class="md-nav__icon md-icon"></span> dataset_utils </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../dataset/dataset_utils/dataset_utils/ class=md-nav__link> <span class=md-ellipsis> dataset_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/dataset_utils/file_utils/ class=md-nav__link> <span class=md-ellipsis> file_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/dataset_utils/iterate_over_files/ class=md-nav__link> <span class=md-ellipsis> iterate_over_files </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/dataset_utils/running_stats/ class=md-nav__link> <span class=md-ellipsis> running_stats </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../dataset/in_memory_dataset/ class=md-nav__link> <span class=md-ellipsis> in_memory_dataset </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/in_memory_pred_dataset/ class=md-nav__link> <span class=md-ellipsis> in_memory_pred_dataset </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/in_memory_tiled_pred_dataset/ class=md-nav__link> <span class=md-ellipsis> in_memory_tiled_pred_dataset </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/iterable_dataset/ class=md-nav__link> <span class=md-ellipsis> iterable_dataset </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/iterable_pred_dataset/ class=md-nav__link> <span class=md-ellipsis> iterable_pred_dataset </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/iterable_tiled_pred_dataset/ class=md-nav__link> <span class=md-ellipsis> iterable_tiled_pred_dataset </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_5_8> <label class=md-nav__link for=__nav_7_2_5_8 id=__nav_7_2_5_8_label tabindex=0> <span class=md-ellipsis> patching </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_5_8_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_5_8> <span class="md-nav__icon md-icon"></span> patching </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../dataset/patching/patching/ class=md-nav__link> <span class=md-ellipsis> patching </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/patching/random_patching/ class=md-nav__link> <span class=md-ellipsis> random_patching </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/patching/sequential_patching/ class=md-nav__link> <span class=md-ellipsis> sequential_patching </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/patching/validate_patch_dimension/ class=md-nav__link> <span class=md-ellipsis> validate_patch_dimension </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_5_9> <label class=md-nav__link for=__nav_7_2_5_9 id=__nav_7_2_5_9_label tabindex=0> <span class=md-ellipsis> tiling </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_5_9_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_5_9> <span class="md-nav__icon md-icon"></span> tiling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../dataset/tiling/collate_tiles/ class=md-nav__link> <span class=md-ellipsis> collate_tiles </span> </a> </li> <li class=md-nav__item> <a href=../../../dataset/tiling/tiled_patching/ class=md-nav__link> <span class=md-ellipsis> tiled_patching </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../dataset/zarr_dataset/ class=md-nav__link> <span class=md-ellipsis> zarr_dataset </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_6> <label class=md-nav__link for=__nav_7_2_6 id=__nav_7_2_6_label tabindex=0> <span class=md-ellipsis> file_io </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_6> <span class="md-nav__icon md-icon"></span> file_io </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_6_1> <label class=md-nav__link for=__nav_7_2_6_1 id=__nav_7_2_6_1_label tabindex=0> <span class=md-ellipsis> read </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_6_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_6_1> <span class="md-nav__icon md-icon"></span> read </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../file_io/read/get_func/ class=md-nav__link> <span class=md-ellipsis> get_func </span> </a> </li> <li class=md-nav__item> <a href=../../../file_io/read/tiff/ class=md-nav__link> <span class=md-ellipsis> tiff </span> </a> </li> <li class=md-nav__item> <a href=../../../file_io/read/zarr/ class=md-nav__link> <span class=md-ellipsis> zarr </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_6_2> <label class=md-nav__link for=__nav_7_2_6_2 id=__nav_7_2_6_2_label tabindex=0> <span class=md-ellipsis> write </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_6_2_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_6_2> <span class="md-nav__icon md-icon"></span> write </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../file_io/write/get_func/ class=md-nav__link> <span class=md-ellipsis> get_func </span> </a> </li> <li class=md-nav__item> <a href=../../../file_io/write/tiff/ class=md-nav__link> <span class=md-ellipsis> tiff </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_7> <label class=md-nav__link for=__nav_7_2_7 id=__nav_7_2_7_label tabindex=0> <span class=md-ellipsis> lightning </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_7> <span class="md-nav__icon md-icon"></span> lightning </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_7_1> <label class=md-nav__link for=__nav_7_2_7_1 id=__nav_7_2_7_1_label tabindex=0> <span class=md-ellipsis> callbacks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_7_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_7_1> <span class="md-nav__icon md-icon"></span> callbacks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../lightning/callbacks/hyperparameters_callback/ class=md-nav__link> <span class=md-ellipsis> hyperparameters_callback </span> </a> </li> <li class=md-nav__item> <a href=../../../lightning/callbacks/progress_bar_callback/ class=md-nav__link> <span class=md-ellipsis> progress_bar_callback </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../lightning/lightning_module/ class=md-nav__link> <span class=md-ellipsis> lightning_module </span> </a> </li> <li class=md-nav__item> <a href=../../../lightning/predict_data_module/ class=md-nav__link> <span class=md-ellipsis> predict_data_module </span> </a> </li> <li class=md-nav__item> <a href=../../../lightning/train_data_module/ class=md-nav__link> <span class=md-ellipsis> train_data_module </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_8> <label class=md-nav__link for=__nav_7_2_8 id=__nav_7_2_8_label tabindex=0> <span class=md-ellipsis> losses </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_8> <span class="md-nav__icon md-icon"></span> losses </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../losses/loss_factory/ class=md-nav__link> <span class=md-ellipsis> loss_factory </span> </a> </li> <li class=md-nav__item> <a href=../../../losses/losses/ class=md-nav__link> <span class=md-ellipsis> losses </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_9> <label class=md-nav__link for=__nav_7_2_9 id=__nav_7_2_9_label tabindex=0> <span class=md-ellipsis> lvae_training </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_9> <span class="md-nav__icon md-icon"></span> lvae_training </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../lvae_training/data_modules/ class=md-nav__link> <span class=md-ellipsis> data_modules </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/data_utils/ class=md-nav__link> <span class=md-ellipsis> data_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/eval_utils/ class=md-nav__link> <span class=md-ellipsis> eval_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/get_config/ class=md-nav__link> <span class=md-ellipsis> get_config </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/lightning_module/ class=md-nav__link> <span class=md-ellipsis> lightning_module </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/metrics/ class=md-nav__link> <span class=md-ellipsis> metrics </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/train_lvae/ class=md-nav__link> <span class=md-ellipsis> train_lvae </span> </a> </li> <li class=md-nav__item> <a href=../../../lvae_training/train_utils/ class=md-nav__link> <span class=md-ellipsis> train_utils </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_10> <label class=md-nav__link for=__nav_7_2_10 id=__nav_7_2_10_label tabindex=0> <span class=md-ellipsis> model_io </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_10_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_10> <span class="md-nav__icon md-icon"></span> model_io </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_10_1> <label class=md-nav__link for=__nav_7_2_10_1 id=__nav_7_2_10_1_label tabindex=0> <span class=md-ellipsis> bioimage </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_10_1_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_10_1> <span class="md-nav__icon md-icon"></span> bioimage </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../model_io/bioimage/_readme_factory/ class=md-nav__link> <span class=md-ellipsis> _readme_factory </span> </a> </li> <li class=md-nav__item> <a href=../../../model_io/bioimage/bioimage_utils/ class=md-nav__link> <span class=md-ellipsis> bioimage_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../model_io/bioimage/model_description/ class=md-nav__link> <span class=md-ellipsis> model_description </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../model_io/bmz_io/ class=md-nav__link> <span class=md-ellipsis> bmz_io </span> </a> </li> <li class=md-nav__item> <a href=../../../model_io/model_io_utils/ class=md-nav__link> <span class=md-ellipsis> model_io_utils </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_11 checked> <label class=md-nav__link for=__nav_7_2_11 id=__nav_7_2_11_label tabindex=0> <span class=md-ellipsis> models </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_11_label aria-expanded=true> <label class=md-nav__title for=__nav_7_2_11> <span class="md-nav__icon md-icon"></span> models </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../activation/ class=md-nav__link> <span class=md-ellipsis> activation </span> </a> </li> <li class=md-nav__item> <a href=../../layers/ class=md-nav__link> <span class=md-ellipsis> layers </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_11_3 checked> <label class=md-nav__link for=__nav_7_2_11_3 id=__nav_7_2_11_3_label tabindex=0> <span class=md-ellipsis> lvae </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=4 aria-labelledby=__nav_7_2_11_3_label aria-expanded=true> <label class=md-nav__title for=__nav_7_2_11_3> <span class="md-nav__icon md-icon"></span> lvae </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> layers </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> layers </span> </a> <nav class="md-nav md-nav--secondary" aria-label="On This Page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On This Page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#careamics.models.lvae.layers class=md-nav__link> <span class=md-ellipsis> layers </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer class=md-nav__link> <span class=md-ellipsis> BottomUpLayer </span> </a> <nav class=md-nav aria-label=BottomUpLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.GateLayer2d class=md-nav__link> <span class=md-ellipsis> GateLayer2d </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLayer class=md-nav__link> <span class=md-ellipsis> MergeLayer </span> </a> <nav class=md-nav aria-label=MergeLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLowRes class=md-nav__link> <span class=md-ellipsis> MergeLowRes </span> </a> <nav class=md-nav aria-label=MergeLowRes> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLowRes.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d class=md-nav__link> <span class=md-ellipsis> NonStochasticBlock2d </span> </a> <nav class=md-nav aria-label=NonStochasticBlock2d> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.compute_kl_metrics class=md-nav__link> <span class=md-ellipsis> compute_kl_metrics </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d class=md-nav__link> <span class=md-ellipsis> NormalStochasticBlock2d </span> </a> <nav class=md-nav aria-label=NormalStochasticBlock2d> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.compute_kl_metrics class=md-nav__link> <span class=md-ellipsis> compute_kl_metrics </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.get_z class=md-nav__link> <span class=md-ellipsis> get_z </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_p_params class=md-nav__link> <span class=md-ellipsis> process_p_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_q_params class=md-nav__link> <span class=md-ellipsis> process_q_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.sample_from_q class=md-nav__link> <span class=md-ellipsis> sample_from_q </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResBlockWithResampling class=md-nav__link> <span class=md-ellipsis> ResBlockWithResampling </span> </a> <nav class=md-nav aria-label=ResBlockWithResampling> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResBlockWithResampling.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResidualBlock class=md-nav__link> <span class=md-ellipsis> ResidualBlock </span> </a> <nav class=md-nav aria-label=ResidualBlock> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResidualBlock.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.SkipConnectionMerger class=md-nav__link> <span class=md-ellipsis> SkipConnectionMerger </span> </a> <nav class=md-nav aria-label=SkipConnectionMerger> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.SkipConnectionMerger.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer class=md-nav__link> <span class=md-ellipsis> TopDownLayer </span> </a> <nav class=md-nav aria-label=TopDownLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.align_pparams_buvalue class=md-nav__link> <span class=md-ellipsis> align_pparams_buvalue </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.get_p_params class=md-nav__link> <span class=md-ellipsis> get_p_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.sample_from_q class=md-nav__link> <span class=md-ellipsis> sample_from_q </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../likelihoods/ class=md-nav__link> <span class=md-ellipsis> likelihoods </span> </a> </li> <li class=md-nav__item> <a href=../lvae/ class=md-nav__link> <span class=md-ellipsis> lvae </span> </a> </li> <li class=md-nav__item> <a href=../noise_models/ class=md-nav__link> <span class=md-ellipsis> noise_models </span> </a> </li> <li class=md-nav__item> <a href=../utils/ class=md-nav__link> <span class=md-ellipsis> utils </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../model_factory/ class=md-nav__link> <span class=md-ellipsis> model_factory </span> </a> </li> <li class=md-nav__item> <a href=../../unet/ class=md-nav__link> <span class=md-ellipsis> unet </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_12> <label class=md-nav__link for=__nav_7_2_12 id=__nav_7_2_12_label tabindex=0> <span class=md-ellipsis> prediction_utils </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_12_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_12> <span class="md-nav__icon md-icon"></span> prediction_utils </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../prediction_utils/prediction_outputs/ class=md-nav__link> <span class=md-ellipsis> prediction_outputs </span> </a> </li> <li class=md-nav__item> <a href=../../../prediction_utils/stitch_prediction/ class=md-nav__link> <span class=md-ellipsis> stitch_prediction </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_13> <label class=md-nav__link for=__nav_7_2_13 id=__nav_7_2_13_label tabindex=0> <span class=md-ellipsis> transforms </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_13_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_13> <span class="md-nav__icon md-icon"></span> transforms </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../transforms/compose/ class=md-nav__link> <span class=md-ellipsis> compose </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/n2v_manipulate/ class=md-nav__link> <span class=md-ellipsis> n2v_manipulate </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/normalize/ class=md-nav__link> <span class=md-ellipsis> normalize </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/pixel_manipulation/ class=md-nav__link> <span class=md-ellipsis> pixel_manipulation </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/struct_mask_parameters/ class=md-nav__link> <span class=md-ellipsis> struct_mask_parameters </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/transform/ class=md-nav__link> <span class=md-ellipsis> transform </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/tta/ class=md-nav__link> <span class=md-ellipsis> tta </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/xy_flip/ class=md-nav__link> <span class=md-ellipsis> xy_flip </span> </a> </li> <li class=md-nav__item> <a href=../../../transforms/xy_random_rotate90/ class=md-nav__link> <span class=md-ellipsis> xy_random_rotate90 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_2_14> <label class=md-nav__link for=__nav_7_2_14 id=__nav_7_2_14_label tabindex=0> <span class=md-ellipsis> utils </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_2_14_label aria-expanded=false> <label class=md-nav__title for=__nav_7_2_14> <span class="md-nav__icon md-icon"></span> utils </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../utils/autocorrelation/ class=md-nav__link> <span class=md-ellipsis> autocorrelation </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/base_enum/ class=md-nav__link> <span class=md-ellipsis> base_enum </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/context/ class=md-nav__link> <span class=md-ellipsis> context </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/logging/ class=md-nav__link> <span class=md-ellipsis> logging </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/metrics/ class=md-nav__link> <span class=md-ellipsis> metrics </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/path_utils/ class=md-nav__link> <span class=md-ellipsis> path_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/ram/ class=md-nav__link> <span class=md-ellipsis> ram </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/receptive_field/ class=md-nav__link> <span class=md-ellipsis> receptive_field </span> </a> </li> <li class=md-nav__item> <a href=../../../utils/torch_utils/ class=md-nav__link> <span class=md-ellipsis> torch_utils </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_3> <div class="md-nav__link md-nav__container"> <a href=../../../../careamics_portfolio/ class="md-nav__link "> <span class=md-ellipsis> careamics_portfolio </span> </a> <label class="md-nav__link " for=__nav_7_3 id=__nav_7_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_7_3_label aria-expanded=false> <label class=md-nav__title for=__nav_7_3> <span class="md-nav__icon md-icon"></span> careamics_portfolio </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../careamics_portfolio/denoiseg_datasets/ class=md-nav__link> <span class=md-ellipsis> denoiseg_datasets </span> </a> </li> <li class=md-nav__item> <a href=../../../../careamics_portfolio/denoising_datasets/ class=md-nav__link> <span class=md-ellipsis> denoising_datasets </span> </a> </li> <li class=md-nav__item> <a href=../../../../careamics_portfolio/portfolio/ class=md-nav__link> <span class=md-ellipsis> portfolio </span> </a> </li> <li class=md-nav__item> <a href=../../../../careamics_portfolio/portfolio_entry/ class=md-nav__link> <span class=md-ellipsis> portfolio_entry </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7_3_6> <label class=md-nav__link for=__nav_7_3_6 id=__nav_7_3_6_label tabindex=0> <span class=md-ellipsis> utils </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_7_3_6_label aria-expanded=false> <label class=md-nav__title for=__nav_7_3_6> <span class="md-nav__icon md-icon"></span> utils </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../../careamics_portfolio/utils/download_utils/ class=md-nav__link> <span class=md-ellipsis> download_utils </span> </a> </li> <li class=md-nav__item> <a href=../../../../careamics_portfolio/utils/pale_blue_dot/ class=md-nav__link> <span class=md-ellipsis> pale_blue_dot </span> </a> </li> <li class=md-nav__item> <a href=../../../../careamics_portfolio/utils/pale_blue_dot_zip/ class=md-nav__link> <span class=md-ellipsis> pale_blue_dot_zip </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="On This Page"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> On This Page </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#careamics.models.lvae.layers class=md-nav__link> <span class=md-ellipsis> layers </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer class=md-nav__link> <span class=md-ellipsis> BottomUpLayer </span> </a> <nav class=md-nav aria-label=BottomUpLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.BottomUpLayer.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.GateLayer2d class=md-nav__link> <span class=md-ellipsis> GateLayer2d </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLayer class=md-nav__link> <span class=md-ellipsis> MergeLayer </span> </a> <nav class=md-nav aria-label=MergeLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLowRes class=md-nav__link> <span class=md-ellipsis> MergeLowRes </span> </a> <nav class=md-nav aria-label=MergeLowRes> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.MergeLowRes.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d class=md-nav__link> <span class=md-ellipsis> NonStochasticBlock2d </span> </a> <nav class=md-nav aria-label=NonStochasticBlock2d> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.compute_kl_metrics class=md-nav__link> <span class=md-ellipsis> compute_kl_metrics </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d class=md-nav__link> <span class=md-ellipsis> NormalStochasticBlock2d </span> </a> <nav class=md-nav aria-label=NormalStochasticBlock2d> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.compute_kl_metrics class=md-nav__link> <span class=md-ellipsis> compute_kl_metrics </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.get_z class=md-nav__link> <span class=md-ellipsis> get_z </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_p_params class=md-nav__link> <span class=md-ellipsis> process_p_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_q_params class=md-nav__link> <span class=md-ellipsis> process_q_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.sample_from_q class=md-nav__link> <span class=md-ellipsis> sample_from_q </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResBlockWithResampling class=md-nav__link> <span class=md-ellipsis> ResBlockWithResampling </span> </a> <nav class=md-nav aria-label=ResBlockWithResampling> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResBlockWithResampling.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResidualBlock class=md-nav__link> <span class=md-ellipsis> ResidualBlock </span> </a> <nav class=md-nav aria-label=ResidualBlock> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.ResidualBlock.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.SkipConnectionMerger class=md-nav__link> <span class=md-ellipsis> SkipConnectionMerger </span> </a> <nav class=md-nav aria-label=SkipConnectionMerger> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.SkipConnectionMerger.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer class=md-nav__link> <span class=md-ellipsis> TopDownLayer </span> </a> <nav class=md-nav aria-label=TopDownLayer> <ul class=md-nav__list> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.__init__ class=md-nav__link> <span class=md-ellipsis> __init__ </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.align_pparams_buvalue class=md-nav__link> <span class=md-ellipsis> align_pparams_buvalue </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.forward class=md-nav__link> <span class=md-ellipsis> forward </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.get_p_params class=md-nav__link> <span class=md-ellipsis> get_p_params </span> </a> </li> <li class=md-nav__item> <a href=#careamics.models.lvae.layers.TopDownLayer.sample_from_q class=md-nav__link> <span class=md-ellipsis> sample_from_q </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <div role=navigation aria-label="breadcrumbs navigation"> <ul class=breadcrumbs> <li> <a href=../../../../..> <span class=twemoji title=Home><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M11.03 2.59a1.501 1.501 0 0 1 1.94 0l7.5 6.363a1.5 1.5 0 0 1 .53 1.144V19.5a1.5 1.5 0 0 1-1.5 1.5h-5.75a.75.75 0 0 1-.75-.75V14h-2v6.25a.75.75 0 0 1-.75.75H4.5A1.5 1.5 0 0 1 3 19.5v-9.403c0-.44.194-.859.53-1.144ZM12 3.734l-7.5 6.363V19.5h5v-6.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v6.25h5v-9.403Z"/></svg></span> </a> </li> <li>Code Reference</li><li>careamics</li><li>models</li><li>lvae</li> <li>layers</li> </ul> </div> <nav class=md-tags> </nav> <h1>layers</h1> <div class="doc doc-object doc-module"> <a id=careamics.models.lvae.layers></a> <div class="doc doc-contents first"> <p>Script containing the common basic blocks (nn.Module) reused by the LadderVAE architecture.</p> <p>Hierarchy in the model blocks:</p> <div class="doc doc-children"> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.BottomUpLayer class="doc doc-heading"> <code>BottomUpLayer</code> <a href=#careamics.models.lvae.layers.BottomUpLayer class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Bottom-up deterministic layer. It consists of one or a stack of <code>BottomUpDeterministicResBlock</code>'s. The outputs are the so-called <code>bu_values</code> that are later used in the Decoder to update the generative distributions.</p> <p>NOTE: When Lateral Contextualization is Enabled (i.e., <code>enable_multiscale=True</code>), the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB) (without downsampling), and then merged to the latent tensor produced by the primary flow of the <code>BottomUpLayer</code> through the <code>MergeLowRes</code> layer. It is meaningful to remark that the BUDB that takes care of encoding the low-res input can be either shared with the primary flow (and in that case it is the "same_size" BUDB (or stack of BUDBs) -&gt; see <code>self.net</code>), or can be a deep-copy of the primary flow's BUDB. This behaviour is controlled by <code>lowres_separate_branch</code> parameter.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-367>367</a></span>
<span class=normal><a href=#__codelineno-0-368>368</a></span>
<span class=normal><a href=#__codelineno-0-369>369</a></span>
<span class=normal><a href=#__codelineno-0-370>370</a></span>
<span class=normal><a href=#__codelineno-0-371>371</a></span>
<span class=normal><a href=#__codelineno-0-372>372</a></span>
<span class=normal><a href=#__codelineno-0-373>373</a></span>
<span class=normal><a href=#__codelineno-0-374>374</a></span>
<span class=normal><a href=#__codelineno-0-375>375</a></span>
<span class=normal><a href=#__codelineno-0-376>376</a></span>
<span class=normal><a href=#__codelineno-0-377>377</a></span>
<span class=normal><a href=#__codelineno-0-378>378</a></span>
<span class=normal><a href=#__codelineno-0-379>379</a></span>
<span class=normal><a href=#__codelineno-0-380>380</a></span>
<span class=normal><a href=#__codelineno-0-381>381</a></span>
<span class=normal><a href=#__codelineno-0-382>382</a></span>
<span class=normal><a href=#__codelineno-0-383>383</a></span>
<span class=normal><a href=#__codelineno-0-384>384</a></span>
<span class=normal><a href=#__codelineno-0-385>385</a></span>
<span class=normal><a href=#__codelineno-0-386>386</a></span>
<span class=normal><a href=#__codelineno-0-387>387</a></span>
<span class=normal><a href=#__codelineno-0-388>388</a></span>
<span class=normal><a href=#__codelineno-0-389>389</a></span>
<span class=normal><a href=#__codelineno-0-390>390</a></span>
<span class=normal><a href=#__codelineno-0-391>391</a></span>
<span class=normal><a href=#__codelineno-0-392>392</a></span>
<span class=normal><a href=#__codelineno-0-393>393</a></span>
<span class=normal><a href=#__codelineno-0-394>394</a></span>
<span class=normal><a href=#__codelineno-0-395>395</a></span>
<span class=normal><a href=#__codelineno-0-396>396</a></span>
<span class=normal><a href=#__codelineno-0-397>397</a></span>
<span class=normal><a href=#__codelineno-0-398>398</a></span>
<span class=normal><a href=#__codelineno-0-399>399</a></span>
<span class=normal><a href=#__codelineno-0-400>400</a></span>
<span class=normal><a href=#__codelineno-0-401>401</a></span>
<span class=normal><a href=#__codelineno-0-402>402</a></span>
<span class=normal><a href=#__codelineno-0-403>403</a></span>
<span class=normal><a href=#__codelineno-0-404>404</a></span>
<span class=normal><a href=#__codelineno-0-405>405</a></span>
<span class=normal><a href=#__codelineno-0-406>406</a></span>
<span class=normal><a href=#__codelineno-0-407>407</a></span>
<span class=normal><a href=#__codelineno-0-408>408</a></span>
<span class=normal><a href=#__codelineno-0-409>409</a></span>
<span class=normal><a href=#__codelineno-0-410>410</a></span>
<span class=normal><a href=#__codelineno-0-411>411</a></span>
<span class=normal><a href=#__codelineno-0-412>412</a></span>
<span class=normal><a href=#__codelineno-0-413>413</a></span>
<span class=normal><a href=#__codelineno-0-414>414</a></span>
<span class=normal><a href=#__codelineno-0-415>415</a></span>
<span class=normal><a href=#__codelineno-0-416>416</a></span>
<span class=normal><a href=#__codelineno-0-417>417</a></span>
<span class=normal><a href=#__codelineno-0-418>418</a></span>
<span class=normal><a href=#__codelineno-0-419>419</a></span>
<span class=normal><a href=#__codelineno-0-420>420</a></span>
<span class=normal><a href=#__codelineno-0-421>421</a></span>
<span class=normal><a href=#__codelineno-0-422>422</a></span>
<span class=normal><a href=#__codelineno-0-423>423</a></span>
<span class=normal><a href=#__codelineno-0-424>424</a></span>
<span class=normal><a href=#__codelineno-0-425>425</a></span>
<span class=normal><a href=#__codelineno-0-426>426</a></span>
<span class=normal><a href=#__codelineno-0-427>427</a></span>
<span class=normal><a href=#__codelineno-0-428>428</a></span>
<span class=normal><a href=#__codelineno-0-429>429</a></span>
<span class=normal><a href=#__codelineno-0-430>430</a></span>
<span class=normal><a href=#__codelineno-0-431>431</a></span>
<span class=normal><a href=#__codelineno-0-432>432</a></span>
<span class=normal><a href=#__codelineno-0-433>433</a></span>
<span class=normal><a href=#__codelineno-0-434>434</a></span>
<span class=normal><a href=#__codelineno-0-435>435</a></span>
<span class=normal><a href=#__codelineno-0-436>436</a></span>
<span class=normal><a href=#__codelineno-0-437>437</a></span>
<span class=normal><a href=#__codelineno-0-438>438</a></span>
<span class=normal><a href=#__codelineno-0-439>439</a></span>
<span class=normal><a href=#__codelineno-0-440>440</a></span>
<span class=normal><a href=#__codelineno-0-441>441</a></span>
<span class=normal><a href=#__codelineno-0-442>442</a></span>
<span class=normal><a href=#__codelineno-0-443>443</a></span>
<span class=normal><a href=#__codelineno-0-444>444</a></span>
<span class=normal><a href=#__codelineno-0-445>445</a></span>
<span class=normal><a href=#__codelineno-0-446>446</a></span>
<span class=normal><a href=#__codelineno-0-447>447</a></span>
<span class=normal><a href=#__codelineno-0-448>448</a></span>
<span class=normal><a href=#__codelineno-0-449>449</a></span>
<span class=normal><a href=#__codelineno-0-450>450</a></span>
<span class=normal><a href=#__codelineno-0-451>451</a></span>
<span class=normal><a href=#__codelineno-0-452>452</a></span>
<span class=normal><a href=#__codelineno-0-453>453</a></span>
<span class=normal><a href=#__codelineno-0-454>454</a></span>
<span class=normal><a href=#__codelineno-0-455>455</a></span>
<span class=normal><a href=#__codelineno-0-456>456</a></span>
<span class=normal><a href=#__codelineno-0-457>457</a></span>
<span class=normal><a href=#__codelineno-0-458>458</a></span>
<span class=normal><a href=#__codelineno-0-459>459</a></span>
<span class=normal><a href=#__codelineno-0-460>460</a></span>
<span class=normal><a href=#__codelineno-0-461>461</a></span>
<span class=normal><a href=#__codelineno-0-462>462</a></span>
<span class=normal><a href=#__codelineno-0-463>463</a></span>
<span class=normal><a href=#__codelineno-0-464>464</a></span>
<span class=normal><a href=#__codelineno-0-465>465</a></span>
<span class=normal><a href=#__codelineno-0-466>466</a></span>
<span class=normal><a href=#__codelineno-0-467>467</a></span>
<span class=normal><a href=#__codelineno-0-468>468</a></span>
<span class=normal><a href=#__codelineno-0-469>469</a></span>
<span class=normal><a href=#__codelineno-0-470>470</a></span>
<span class=normal><a href=#__codelineno-0-471>471</a></span>
<span class=normal><a href=#__codelineno-0-472>472</a></span>
<span class=normal><a href=#__codelineno-0-473>473</a></span>
<span class=normal><a href=#__codelineno-0-474>474</a></span>
<span class=normal><a href=#__codelineno-0-475>475</a></span>
<span class=normal><a href=#__codelineno-0-476>476</a></span>
<span class=normal><a href=#__codelineno-0-477>477</a></span>
<span class=normal><a href=#__codelineno-0-478>478</a></span>
<span class=normal><a href=#__codelineno-0-479>479</a></span>
<span class=normal><a href=#__codelineno-0-480>480</a></span>
<span class=normal><a href=#__codelineno-0-481>481</a></span>
<span class=normal><a href=#__codelineno-0-482>482</a></span>
<span class=normal><a href=#__codelineno-0-483>483</a></span>
<span class=normal><a href=#__codelineno-0-484>484</a></span>
<span class=normal><a href=#__codelineno-0-485>485</a></span>
<span class=normal><a href=#__codelineno-0-486>486</a></span>
<span class=normal><a href=#__codelineno-0-487>487</a></span>
<span class=normal><a href=#__codelineno-0-488>488</a></span>
<span class=normal><a href=#__codelineno-0-489>489</a></span>
<span class=normal><a href=#__codelineno-0-490>490</a></span>
<span class=normal><a href=#__codelineno-0-491>491</a></span>
<span class=normal><a href=#__codelineno-0-492>492</a></span>
<span class=normal><a href=#__codelineno-0-493>493</a></span>
<span class=normal><a href=#__codelineno-0-494>494</a></span>
<span class=normal><a href=#__codelineno-0-495>495</a></span>
<span class=normal><a href=#__codelineno-0-496>496</a></span>
<span class=normal><a href=#__codelineno-0-497>497</a></span>
<span class=normal><a href=#__codelineno-0-498>498</a></span>
<span class=normal><a href=#__codelineno-0-499>499</a></span>
<span class=normal><a href=#__codelineno-0-500>500</a></span>
<span class=normal><a href=#__codelineno-0-501>501</a></span>
<span class=normal><a href=#__codelineno-0-502>502</a></span>
<span class=normal><a href=#__codelineno-0-503>503</a></span>
<span class=normal><a href=#__codelineno-0-504>504</a></span>
<span class=normal><a href=#__codelineno-0-505>505</a></span>
<span class=normal><a href=#__codelineno-0-506>506</a></span>
<span class=normal><a href=#__codelineno-0-507>507</a></span>
<span class=normal><a href=#__codelineno-0-508>508</a></span>
<span class=normal><a href=#__codelineno-0-509>509</a></span>
<span class=normal><a href=#__codelineno-0-510>510</a></span>
<span class=normal><a href=#__codelineno-0-511>511</a></span>
<span class=normal><a href=#__codelineno-0-512>512</a></span>
<span class=normal><a href=#__codelineno-0-513>513</a></span>
<span class=normal><a href=#__codelineno-0-514>514</a></span>
<span class=normal><a href=#__codelineno-0-515>515</a></span>
<span class=normal><a href=#__codelineno-0-516>516</a></span>
<span class=normal><a href=#__codelineno-0-517>517</a></span>
<span class=normal><a href=#__codelineno-0-518>518</a></span>
<span class=normal><a href=#__codelineno-0-519>519</a></span>
<span class=normal><a href=#__codelineno-0-520>520</a></span>
<span class=normal><a href=#__codelineno-0-521>521</a></span>
<span class=normal><a href=#__codelineno-0-522>522</a></span>
<span class=normal><a href=#__codelineno-0-523>523</a></span>
<span class=normal><a href=#__codelineno-0-524>524</a></span>
<span class=normal><a href=#__codelineno-0-525>525</a></span>
<span class=normal><a href=#__codelineno-0-526>526</a></span>
<span class=normal><a href=#__codelineno-0-527>527</a></span>
<span class=normal><a href=#__codelineno-0-528>528</a></span>
<span class=normal><a href=#__codelineno-0-529>529</a></span>
<span class=normal><a href=#__codelineno-0-530>530</a></span>
<span class=normal><a href=#__codelineno-0-531>531</a></span>
<span class=normal><a href=#__codelineno-0-532>532</a></span>
<span class=normal><a href=#__codelineno-0-533>533</a></span>
<span class=normal><a href=#__codelineno-0-534>534</a></span>
<span class=normal><a href=#__codelineno-0-535>535</a></span>
<span class=normal><a href=#__codelineno-0-536>536</a></span>
<span class=normal><a href=#__codelineno-0-537>537</a></span>
<span class=normal><a href=#__codelineno-0-538>538</a></span>
<span class=normal><a href=#__codelineno-0-539>539</a></span>
<span class=normal><a href=#__codelineno-0-540>540</a></span>
<span class=normal><a href=#__codelineno-0-541>541</a></span>
<span class=normal><a href=#__codelineno-0-542>542</a></span>
<span class=normal><a href=#__codelineno-0-543>543</a></span>
<span class=normal><a href=#__codelineno-0-544>544</a></span>
<span class=normal><a href=#__codelineno-0-545>545</a></span>
<span class=normal><a href=#__codelineno-0-546>546</a></span>
<span class=normal><a href=#__codelineno-0-547>547</a></span>
<span class=normal><a href=#__codelineno-0-548>548</a></span>
<span class=normal><a href=#__codelineno-0-549>549</a></span>
<span class=normal><a href=#__codelineno-0-550>550</a></span>
<span class=normal><a href=#__codelineno-0-551>551</a></span>
<span class=normal><a href=#__codelineno-0-552>552</a></span>
<span class=normal><a href=#__codelineno-0-553>553</a></span>
<span class=normal><a href=#__codelineno-0-554>554</a></span>
<span class=normal><a href=#__codelineno-0-555>555</a></span>
<span class=normal><a href=#__codelineno-0-556>556</a></span>
<span class=normal><a href=#__codelineno-0-557>557</a></span>
<span class=normal><a href=#__codelineno-0-558>558</a></span>
<span class=normal><a href=#__codelineno-0-559>559</a></span>
<span class=normal><a href=#__codelineno-0-560>560</a></span>
<span class=normal><a href=#__codelineno-0-561>561</a></span>
<span class=normal><a href=#__codelineno-0-562>562</a></span>
<span class=normal><a href=#__codelineno-0-563>563</a></span>
<span class=normal><a href=#__codelineno-0-564>564</a></span>
<span class=normal><a href=#__codelineno-0-565>565</a></span>
<span class=normal><a href=#__codelineno-0-566>566</a></span>
<span class=normal><a href=#__codelineno-0-567>567</a></span>
<span class=normal><a href=#__codelineno-0-568>568</a></span>
<span class=normal><a href=#__codelineno-0-569>569</a></span>
<span class=normal><a href=#__codelineno-0-570>570</a></span>
<span class=normal><a href=#__codelineno-0-571>571</a></span>
<span class=normal><a href=#__codelineno-0-572>572</a></span>
<span class=normal><a href=#__codelineno-0-573>573</a></span>
<span class=normal><a href=#__codelineno-0-574>574</a></span>
<span class=normal><a href=#__codelineno-0-575>575</a></span>
<span class=normal><a href=#__codelineno-0-576>576</a></span>
<span class=normal><a href=#__codelineno-0-577>577</a></span>
<span class=normal><a href=#__codelineno-0-578>578</a></span>
<span class=normal><a href=#__codelineno-0-579>579</a></span>
<span class=normal><a href=#__codelineno-0-580>580</a></span>
<span class=normal><a href=#__codelineno-0-581>581</a></span>
<span class=normal><a href=#__codelineno-0-582>582</a></span>
<span class=normal><a href=#__codelineno-0-583>583</a></span>
<span class=normal><a href=#__codelineno-0-584>584</a></span>
<span class=normal><a href=#__codelineno-0-585>585</a></span>
<span class=normal><a href=#__codelineno-0-586>586</a></span>
<span class=normal><a href=#__codelineno-0-587>587</a></span>
<span class=normal><a href=#__codelineno-0-588>588</a></span>
<span class=normal><a href=#__codelineno-0-589>589</a></span>
<span class=normal><a href=#__codelineno-0-590>590</a></span>
<span class=normal><a href=#__codelineno-0-591>591</a></span>
<span class=normal><a href=#__codelineno-0-592>592</a></span>
<span class=normal><a href=#__codelineno-0-593>593</a></span>
<span class=normal><a href=#__codelineno-0-594>594</a></span>
<span class=normal><a href=#__codelineno-0-595>595</a></span>
<span class=normal><a href=#__codelineno-0-596>596</a></span>
<span class=normal><a href=#__codelineno-0-597>597</a></span>
<span class=normal><a href=#__codelineno-0-598>598</a></span>
<span class=normal><a href=#__codelineno-0-599>599</a></span>
<span class=normal><a href=#__codelineno-0-600>600</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-367><a id=__codelineno-0-367 name=__codelineno-0-367></a><span class=k>class</span> <span class=nc>BottomUpLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-368><a id=__codelineno-0-368 name=__codelineno-0-368></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-369><a id=__codelineno-0-369 name=__codelineno-0-369></a><span class=sd>    Bottom-up deterministic layer.</span>
</span><span id=__span-0-370><a id=__codelineno-0-370 name=__codelineno-0-370></a><span class=sd>    It consists of one or a stack of `BottomUpDeterministicResBlock`&#39;s.</span>
</span><span id=__span-0-371><a id=__codelineno-0-371 name=__codelineno-0-371></a><span class=sd>    The outputs are the so-called `bu_values` that are later used in the Decoder to update the</span>
</span><span id=__span-0-372><a id=__codelineno-0-372 name=__codelineno-0-372></a><span class=sd>    generative distributions.</span>
</span><span id=__span-0-373><a id=__codelineno-0-373 name=__codelineno-0-373></a>
</span><span id=__span-0-374><a id=__codelineno-0-374 name=__codelineno-0-374></a><span class=sd>    NOTE: When Lateral Contextualization is Enabled (i.e., `enable_multiscale=True`),</span>
</span><span id=__span-0-375><a id=__codelineno-0-375 name=__codelineno-0-375></a><span class=sd>    the low-res lateral input is first fed through a BottomUpDeterministicBlock (BUDB)</span>
</span><span id=__span-0-376><a id=__codelineno-0-376 name=__codelineno-0-376></a><span class=sd>    (without downsampling), and then merged to the latent tensor produced by the primary flow</span>
</span><span id=__span-0-377><a id=__codelineno-0-377 name=__codelineno-0-377></a><span class=sd>    of the `BottomUpLayer` through the `MergeLowRes` layer. It is meaningful to remark that</span>
</span><span id=__span-0-378><a id=__codelineno-0-378 name=__codelineno-0-378></a><span class=sd>    the BUDB that takes care of encoding the low-res input can be either shared with the</span>
</span><span id=__span-0-379><a id=__codelineno-0-379 name=__codelineno-0-379></a><span class=sd>    primary flow (and in that case it is the &quot;same_size&quot; BUDB (or stack of BUDBs) -&gt; see `self.net`),</span>
</span><span id=__span-0-380><a id=__codelineno-0-380 name=__codelineno-0-380></a><span class=sd>    or can be a deep-copy of the primary flow&#39;s BUDB.</span>
</span><span id=__span-0-381><a id=__codelineno-0-381 name=__codelineno-0-381></a><span class=sd>    This behaviour is controlled by `lowres_separate_branch` parameter.</span>
</span><span id=__span-0-382><a id=__codelineno-0-382 name=__codelineno-0-382></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-383><a id=__codelineno-0-383 name=__codelineno-0-383></a>
</span><span id=__span-0-384><a id=__codelineno-0-384 name=__codelineno-0-384></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-385><a id=__codelineno-0-385 name=__codelineno-0-385></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-386><a id=__codelineno-0-386 name=__codelineno-0-386></a>        <span class=n>n_res_blocks</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-387><a id=__codelineno-0-387 name=__codelineno-0-387></a>        <span class=n>n_filters</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-388><a id=__codelineno-0-388 name=__codelineno-0-388></a>        <span class=n>downsampling_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span><span id=__span-0-389><a id=__codelineno-0-389 name=__codelineno-0-389></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-390><a id=__codelineno-0-390 name=__codelineno-0-390></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-391><a id=__codelineno-0-391 name=__codelineno-0-391></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-392><a id=__codelineno-0-392 name=__codelineno-0-392></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-393><a id=__codelineno-0-393 name=__codelineno-0-393></a>        <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-394><a id=__codelineno-0-394 name=__codelineno-0-394></a>        <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-395><a id=__codelineno-0-395 name=__codelineno-0-395></a>        <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-396><a id=__codelineno-0-396 name=__codelineno-0-396></a>        <span class=n>enable_multiscale</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-397><a id=__codelineno-0-397 name=__codelineno-0-397></a>        <span class=n>multiscale_lowres_size_factor</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-398><a id=__codelineno-0-398 name=__codelineno-0-398></a>        <span class=n>lowres_separate_branch</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-399><a id=__codelineno-0-399 name=__codelineno-0-399></a>        <span class=n>multiscale_retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-400><a id=__codelineno-0-400 name=__codelineno-0-400></a>        <span class=n>decoder_retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-401><a id=__codelineno-0-401 name=__codelineno-0-401></a>        <span class=n>output_expected_shape</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-402><a id=__codelineno-0-402 name=__codelineno-0-402></a>    <span class=p>):</span>
</span><span id=__span-0-403><a id=__codelineno-0-403 name=__codelineno-0-403></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-404><a id=__codelineno-0-404 name=__codelineno-0-404></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-405><a id=__codelineno-0-405 name=__codelineno-0-405></a>
</span><span id=__span-0-406><a id=__codelineno-0-406 name=__codelineno-0-406></a><span class=sd>        Parameters</span>
</span><span id=__span-0-407><a id=__codelineno-0-407 name=__codelineno-0-407></a><span class=sd>        ----------</span>
</span><span id=__span-0-408><a id=__codelineno-0-408 name=__codelineno-0-408></a><span class=sd>        n_res_blocks: int</span>
</span><span id=__span-0-409><a id=__codelineno-0-409 name=__codelineno-0-409></a><span class=sd>            Number of `BottomUpDeterministicResBlock` modules stacked in this layer.</span>
</span><span id=__span-0-410><a id=__codelineno-0-410 name=__codelineno-0-410></a><span class=sd>        n_filters: int</span>
</span><span id=__span-0-411><a id=__codelineno-0-411 name=__codelineno-0-411></a><span class=sd>            Number of channels present through out the layers of this block.</span>
</span><span id=__span-0-412><a id=__codelineno-0-412 name=__codelineno-0-412></a><span class=sd>        downsampling_steps: int, optional</span>
</span><span id=__span-0-413><a id=__codelineno-0-413 name=__codelineno-0-413></a><span class=sd>            Number of downsampling steps that has to be done in this layer (typically 1).</span>
</span><span id=__span-0-414><a id=__codelineno-0-414 name=__codelineno-0-414></a><span class=sd>            Default is 0.</span>
</span><span id=__span-0-415><a id=__codelineno-0-415 name=__codelineno-0-415></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-416><a id=__codelineno-0-416 name=__codelineno-0-416></a><span class=sd>            The non-linearity function used in the block. Default is `None`.</span>
</span><span id=__span-0-417><a id=__codelineno-0-417 name=__codelineno-0-417></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-418><a id=__codelineno-0-418 name=__codelineno-0-418></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-419><a id=__codelineno-0-419 name=__codelineno-0-419></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-420><a id=__codelineno-0-420 name=__codelineno-0-420></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-421><a id=__codelineno-0-421 name=__codelineno-0-421></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-422><a id=__codelineno-0-422 name=__codelineno-0-422></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-423><a id=__codelineno-0-423 name=__codelineno-0-423></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-424><a id=__codelineno-0-424 name=__codelineno-0-424></a><span class=sd>            Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-425><a id=__codelineno-0-425 name=__codelineno-0-425></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-426><a id=__codelineno-0-426 name=__codelineno-0-426></a><span class=sd>        res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-427><a id=__codelineno-0-427 name=__codelineno-0-427></a><span class=sd>            The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-428><a id=__codelineno-0-428 name=__codelineno-0-428></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-429><a id=__codelineno-0-429 name=__codelineno-0-429></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-430><a id=__codelineno-0-430 name=__codelineno-0-430></a><span class=sd>        res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-431><a id=__codelineno-0-431 name=__codelineno-0-431></a><span class=sd>            Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-432><a id=__codelineno-0-432 name=__codelineno-0-432></a><span class=sd>        gated: bool, optional</span>
</span><span id=__span-0-433><a id=__codelineno-0-433 name=__codelineno-0-433></a><span class=sd>            Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-434><a id=__codelineno-0-434 name=__codelineno-0-434></a><span class=sd>        enable_multiscale: bool, optional</span>
</span><span id=__span-0-435><a id=__codelineno-0-435 name=__codelineno-0-435></a><span class=sd>            Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.</span>
</span><span id=__span-0-436><a id=__codelineno-0-436 name=__codelineno-0-436></a><span class=sd>        multiscale_lowres_size_factor: int, optional</span>
</span><span id=__span-0-437><a id=__codelineno-0-437 name=__codelineno-0-437></a><span class=sd>            A factor the expresses the relative size of the primary flow tensor with respect to the</span>
</span><span id=__span-0-438><a id=__codelineno-0-438 name=__codelineno-0-438></a><span class=sd>            lower-resolution lateral input tensor. Default in `None`.</span>
</span><span id=__span-0-439><a id=__codelineno-0-439 name=__codelineno-0-439></a><span class=sd>        lowres_separate_branch: bool, optional</span>
</span><span id=__span-0-440><a id=__codelineno-0-440 name=__codelineno-0-440></a><span class=sd>            Whether the residual block(s) encoding the low-res input should be shared (`False`) or</span>
</span><span id=__span-0-441><a id=__codelineno-0-441 name=__codelineno-0-441></a><span class=sd>            not (`True`) with the primary flow &quot;same-size&quot; residual block(s). Default is `False`.</span>
</span><span id=__span-0-442><a id=__codelineno-0-442 name=__codelineno-0-442></a><span class=sd>        multiscale_retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-443><a id=__codelineno-0-443 name=__codelineno-0-443></a><span class=sd>            Whether to pad the latent tensor resulting from the bottom-up layer&#39;s primary flow</span>
</span><span id=__span-0-444><a id=__codelineno-0-444 name=__codelineno-0-444></a><span class=sd>            to match the size of the low-res input. Default is `False`.</span>
</span><span id=__span-0-445><a id=__codelineno-0-445 name=__codelineno-0-445></a><span class=sd>        decoder_retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-446><a id=__codelineno-0-446 name=__codelineno-0-446></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-447><a id=__codelineno-0-447 name=__codelineno-0-447></a><span class=sd>        output_expected_shape: Iterable[int], optional</span>
</span><span id=__span-0-448><a id=__codelineno-0-448 name=__codelineno-0-448></a><span class=sd>            The expected shape of the layer output (only used if `enable_multiscale == True`).</span>
</span><span id=__span-0-449><a id=__codelineno-0-449 name=__codelineno-0-449></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-450><a id=__codelineno-0-450 name=__codelineno-0-450></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-451><a id=__codelineno-0-451 name=__codelineno-0-451></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-452><a id=__codelineno-0-452 name=__codelineno-0-452></a>
</span><span id=__span-0-453><a id=__codelineno-0-453 name=__codelineno-0-453></a>        <span class=c1># Define attributes for Lateral Contextualization</span>
</span><span id=__span-0-454><a id=__codelineno-0-454 name=__codelineno-0-454></a>        <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=o>=</span> <span class=n>enable_multiscale</span>
</span><span id=__span-0-455><a id=__codelineno-0-455 name=__codelineno-0-455></a>        <span class=bp>self</span><span class=o>.</span><span class=n>lowres_separate_branch</span> <span class=o>=</span> <span class=n>lowres_separate_branch</span>
</span><span id=__span-0-456><a id=__codelineno-0-456 name=__codelineno-0-456></a>        <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_retain_spatial_dims</span> <span class=o>=</span> <span class=n>multiscale_retain_spatial_dims</span>
</span><span id=__span-0-457><a id=__codelineno-0-457 name=__codelineno-0-457></a>        <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span> <span class=o>=</span> <span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-458><a id=__codelineno-0-458 name=__codelineno-0-458></a>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_retain_spatial_dims</span> <span class=o>=</span> <span class=n>decoder_retain_spatial_dims</span>
</span><span id=__span-0-459><a id=__codelineno-0-459 name=__codelineno-0-459></a>        <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=o>=</span> <span class=n>output_expected_shape</span>
</span><span id=__span-0-460><a id=__codelineno-0-460 name=__codelineno-0-460></a>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-461><a id=__codelineno-0-461 name=__codelineno-0-461></a>
</span><span id=__span-0-462><a id=__codelineno-0-462 name=__codelineno-0-462></a>        <span class=n>bu_blocks_downsized</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-463><a id=__codelineno-0-463 name=__codelineno-0-463></a>        <span class=n>bu_blocks_samesize</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-464><a id=__codelineno-0-464 name=__codelineno-0-464></a>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_res_blocks</span><span class=p>):</span>
</span><span id=__span-0-465><a id=__codelineno-0-465 name=__codelineno-0-465></a>            <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-466><a id=__codelineno-0-466 name=__codelineno-0-466></a>            <span class=k>if</span> <span class=n>downsampling_steps</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-467><a id=__codelineno-0-467 name=__codelineno-0-467></a>                <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>True</span>
</span><span id=__span-0-468><a id=__codelineno-0-468 name=__codelineno-0-468></a>                <span class=n>downsampling_steps</span> <span class=o>-=</span> <span class=mi>1</span>
</span><span id=__span-0-469><a id=__codelineno-0-469 name=__codelineno-0-469></a>            <span class=n>block</span> <span class=o>=</span> <span class=n>BottomUpDeterministicResBlock</span><span class=p>(</span>
</span><span id=__span-0-470><a id=__codelineno-0-470 name=__codelineno-0-470></a>                <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-471><a id=__codelineno-0-471 name=__codelineno-0-471></a>                <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-472><a id=__codelineno-0-472 name=__codelineno-0-472></a>                <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-473><a id=__codelineno-0-473 name=__codelineno-0-473></a>                <span class=n>downsample</span><span class=o>=</span><span class=n>do_resample</span><span class=p>,</span>
</span><span id=__span-0-474><a id=__codelineno-0-474 name=__codelineno-0-474></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-475><a id=__codelineno-0-475 name=__codelineno-0-475></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-476><a id=__codelineno-0-476 name=__codelineno-0-476></a>                <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-477><a id=__codelineno-0-477 name=__codelineno-0-477></a>                <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-478><a id=__codelineno-0-478 name=__codelineno-0-478></a>                <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-479><a id=__codelineno-0-479 name=__codelineno-0-479></a>                <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-480><a id=__codelineno-0-480 name=__codelineno-0-480></a>            <span class=p>)</span>
</span><span id=__span-0-481><a id=__codelineno-0-481 name=__codelineno-0-481></a>            <span class=k>if</span> <span class=n>do_resample</span><span class=p>:</span>
</span><span id=__span-0-482><a id=__codelineno-0-482 name=__codelineno-0-482></a>                <span class=n>bu_blocks_downsized</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>block</span><span class=p>)</span>
</span><span id=__span-0-483><a id=__codelineno-0-483 name=__codelineno-0-483></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-484><a id=__codelineno-0-484 name=__codelineno-0-484></a>                <span class=n>bu_blocks_samesize</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>block</span><span class=p>)</span>
</span><span id=__span-0-485><a id=__codelineno-0-485 name=__codelineno-0-485></a>
</span><span id=__span-0-486><a id=__codelineno-0-486 name=__codelineno-0-486></a>        <span class=bp>self</span><span class=o>.</span><span class=n>net_downsized</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>bu_blocks_downsized</span><span class=p>)</span>
</span><span id=__span-0-487><a id=__codelineno-0-487 name=__codelineno-0-487></a>        <span class=bp>self</span><span class=o>.</span><span class=n>net</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>bu_blocks_samesize</span><span class=p>)</span>
</span><span id=__span-0-488><a id=__codelineno-0-488 name=__codelineno-0-488></a>
</span><span id=__span-0-489><a id=__codelineno-0-489 name=__codelineno-0-489></a>        <span class=c1># Using the same net for the low resolution (and larger sized image)</span>
</span><span id=__span-0-490><a id=__codelineno-0-490 name=__codelineno-0-490></a>        <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_merge</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-491><a id=__codelineno-0-491 name=__codelineno-0-491></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span><span class=p>:</span>
</span><span id=__span-0-492><a id=__codelineno-0-492 name=__codelineno-0-492></a>            <span class=bp>self</span><span class=o>.</span><span class=n>_init_multiscale</span><span class=p>(</span>
</span><span id=__span-0-493><a id=__codelineno-0-493 name=__codelineno-0-493></a>                <span class=n>n_filters</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-494><a id=__codelineno-0-494 name=__codelineno-0-494></a>                <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-495><a id=__codelineno-0-495 name=__codelineno-0-495></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-496><a id=__codelineno-0-496 name=__codelineno-0-496></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-497><a id=__codelineno-0-497 name=__codelineno-0-497></a>                <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-498><a id=__codelineno-0-498 name=__codelineno-0-498></a>            <span class=p>)</span>
</span><span id=__span-0-499><a id=__codelineno-0-499 name=__codelineno-0-499></a>
</span><span id=__span-0-500><a id=__codelineno-0-500 name=__codelineno-0-500></a>        <span class=c1># msg = f&#39;[{self.__class__.__name__}] McEnabled:{int(enable_multiscale)} &#39;</span>
</span><span id=__span-0-501><a id=__codelineno-0-501 name=__codelineno-0-501></a>        <span class=c1># if enable_multiscale:</span>
</span><span id=__span-0-502><a id=__codelineno-0-502 name=__codelineno-0-502></a>        <span class=c1>#     msg += f&#39;McParallelBeam:{int(multiscale_retain_spatial_dims)} McFactor{multiscale_lowres_size_factor}&#39;</span>
</span><span id=__span-0-503><a id=__codelineno-0-503 name=__codelineno-0-503></a>        <span class=c1># print(msg)</span>
</span><span id=__span-0-504><a id=__codelineno-0-504 name=__codelineno-0-504></a>
</span><span id=__span-0-505><a id=__codelineno-0-505 name=__codelineno-0-505></a>    <span class=k>def</span> <span class=nf>_init_multiscale</span><span class=p>(</span>
</span><span id=__span-0-506><a id=__codelineno-0-506 name=__codelineno-0-506></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-507><a id=__codelineno-0-507 name=__codelineno-0-507></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-508><a id=__codelineno-0-508 name=__codelineno-0-508></a>        <span class=n>n_filters</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-509><a id=__codelineno-0-509 name=__codelineno-0-509></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-510><a id=__codelineno-0-510 name=__codelineno-0-510></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-511><a id=__codelineno-0-511 name=__codelineno-0-511></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-512><a id=__codelineno-0-512 name=__codelineno-0-512></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-513><a id=__codelineno-0-513 name=__codelineno-0-513></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-514><a id=__codelineno-0-514 name=__codelineno-0-514></a><span class=sd>        This method defines the modules responsible of merging compressed lateral inputs to the outputs</span>
</span><span id=__span-0-515><a id=__codelineno-0-515 name=__codelineno-0-515></a><span class=sd>        of the primary flow at different hierarchical levels in the multiresolution approach (LC).</span>
</span><span id=__span-0-516><a id=__codelineno-0-516 name=__codelineno-0-516></a>
</span><span id=__span-0-517><a id=__codelineno-0-517 name=__codelineno-0-517></a><span class=sd>        Specifically, the method initializes `lowres_net`, which is a stack of `BottomUpDeterministicBlock`&#39;s</span>
</span><span id=__span-0-518><a id=__codelineno-0-518 name=__codelineno-0-518></a><span class=sd>        (w/out downsampling) that takes care of additionally processing the low-res input, and `lowres_merge`,</span>
</span><span id=__span-0-519><a id=__codelineno-0-519 name=__codelineno-0-519></a><span class=sd>        which is the module responsible of merging the compressed lateral input to the main flow.</span>
</span><span id=__span-0-520><a id=__codelineno-0-520 name=__codelineno-0-520></a>
</span><span id=__span-0-521><a id=__codelineno-0-521 name=__codelineno-0-521></a><span class=sd>        NOTE: The merge modality is set by default to &quot;residual&quot;, meaning that the merge layer</span>
</span><span id=__span-0-522><a id=__codelineno-0-522 name=__codelineno-0-522></a><span class=sd>        performs concatenation on dim=1, followed by 1x1 convolution and a Residual Gated block.</span>
</span><span id=__span-0-523><a id=__codelineno-0-523 name=__codelineno-0-523></a>
</span><span id=__span-0-524><a id=__codelineno-0-524 name=__codelineno-0-524></a><span class=sd>        Parameters</span>
</span><span id=__span-0-525><a id=__codelineno-0-525 name=__codelineno-0-525></a><span class=sd>        ----------</span>
</span><span id=__span-0-526><a id=__codelineno-0-526 name=__codelineno-0-526></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-527><a id=__codelineno-0-527 name=__codelineno-0-527></a><span class=sd>            The non-linearity function used in the block. Default is `None`.</span>
</span><span id=__span-0-528><a id=__codelineno-0-528 name=__codelineno-0-528></a><span class=sd>        n_filters: int</span>
</span><span id=__span-0-529><a id=__codelineno-0-529 name=__codelineno-0-529></a><span class=sd>            Number of channels present through out the layers of this block.</span>
</span><span id=__span-0-530><a id=__codelineno-0-530 name=__codelineno-0-530></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-531><a id=__codelineno-0-531 name=__codelineno-0-531></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-532><a id=__codelineno-0-532 name=__codelineno-0-532></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-533><a id=__codelineno-0-533 name=__codelineno-0-533></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-534><a id=__codelineno-0-534 name=__codelineno-0-534></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-535><a id=__codelineno-0-535 name=__codelineno-0-535></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-536><a id=__codelineno-0-536 name=__codelineno-0-536></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-537><a id=__codelineno-0-537 name=__codelineno-0-537></a><span class=sd>            Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-538><a id=__codelineno-0-538 name=__codelineno-0-538></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-539><a id=__codelineno-0-539 name=__codelineno-0-539></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-540><a id=__codelineno-0-540 name=__codelineno-0-540></a>        <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net</span>
</span><span id=__span-0-541><a id=__codelineno-0-541 name=__codelineno-0-541></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_separate_branch</span><span class=p>:</span>
</span><span id=__span-0-542><a id=__codelineno-0-542 name=__codelineno-0-542></a>            <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span> <span class=o>=</span> <span class=n>deepcopy</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>net</span><span class=p>)</span>
</span><span id=__span-0-543><a id=__codelineno-0-543 name=__codelineno-0-543></a>
</span><span id=__span-0-544><a id=__codelineno-0-544 name=__codelineno-0-544></a>        <span class=bp>self</span><span class=o>.</span><span class=n>lowres_merge</span> <span class=o>=</span> <span class=n>MergeLowRes</span><span class=p>(</span>
</span><span id=__span-0-545><a id=__codelineno-0-545 name=__codelineno-0-545></a>            <span class=n>channels</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-546><a id=__codelineno-0-546 name=__codelineno-0-546></a>            <span class=n>merge_type</span><span class=o>=</span><span class=s2>&quot;residual&quot;</span><span class=p>,</span>
</span><span id=__span-0-547><a id=__codelineno-0-547 name=__codelineno-0-547></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-548><a id=__codelineno-0-548 name=__codelineno-0-548></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-549><a id=__codelineno-0-549 name=__codelineno-0-549></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-550><a id=__codelineno-0-550 name=__codelineno-0-550></a>            <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-551><a id=__codelineno-0-551 name=__codelineno-0-551></a>            <span class=n>multiscale_retain_spatial_dims</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>multiscale_retain_spatial_dims</span><span class=p>,</span>
</span><span id=__span-0-552><a id=__codelineno-0-552 name=__codelineno-0-552></a>            <span class=n>multiscale_lowres_size_factor</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span><span class=p>,</span>
</span><span id=__span-0-553><a id=__codelineno-0-553 name=__codelineno-0-553></a>        <span class=p>)</span>
</span><span id=__span-0-554><a id=__codelineno-0-554 name=__codelineno-0-554></a>
</span><span id=__span-0-555><a id=__codelineno-0-555 name=__codelineno-0-555></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-556><a id=__codelineno-0-556 name=__codelineno-0-556></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>lowres_x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-557><a id=__codelineno-0-557 name=__codelineno-0-557></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-558><a id=__codelineno-0-558 name=__codelineno-0-558></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-559><a id=__codelineno-0-559 name=__codelineno-0-559></a><span class=sd>        Parameters</span>
</span><span id=__span-0-560><a id=__codelineno-0-560 name=__codelineno-0-560></a><span class=sd>        ----------</span>
</span><span id=__span-0-561><a id=__codelineno-0-561 name=__codelineno-0-561></a><span class=sd>        x: torch.Tensor</span>
</span><span id=__span-0-562><a id=__codelineno-0-562 name=__codelineno-0-562></a><span class=sd>            The input of the `BottomUpLayer`, i.e., the input image or the output of the</span>
</span><span id=__span-0-563><a id=__codelineno-0-563 name=__codelineno-0-563></a><span class=sd>            previous layer.</span>
</span><span id=__span-0-564><a id=__codelineno-0-564 name=__codelineno-0-564></a><span class=sd>        lowres_x: torch.Tensor, optional</span>
</span><span id=__span-0-565><a id=__codelineno-0-565 name=__codelineno-0-565></a><span class=sd>            The low-res input used for Lateral Contextualization (LC). Default is `None`.</span>
</span><span id=__span-0-566><a id=__codelineno-0-566 name=__codelineno-0-566></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-567><a id=__codelineno-0-567 name=__codelineno-0-567></a>        <span class=c1># The input is fed through the residual downsampling block(s)</span>
</span><span id=__span-0-568><a id=__codelineno-0-568 name=__codelineno-0-568></a>        <span class=n>primary_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net_downsized</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-569><a id=__codelineno-0-569 name=__codelineno-0-569></a>        <span class=c1># The downsampling output is fed through additional residual block(s)</span>
</span><span id=__span-0-570><a id=__codelineno-0-570 name=__codelineno-0-570></a>        <span class=n>primary_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net</span><span class=p>(</span><span class=n>primary_flow</span><span class=p>)</span>
</span><span id=__span-0-571><a id=__codelineno-0-571 name=__codelineno-0-571></a>
</span><span id=__span-0-572><a id=__codelineno-0-572 name=__codelineno-0-572></a>        <span class=c1># If LC is not used, simply return output of primary-flow</span>
</span><span id=__span-0-573><a id=__codelineno-0-573 name=__codelineno-0-573></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-574><a id=__codelineno-0-574 name=__codelineno-0-574></a>            <span class=k>assert</span> <span class=n>lowres_x</span> <span class=ow>is</span> <span class=kc>None</span>
</span><span id=__span-0-575><a id=__codelineno-0-575 name=__codelineno-0-575></a>            <span class=k>return</span> <span class=n>primary_flow</span><span class=p>,</span> <span class=n>primary_flow</span>
</span><span id=__span-0-576><a id=__codelineno-0-576 name=__codelineno-0-576></a>
</span><span id=__span-0-577><a id=__codelineno-0-577 name=__codelineno-0-577></a>        <span class=k>if</span> <span class=n>lowres_x</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-578><a id=__codelineno-0-578 name=__codelineno-0-578></a>            <span class=c1># First encode the low-res lateral input</span>
</span><span id=__span-0-579><a id=__codelineno-0-579 name=__codelineno-0-579></a>            <span class=n>lowres_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span><span class=p>(</span><span class=n>lowres_x</span><span class=p>)</span>
</span><span id=__span-0-580><a id=__codelineno-0-580 name=__codelineno-0-580></a>            <span class=c1># Then pass the result through the MergeLowRes layer</span>
</span><span id=__span-0-581><a id=__codelineno-0-581 name=__codelineno-0-581></a>            <span class=n>merged</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_merge</span><span class=p>(</span><span class=n>primary_flow</span><span class=p>,</span> <span class=n>lowres_flow</span><span class=p>)</span>
</span><span id=__span-0-582><a id=__codelineno-0-582 name=__codelineno-0-582></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-583><a id=__codelineno-0-583 name=__codelineno-0-583></a>            <span class=n>merged</span> <span class=o>=</span> <span class=n>primary_flow</span>
</span><span id=__span-0-584><a id=__codelineno-0-584 name=__codelineno-0-584></a>
</span><span id=__span-0-585><a id=__codelineno-0-585 name=__codelineno-0-585></a>        <span class=k>if</span> <span class=p>(</span>
</span><span id=__span-0-586><a id=__codelineno-0-586 name=__codelineno-0-586></a>            <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_retain_spatial_dims</span> <span class=ow>is</span> <span class=kc>False</span>
</span><span id=__span-0-587><a id=__codelineno-0-587 name=__codelineno-0-587></a>            <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder_retain_spatial_dims</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-588><a id=__codelineno-0-588 name=__codelineno-0-588></a>        <span class=p>):</span>
</span><span id=__span-0-589><a id=__codelineno-0-589 name=__codelineno-0-589></a>            <span class=k>return</span> <span class=n>merged</span><span class=p>,</span> <span class=n>merged</span>
</span><span id=__span-0-590><a id=__codelineno-0-590 name=__codelineno-0-590></a>
</span><span id=__span-0-591><a id=__codelineno-0-591 name=__codelineno-0-591></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-592><a id=__codelineno-0-592 name=__codelineno-0-592></a>            <span class=n>expected_shape</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span>
</span><span id=__span-0-593><a id=__codelineno-0-593 name=__codelineno-0-593></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-594><a id=__codelineno-0-594 name=__codelineno-0-594></a>            <span class=n>fac</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-595><a id=__codelineno-0-595 name=__codelineno-0-595></a>            <span class=n>expected_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>//</span> <span class=n>fac</span><span class=p>,</span> <span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=n>fac</span><span class=p>)</span>
</span><span id=__span-0-596><a id=__codelineno-0-596 name=__codelineno-0-596></a>            <span class=k>assert</span> <span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span> <span class=o>!=</span> <span class=n>expected_shape</span>
</span><span id=__span-0-597><a id=__codelineno-0-597 name=__codelineno-0-597></a>
</span><span id=__span-0-598><a id=__codelineno-0-598 name=__codelineno-0-598></a>        <span class=c1># Crop the resulting tensor so that it matches with the Decoder</span>
</span><span id=__span-0-599><a id=__codelineno-0-599 name=__codelineno-0-599></a>        <span class=n>value_to_use_in_topdown</span> <span class=o>=</span> <span class=n>crop_img_tensor</span><span class=p>(</span><span class=n>merged</span><span class=p>,</span> <span class=n>expected_shape</span><span class=p>)</span>
</span><span id=__span-0-600><a id=__codelineno-0-600 name=__codelineno-0-600></a>        <span class=k>return</span> <span class=n>merged</span><span class=p>,</span> <span class=n>value_to_use_in_topdown</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.BottomUpLayer.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>n_res_blocks</span><span class=p>,</span> <span class=n>n_filters</span><span class=p>,</span> <span class=n>downsampling_steps</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>nonlin</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>batchnorm</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>gated</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>enable_multiscale</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>multiscale_lowres_size_factor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>lowres_separate_branch</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>multiscale_retain_spatial_dims</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>decoder_retain_spatial_dims</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>output_expected_shape</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.BottomUpLayer.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>n_res_blocks</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>Number of <code>BottomUpDeterministicResBlock</code> modules stacked in this layer.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>n_filters</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>Number of channels present through out the layers of this block.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>downsampling_steps</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>Number of downsampling steps that has to be done in this layer (typically 1). Default is 0.</p> </div> </td> <td> <code>0</code> </td> </tr> <tr class=doc-section-item> <td><code>nonlin</code></td> <td> <code><span title="typing.Callable">Callable</span></code> </td> <td> <div class=doc-md-description> <p>The non-linearity function used in the block. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>batchnorm</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>dropout</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_type</code></td> <td> <code>str</code> </td> <td> <div class=doc-md-description> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_kernel</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_skip_padding</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to skip padding in convolutions in the Residual block. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>gated</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use gated layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>enable_multiscale</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to enable multiscale (Lateral Contextualization) or not. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>multiscale_lowres_size_factor</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>A factor the expresses the relative size of the primary flow tensor with respect to the lower-resolution lateral input tensor. Default in <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>lowres_separate_branch</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the residual block(s) encoding the low-res input should be shared (<code>False</code>) or not (<code>True</code>) with the primary flow "same-size" residual block(s). Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>multiscale_retain_spatial_dims</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to pad the latent tensor resulting from the bottom-up layer's primary flow to match the size of the low-res input. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>decoder_retain_spatial_dims</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>output_expected_shape</code></td> <td> <code><span title="typing.Iterable">Iterable</span>[int]</code> </td> <td> <div class=doc-md-description> <p>The expected shape of the layer output (only used if <code>enable_multiscale == True</code>). Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-384>384</a></span>
<span class=normal><a href=#__codelineno-0-385>385</a></span>
<span class=normal><a href=#__codelineno-0-386>386</a></span>
<span class=normal><a href=#__codelineno-0-387>387</a></span>
<span class=normal><a href=#__codelineno-0-388>388</a></span>
<span class=normal><a href=#__codelineno-0-389>389</a></span>
<span class=normal><a href=#__codelineno-0-390>390</a></span>
<span class=normal><a href=#__codelineno-0-391>391</a></span>
<span class=normal><a href=#__codelineno-0-392>392</a></span>
<span class=normal><a href=#__codelineno-0-393>393</a></span>
<span class=normal><a href=#__codelineno-0-394>394</a></span>
<span class=normal><a href=#__codelineno-0-395>395</a></span>
<span class=normal><a href=#__codelineno-0-396>396</a></span>
<span class=normal><a href=#__codelineno-0-397>397</a></span>
<span class=normal><a href=#__codelineno-0-398>398</a></span>
<span class=normal><a href=#__codelineno-0-399>399</a></span>
<span class=normal><a href=#__codelineno-0-400>400</a></span>
<span class=normal><a href=#__codelineno-0-401>401</a></span>
<span class=normal><a href=#__codelineno-0-402>402</a></span>
<span class=normal><a href=#__codelineno-0-403>403</a></span>
<span class=normal><a href=#__codelineno-0-404>404</a></span>
<span class=normal><a href=#__codelineno-0-405>405</a></span>
<span class=normal><a href=#__codelineno-0-406>406</a></span>
<span class=normal><a href=#__codelineno-0-407>407</a></span>
<span class=normal><a href=#__codelineno-0-408>408</a></span>
<span class=normal><a href=#__codelineno-0-409>409</a></span>
<span class=normal><a href=#__codelineno-0-410>410</a></span>
<span class=normal><a href=#__codelineno-0-411>411</a></span>
<span class=normal><a href=#__codelineno-0-412>412</a></span>
<span class=normal><a href=#__codelineno-0-413>413</a></span>
<span class=normal><a href=#__codelineno-0-414>414</a></span>
<span class=normal><a href=#__codelineno-0-415>415</a></span>
<span class=normal><a href=#__codelineno-0-416>416</a></span>
<span class=normal><a href=#__codelineno-0-417>417</a></span>
<span class=normal><a href=#__codelineno-0-418>418</a></span>
<span class=normal><a href=#__codelineno-0-419>419</a></span>
<span class=normal><a href=#__codelineno-0-420>420</a></span>
<span class=normal><a href=#__codelineno-0-421>421</a></span>
<span class=normal><a href=#__codelineno-0-422>422</a></span>
<span class=normal><a href=#__codelineno-0-423>423</a></span>
<span class=normal><a href=#__codelineno-0-424>424</a></span>
<span class=normal><a href=#__codelineno-0-425>425</a></span>
<span class=normal><a href=#__codelineno-0-426>426</a></span>
<span class=normal><a href=#__codelineno-0-427>427</a></span>
<span class=normal><a href=#__codelineno-0-428>428</a></span>
<span class=normal><a href=#__codelineno-0-429>429</a></span>
<span class=normal><a href=#__codelineno-0-430>430</a></span>
<span class=normal><a href=#__codelineno-0-431>431</a></span>
<span class=normal><a href=#__codelineno-0-432>432</a></span>
<span class=normal><a href=#__codelineno-0-433>433</a></span>
<span class=normal><a href=#__codelineno-0-434>434</a></span>
<span class=normal><a href=#__codelineno-0-435>435</a></span>
<span class=normal><a href=#__codelineno-0-436>436</a></span>
<span class=normal><a href=#__codelineno-0-437>437</a></span>
<span class=normal><a href=#__codelineno-0-438>438</a></span>
<span class=normal><a href=#__codelineno-0-439>439</a></span>
<span class=normal><a href=#__codelineno-0-440>440</a></span>
<span class=normal><a href=#__codelineno-0-441>441</a></span>
<span class=normal><a href=#__codelineno-0-442>442</a></span>
<span class=normal><a href=#__codelineno-0-443>443</a></span>
<span class=normal><a href=#__codelineno-0-444>444</a></span>
<span class=normal><a href=#__codelineno-0-445>445</a></span>
<span class=normal><a href=#__codelineno-0-446>446</a></span>
<span class=normal><a href=#__codelineno-0-447>447</a></span>
<span class=normal><a href=#__codelineno-0-448>448</a></span>
<span class=normal><a href=#__codelineno-0-449>449</a></span>
<span class=normal><a href=#__codelineno-0-450>450</a></span>
<span class=normal><a href=#__codelineno-0-451>451</a></span>
<span class=normal><a href=#__codelineno-0-452>452</a></span>
<span class=normal><a href=#__codelineno-0-453>453</a></span>
<span class=normal><a href=#__codelineno-0-454>454</a></span>
<span class=normal><a href=#__codelineno-0-455>455</a></span>
<span class=normal><a href=#__codelineno-0-456>456</a></span>
<span class=normal><a href=#__codelineno-0-457>457</a></span>
<span class=normal><a href=#__codelineno-0-458>458</a></span>
<span class=normal><a href=#__codelineno-0-459>459</a></span>
<span class=normal><a href=#__codelineno-0-460>460</a></span>
<span class=normal><a href=#__codelineno-0-461>461</a></span>
<span class=normal><a href=#__codelineno-0-462>462</a></span>
<span class=normal><a href=#__codelineno-0-463>463</a></span>
<span class=normal><a href=#__codelineno-0-464>464</a></span>
<span class=normal><a href=#__codelineno-0-465>465</a></span>
<span class=normal><a href=#__codelineno-0-466>466</a></span>
<span class=normal><a href=#__codelineno-0-467>467</a></span>
<span class=normal><a href=#__codelineno-0-468>468</a></span>
<span class=normal><a href=#__codelineno-0-469>469</a></span>
<span class=normal><a href=#__codelineno-0-470>470</a></span>
<span class=normal><a href=#__codelineno-0-471>471</a></span>
<span class=normal><a href=#__codelineno-0-472>472</a></span>
<span class=normal><a href=#__codelineno-0-473>473</a></span>
<span class=normal><a href=#__codelineno-0-474>474</a></span>
<span class=normal><a href=#__codelineno-0-475>475</a></span>
<span class=normal><a href=#__codelineno-0-476>476</a></span>
<span class=normal><a href=#__codelineno-0-477>477</a></span>
<span class=normal><a href=#__codelineno-0-478>478</a></span>
<span class=normal><a href=#__codelineno-0-479>479</a></span>
<span class=normal><a href=#__codelineno-0-480>480</a></span>
<span class=normal><a href=#__codelineno-0-481>481</a></span>
<span class=normal><a href=#__codelineno-0-482>482</a></span>
<span class=normal><a href=#__codelineno-0-483>483</a></span>
<span class=normal><a href=#__codelineno-0-484>484</a></span>
<span class=normal><a href=#__codelineno-0-485>485</a></span>
<span class=normal><a href=#__codelineno-0-486>486</a></span>
<span class=normal><a href=#__codelineno-0-487>487</a></span>
<span class=normal><a href=#__codelineno-0-488>488</a></span>
<span class=normal><a href=#__codelineno-0-489>489</a></span>
<span class=normal><a href=#__codelineno-0-490>490</a></span>
<span class=normal><a href=#__codelineno-0-491>491</a></span>
<span class=normal><a href=#__codelineno-0-492>492</a></span>
<span class=normal><a href=#__codelineno-0-493>493</a></span>
<span class=normal><a href=#__codelineno-0-494>494</a></span>
<span class=normal><a href=#__codelineno-0-495>495</a></span>
<span class=normal><a href=#__codelineno-0-496>496</a></span>
<span class=normal><a href=#__codelineno-0-497>497</a></span>
<span class=normal><a href=#__codelineno-0-498>498</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-384><a id=__codelineno-0-384 name=__codelineno-0-384></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-385><a id=__codelineno-0-385 name=__codelineno-0-385></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-386><a id=__codelineno-0-386 name=__codelineno-0-386></a>    <span class=n>n_res_blocks</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-387><a id=__codelineno-0-387 name=__codelineno-0-387></a>    <span class=n>n_filters</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-388><a id=__codelineno-0-388 name=__codelineno-0-388></a>    <span class=n>downsampling_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span>
</span><span id=__span-0-389><a id=__codelineno-0-389 name=__codelineno-0-389></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-390><a id=__codelineno-0-390 name=__codelineno-0-390></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-391><a id=__codelineno-0-391 name=__codelineno-0-391></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-392><a id=__codelineno-0-392 name=__codelineno-0-392></a>    <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-393><a id=__codelineno-0-393 name=__codelineno-0-393></a>    <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-394><a id=__codelineno-0-394 name=__codelineno-0-394></a>    <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-395><a id=__codelineno-0-395 name=__codelineno-0-395></a>    <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-396><a id=__codelineno-0-396 name=__codelineno-0-396></a>    <span class=n>enable_multiscale</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-397><a id=__codelineno-0-397 name=__codelineno-0-397></a>    <span class=n>multiscale_lowres_size_factor</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-398><a id=__codelineno-0-398 name=__codelineno-0-398></a>    <span class=n>lowres_separate_branch</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-399><a id=__codelineno-0-399 name=__codelineno-0-399></a>    <span class=n>multiscale_retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-400><a id=__codelineno-0-400 name=__codelineno-0-400></a>    <span class=n>decoder_retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-401><a id=__codelineno-0-401 name=__codelineno-0-401></a>    <span class=n>output_expected_shape</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-402><a id=__codelineno-0-402 name=__codelineno-0-402></a><span class=p>):</span>
</span><span id=__span-0-403><a id=__codelineno-0-403 name=__codelineno-0-403></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-404><a id=__codelineno-0-404 name=__codelineno-0-404></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-405><a id=__codelineno-0-405 name=__codelineno-0-405></a>
</span><span id=__span-0-406><a id=__codelineno-0-406 name=__codelineno-0-406></a><span class=sd>    Parameters</span>
</span><span id=__span-0-407><a id=__codelineno-0-407 name=__codelineno-0-407></a><span class=sd>    ----------</span>
</span><span id=__span-0-408><a id=__codelineno-0-408 name=__codelineno-0-408></a><span class=sd>    n_res_blocks: int</span>
</span><span id=__span-0-409><a id=__codelineno-0-409 name=__codelineno-0-409></a><span class=sd>        Number of `BottomUpDeterministicResBlock` modules stacked in this layer.</span>
</span><span id=__span-0-410><a id=__codelineno-0-410 name=__codelineno-0-410></a><span class=sd>    n_filters: int</span>
</span><span id=__span-0-411><a id=__codelineno-0-411 name=__codelineno-0-411></a><span class=sd>        Number of channels present through out the layers of this block.</span>
</span><span id=__span-0-412><a id=__codelineno-0-412 name=__codelineno-0-412></a><span class=sd>    downsampling_steps: int, optional</span>
</span><span id=__span-0-413><a id=__codelineno-0-413 name=__codelineno-0-413></a><span class=sd>        Number of downsampling steps that has to be done in this layer (typically 1).</span>
</span><span id=__span-0-414><a id=__codelineno-0-414 name=__codelineno-0-414></a><span class=sd>        Default is 0.</span>
</span><span id=__span-0-415><a id=__codelineno-0-415 name=__codelineno-0-415></a><span class=sd>    nonlin: Callable, optional</span>
</span><span id=__span-0-416><a id=__codelineno-0-416 name=__codelineno-0-416></a><span class=sd>        The non-linearity function used in the block. Default is `None`.</span>
</span><span id=__span-0-417><a id=__codelineno-0-417 name=__codelineno-0-417></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-418><a id=__codelineno-0-418 name=__codelineno-0-418></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-419><a id=__codelineno-0-419 name=__codelineno-0-419></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-420><a id=__codelineno-0-420 name=__codelineno-0-420></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-421><a id=__codelineno-0-421 name=__codelineno-0-421></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-422><a id=__codelineno-0-422 name=__codelineno-0-422></a><span class=sd>    res_block_type: str, optional</span>
</span><span id=__span-0-423><a id=__codelineno-0-423 name=__codelineno-0-423></a><span class=sd>        A string specifying the structure of residual block.</span>
</span><span id=__span-0-424><a id=__codelineno-0-424 name=__codelineno-0-424></a><span class=sd>        Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-425><a id=__codelineno-0-425 name=__codelineno-0-425></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-426><a id=__codelineno-0-426 name=__codelineno-0-426></a><span class=sd>    res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-427><a id=__codelineno-0-427 name=__codelineno-0-427></a><span class=sd>        The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-428><a id=__codelineno-0-428 name=__codelineno-0-428></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-429><a id=__codelineno-0-429 name=__codelineno-0-429></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-430><a id=__codelineno-0-430 name=__codelineno-0-430></a><span class=sd>    res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-431><a id=__codelineno-0-431 name=__codelineno-0-431></a><span class=sd>        Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-432><a id=__codelineno-0-432 name=__codelineno-0-432></a><span class=sd>    gated: bool, optional</span>
</span><span id=__span-0-433><a id=__codelineno-0-433 name=__codelineno-0-433></a><span class=sd>        Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-434><a id=__codelineno-0-434 name=__codelineno-0-434></a><span class=sd>    enable_multiscale: bool, optional</span>
</span><span id=__span-0-435><a id=__codelineno-0-435 name=__codelineno-0-435></a><span class=sd>        Whether to enable multiscale (Lateral Contextualization) or not. Default is `False`.</span>
</span><span id=__span-0-436><a id=__codelineno-0-436 name=__codelineno-0-436></a><span class=sd>    multiscale_lowres_size_factor: int, optional</span>
</span><span id=__span-0-437><a id=__codelineno-0-437 name=__codelineno-0-437></a><span class=sd>        A factor the expresses the relative size of the primary flow tensor with respect to the</span>
</span><span id=__span-0-438><a id=__codelineno-0-438 name=__codelineno-0-438></a><span class=sd>        lower-resolution lateral input tensor. Default in `None`.</span>
</span><span id=__span-0-439><a id=__codelineno-0-439 name=__codelineno-0-439></a><span class=sd>    lowres_separate_branch: bool, optional</span>
</span><span id=__span-0-440><a id=__codelineno-0-440 name=__codelineno-0-440></a><span class=sd>        Whether the residual block(s) encoding the low-res input should be shared (`False`) or</span>
</span><span id=__span-0-441><a id=__codelineno-0-441 name=__codelineno-0-441></a><span class=sd>        not (`True`) with the primary flow &quot;same-size&quot; residual block(s). Default is `False`.</span>
</span><span id=__span-0-442><a id=__codelineno-0-442 name=__codelineno-0-442></a><span class=sd>    multiscale_retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-443><a id=__codelineno-0-443 name=__codelineno-0-443></a><span class=sd>        Whether to pad the latent tensor resulting from the bottom-up layer&#39;s primary flow</span>
</span><span id=__span-0-444><a id=__codelineno-0-444 name=__codelineno-0-444></a><span class=sd>        to match the size of the low-res input. Default is `False`.</span>
</span><span id=__span-0-445><a id=__codelineno-0-445 name=__codelineno-0-445></a><span class=sd>    decoder_retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-446><a id=__codelineno-0-446 name=__codelineno-0-446></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-447><a id=__codelineno-0-447 name=__codelineno-0-447></a><span class=sd>    output_expected_shape: Iterable[int], optional</span>
</span><span id=__span-0-448><a id=__codelineno-0-448 name=__codelineno-0-448></a><span class=sd>        The expected shape of the layer output (only used if `enable_multiscale == True`).</span>
</span><span id=__span-0-449><a id=__codelineno-0-449 name=__codelineno-0-449></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-450><a id=__codelineno-0-450 name=__codelineno-0-450></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-451><a id=__codelineno-0-451 name=__codelineno-0-451></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-452><a id=__codelineno-0-452 name=__codelineno-0-452></a>
</span><span id=__span-0-453><a id=__codelineno-0-453 name=__codelineno-0-453></a>    <span class=c1># Define attributes for Lateral Contextualization</span>
</span><span id=__span-0-454><a id=__codelineno-0-454 name=__codelineno-0-454></a>    <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=o>=</span> <span class=n>enable_multiscale</span>
</span><span id=__span-0-455><a id=__codelineno-0-455 name=__codelineno-0-455></a>    <span class=bp>self</span><span class=o>.</span><span class=n>lowres_separate_branch</span> <span class=o>=</span> <span class=n>lowres_separate_branch</span>
</span><span id=__span-0-456><a id=__codelineno-0-456 name=__codelineno-0-456></a>    <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_retain_spatial_dims</span> <span class=o>=</span> <span class=n>multiscale_retain_spatial_dims</span>
</span><span id=__span-0-457><a id=__codelineno-0-457 name=__codelineno-0-457></a>    <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span> <span class=o>=</span> <span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-458><a id=__codelineno-0-458 name=__codelineno-0-458></a>    <span class=bp>self</span><span class=o>.</span><span class=n>decoder_retain_spatial_dims</span> <span class=o>=</span> <span class=n>decoder_retain_spatial_dims</span>
</span><span id=__span-0-459><a id=__codelineno-0-459 name=__codelineno-0-459></a>    <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=o>=</span> <span class=n>output_expected_shape</span>
</span><span id=__span-0-460><a id=__codelineno-0-460 name=__codelineno-0-460></a>    <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-461><a id=__codelineno-0-461 name=__codelineno-0-461></a>
</span><span id=__span-0-462><a id=__codelineno-0-462 name=__codelineno-0-462></a>    <span class=n>bu_blocks_downsized</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-463><a id=__codelineno-0-463 name=__codelineno-0-463></a>    <span class=n>bu_blocks_samesize</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-464><a id=__codelineno-0-464 name=__codelineno-0-464></a>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_res_blocks</span><span class=p>):</span>
</span><span id=__span-0-465><a id=__codelineno-0-465 name=__codelineno-0-465></a>        <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-466><a id=__codelineno-0-466 name=__codelineno-0-466></a>        <span class=k>if</span> <span class=n>downsampling_steps</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-467><a id=__codelineno-0-467 name=__codelineno-0-467></a>            <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>True</span>
</span><span id=__span-0-468><a id=__codelineno-0-468 name=__codelineno-0-468></a>            <span class=n>downsampling_steps</span> <span class=o>-=</span> <span class=mi>1</span>
</span><span id=__span-0-469><a id=__codelineno-0-469 name=__codelineno-0-469></a>        <span class=n>block</span> <span class=o>=</span> <span class=n>BottomUpDeterministicResBlock</span><span class=p>(</span>
</span><span id=__span-0-470><a id=__codelineno-0-470 name=__codelineno-0-470></a>            <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-471><a id=__codelineno-0-471 name=__codelineno-0-471></a>            <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-472><a id=__codelineno-0-472 name=__codelineno-0-472></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-473><a id=__codelineno-0-473 name=__codelineno-0-473></a>            <span class=n>downsample</span><span class=o>=</span><span class=n>do_resample</span><span class=p>,</span>
</span><span id=__span-0-474><a id=__codelineno-0-474 name=__codelineno-0-474></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-475><a id=__codelineno-0-475 name=__codelineno-0-475></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-476><a id=__codelineno-0-476 name=__codelineno-0-476></a>            <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-477><a id=__codelineno-0-477 name=__codelineno-0-477></a>            <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-478><a id=__codelineno-0-478 name=__codelineno-0-478></a>            <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-479><a id=__codelineno-0-479 name=__codelineno-0-479></a>            <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-480><a id=__codelineno-0-480 name=__codelineno-0-480></a>        <span class=p>)</span>
</span><span id=__span-0-481><a id=__codelineno-0-481 name=__codelineno-0-481></a>        <span class=k>if</span> <span class=n>do_resample</span><span class=p>:</span>
</span><span id=__span-0-482><a id=__codelineno-0-482 name=__codelineno-0-482></a>            <span class=n>bu_blocks_downsized</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>block</span><span class=p>)</span>
</span><span id=__span-0-483><a id=__codelineno-0-483 name=__codelineno-0-483></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-484><a id=__codelineno-0-484 name=__codelineno-0-484></a>            <span class=n>bu_blocks_samesize</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>block</span><span class=p>)</span>
</span><span id=__span-0-485><a id=__codelineno-0-485 name=__codelineno-0-485></a>
</span><span id=__span-0-486><a id=__codelineno-0-486 name=__codelineno-0-486></a>    <span class=bp>self</span><span class=o>.</span><span class=n>net_downsized</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>bu_blocks_downsized</span><span class=p>)</span>
</span><span id=__span-0-487><a id=__codelineno-0-487 name=__codelineno-0-487></a>    <span class=bp>self</span><span class=o>.</span><span class=n>net</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>bu_blocks_samesize</span><span class=p>)</span>
</span><span id=__span-0-488><a id=__codelineno-0-488 name=__codelineno-0-488></a>
</span><span id=__span-0-489><a id=__codelineno-0-489 name=__codelineno-0-489></a>    <span class=c1># Using the same net for the low resolution (and larger sized image)</span>
</span><span id=__span-0-490><a id=__codelineno-0-490 name=__codelineno-0-490></a>    <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_merge</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-491><a id=__codelineno-0-491 name=__codelineno-0-491></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span><span class=p>:</span>
</span><span id=__span-0-492><a id=__codelineno-0-492 name=__codelineno-0-492></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_init_multiscale</span><span class=p>(</span>
</span><span id=__span-0-493><a id=__codelineno-0-493 name=__codelineno-0-493></a>            <span class=n>n_filters</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-494><a id=__codelineno-0-494 name=__codelineno-0-494></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-495><a id=__codelineno-0-495 name=__codelineno-0-495></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-496><a id=__codelineno-0-496 name=__codelineno-0-496></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-497><a id=__codelineno-0-497 name=__codelineno-0-497></a>            <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-498><a id=__codelineno-0-498 name=__codelineno-0-498></a>        <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.BottomUpLayer.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>lowres_x</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.BottomUpLayer.forward class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>x</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input of the <code>BottomUpLayer</code>, i.e., the input image or the output of the previous layer.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>lowres_x</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The low-res input used for Lateral Contextualization (LC). Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-555>555</a></span>
<span class=normal><a href=#__codelineno-0-556>556</a></span>
<span class=normal><a href=#__codelineno-0-557>557</a></span>
<span class=normal><a href=#__codelineno-0-558>558</a></span>
<span class=normal><a href=#__codelineno-0-559>559</a></span>
<span class=normal><a href=#__codelineno-0-560>560</a></span>
<span class=normal><a href=#__codelineno-0-561>561</a></span>
<span class=normal><a href=#__codelineno-0-562>562</a></span>
<span class=normal><a href=#__codelineno-0-563>563</a></span>
<span class=normal><a href=#__codelineno-0-564>564</a></span>
<span class=normal><a href=#__codelineno-0-565>565</a></span>
<span class=normal><a href=#__codelineno-0-566>566</a></span>
<span class=normal><a href=#__codelineno-0-567>567</a></span>
<span class=normal><a href=#__codelineno-0-568>568</a></span>
<span class=normal><a href=#__codelineno-0-569>569</a></span>
<span class=normal><a href=#__codelineno-0-570>570</a></span>
<span class=normal><a href=#__codelineno-0-571>571</a></span>
<span class=normal><a href=#__codelineno-0-572>572</a></span>
<span class=normal><a href=#__codelineno-0-573>573</a></span>
<span class=normal><a href=#__codelineno-0-574>574</a></span>
<span class=normal><a href=#__codelineno-0-575>575</a></span>
<span class=normal><a href=#__codelineno-0-576>576</a></span>
<span class=normal><a href=#__codelineno-0-577>577</a></span>
<span class=normal><a href=#__codelineno-0-578>578</a></span>
<span class=normal><a href=#__codelineno-0-579>579</a></span>
<span class=normal><a href=#__codelineno-0-580>580</a></span>
<span class=normal><a href=#__codelineno-0-581>581</a></span>
<span class=normal><a href=#__codelineno-0-582>582</a></span>
<span class=normal><a href=#__codelineno-0-583>583</a></span>
<span class=normal><a href=#__codelineno-0-584>584</a></span>
<span class=normal><a href=#__codelineno-0-585>585</a></span>
<span class=normal><a href=#__codelineno-0-586>586</a></span>
<span class=normal><a href=#__codelineno-0-587>587</a></span>
<span class=normal><a href=#__codelineno-0-588>588</a></span>
<span class=normal><a href=#__codelineno-0-589>589</a></span>
<span class=normal><a href=#__codelineno-0-590>590</a></span>
<span class=normal><a href=#__codelineno-0-591>591</a></span>
<span class=normal><a href=#__codelineno-0-592>592</a></span>
<span class=normal><a href=#__codelineno-0-593>593</a></span>
<span class=normal><a href=#__codelineno-0-594>594</a></span>
<span class=normal><a href=#__codelineno-0-595>595</a></span>
<span class=normal><a href=#__codelineno-0-596>596</a></span>
<span class=normal><a href=#__codelineno-0-597>597</a></span>
<span class=normal><a href=#__codelineno-0-598>598</a></span>
<span class=normal><a href=#__codelineno-0-599>599</a></span>
<span class=normal><a href=#__codelineno-0-600>600</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-555><a id=__codelineno-0-555 name=__codelineno-0-555></a><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-556><a id=__codelineno-0-556 name=__codelineno-0-556></a>    <span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>lowres_x</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-557><a id=__codelineno-0-557 name=__codelineno-0-557></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-558><a id=__codelineno-0-558 name=__codelineno-0-558></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-559><a id=__codelineno-0-559 name=__codelineno-0-559></a><span class=sd>    Parameters</span>
</span><span id=__span-0-560><a id=__codelineno-0-560 name=__codelineno-0-560></a><span class=sd>    ----------</span>
</span><span id=__span-0-561><a id=__codelineno-0-561 name=__codelineno-0-561></a><span class=sd>    x: torch.Tensor</span>
</span><span id=__span-0-562><a id=__codelineno-0-562 name=__codelineno-0-562></a><span class=sd>        The input of the `BottomUpLayer`, i.e., the input image or the output of the</span>
</span><span id=__span-0-563><a id=__codelineno-0-563 name=__codelineno-0-563></a><span class=sd>        previous layer.</span>
</span><span id=__span-0-564><a id=__codelineno-0-564 name=__codelineno-0-564></a><span class=sd>    lowres_x: torch.Tensor, optional</span>
</span><span id=__span-0-565><a id=__codelineno-0-565 name=__codelineno-0-565></a><span class=sd>        The low-res input used for Lateral Contextualization (LC). Default is `None`.</span>
</span><span id=__span-0-566><a id=__codelineno-0-566 name=__codelineno-0-566></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-567><a id=__codelineno-0-567 name=__codelineno-0-567></a>    <span class=c1># The input is fed through the residual downsampling block(s)</span>
</span><span id=__span-0-568><a id=__codelineno-0-568 name=__codelineno-0-568></a>    <span class=n>primary_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net_downsized</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-569><a id=__codelineno-0-569 name=__codelineno-0-569></a>    <span class=c1># The downsampling output is fed through additional residual block(s)</span>
</span><span id=__span-0-570><a id=__codelineno-0-570 name=__codelineno-0-570></a>    <span class=n>primary_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net</span><span class=p>(</span><span class=n>primary_flow</span><span class=p>)</span>
</span><span id=__span-0-571><a id=__codelineno-0-571 name=__codelineno-0-571></a>
</span><span id=__span-0-572><a id=__codelineno-0-572 name=__codelineno-0-572></a>    <span class=c1># If LC is not used, simply return output of primary-flow</span>
</span><span id=__span-0-573><a id=__codelineno-0-573 name=__codelineno-0-573></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>enable_multiscale</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-574><a id=__codelineno-0-574 name=__codelineno-0-574></a>        <span class=k>assert</span> <span class=n>lowres_x</span> <span class=ow>is</span> <span class=kc>None</span>
</span><span id=__span-0-575><a id=__codelineno-0-575 name=__codelineno-0-575></a>        <span class=k>return</span> <span class=n>primary_flow</span><span class=p>,</span> <span class=n>primary_flow</span>
</span><span id=__span-0-576><a id=__codelineno-0-576 name=__codelineno-0-576></a>
</span><span id=__span-0-577><a id=__codelineno-0-577 name=__codelineno-0-577></a>    <span class=k>if</span> <span class=n>lowres_x</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-578><a id=__codelineno-0-578 name=__codelineno-0-578></a>        <span class=c1># First encode the low-res lateral input</span>
</span><span id=__span-0-579><a id=__codelineno-0-579 name=__codelineno-0-579></a>        <span class=n>lowres_flow</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_net</span><span class=p>(</span><span class=n>lowres_x</span><span class=p>)</span>
</span><span id=__span-0-580><a id=__codelineno-0-580 name=__codelineno-0-580></a>        <span class=c1># Then pass the result through the MergeLowRes layer</span>
</span><span id=__span-0-581><a id=__codelineno-0-581 name=__codelineno-0-581></a>        <span class=n>merged</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lowres_merge</span><span class=p>(</span><span class=n>primary_flow</span><span class=p>,</span> <span class=n>lowres_flow</span><span class=p>)</span>
</span><span id=__span-0-582><a id=__codelineno-0-582 name=__codelineno-0-582></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-583><a id=__codelineno-0-583 name=__codelineno-0-583></a>        <span class=n>merged</span> <span class=o>=</span> <span class=n>primary_flow</span>
</span><span id=__span-0-584><a id=__codelineno-0-584 name=__codelineno-0-584></a>
</span><span id=__span-0-585><a id=__codelineno-0-585 name=__codelineno-0-585></a>    <span class=k>if</span> <span class=p>(</span>
</span><span id=__span-0-586><a id=__codelineno-0-586 name=__codelineno-0-586></a>        <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_retain_spatial_dims</span> <span class=ow>is</span> <span class=kc>False</span>
</span><span id=__span-0-587><a id=__codelineno-0-587 name=__codelineno-0-587></a>        <span class=ow>or</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder_retain_spatial_dims</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-588><a id=__codelineno-0-588 name=__codelineno-0-588></a>    <span class=p>):</span>
</span><span id=__span-0-589><a id=__codelineno-0-589 name=__codelineno-0-589></a>        <span class=k>return</span> <span class=n>merged</span><span class=p>,</span> <span class=n>merged</span>
</span><span id=__span-0-590><a id=__codelineno-0-590 name=__codelineno-0-590></a>
</span><span id=__span-0-591><a id=__codelineno-0-591 name=__codelineno-0-591></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-592><a id=__codelineno-0-592 name=__codelineno-0-592></a>        <span class=n>expected_shape</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_expected_shape</span>
</span><span id=__span-0-593><a id=__codelineno-0-593 name=__codelineno-0-593></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-594><a id=__codelineno-0-594 name=__codelineno-0-594></a>        <span class=n>fac</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-595><a id=__codelineno-0-595 name=__codelineno-0-595></a>        <span class=n>expected_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>//</span> <span class=n>fac</span><span class=p>,</span> <span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=n>fac</span><span class=p>)</span>
</span><span id=__span-0-596><a id=__codelineno-0-596 name=__codelineno-0-596></a>        <span class=k>assert</span> <span class=n>merged</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span> <span class=o>!=</span> <span class=n>expected_shape</span>
</span><span id=__span-0-597><a id=__codelineno-0-597 name=__codelineno-0-597></a>
</span><span id=__span-0-598><a id=__codelineno-0-598 name=__codelineno-0-598></a>    <span class=c1># Crop the resulting tensor so that it matches with the Decoder</span>
</span><span id=__span-0-599><a id=__codelineno-0-599 name=__codelineno-0-599></a>    <span class=n>value_to_use_in_topdown</span> <span class=o>=</span> <span class=n>crop_img_tensor</span><span class=p>(</span><span class=n>merged</span><span class=p>,</span> <span class=n>expected_shape</span><span class=p>)</span>
</span><span id=__span-0-600><a id=__codelineno-0-600 name=__codelineno-0-600></a>    <span class=k>return</span> <span class=n>merged</span><span class=p>,</span> <span class=n>value_to_use_in_topdown</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.GateLayer2d class="doc doc-heading"> <code>GateLayer2d</code> <a href=#careamics.models.lvae.layers.GateLayer2d class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Double the number of channels through a convolutional layer, then use half the channels as gate for the other half.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-181>181</a></span>
<span class=normal><a href=#__codelineno-0-182>182</a></span>
<span class=normal><a href=#__codelineno-0-183>183</a></span>
<span class=normal><a href=#__codelineno-0-184>184</a></span>
<span class=normal><a href=#__codelineno-0-185>185</a></span>
<span class=normal><a href=#__codelineno-0-186>186</a></span>
<span class=normal><a href=#__codelineno-0-187>187</a></span>
<span class=normal><a href=#__codelineno-0-188>188</a></span>
<span class=normal><a href=#__codelineno-0-189>189</a></span>
<span class=normal><a href=#__codelineno-0-190>190</a></span>
<span class=normal><a href=#__codelineno-0-191>191</a></span>
<span class=normal><a href=#__codelineno-0-192>192</a></span>
<span class=normal><a href=#__codelineno-0-193>193</a></span>
<span class=normal><a href=#__codelineno-0-194>194</a></span>
<span class=normal><a href=#__codelineno-0-195>195</a></span>
<span class=normal><a href=#__codelineno-0-196>196</a></span>
<span class=normal><a href=#__codelineno-0-197>197</a></span>
<span class=normal><a href=#__codelineno-0-198>198</a></span>
<span class=normal><a href=#__codelineno-0-199>199</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-181><a id=__codelineno-0-181 name=__codelineno-0-181></a><span class=k>class</span> <span class=nc>GateLayer2d</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-182><a id=__codelineno-0-182 name=__codelineno-0-182></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-183><a id=__codelineno-0-183 name=__codelineno-0-183></a><span class=sd>    Double the number of channels through a convolutional layer, then use</span>
</span><span id=__span-0-184><a id=__codelineno-0-184 name=__codelineno-0-184></a><span class=sd>    half the channels as gate for the other half.</span>
</span><span id=__span-0-185><a id=__codelineno-0-185 name=__codelineno-0-185></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-186><a id=__codelineno-0-186 name=__codelineno-0-186></a>
</span><span id=__span-0-187><a id=__codelineno-0-187 name=__codelineno-0-187></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>,</span> <span class=n>nonlin</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>):</span>
</span><span id=__span-0-188><a id=__codelineno-0-188 name=__codelineno-0-188></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-189><a id=__codelineno-0-189 name=__codelineno-0-189></a>        <span class=k>assert</span> <span class=n>kernel_size</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span>
</span><span id=__span-0-190><a id=__codelineno-0-190 name=__codelineno-0-190></a>        <span class=n>pad</span> <span class=o>=</span> <span class=n>kernel_size</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-191><a id=__codelineno-0-191 name=__codelineno-0-191></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>channels</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>channels</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-192><a id=__codelineno-0-192 name=__codelineno-0-192></a>        <span class=bp>self</span><span class=o>.</span><span class=n>nonlin</span> <span class=o>=</span> <span class=n>nonlin</span><span class=p>()</span>
</span><span id=__span-0-193><a id=__codelineno-0-193 name=__codelineno-0-193></a>
</span><span id=__span-0-194><a id=__codelineno-0-194 name=__codelineno-0-194></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-0-195><a id=__codelineno-0-195 name=__codelineno-0-195></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-196><a id=__codelineno-0-196 name=__codelineno-0-196></a>        <span class=n>x</span><span class=p>,</span> <span class=n>gate</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-197><a id=__codelineno-0-197 name=__codelineno-0-197></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>nonlin</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># TODO remove this?</span>
</span><span id=__span-0-198><a id=__codelineno-0-198 name=__codelineno-0-198></a>        <span class=n>gate</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>gate</span><span class=p>)</span>
</span><span id=__span-0-199><a id=__codelineno-0-199 name=__codelineno-0-199></a>        <span class=k>return</span> <span class=n>x</span> <span class=o>*</span> <span class=n>gate</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.MergeLayer class="doc doc-heading"> <code>MergeLayer</code> <a href=#careamics.models.lvae.layers.MergeLayer class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>This layer merges two or more 4D input tensors by concatenating along dim=1 and passes the result through: a) a convolutional 1x1 layer (<code>merge_type == "linear"</code>), or b) a convolutional 1x1 layer and then a gated residual block (<code>merge_type == "residual"</code>), or c) a convolutional 1x1 layer and then an ungated residual block (<code>merge_type == "residual_ungated"</code>).</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-603>603</a></span>
<span class=normal><a href=#__codelineno-0-604>604</a></span>
<span class=normal><a href=#__codelineno-0-605>605</a></span>
<span class=normal><a href=#__codelineno-0-606>606</a></span>
<span class=normal><a href=#__codelineno-0-607>607</a></span>
<span class=normal><a href=#__codelineno-0-608>608</a></span>
<span class=normal><a href=#__codelineno-0-609>609</a></span>
<span class=normal><a href=#__codelineno-0-610>610</a></span>
<span class=normal><a href=#__codelineno-0-611>611</a></span>
<span class=normal><a href=#__codelineno-0-612>612</a></span>
<span class=normal><a href=#__codelineno-0-613>613</a></span>
<span class=normal><a href=#__codelineno-0-614>614</a></span>
<span class=normal><a href=#__codelineno-0-615>615</a></span>
<span class=normal><a href=#__codelineno-0-616>616</a></span>
<span class=normal><a href=#__codelineno-0-617>617</a></span>
<span class=normal><a href=#__codelineno-0-618>618</a></span>
<span class=normal><a href=#__codelineno-0-619>619</a></span>
<span class=normal><a href=#__codelineno-0-620>620</a></span>
<span class=normal><a href=#__codelineno-0-621>621</a></span>
<span class=normal><a href=#__codelineno-0-622>622</a></span>
<span class=normal><a href=#__codelineno-0-623>623</a></span>
<span class=normal><a href=#__codelineno-0-624>624</a></span>
<span class=normal><a href=#__codelineno-0-625>625</a></span>
<span class=normal><a href=#__codelineno-0-626>626</a></span>
<span class=normal><a href=#__codelineno-0-627>627</a></span>
<span class=normal><a href=#__codelineno-0-628>628</a></span>
<span class=normal><a href=#__codelineno-0-629>629</a></span>
<span class=normal><a href=#__codelineno-0-630>630</a></span>
<span class=normal><a href=#__codelineno-0-631>631</a></span>
<span class=normal><a href=#__codelineno-0-632>632</a></span>
<span class=normal><a href=#__codelineno-0-633>633</a></span>
<span class=normal><a href=#__codelineno-0-634>634</a></span>
<span class=normal><a href=#__codelineno-0-635>635</a></span>
<span class=normal><a href=#__codelineno-0-636>636</a></span>
<span class=normal><a href=#__codelineno-0-637>637</a></span>
<span class=normal><a href=#__codelineno-0-638>638</a></span>
<span class=normal><a href=#__codelineno-0-639>639</a></span>
<span class=normal><a href=#__codelineno-0-640>640</a></span>
<span class=normal><a href=#__codelineno-0-641>641</a></span>
<span class=normal><a href=#__codelineno-0-642>642</a></span>
<span class=normal><a href=#__codelineno-0-643>643</a></span>
<span class=normal><a href=#__codelineno-0-644>644</a></span>
<span class=normal><a href=#__codelineno-0-645>645</a></span>
<span class=normal><a href=#__codelineno-0-646>646</a></span>
<span class=normal><a href=#__codelineno-0-647>647</a></span>
<span class=normal><a href=#__codelineno-0-648>648</a></span>
<span class=normal><a href=#__codelineno-0-649>649</a></span>
<span class=normal><a href=#__codelineno-0-650>650</a></span>
<span class=normal><a href=#__codelineno-0-651>651</a></span>
<span class=normal><a href=#__codelineno-0-652>652</a></span>
<span class=normal><a href=#__codelineno-0-653>653</a></span>
<span class=normal><a href=#__codelineno-0-654>654</a></span>
<span class=normal><a href=#__codelineno-0-655>655</a></span>
<span class=normal><a href=#__codelineno-0-656>656</a></span>
<span class=normal><a href=#__codelineno-0-657>657</a></span>
<span class=normal><a href=#__codelineno-0-658>658</a></span>
<span class=normal><a href=#__codelineno-0-659>659</a></span>
<span class=normal><a href=#__codelineno-0-660>660</a></span>
<span class=normal><a href=#__codelineno-0-661>661</a></span>
<span class=normal><a href=#__codelineno-0-662>662</a></span>
<span class=normal><a href=#__codelineno-0-663>663</a></span>
<span class=normal><a href=#__codelineno-0-664>664</a></span>
<span class=normal><a href=#__codelineno-0-665>665</a></span>
<span class=normal><a href=#__codelineno-0-666>666</a></span>
<span class=normal><a href=#__codelineno-0-667>667</a></span>
<span class=normal><a href=#__codelineno-0-668>668</a></span>
<span class=normal><a href=#__codelineno-0-669>669</a></span>
<span class=normal><a href=#__codelineno-0-670>670</a></span>
<span class=normal><a href=#__codelineno-0-671>671</a></span>
<span class=normal><a href=#__codelineno-0-672>672</a></span>
<span class=normal><a href=#__codelineno-0-673>673</a></span>
<span class=normal><a href=#__codelineno-0-674>674</a></span>
<span class=normal><a href=#__codelineno-0-675>675</a></span>
<span class=normal><a href=#__codelineno-0-676>676</a></span>
<span class=normal><a href=#__codelineno-0-677>677</a></span>
<span class=normal><a href=#__codelineno-0-678>678</a></span>
<span class=normal><a href=#__codelineno-0-679>679</a></span>
<span class=normal><a href=#__codelineno-0-680>680</a></span>
<span class=normal><a href=#__codelineno-0-681>681</a></span>
<span class=normal><a href=#__codelineno-0-682>682</a></span>
<span class=normal><a href=#__codelineno-0-683>683</a></span>
<span class=normal><a href=#__codelineno-0-684>684</a></span>
<span class=normal><a href=#__codelineno-0-685>685</a></span>
<span class=normal><a href=#__codelineno-0-686>686</a></span>
<span class=normal><a href=#__codelineno-0-687>687</a></span>
<span class=normal><a href=#__codelineno-0-688>688</a></span>
<span class=normal><a href=#__codelineno-0-689>689</a></span>
<span class=normal><a href=#__codelineno-0-690>690</a></span>
<span class=normal><a href=#__codelineno-0-691>691</a></span>
<span class=normal><a href=#__codelineno-0-692>692</a></span>
<span class=normal><a href=#__codelineno-0-693>693</a></span>
<span class=normal><a href=#__codelineno-0-694>694</a></span>
<span class=normal><a href=#__codelineno-0-695>695</a></span>
<span class=normal><a href=#__codelineno-0-696>696</a></span>
<span class=normal><a href=#__codelineno-0-697>697</a></span>
<span class=normal><a href=#__codelineno-0-698>698</a></span>
<span class=normal><a href=#__codelineno-0-699>699</a></span>
<span class=normal><a href=#__codelineno-0-700>700</a></span>
<span class=normal><a href=#__codelineno-0-701>701</a></span>
<span class=normal><a href=#__codelineno-0-702>702</a></span>
<span class=normal><a href=#__codelineno-0-703>703</a></span>
<span class=normal><a href=#__codelineno-0-704>704</a></span>
<span class=normal><a href=#__codelineno-0-705>705</a></span>
<span class=normal><a href=#__codelineno-0-706>706</a></span>
<span class=normal><a href=#__codelineno-0-707>707</a></span>
<span class=normal><a href=#__codelineno-0-708>708</a></span>
<span class=normal><a href=#__codelineno-0-709>709</a></span>
<span class=normal><a href=#__codelineno-0-710>710</a></span>
<span class=normal><a href=#__codelineno-0-711>711</a></span>
<span class=normal><a href=#__codelineno-0-712>712</a></span>
<span class=normal><a href=#__codelineno-0-713>713</a></span>
<span class=normal><a href=#__codelineno-0-714>714</a></span>
<span class=normal><a href=#__codelineno-0-715>715</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-603><a id=__codelineno-0-603 name=__codelineno-0-603></a><span class=k>class</span> <span class=nc>MergeLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-604><a id=__codelineno-0-604 name=__codelineno-0-604></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-605><a id=__codelineno-0-605 name=__codelineno-0-605></a><span class=sd>    This layer merges two or more 4D input tensors by concatenating along dim=1 and passes the result through:</span>
</span><span id=__span-0-606><a id=__codelineno-0-606 name=__codelineno-0-606></a><span class=sd>    a) a convolutional 1x1 layer (`merge_type == &quot;linear&quot;`), or</span>
</span><span id=__span-0-607><a id=__codelineno-0-607 name=__codelineno-0-607></a><span class=sd>    b) a convolutional 1x1 layer and then a gated residual block (`merge_type == &quot;residual&quot;`), or</span>
</span><span id=__span-0-608><a id=__codelineno-0-608 name=__codelineno-0-608></a><span class=sd>    c) a convolutional 1x1 layer and then an ungated residual block (`merge_type == &quot;residual_ungated&quot;`).</span>
</span><span id=__span-0-609><a id=__codelineno-0-609 name=__codelineno-0-609></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-610><a id=__codelineno-0-610 name=__codelineno-0-610></a>
</span><span id=__span-0-611><a id=__codelineno-0-611 name=__codelineno-0-611></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-612><a id=__codelineno-0-612 name=__codelineno-0-612></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-613><a id=__codelineno-0-613 name=__codelineno-0-613></a>        <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>],</span>
</span><span id=__span-0-614><a id=__codelineno-0-614 name=__codelineno-0-614></a>        <span class=n>channels</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span>
</span><span id=__span-0-615><a id=__codelineno-0-615 name=__codelineno-0-615></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span>
</span><span id=__span-0-616><a id=__codelineno-0-616 name=__codelineno-0-616></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-617><a id=__codelineno-0-617 name=__codelineno-0-617></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-618><a id=__codelineno-0-618 name=__codelineno-0-618></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-619><a id=__codelineno-0-619 name=__codelineno-0-619></a>        <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-620><a id=__codelineno-0-620 name=__codelineno-0-620></a>        <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-621><a id=__codelineno-0-621 name=__codelineno-0-621></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-622><a id=__codelineno-0-622 name=__codelineno-0-622></a>    <span class=p>):</span>
</span><span id=__span-0-623><a id=__codelineno-0-623 name=__codelineno-0-623></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-624><a id=__codelineno-0-624 name=__codelineno-0-624></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-625><a id=__codelineno-0-625 name=__codelineno-0-625></a>
</span><span id=__span-0-626><a id=__codelineno-0-626 name=__codelineno-0-626></a><span class=sd>        Parameters</span>
</span><span id=__span-0-627><a id=__codelineno-0-627 name=__codelineno-0-627></a><span class=sd>        ----------</span>
</span><span id=__span-0-628><a id=__codelineno-0-628 name=__codelineno-0-628></a><span class=sd>        merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;]</span>
</span><span id=__span-0-629><a id=__codelineno-0-629 name=__codelineno-0-629></a><span class=sd>            The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;, and &quot;residual_ungated&quot;.</span>
</span><span id=__span-0-630><a id=__codelineno-0-630 name=__codelineno-0-630></a><span class=sd>            Check the class docstring for more information about the behaviour of different merge modalities.</span>
</span><span id=__span-0-631><a id=__codelineno-0-631 name=__codelineno-0-631></a><span class=sd>        channels: Union[int, Iterable[int]]</span>
</span><span id=__span-0-632><a id=__codelineno-0-632 name=__codelineno-0-632></a><span class=sd>            The number of channels used in the convolutional blocks of this layer.</span>
</span><span id=__span-0-633><a id=__codelineno-0-633 name=__codelineno-0-633></a><span class=sd>            If it is an `int`:</span>
</span><span id=__span-0-634><a id=__codelineno-0-634 name=__codelineno-0-634></a><span class=sd>                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels</span>
</span><span id=__span-0-635><a id=__codelineno-0-635 name=__codelineno-0-635></a><span class=sd>                - (Optional) ResBlock: in_channels=channels, out_channels=channels</span>
</span><span id=__span-0-636><a id=__codelineno-0-636 name=__codelineno-0-636></a><span class=sd>            If it is an Iterable (must have `len(channels)==3`):</span>
</span><span id=__span-0-637><a id=__codelineno-0-637 name=__codelineno-0-637></a><span class=sd>                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]</span>
</span><span id=__span-0-638><a id=__codelineno-0-638 name=__codelineno-0-638></a><span class=sd>                - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]</span>
</span><span id=__span-0-639><a id=__codelineno-0-639 name=__codelineno-0-639></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-640><a id=__codelineno-0-640 name=__codelineno-0-640></a><span class=sd>            The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-641><a id=__codelineno-0-641 name=__codelineno-0-641></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-642><a id=__codelineno-0-642 name=__codelineno-0-642></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-643><a id=__codelineno-0-643 name=__codelineno-0-643></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-644><a id=__codelineno-0-644 name=__codelineno-0-644></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-645><a id=__codelineno-0-645 name=__codelineno-0-645></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-646><a id=__codelineno-0-646 name=__codelineno-0-646></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-647><a id=__codelineno-0-647 name=__codelineno-0-647></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-648><a id=__codelineno-0-648 name=__codelineno-0-648></a><span class=sd>            Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-649><a id=__codelineno-0-649 name=__codelineno-0-649></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-650><a id=__codelineno-0-650 name=__codelineno-0-650></a><span class=sd>        res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-651><a id=__codelineno-0-651 name=__codelineno-0-651></a><span class=sd>            The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-652><a id=__codelineno-0-652 name=__codelineno-0-652></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-653><a id=__codelineno-0-653 name=__codelineno-0-653></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-654><a id=__codelineno-0-654 name=__codelineno-0-654></a><span class=sd>        res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-655><a id=__codelineno-0-655 name=__codelineno-0-655></a><span class=sd>            Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-656><a id=__codelineno-0-656 name=__codelineno-0-656></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-657><a id=__codelineno-0-657 name=__codelineno-0-657></a><span class=sd>            Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-658><a id=__codelineno-0-658 name=__codelineno-0-658></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-659><a id=__codelineno-0-659 name=__codelineno-0-659></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-660><a id=__codelineno-0-660 name=__codelineno-0-660></a>        <span class=k>try</span><span class=p>:</span>
</span><span id=__span-0-661><a id=__codelineno-0-661 name=__codelineno-0-661></a>            <span class=nb>iter</span><span class=p>(</span><span class=n>channels</span><span class=p>)</span>
</span><span id=__span-0-662><a id=__codelineno-0-662 name=__codelineno-0-662></a>        <span class=k>except</span> <span class=ne>TypeError</span><span class=p>:</span>  <span class=c1># it is not iterable</span>
</span><span id=__span-0-663><a id=__codelineno-0-663 name=__codelineno-0-663></a>            <span class=n>channels</span> <span class=o>=</span> <span class=p>[</span><span class=n>channels</span><span class=p>]</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-664><a id=__codelineno-0-664 name=__codelineno-0-664></a>        <span class=k>else</span><span class=p>:</span>  <span class=c1># it is iterable</span>
</span><span id=__span-0-665><a id=__codelineno-0-665 name=__codelineno-0-665></a>            <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>channels</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-0-666><a id=__codelineno-0-666 name=__codelineno-0-666></a>                <span class=n>channels</span> <span class=o>=</span> <span class=p>[</span><span class=n>channels</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-667><a id=__codelineno-0-667 name=__codelineno-0-667></a>
</span><span id=__span-0-668><a id=__codelineno-0-668 name=__codelineno-0-668></a>        <span class=c1># assert len(channels) == 3</span>
</span><span id=__span-0-669><a id=__codelineno-0-669 name=__codelineno-0-669></a>
</span><span id=__span-0-670><a id=__codelineno-0-670 name=__codelineno-0-670></a>        <span class=k>if</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;linear&quot;</span><span class=p>:</span>
</span><span id=__span-0-671><a id=__codelineno-0-671 name=__codelineno-0-671></a>            <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-672><a id=__codelineno-0-672 name=__codelineno-0-672></a>                <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-673><a id=__codelineno-0-673 name=__codelineno-0-673></a>            <span class=p>)</span>
</span><span id=__span-0-674><a id=__codelineno-0-674 name=__codelineno-0-674></a>        <span class=k>elif</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;residual&quot;</span><span class=p>:</span>
</span><span id=__span-0-675><a id=__codelineno-0-675 name=__codelineno-0-675></a>            <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-0-676><a id=__codelineno-0-676 name=__codelineno-0-676></a>                <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-677><a id=__codelineno-0-677 name=__codelineno-0-677></a>                    <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-678><a id=__codelineno-0-678 name=__codelineno-0-678></a>                <span class=p>),</span>
</span><span id=__span-0-679><a id=__codelineno-0-679 name=__codelineno-0-679></a>                <span class=n>ResidualGatedBlock</span><span class=p>(</span>
</span><span id=__span-0-680><a id=__codelineno-0-680 name=__codelineno-0-680></a>                    <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span><span id=__span-0-681><a id=__codelineno-0-681 name=__codelineno-0-681></a>                    <span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-682><a id=__codelineno-0-682 name=__codelineno-0-682></a>                    <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-683><a id=__codelineno-0-683 name=__codelineno-0-683></a>                    <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-684><a id=__codelineno-0-684 name=__codelineno-0-684></a>                    <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-685><a id=__codelineno-0-685 name=__codelineno-0-685></a>                    <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-686><a id=__codelineno-0-686 name=__codelineno-0-686></a>                    <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-687><a id=__codelineno-0-687 name=__codelineno-0-687></a>                    <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-688><a id=__codelineno-0-688 name=__codelineno-0-688></a>                <span class=p>),</span>
</span><span id=__span-0-689><a id=__codelineno-0-689 name=__codelineno-0-689></a>            <span class=p>)</span>
</span><span id=__span-0-690><a id=__codelineno-0-690 name=__codelineno-0-690></a>        <span class=k>elif</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>:</span>
</span><span id=__span-0-691><a id=__codelineno-0-691 name=__codelineno-0-691></a>            <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-0-692><a id=__codelineno-0-692 name=__codelineno-0-692></a>                <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-693><a id=__codelineno-0-693 name=__codelineno-0-693></a>                    <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-694><a id=__codelineno-0-694 name=__codelineno-0-694></a>                <span class=p>),</span>
</span><span id=__span-0-695><a id=__codelineno-0-695 name=__codelineno-0-695></a>                <span class=n>ResidualBlock</span><span class=p>(</span>
</span><span id=__span-0-696><a id=__codelineno-0-696 name=__codelineno-0-696></a>                    <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span><span id=__span-0-697><a id=__codelineno-0-697 name=__codelineno-0-697></a>                    <span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-698><a id=__codelineno-0-698 name=__codelineno-0-698></a>                    <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-699><a id=__codelineno-0-699 name=__codelineno-0-699></a>                    <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-700><a id=__codelineno-0-700 name=__codelineno-0-700></a>                    <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-701><a id=__codelineno-0-701 name=__codelineno-0-701></a>                    <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-702><a id=__codelineno-0-702 name=__codelineno-0-702></a>                    <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-703><a id=__codelineno-0-703 name=__codelineno-0-703></a>                    <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-704><a id=__codelineno-0-704 name=__codelineno-0-704></a>                <span class=p>),</span>
</span><span id=__span-0-705><a id=__codelineno-0-705 name=__codelineno-0-705></a>            <span class=p>)</span>
</span><span id=__span-0-706><a id=__codelineno-0-706 name=__codelineno-0-706></a>
</span><span id=__span-0-707><a id=__codelineno-0-707 name=__codelineno-0-707></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-708><a id=__codelineno-0-708 name=__codelineno-0-708></a>
</span><span id=__span-0-709><a id=__codelineno-0-709 name=__codelineno-0-709></a>        <span class=c1># Concatenate the input tensors along dim=1</span>
</span><span id=__span-0-710><a id=__codelineno-0-710 name=__codelineno-0-710></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>args</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-711><a id=__codelineno-0-711 name=__codelineno-0-711></a>
</span><span id=__span-0-712><a id=__codelineno-0-712 name=__codelineno-0-712></a>        <span class=c1># Pass the concatenated tensor through the conv layer</span>
</span><span id=__span-0-713><a id=__codelineno-0-713 name=__codelineno-0-713></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-714><a id=__codelineno-0-714 name=__codelineno-0-714></a>
</span><span id=__span-0-715><a id=__codelineno-0-715 name=__codelineno-0-715></a>        <span class=k>return</span> <span class=n>x</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.MergeLayer.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>merge_type</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>nonlin</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span> <span class=n>batchnorm</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.MergeLayer.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>merge_type</code></td> <td> <code><span title="typing.Literal">Literal</span>[&#39;linear&#39;, &#39;residual&#39;, &#39;residual_ungated&#39;]</code> </td> <td> <div class=doc-md-description> <p>The type of merge done in the layer. It can be chosen between "linear", "residual", and "residual_ungated". Check the class docstring for more information about the behaviour of different merge modalities.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>channels</code></td> <td> <code><span title="typing.Union">Union</span>[int, <span title="typing.Iterable">Iterable</span>[int]]</code> </td> <td> <div class=doc-md-description> <p>The number of channels used in the convolutional blocks of this layer. If it is an <code>int</code>: - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels - (Optional) ResBlock: in_channels=channels, out_channels=channels If it is an Iterable (must have <code>len(channels)==3</code>): - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1] - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>nonlin</code></td> <td> <code><span title="typing.Callable">Callable</span></code> </td> <td> <div class=doc-md-description> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> </div> </td> <td> <code><span title="torch.nn.LeakyReLU">LeakyReLU</span></code> </td> </tr> <tr class=doc-section-item> <td><code>batchnorm</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>dropout</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_type</code></td> <td> <code>str</code> </td> <td> <div class=doc-md-description> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_kernel</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_skip_padding</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to skip padding in convolutions in the Residual block. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>conv2d_bias</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-611>611</a></span>
<span class=normal><a href=#__codelineno-0-612>612</a></span>
<span class=normal><a href=#__codelineno-0-613>613</a></span>
<span class=normal><a href=#__codelineno-0-614>614</a></span>
<span class=normal><a href=#__codelineno-0-615>615</a></span>
<span class=normal><a href=#__codelineno-0-616>616</a></span>
<span class=normal><a href=#__codelineno-0-617>617</a></span>
<span class=normal><a href=#__codelineno-0-618>618</a></span>
<span class=normal><a href=#__codelineno-0-619>619</a></span>
<span class=normal><a href=#__codelineno-0-620>620</a></span>
<span class=normal><a href=#__codelineno-0-621>621</a></span>
<span class=normal><a href=#__codelineno-0-622>622</a></span>
<span class=normal><a href=#__codelineno-0-623>623</a></span>
<span class=normal><a href=#__codelineno-0-624>624</a></span>
<span class=normal><a href=#__codelineno-0-625>625</a></span>
<span class=normal><a href=#__codelineno-0-626>626</a></span>
<span class=normal><a href=#__codelineno-0-627>627</a></span>
<span class=normal><a href=#__codelineno-0-628>628</a></span>
<span class=normal><a href=#__codelineno-0-629>629</a></span>
<span class=normal><a href=#__codelineno-0-630>630</a></span>
<span class=normal><a href=#__codelineno-0-631>631</a></span>
<span class=normal><a href=#__codelineno-0-632>632</a></span>
<span class=normal><a href=#__codelineno-0-633>633</a></span>
<span class=normal><a href=#__codelineno-0-634>634</a></span>
<span class=normal><a href=#__codelineno-0-635>635</a></span>
<span class=normal><a href=#__codelineno-0-636>636</a></span>
<span class=normal><a href=#__codelineno-0-637>637</a></span>
<span class=normal><a href=#__codelineno-0-638>638</a></span>
<span class=normal><a href=#__codelineno-0-639>639</a></span>
<span class=normal><a href=#__codelineno-0-640>640</a></span>
<span class=normal><a href=#__codelineno-0-641>641</a></span>
<span class=normal><a href=#__codelineno-0-642>642</a></span>
<span class=normal><a href=#__codelineno-0-643>643</a></span>
<span class=normal><a href=#__codelineno-0-644>644</a></span>
<span class=normal><a href=#__codelineno-0-645>645</a></span>
<span class=normal><a href=#__codelineno-0-646>646</a></span>
<span class=normal><a href=#__codelineno-0-647>647</a></span>
<span class=normal><a href=#__codelineno-0-648>648</a></span>
<span class=normal><a href=#__codelineno-0-649>649</a></span>
<span class=normal><a href=#__codelineno-0-650>650</a></span>
<span class=normal><a href=#__codelineno-0-651>651</a></span>
<span class=normal><a href=#__codelineno-0-652>652</a></span>
<span class=normal><a href=#__codelineno-0-653>653</a></span>
<span class=normal><a href=#__codelineno-0-654>654</a></span>
<span class=normal><a href=#__codelineno-0-655>655</a></span>
<span class=normal><a href=#__codelineno-0-656>656</a></span>
<span class=normal><a href=#__codelineno-0-657>657</a></span>
<span class=normal><a href=#__codelineno-0-658>658</a></span>
<span class=normal><a href=#__codelineno-0-659>659</a></span>
<span class=normal><a href=#__codelineno-0-660>660</a></span>
<span class=normal><a href=#__codelineno-0-661>661</a></span>
<span class=normal><a href=#__codelineno-0-662>662</a></span>
<span class=normal><a href=#__codelineno-0-663>663</a></span>
<span class=normal><a href=#__codelineno-0-664>664</a></span>
<span class=normal><a href=#__codelineno-0-665>665</a></span>
<span class=normal><a href=#__codelineno-0-666>666</a></span>
<span class=normal><a href=#__codelineno-0-667>667</a></span>
<span class=normal><a href=#__codelineno-0-668>668</a></span>
<span class=normal><a href=#__codelineno-0-669>669</a></span>
<span class=normal><a href=#__codelineno-0-670>670</a></span>
<span class=normal><a href=#__codelineno-0-671>671</a></span>
<span class=normal><a href=#__codelineno-0-672>672</a></span>
<span class=normal><a href=#__codelineno-0-673>673</a></span>
<span class=normal><a href=#__codelineno-0-674>674</a></span>
<span class=normal><a href=#__codelineno-0-675>675</a></span>
<span class=normal><a href=#__codelineno-0-676>676</a></span>
<span class=normal><a href=#__codelineno-0-677>677</a></span>
<span class=normal><a href=#__codelineno-0-678>678</a></span>
<span class=normal><a href=#__codelineno-0-679>679</a></span>
<span class=normal><a href=#__codelineno-0-680>680</a></span>
<span class=normal><a href=#__codelineno-0-681>681</a></span>
<span class=normal><a href=#__codelineno-0-682>682</a></span>
<span class=normal><a href=#__codelineno-0-683>683</a></span>
<span class=normal><a href=#__codelineno-0-684>684</a></span>
<span class=normal><a href=#__codelineno-0-685>685</a></span>
<span class=normal><a href=#__codelineno-0-686>686</a></span>
<span class=normal><a href=#__codelineno-0-687>687</a></span>
<span class=normal><a href=#__codelineno-0-688>688</a></span>
<span class=normal><a href=#__codelineno-0-689>689</a></span>
<span class=normal><a href=#__codelineno-0-690>690</a></span>
<span class=normal><a href=#__codelineno-0-691>691</a></span>
<span class=normal><a href=#__codelineno-0-692>692</a></span>
<span class=normal><a href=#__codelineno-0-693>693</a></span>
<span class=normal><a href=#__codelineno-0-694>694</a></span>
<span class=normal><a href=#__codelineno-0-695>695</a></span>
<span class=normal><a href=#__codelineno-0-696>696</a></span>
<span class=normal><a href=#__codelineno-0-697>697</a></span>
<span class=normal><a href=#__codelineno-0-698>698</a></span>
<span class=normal><a href=#__codelineno-0-699>699</a></span>
<span class=normal><a href=#__codelineno-0-700>700</a></span>
<span class=normal><a href=#__codelineno-0-701>701</a></span>
<span class=normal><a href=#__codelineno-0-702>702</a></span>
<span class=normal><a href=#__codelineno-0-703>703</a></span>
<span class=normal><a href=#__codelineno-0-704>704</a></span>
<span class=normal><a href=#__codelineno-0-705>705</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-611><a id=__codelineno-0-611 name=__codelineno-0-611></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-612><a id=__codelineno-0-612 name=__codelineno-0-612></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-613><a id=__codelineno-0-613 name=__codelineno-0-613></a>    <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>],</span>
</span><span id=__span-0-614><a id=__codelineno-0-614 name=__codelineno-0-614></a>    <span class=n>channels</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span>
</span><span id=__span-0-615><a id=__codelineno-0-615 name=__codelineno-0-615></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span>
</span><span id=__span-0-616><a id=__codelineno-0-616 name=__codelineno-0-616></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-617><a id=__codelineno-0-617 name=__codelineno-0-617></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-618><a id=__codelineno-0-618 name=__codelineno-0-618></a>    <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-619><a id=__codelineno-0-619 name=__codelineno-0-619></a>    <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-620><a id=__codelineno-0-620 name=__codelineno-0-620></a>    <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-621><a id=__codelineno-0-621 name=__codelineno-0-621></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-622><a id=__codelineno-0-622 name=__codelineno-0-622></a><span class=p>):</span>
</span><span id=__span-0-623><a id=__codelineno-0-623 name=__codelineno-0-623></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-624><a id=__codelineno-0-624 name=__codelineno-0-624></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-625><a id=__codelineno-0-625 name=__codelineno-0-625></a>
</span><span id=__span-0-626><a id=__codelineno-0-626 name=__codelineno-0-626></a><span class=sd>    Parameters</span>
</span><span id=__span-0-627><a id=__codelineno-0-627 name=__codelineno-0-627></a><span class=sd>    ----------</span>
</span><span id=__span-0-628><a id=__codelineno-0-628 name=__codelineno-0-628></a><span class=sd>    merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;]</span>
</span><span id=__span-0-629><a id=__codelineno-0-629 name=__codelineno-0-629></a><span class=sd>        The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;, and &quot;residual_ungated&quot;.</span>
</span><span id=__span-0-630><a id=__codelineno-0-630 name=__codelineno-0-630></a><span class=sd>        Check the class docstring for more information about the behaviour of different merge modalities.</span>
</span><span id=__span-0-631><a id=__codelineno-0-631 name=__codelineno-0-631></a><span class=sd>    channels: Union[int, Iterable[int]]</span>
</span><span id=__span-0-632><a id=__codelineno-0-632 name=__codelineno-0-632></a><span class=sd>        The number of channels used in the convolutional blocks of this layer.</span>
</span><span id=__span-0-633><a id=__codelineno-0-633 name=__codelineno-0-633></a><span class=sd>        If it is an `int`:</span>
</span><span id=__span-0-634><a id=__codelineno-0-634 name=__codelineno-0-634></a><span class=sd>            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels</span>
</span><span id=__span-0-635><a id=__codelineno-0-635 name=__codelineno-0-635></a><span class=sd>            - (Optional) ResBlock: in_channels=channels, out_channels=channels</span>
</span><span id=__span-0-636><a id=__codelineno-0-636 name=__codelineno-0-636></a><span class=sd>        If it is an Iterable (must have `len(channels)==3`):</span>
</span><span id=__span-0-637><a id=__codelineno-0-637 name=__codelineno-0-637></a><span class=sd>            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]</span>
</span><span id=__span-0-638><a id=__codelineno-0-638 name=__codelineno-0-638></a><span class=sd>            - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]</span>
</span><span id=__span-0-639><a id=__codelineno-0-639 name=__codelineno-0-639></a><span class=sd>    nonlin: Callable, optional</span>
</span><span id=__span-0-640><a id=__codelineno-0-640 name=__codelineno-0-640></a><span class=sd>        The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-641><a id=__codelineno-0-641 name=__codelineno-0-641></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-642><a id=__codelineno-0-642 name=__codelineno-0-642></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-643><a id=__codelineno-0-643 name=__codelineno-0-643></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-644><a id=__codelineno-0-644 name=__codelineno-0-644></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-645><a id=__codelineno-0-645 name=__codelineno-0-645></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-646><a id=__codelineno-0-646 name=__codelineno-0-646></a><span class=sd>    res_block_type: str, optional</span>
</span><span id=__span-0-647><a id=__codelineno-0-647 name=__codelineno-0-647></a><span class=sd>        A string specifying the structure of residual block.</span>
</span><span id=__span-0-648><a id=__codelineno-0-648 name=__codelineno-0-648></a><span class=sd>        Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-649><a id=__codelineno-0-649 name=__codelineno-0-649></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-650><a id=__codelineno-0-650 name=__codelineno-0-650></a><span class=sd>    res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-651><a id=__codelineno-0-651 name=__codelineno-0-651></a><span class=sd>        The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-652><a id=__codelineno-0-652 name=__codelineno-0-652></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-653><a id=__codelineno-0-653 name=__codelineno-0-653></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-654><a id=__codelineno-0-654 name=__codelineno-0-654></a><span class=sd>    res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-655><a id=__codelineno-0-655 name=__codelineno-0-655></a><span class=sd>        Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-656><a id=__codelineno-0-656 name=__codelineno-0-656></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-657><a id=__codelineno-0-657 name=__codelineno-0-657></a><span class=sd>        Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-658><a id=__codelineno-0-658 name=__codelineno-0-658></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-659><a id=__codelineno-0-659 name=__codelineno-0-659></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-660><a id=__codelineno-0-660 name=__codelineno-0-660></a>    <span class=k>try</span><span class=p>:</span>
</span><span id=__span-0-661><a id=__codelineno-0-661 name=__codelineno-0-661></a>        <span class=nb>iter</span><span class=p>(</span><span class=n>channels</span><span class=p>)</span>
</span><span id=__span-0-662><a id=__codelineno-0-662 name=__codelineno-0-662></a>    <span class=k>except</span> <span class=ne>TypeError</span><span class=p>:</span>  <span class=c1># it is not iterable</span>
</span><span id=__span-0-663><a id=__codelineno-0-663 name=__codelineno-0-663></a>        <span class=n>channels</span> <span class=o>=</span> <span class=p>[</span><span class=n>channels</span><span class=p>]</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-664><a id=__codelineno-0-664 name=__codelineno-0-664></a>    <span class=k>else</span><span class=p>:</span>  <span class=c1># it is iterable</span>
</span><span id=__span-0-665><a id=__codelineno-0-665 name=__codelineno-0-665></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>channels</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span>
</span><span id=__span-0-666><a id=__codelineno-0-666 name=__codelineno-0-666></a>            <span class=n>channels</span> <span class=o>=</span> <span class=p>[</span><span class=n>channels</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-667><a id=__codelineno-0-667 name=__codelineno-0-667></a>
</span><span id=__span-0-668><a id=__codelineno-0-668 name=__codelineno-0-668></a>    <span class=c1># assert len(channels) == 3</span>
</span><span id=__span-0-669><a id=__codelineno-0-669 name=__codelineno-0-669></a>
</span><span id=__span-0-670><a id=__codelineno-0-670 name=__codelineno-0-670></a>    <span class=k>if</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;linear&quot;</span><span class=p>:</span>
</span><span id=__span-0-671><a id=__codelineno-0-671 name=__codelineno-0-671></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-672><a id=__codelineno-0-672 name=__codelineno-0-672></a>            <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-673><a id=__codelineno-0-673 name=__codelineno-0-673></a>        <span class=p>)</span>
</span><span id=__span-0-674><a id=__codelineno-0-674 name=__codelineno-0-674></a>    <span class=k>elif</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;residual&quot;</span><span class=p>:</span>
</span><span id=__span-0-675><a id=__codelineno-0-675 name=__codelineno-0-675></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-0-676><a id=__codelineno-0-676 name=__codelineno-0-676></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-677><a id=__codelineno-0-677 name=__codelineno-0-677></a>                <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-678><a id=__codelineno-0-678 name=__codelineno-0-678></a>            <span class=p>),</span>
</span><span id=__span-0-679><a id=__codelineno-0-679 name=__codelineno-0-679></a>            <span class=n>ResidualGatedBlock</span><span class=p>(</span>
</span><span id=__span-0-680><a id=__codelineno-0-680 name=__codelineno-0-680></a>                <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span><span id=__span-0-681><a id=__codelineno-0-681 name=__codelineno-0-681></a>                <span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-682><a id=__codelineno-0-682 name=__codelineno-0-682></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-683><a id=__codelineno-0-683 name=__codelineno-0-683></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-684><a id=__codelineno-0-684 name=__codelineno-0-684></a>                <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-685><a id=__codelineno-0-685 name=__codelineno-0-685></a>                <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-686><a id=__codelineno-0-686 name=__codelineno-0-686></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-687><a id=__codelineno-0-687 name=__codelineno-0-687></a>                <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-688><a id=__codelineno-0-688 name=__codelineno-0-688></a>            <span class=p>),</span>
</span><span id=__span-0-689><a id=__codelineno-0-689 name=__codelineno-0-689></a>        <span class=p>)</span>
</span><span id=__span-0-690><a id=__codelineno-0-690 name=__codelineno-0-690></a>    <span class=k>elif</span> <span class=n>merge_type</span> <span class=o>==</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>:</span>
</span><span id=__span-0-691><a id=__codelineno-0-691 name=__codelineno-0-691></a>        <span class=bp>self</span><span class=o>.</span><span class=n>layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span><span id=__span-0-692><a id=__codelineno-0-692 name=__codelineno-0-692></a>            <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-693><a id=__codelineno-0-693 name=__codelineno-0-693></a>                <span class=nb>sum</span><span class=p>(</span><span class=n>channels</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]),</span> <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-694><a id=__codelineno-0-694 name=__codelineno-0-694></a>            <span class=p>),</span>
</span><span id=__span-0-695><a id=__codelineno-0-695 name=__codelineno-0-695></a>            <span class=n>ResidualBlock</span><span class=p>(</span>
</span><span id=__span-0-696><a id=__codelineno-0-696 name=__codelineno-0-696></a>                <span class=n>channels</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span>
</span><span id=__span-0-697><a id=__codelineno-0-697 name=__codelineno-0-697></a>                <span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-698><a id=__codelineno-0-698 name=__codelineno-0-698></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-699><a id=__codelineno-0-699 name=__codelineno-0-699></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-700><a id=__codelineno-0-700 name=__codelineno-0-700></a>                <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-701><a id=__codelineno-0-701 name=__codelineno-0-701></a>                <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-702><a id=__codelineno-0-702 name=__codelineno-0-702></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-703><a id=__codelineno-0-703 name=__codelineno-0-703></a>                <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-704><a id=__codelineno-0-704 name=__codelineno-0-704></a>            <span class=p>),</span>
</span><span id=__span-0-705><a id=__codelineno-0-705 name=__codelineno-0-705></a>        <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.MergeLowRes class="doc doc-heading"> <code>MergeLowRes</code> <a href=#careamics.models.lvae.layers.MergeLowRes class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title="careamics.models.lvae.layers.MergeLayer" href="#careamics.models.lvae.layers.MergeLayer">MergeLayer</a></code></p> <p>Child class of <code>MergeLayer</code>, specifically designed to merge the low-resolution patches that are used in Lateral Contextualization approach.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-718>718</a></span>
<span class=normal><a href=#__codelineno-0-719>719</a></span>
<span class=normal><a href=#__codelineno-0-720>720</a></span>
<span class=normal><a href=#__codelineno-0-721>721</a></span>
<span class=normal><a href=#__codelineno-0-722>722</a></span>
<span class=normal><a href=#__codelineno-0-723>723</a></span>
<span class=normal><a href=#__codelineno-0-724>724</a></span>
<span class=normal><a href=#__codelineno-0-725>725</a></span>
<span class=normal><a href=#__codelineno-0-726>726</a></span>
<span class=normal><a href=#__codelineno-0-727>727</a></span>
<span class=normal><a href=#__codelineno-0-728>728</a></span>
<span class=normal><a href=#__codelineno-0-729>729</a></span>
<span class=normal><a href=#__codelineno-0-730>730</a></span>
<span class=normal><a href=#__codelineno-0-731>731</a></span>
<span class=normal><a href=#__codelineno-0-732>732</a></span>
<span class=normal><a href=#__codelineno-0-733>733</a></span>
<span class=normal><a href=#__codelineno-0-734>734</a></span>
<span class=normal><a href=#__codelineno-0-735>735</a></span>
<span class=normal><a href=#__codelineno-0-736>736</a></span>
<span class=normal><a href=#__codelineno-0-737>737</a></span>
<span class=normal><a href=#__codelineno-0-738>738</a></span>
<span class=normal><a href=#__codelineno-0-739>739</a></span>
<span class=normal><a href=#__codelineno-0-740>740</a></span>
<span class=normal><a href=#__codelineno-0-741>741</a></span>
<span class=normal><a href=#__codelineno-0-742>742</a></span>
<span class=normal><a href=#__codelineno-0-743>743</a></span>
<span class=normal><a href=#__codelineno-0-744>744</a></span>
<span class=normal><a href=#__codelineno-0-745>745</a></span>
<span class=normal><a href=#__codelineno-0-746>746</a></span>
<span class=normal><a href=#__codelineno-0-747>747</a></span>
<span class=normal><a href=#__codelineno-0-748>748</a></span>
<span class=normal><a href=#__codelineno-0-749>749</a></span>
<span class=normal><a href=#__codelineno-0-750>750</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-718><a id=__codelineno-0-718 name=__codelineno-0-718></a><span class=k>class</span> <span class=nc>MergeLowRes</span><span class=p>(</span><span class=n>MergeLayer</span><span class=p>):</span>
</span><span id=__span-0-719><a id=__codelineno-0-719 name=__codelineno-0-719></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-720><a id=__codelineno-0-720 name=__codelineno-0-720></a><span class=sd>    Child class of `MergeLayer`, specifically designed to merge the low-resolution patches</span>
</span><span id=__span-0-721><a id=__codelineno-0-721 name=__codelineno-0-721></a><span class=sd>    that are used in Lateral Contextualization approach.</span>
</span><span id=__span-0-722><a id=__codelineno-0-722 name=__codelineno-0-722></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-723><a id=__codelineno-0-723 name=__codelineno-0-723></a>
</span><span id=__span-0-724><a id=__codelineno-0-724 name=__codelineno-0-724></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span><span id=__span-0-725><a id=__codelineno-0-725 name=__codelineno-0-725></a>        <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span> <span class=o>=</span> <span class=n>kwargs</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;multiscale_retain_spatial_dims&quot;</span><span class=p>)</span>
</span><span id=__span-0-726><a id=__codelineno-0-726 name=__codelineno-0-726></a>        <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span> <span class=o>=</span> <span class=n>kwargs</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=s2>&quot;multiscale_lowres_size_factor&quot;</span><span class=p>)</span>
</span><span id=__span-0-727><a id=__codelineno-0-727 name=__codelineno-0-727></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span><span id=__span-0-728><a id=__codelineno-0-728 name=__codelineno-0-728></a>
</span><span id=__span-0-729><a id=__codelineno-0-729 name=__codelineno-0-729></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>lowres</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-730><a id=__codelineno-0-730 name=__codelineno-0-730></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-731><a id=__codelineno-0-731 name=__codelineno-0-731></a><span class=sd>        Parameters</span>
</span><span id=__span-0-732><a id=__codelineno-0-732 name=__codelineno-0-732></a><span class=sd>        ----------</span>
</span><span id=__span-0-733><a id=__codelineno-0-733 name=__codelineno-0-733></a><span class=sd>        latent: torch.Tensor</span>
</span><span id=__span-0-734><a id=__codelineno-0-734 name=__codelineno-0-734></a><span class=sd>            The output latent tensor from previous layer in the LVAE hierarchy.</span>
</span><span id=__span-0-735><a id=__codelineno-0-735 name=__codelineno-0-735></a><span class=sd>        lowres: torch.Tensor</span>
</span><span id=__span-0-736><a id=__codelineno-0-736 name=__codelineno-0-736></a><span class=sd>            The low-res patch image to be merged to increase the context.</span>
</span><span id=__span-0-737><a id=__codelineno-0-737 name=__codelineno-0-737></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-738><a id=__codelineno-0-738 name=__codelineno-0-738></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span><span class=p>:</span>
</span><span id=__span-0-739><a id=__codelineno-0-739 name=__codelineno-0-739></a>            <span class=c1># Pad latent tensor to match lowres tensor&#39;s shape</span>
</span><span id=__span-0-740><a id=__codelineno-0-740 name=__codelineno-0-740></a>            <span class=n>latent</span> <span class=o>=</span> <span class=n>pad_img_tensor</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>lowres</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-741><a id=__codelineno-0-741 name=__codelineno-0-741></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-742><a id=__codelineno-0-742 name=__codelineno-0-742></a>            <span class=c1># Crop lowres tensor to match latent tensor&#39;s shape</span>
</span><span id=__span-0-743><a id=__codelineno-0-743 name=__codelineno-0-743></a>            <span class=n>lh</span><span class=p>,</span> <span class=n>lw</span> <span class=o>=</span> <span class=n>lowres</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-0-744><a id=__codelineno-0-744 name=__codelineno-0-744></a>            <span class=n>h</span> <span class=o>=</span> <span class=n>lh</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-745><a id=__codelineno-0-745 name=__codelineno-0-745></a>            <span class=n>w</span> <span class=o>=</span> <span class=n>lw</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-746><a id=__codelineno-0-746 name=__codelineno-0-746></a>            <span class=n>h_pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>lh</span> <span class=o>-</span> <span class=n>h</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-747><a id=__codelineno-0-747 name=__codelineno-0-747></a>            <span class=n>w_pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>lw</span> <span class=o>-</span> <span class=n>w</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-748><a id=__codelineno-0-748 name=__codelineno-0-748></a>            <span class=n>lowres</span> <span class=o>=</span> <span class=n>lowres</span><span class=p>[:,</span> <span class=p>:,</span> <span class=n>h_pad</span><span class=p>:</span><span class=o>-</span><span class=n>h_pad</span><span class=p>,</span> <span class=n>w_pad</span><span class=p>:</span><span class=o>-</span><span class=n>w_pad</span><span class=p>]</span>
</span><span id=__span-0-749><a id=__codelineno-0-749 name=__codelineno-0-749></a>
</span><span id=__span-0-750><a id=__codelineno-0-750 name=__codelineno-0-750></a>        <span class=k>return</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>lowres</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.MergeLowRes.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>lowres</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.MergeLowRes.forward class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>latent</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The output latent tensor from previous layer in the LVAE hierarchy.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>lowres</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The low-res patch image to be merged to increase the context.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-729>729</a></span>
<span class=normal><a href=#__codelineno-0-730>730</a></span>
<span class=normal><a href=#__codelineno-0-731>731</a></span>
<span class=normal><a href=#__codelineno-0-732>732</a></span>
<span class=normal><a href=#__codelineno-0-733>733</a></span>
<span class=normal><a href=#__codelineno-0-734>734</a></span>
<span class=normal><a href=#__codelineno-0-735>735</a></span>
<span class=normal><a href=#__codelineno-0-736>736</a></span>
<span class=normal><a href=#__codelineno-0-737>737</a></span>
<span class=normal><a href=#__codelineno-0-738>738</a></span>
<span class=normal><a href=#__codelineno-0-739>739</a></span>
<span class=normal><a href=#__codelineno-0-740>740</a></span>
<span class=normal><a href=#__codelineno-0-741>741</a></span>
<span class=normal><a href=#__codelineno-0-742>742</a></span>
<span class=normal><a href=#__codelineno-0-743>743</a></span>
<span class=normal><a href=#__codelineno-0-744>744</a></span>
<span class=normal><a href=#__codelineno-0-745>745</a></span>
<span class=normal><a href=#__codelineno-0-746>746</a></span>
<span class=normal><a href=#__codelineno-0-747>747</a></span>
<span class=normal><a href=#__codelineno-0-748>748</a></span>
<span class=normal><a href=#__codelineno-0-749>749</a></span>
<span class=normal><a href=#__codelineno-0-750>750</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-729><a id=__codelineno-0-729 name=__codelineno-0-729></a><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>lowres</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-730><a id=__codelineno-0-730 name=__codelineno-0-730></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-731><a id=__codelineno-0-731 name=__codelineno-0-731></a><span class=sd>    Parameters</span>
</span><span id=__span-0-732><a id=__codelineno-0-732 name=__codelineno-0-732></a><span class=sd>    ----------</span>
</span><span id=__span-0-733><a id=__codelineno-0-733 name=__codelineno-0-733></a><span class=sd>    latent: torch.Tensor</span>
</span><span id=__span-0-734><a id=__codelineno-0-734 name=__codelineno-0-734></a><span class=sd>        The output latent tensor from previous layer in the LVAE hierarchy.</span>
</span><span id=__span-0-735><a id=__codelineno-0-735 name=__codelineno-0-735></a><span class=sd>    lowres: torch.Tensor</span>
</span><span id=__span-0-736><a id=__codelineno-0-736 name=__codelineno-0-736></a><span class=sd>        The low-res patch image to be merged to increase the context.</span>
</span><span id=__span-0-737><a id=__codelineno-0-737 name=__codelineno-0-737></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-738><a id=__codelineno-0-738 name=__codelineno-0-738></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span><span class=p>:</span>
</span><span id=__span-0-739><a id=__codelineno-0-739 name=__codelineno-0-739></a>        <span class=c1># Pad latent tensor to match lowres tensor&#39;s shape</span>
</span><span id=__span-0-740><a id=__codelineno-0-740 name=__codelineno-0-740></a>        <span class=n>latent</span> <span class=o>=</span> <span class=n>pad_img_tensor</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>lowres</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-741><a id=__codelineno-0-741 name=__codelineno-0-741></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-742><a id=__codelineno-0-742 name=__codelineno-0-742></a>        <span class=c1># Crop lowres tensor to match latent tensor&#39;s shape</span>
</span><span id=__span-0-743><a id=__codelineno-0-743 name=__codelineno-0-743></a>        <span class=n>lh</span><span class=p>,</span> <span class=n>lw</span> <span class=o>=</span> <span class=n>lowres</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-0-744><a id=__codelineno-0-744 name=__codelineno-0-744></a>        <span class=n>h</span> <span class=o>=</span> <span class=n>lh</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-745><a id=__codelineno-0-745 name=__codelineno-0-745></a>        <span class=n>w</span> <span class=o>=</span> <span class=n>lw</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>multiscale_lowres_size_factor</span>
</span><span id=__span-0-746><a id=__codelineno-0-746 name=__codelineno-0-746></a>        <span class=n>h_pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>lh</span> <span class=o>-</span> <span class=n>h</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-747><a id=__codelineno-0-747 name=__codelineno-0-747></a>        <span class=n>w_pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>lw</span> <span class=o>-</span> <span class=n>w</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-748><a id=__codelineno-0-748 name=__codelineno-0-748></a>        <span class=n>lowres</span> <span class=o>=</span> <span class=n>lowres</span><span class=p>[:,</span> <span class=p>:,</span> <span class=n>h_pad</span><span class=p>:</span><span class=o>-</span><span class=n>h_pad</span><span class=p>,</span> <span class=n>w_pad</span><span class=p>:</span><span class=o>-</span><span class=n>w_pad</span><span class=p>]</span>
</span><span id=__span-0-749><a id=__codelineno-0-749 name=__codelineno-0-749></a>
</span><span id=__span-0-750><a id=__codelineno-0-750 name=__codelineno-0-750></a>    <span class=k>return</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>latent</span><span class=p>,</span> <span class=n>lowres</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.NonStochasticBlock2d class="doc doc-heading"> <code>NonStochasticBlock2d</code> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Non-stochastic version of the NormalStochasticBlock2d.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1776>1776</a></span>
<span class=normal><a href=#__codelineno-0-1777>1777</a></span>
<span class=normal><a href=#__codelineno-0-1778>1778</a></span>
<span class=normal><a href=#__codelineno-0-1779>1779</a></span>
<span class=normal><a href=#__codelineno-0-1780>1780</a></span>
<span class=normal><a href=#__codelineno-0-1781>1781</a></span>
<span class=normal><a href=#__codelineno-0-1782>1782</a></span>
<span class=normal><a href=#__codelineno-0-1783>1783</a></span>
<span class=normal><a href=#__codelineno-0-1784>1784</a></span>
<span class=normal><a href=#__codelineno-0-1785>1785</a></span>
<span class=normal><a href=#__codelineno-0-1786>1786</a></span>
<span class=normal><a href=#__codelineno-0-1787>1787</a></span>
<span class=normal><a href=#__codelineno-0-1788>1788</a></span>
<span class=normal><a href=#__codelineno-0-1789>1789</a></span>
<span class=normal><a href=#__codelineno-0-1790>1790</a></span>
<span class=normal><a href=#__codelineno-0-1791>1791</a></span>
<span class=normal><a href=#__codelineno-0-1792>1792</a></span>
<span class=normal><a href=#__codelineno-0-1793>1793</a></span>
<span class=normal><a href=#__codelineno-0-1794>1794</a></span>
<span class=normal><a href=#__codelineno-0-1795>1795</a></span>
<span class=normal><a href=#__codelineno-0-1796>1796</a></span>
<span class=normal><a href=#__codelineno-0-1797>1797</a></span>
<span class=normal><a href=#__codelineno-0-1798>1798</a></span>
<span class=normal><a href=#__codelineno-0-1799>1799</a></span>
<span class=normal><a href=#__codelineno-0-1800>1800</a></span>
<span class=normal><a href=#__codelineno-0-1801>1801</a></span>
<span class=normal><a href=#__codelineno-0-1802>1802</a></span>
<span class=normal><a href=#__codelineno-0-1803>1803</a></span>
<span class=normal><a href=#__codelineno-0-1804>1804</a></span>
<span class=normal><a href=#__codelineno-0-1805>1805</a></span>
<span class=normal><a href=#__codelineno-0-1806>1806</a></span>
<span class=normal><a href=#__codelineno-0-1807>1807</a></span>
<span class=normal><a href=#__codelineno-0-1808>1808</a></span>
<span class=normal><a href=#__codelineno-0-1809>1809</a></span>
<span class=normal><a href=#__codelineno-0-1810>1810</a></span>
<span class=normal><a href=#__codelineno-0-1811>1811</a></span>
<span class=normal><a href=#__codelineno-0-1812>1812</a></span>
<span class=normal><a href=#__codelineno-0-1813>1813</a></span>
<span class=normal><a href=#__codelineno-0-1814>1814</a></span>
<span class=normal><a href=#__codelineno-0-1815>1815</a></span>
<span class=normal><a href=#__codelineno-0-1816>1816</a></span>
<span class=normal><a href=#__codelineno-0-1817>1817</a></span>
<span class=normal><a href=#__codelineno-0-1818>1818</a></span>
<span class=normal><a href=#__codelineno-0-1819>1819</a></span>
<span class=normal><a href=#__codelineno-0-1820>1820</a></span>
<span class=normal><a href=#__codelineno-0-1821>1821</a></span>
<span class=normal><a href=#__codelineno-0-1822>1822</a></span>
<span class=normal><a href=#__codelineno-0-1823>1823</a></span>
<span class=normal><a href=#__codelineno-0-1824>1824</a></span>
<span class=normal><a href=#__codelineno-0-1825>1825</a></span>
<span class=normal><a href=#__codelineno-0-1826>1826</a></span>
<span class=normal><a href=#__codelineno-0-1827>1827</a></span>
<span class=normal><a href=#__codelineno-0-1828>1828</a></span>
<span class=normal><a href=#__codelineno-0-1829>1829</a></span>
<span class=normal><a href=#__codelineno-0-1830>1830</a></span>
<span class=normal><a href=#__codelineno-0-1831>1831</a></span>
<span class=normal><a href=#__codelineno-0-1832>1832</a></span>
<span class=normal><a href=#__codelineno-0-1833>1833</a></span>
<span class=normal><a href=#__codelineno-0-1834>1834</a></span>
<span class=normal><a href=#__codelineno-0-1835>1835</a></span>
<span class=normal><a href=#__codelineno-0-1836>1836</a></span>
<span class=normal><a href=#__codelineno-0-1837>1837</a></span>
<span class=normal><a href=#__codelineno-0-1838>1838</a></span>
<span class=normal><a href=#__codelineno-0-1839>1839</a></span>
<span class=normal><a href=#__codelineno-0-1840>1840</a></span>
<span class=normal><a href=#__codelineno-0-1841>1841</a></span>
<span class=normal><a href=#__codelineno-0-1842>1842</a></span>
<span class=normal><a href=#__codelineno-0-1843>1843</a></span>
<span class=normal><a href=#__codelineno-0-1844>1844</a></span>
<span class=normal><a href=#__codelineno-0-1845>1845</a></span>
<span class=normal><a href=#__codelineno-0-1846>1846</a></span>
<span class=normal><a href=#__codelineno-0-1847>1847</a></span>
<span class=normal><a href=#__codelineno-0-1848>1848</a></span>
<span class=normal><a href=#__codelineno-0-1849>1849</a></span>
<span class=normal><a href=#__codelineno-0-1850>1850</a></span>
<span class=normal><a href=#__codelineno-0-1851>1851</a></span>
<span class=normal><a href=#__codelineno-0-1852>1852</a></span>
<span class=normal><a href=#__codelineno-0-1853>1853</a></span>
<span class=normal><a href=#__codelineno-0-1854>1854</a></span>
<span class=normal><a href=#__codelineno-0-1855>1855</a></span>
<span class=normal><a href=#__codelineno-0-1856>1856</a></span>
<span class=normal><a href=#__codelineno-0-1857>1857</a></span>
<span class=normal><a href=#__codelineno-0-1858>1858</a></span>
<span class=normal><a href=#__codelineno-0-1859>1859</a></span>
<span class=normal><a href=#__codelineno-0-1860>1860</a></span>
<span class=normal><a href=#__codelineno-0-1861>1861</a></span>
<span class=normal><a href=#__codelineno-0-1862>1862</a></span>
<span class=normal><a href=#__codelineno-0-1863>1863</a></span>
<span class=normal><a href=#__codelineno-0-1864>1864</a></span>
<span class=normal><a href=#__codelineno-0-1865>1865</a></span>
<span class=normal><a href=#__codelineno-0-1866>1866</a></span>
<span class=normal><a href=#__codelineno-0-1867>1867</a></span>
<span class=normal><a href=#__codelineno-0-1868>1868</a></span>
<span class=normal><a href=#__codelineno-0-1869>1869</a></span>
<span class=normal><a href=#__codelineno-0-1870>1870</a></span>
<span class=normal><a href=#__codelineno-0-1871>1871</a></span>
<span class=normal><a href=#__codelineno-0-1872>1872</a></span>
<span class=normal><a href=#__codelineno-0-1873>1873</a></span>
<span class=normal><a href=#__codelineno-0-1874>1874</a></span>
<span class=normal><a href=#__codelineno-0-1875>1875</a></span>
<span class=normal><a href=#__codelineno-0-1876>1876</a></span>
<span class=normal><a href=#__codelineno-0-1877>1877</a></span>
<span class=normal><a href=#__codelineno-0-1878>1878</a></span>
<span class=normal><a href=#__codelineno-0-1879>1879</a></span>
<span class=normal><a href=#__codelineno-0-1880>1880</a></span>
<span class=normal><a href=#__codelineno-0-1881>1881</a></span>
<span class=normal><a href=#__codelineno-0-1882>1882</a></span>
<span class=normal><a href=#__codelineno-0-1883>1883</a></span>
<span class=normal><a href=#__codelineno-0-1884>1884</a></span>
<span class=normal><a href=#__codelineno-0-1885>1885</a></span>
<span class=normal><a href=#__codelineno-0-1886>1886</a></span>
<span class=normal><a href=#__codelineno-0-1887>1887</a></span>
<span class=normal><a href=#__codelineno-0-1888>1888</a></span>
<span class=normal><a href=#__codelineno-0-1889>1889</a></span>
<span class=normal><a href=#__codelineno-0-1890>1890</a></span>
<span class=normal><a href=#__codelineno-0-1891>1891</a></span>
<span class=normal><a href=#__codelineno-0-1892>1892</a></span>
<span class=normal><a href=#__codelineno-0-1893>1893</a></span>
<span class=normal><a href=#__codelineno-0-1894>1894</a></span>
<span class=normal><a href=#__codelineno-0-1895>1895</a></span>
<span class=normal><a href=#__codelineno-0-1896>1896</a></span>
<span class=normal><a href=#__codelineno-0-1897>1897</a></span>
<span class=normal><a href=#__codelineno-0-1898>1898</a></span>
<span class=normal><a href=#__codelineno-0-1899>1899</a></span>
<span class=normal><a href=#__codelineno-0-1900>1900</a></span>
<span class=normal><a href=#__codelineno-0-1901>1901</a></span>
<span class=normal><a href=#__codelineno-0-1902>1902</a></span>
<span class=normal><a href=#__codelineno-0-1903>1903</a></span>
<span class=normal><a href=#__codelineno-0-1904>1904</a></span>
<span class=normal><a href=#__codelineno-0-1905>1905</a></span>
<span class=normal><a href=#__codelineno-0-1906>1906</a></span>
<span class=normal><a href=#__codelineno-0-1907>1907</a></span>
<span class=normal><a href=#__codelineno-0-1908>1908</a></span>
<span class=normal><a href=#__codelineno-0-1909>1909</a></span>
<span class=normal><a href=#__codelineno-0-1910>1910</a></span>
<span class=normal><a href=#__codelineno-0-1911>1911</a></span>
<span class=normal><a href=#__codelineno-0-1912>1912</a></span>
<span class=normal><a href=#__codelineno-0-1913>1913</a></span>
<span class=normal><a href=#__codelineno-0-1914>1914</a></span>
<span class=normal><a href=#__codelineno-0-1915>1915</a></span>
<span class=normal><a href=#__codelineno-0-1916>1916</a></span>
<span class=normal><a href=#__codelineno-0-1917>1917</a></span>
<span class=normal><a href=#__codelineno-0-1918>1918</a></span>
<span class=normal><a href=#__codelineno-0-1919>1919</a></span>
<span class=normal><a href=#__codelineno-0-1920>1920</a></span>
<span class=normal><a href=#__codelineno-0-1921>1921</a></span>
<span class=normal><a href=#__codelineno-0-1922>1922</a></span>
<span class=normal><a href=#__codelineno-0-1923>1923</a></span>
<span class=normal><a href=#__codelineno-0-1924>1924</a></span>
<span class=normal><a href=#__codelineno-0-1925>1925</a></span>
<span class=normal><a href=#__codelineno-0-1926>1926</a></span>
<span class=normal><a href=#__codelineno-0-1927>1927</a></span>
<span class=normal><a href=#__codelineno-0-1928>1928</a></span>
<span class=normal><a href=#__codelineno-0-1929>1929</a></span>
<span class=normal><a href=#__codelineno-0-1930>1930</a></span>
<span class=normal><a href=#__codelineno-0-1931>1931</a></span>
<span class=normal><a href=#__codelineno-0-1932>1932</a></span>
<span class=normal><a href=#__codelineno-0-1933>1933</a></span>
<span class=normal><a href=#__codelineno-0-1934>1934</a></span>
<span class=normal><a href=#__codelineno-0-1935>1935</a></span>
<span class=normal><a href=#__codelineno-0-1936>1936</a></span>
<span class=normal><a href=#__codelineno-0-1937>1937</a></span>
<span class=normal><a href=#__codelineno-0-1938>1938</a></span>
<span class=normal><a href=#__codelineno-0-1939>1939</a></span>
<span class=normal><a href=#__codelineno-0-1940>1940</a></span>
<span class=normal><a href=#__codelineno-0-1941>1941</a></span>
<span class=normal><a href=#__codelineno-0-1942>1942</a></span>
<span class=normal><a href=#__codelineno-0-1943>1943</a></span>
<span class=normal><a href=#__codelineno-0-1944>1944</a></span>
<span class=normal><a href=#__codelineno-0-1945>1945</a></span>
<span class=normal><a href=#__codelineno-0-1946>1946</a></span>
<span class=normal><a href=#__codelineno-0-1947>1947</a></span>
<span class=normal><a href=#__codelineno-0-1948>1948</a></span>
<span class=normal><a href=#__codelineno-0-1949>1949</a></span>
<span class=normal><a href=#__codelineno-0-1950>1950</a></span>
<span class=normal><a href=#__codelineno-0-1951>1951</a></span>
<span class=normal><a href=#__codelineno-0-1952>1952</a></span>
<span class=normal><a href=#__codelineno-0-1953>1953</a></span>
<span class=normal><a href=#__codelineno-0-1954>1954</a></span>
<span class=normal><a href=#__codelineno-0-1955>1955</a></span>
<span class=normal><a href=#__codelineno-0-1956>1956</a></span>
<span class=normal><a href=#__codelineno-0-1957>1957</a></span>
<span class=normal><a href=#__codelineno-0-1958>1958</a></span>
<span class=normal><a href=#__codelineno-0-1959>1959</a></span>
<span class=normal><a href=#__codelineno-0-1960>1960</a></span>
<span class=normal><a href=#__codelineno-0-1961>1961</a></span>
<span class=normal><a href=#__codelineno-0-1962>1962</a></span>
<span class=normal><a href=#__codelineno-0-1963>1963</a></span>
<span class=normal><a href=#__codelineno-0-1964>1964</a></span>
<span class=normal><a href=#__codelineno-0-1965>1965</a></span>
<span class=normal><a href=#__codelineno-0-1966>1966</a></span>
<span class=normal><a href=#__codelineno-0-1967>1967</a></span>
<span class=normal><a href=#__codelineno-0-1968>1968</a></span>
<span class=normal><a href=#__codelineno-0-1969>1969</a></span>
<span class=normal><a href=#__codelineno-0-1970>1970</a></span>
<span class=normal><a href=#__codelineno-0-1971>1971</a></span>
<span class=normal><a href=#__codelineno-0-1972>1972</a></span>
<span class=normal><a href=#__codelineno-0-1973>1973</a></span>
<span class=normal><a href=#__codelineno-0-1974>1974</a></span>
<span class=normal><a href=#__codelineno-0-1975>1975</a></span>
<span class=normal><a href=#__codelineno-0-1976>1976</a></span>
<span class=normal><a href=#__codelineno-0-1977>1977</a></span>
<span class=normal><a href=#__codelineno-0-1978>1978</a></span>
<span class=normal><a href=#__codelineno-0-1979>1979</a></span>
<span class=normal><a href=#__codelineno-0-1980>1980</a></span>
<span class=normal><a href=#__codelineno-0-1981>1981</a></span>
<span class=normal><a href=#__codelineno-0-1982>1982</a></span>
<span class=normal><a href=#__codelineno-0-1983>1983</a></span>
<span class=normal><a href=#__codelineno-0-1984>1984</a></span>
<span class=normal><a href=#__codelineno-0-1985>1985</a></span>
<span class=normal><a href=#__codelineno-0-1986>1986</a></span>
<span class=normal><a href=#__codelineno-0-1987>1987</a></span>
<span class=normal><a href=#__codelineno-0-1988>1988</a></span>
<span class=normal><a href=#__codelineno-0-1989>1989</a></span>
<span class=normal><a href=#__codelineno-0-1990>1990</a></span>
<span class=normal><a href=#__codelineno-0-1991>1991</a></span>
<span class=normal><a href=#__codelineno-0-1992>1992</a></span>
<span class=normal><a href=#__codelineno-0-1993>1993</a></span>
<span class=normal><a href=#__codelineno-0-1994>1994</a></span>
<span class=normal><a href=#__codelineno-0-1995>1995</a></span>
<span class=normal><a href=#__codelineno-0-1996>1996</a></span>
<span class=normal><a href=#__codelineno-0-1997>1997</a></span>
<span class=normal><a href=#__codelineno-0-1998>1998</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1776><a id=__codelineno-0-1776 name=__codelineno-0-1776></a><span class=k>class</span> <span class=nc>NonStochasticBlock2d</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-1777><a id=__codelineno-0-1777 name=__codelineno-0-1777></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1778><a id=__codelineno-0-1778 name=__codelineno-0-1778></a><span class=sd>    Non-stochastic version of the NormalStochasticBlock2d.</span>
</span><span id=__span-0-1779><a id=__codelineno-0-1779 name=__codelineno-0-1779></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1780><a id=__codelineno-0-1780 name=__codelineno-0-1780></a>
</span><span id=__span-0-1781><a id=__codelineno-0-1781 name=__codelineno-0-1781></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-1782><a id=__codelineno-0-1782 name=__codelineno-0-1782></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1783><a id=__codelineno-0-1783 name=__codelineno-0-1783></a>        <span class=n>c_vars</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1784><a id=__codelineno-0-1784 name=__codelineno-0-1784></a>        <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1785><a id=__codelineno-0-1785 name=__codelineno-0-1785></a>        <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1786><a id=__codelineno-0-1786 name=__codelineno-0-1786></a>        <span class=n>kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-1787><a id=__codelineno-0-1787 name=__codelineno-0-1787></a>        <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-1788><a id=__codelineno-0-1788 name=__codelineno-0-1788></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1789><a id=__codelineno-0-1789 name=__codelineno-0-1789></a>        <span class=n>transform_p_params</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1790><a id=__codelineno-0-1790 name=__codelineno-0-1790></a>    <span class=p>):</span>
</span><span id=__span-0-1791><a id=__codelineno-0-1791 name=__codelineno-0-1791></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1792><a id=__codelineno-0-1792 name=__codelineno-0-1792></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-1793><a id=__codelineno-0-1793 name=__codelineno-0-1793></a>
</span><span id=__span-0-1794><a id=__codelineno-0-1794 name=__codelineno-0-1794></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1795><a id=__codelineno-0-1795 name=__codelineno-0-1795></a><span class=sd>        ----------</span>
</span><span id=__span-0-1796><a id=__codelineno-0-1796 name=__codelineno-0-1796></a><span class=sd>        c_vars: int</span>
</span><span id=__span-0-1797><a id=__codelineno-0-1797 name=__codelineno-0-1797></a><span class=sd>            The number of channels of the latent space tensor.</span>
</span><span id=__span-0-1798><a id=__codelineno-0-1798 name=__codelineno-0-1798></a><span class=sd>        c_in: int</span>
</span><span id=__span-0-1799><a id=__codelineno-0-1799 name=__codelineno-0-1799></a><span class=sd>            The number of channels of the input tensor.</span>
</span><span id=__span-0-1800><a id=__codelineno-0-1800 name=__codelineno-0-1800></a><span class=sd>        c_out:  int</span>
</span><span id=__span-0-1801><a id=__codelineno-0-1801 name=__codelineno-0-1801></a><span class=sd>            The output of the stochastic layer.</span>
</span><span id=__span-0-1802><a id=__codelineno-0-1802 name=__codelineno-0-1802></a><span class=sd>            Note that this is different from the sampled latent z.</span>
</span><span id=__span-0-1803><a id=__codelineno-0-1803 name=__codelineno-0-1803></a><span class=sd>        kernel: int, optional</span>
</span><span id=__span-0-1804><a id=__codelineno-0-1804 name=__codelineno-0-1804></a><span class=sd>            The size of the kernel used in convolutional layers.</span>
</span><span id=__span-0-1805><a id=__codelineno-0-1805 name=__codelineno-0-1805></a><span class=sd>            Default is 3.</span>
</span><span id=__span-0-1806><a id=__codelineno-0-1806 name=__codelineno-0-1806></a><span class=sd>        groups: int, optional</span>
</span><span id=__span-0-1807><a id=__codelineno-0-1807 name=__codelineno-0-1807></a><span class=sd>            The number of groups to consider in the convolutions of this layer.</span>
</span><span id=__span-0-1808><a id=__codelineno-0-1808 name=__codelineno-0-1808></a><span class=sd>            Default is 1.</span>
</span><span id=__span-0-1809><a id=__codelineno-0-1809 name=__codelineno-0-1809></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-1810><a id=__codelineno-0-1810 name=__codelineno-0-1810></a><span class=sd>            Whether to use bias term is the convolutional blocks of this layer.</span>
</span><span id=__span-0-1811><a id=__codelineno-0-1811 name=__codelineno-0-1811></a><span class=sd>            Default is `True`.</span>
</span><span id=__span-0-1812><a id=__codelineno-0-1812 name=__codelineno-0-1812></a><span class=sd>        transform_p_params: bool, optional</span>
</span><span id=__span-0-1813><a id=__codelineno-0-1813 name=__codelineno-0-1813></a><span class=sd>            Whether a transformation should be applied to the `p_params` tensor.</span>
</span><span id=__span-0-1814><a id=__codelineno-0-1814 name=__codelineno-0-1814></a><span class=sd>            The transformation consists in a 2D convolution ()`conv_in_p()`) that</span>
</span><span id=__span-0-1815><a id=__codelineno-0-1815 name=__codelineno-0-1815></a><span class=sd>            maps the input to a larger number of channels.</span>
</span><span id=__span-0-1816><a id=__codelineno-0-1816 name=__codelineno-0-1816></a><span class=sd>            Default is `True`.</span>
</span><span id=__span-0-1817><a id=__codelineno-0-1817 name=__codelineno-0-1817></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1818><a id=__codelineno-0-1818 name=__codelineno-0-1818></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-1819><a id=__codelineno-0-1819 name=__codelineno-0-1819></a>        <span class=k>assert</span> <span class=n>kernel</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span>
</span><span id=__span-0-1820><a id=__codelineno-0-1820 name=__codelineno-0-1820></a>        <span class=n>pad</span> <span class=o>=</span> <span class=n>kernel</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1821><a id=__codelineno-0-1821 name=__codelineno-0-1821></a>        <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span> <span class=o>=</span> <span class=n>transform_p_params</span>
</span><span id=__span-0-1822><a id=__codelineno-0-1822 name=__codelineno-0-1822></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_in</span> <span class=o>=</span> <span class=n>c_in</span>
</span><span id=__span-0-1823><a id=__codelineno-0-1823 name=__codelineno-0-1823></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_out</span> <span class=o>=</span> <span class=n>c_out</span>
</span><span id=__span-0-1824><a id=__codelineno-0-1824 name=__codelineno-0-1824></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span> <span class=o>=</span> <span class=n>c_vars</span>
</span><span id=__span-0-1825><a id=__codelineno-0-1825 name=__codelineno-0-1825></a>
</span><span id=__span-0-1826><a id=__codelineno-0-1826 name=__codelineno-0-1826></a>        <span class=k>if</span> <span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1827><a id=__codelineno-0-1827 name=__codelineno-0-1827></a>            <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1828><a id=__codelineno-0-1828 name=__codelineno-0-1828></a>                <span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1829><a id=__codelineno-0-1829 name=__codelineno-0-1829></a>            <span class=p>)</span>
</span><span id=__span-0-1830><a id=__codelineno-0-1830 name=__codelineno-0-1830></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1831><a id=__codelineno-0-1831 name=__codelineno-0-1831></a>            <span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1832><a id=__codelineno-0-1832 name=__codelineno-0-1832></a>        <span class=p>)</span>
</span><span id=__span-0-1833><a id=__codelineno-0-1833 name=__codelineno-0-1833></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1834><a id=__codelineno-0-1834 name=__codelineno-0-1834></a>            <span class=n>c_vars</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1835><a id=__codelineno-0-1835 name=__codelineno-0-1835></a>        <span class=p>)</span>
</span><span id=__span-0-1836><a id=__codelineno-0-1836 name=__codelineno-0-1836></a>
</span><span id=__span-0-1837><a id=__codelineno-0-1837 name=__codelineno-0-1837></a>    <span class=k>def</span> <span class=nf>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1838><a id=__codelineno-0-1838 name=__codelineno-0-1838></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1839><a id=__codelineno-0-1839 name=__codelineno-0-1839></a>        <span class=n>p</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1840><a id=__codelineno-0-1840 name=__codelineno-0-1840></a>        <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1841><a id=__codelineno-0-1841 name=__codelineno-0-1841></a>        <span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1842><a id=__codelineno-0-1842 name=__codelineno-0-1842></a>        <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1843><a id=__codelineno-0-1843 name=__codelineno-0-1843></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1844><a id=__codelineno-0-1844 name=__codelineno-0-1844></a>        <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1845><a id=__codelineno-0-1845 name=__codelineno-0-1845></a>        <span class=n>z</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1846><a id=__codelineno-0-1846 name=__codelineno-0-1846></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=kc>None</span><span class=p>]:</span>
</span><span id=__span-0-1847><a id=__codelineno-0-1847 name=__codelineno-0-1847></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1848><a id=__codelineno-0-1848 name=__codelineno-0-1848></a><span class=sd>        Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.</span>
</span><span id=__span-0-1849><a id=__codelineno-0-1849 name=__codelineno-0-1849></a><span class=sd>        Specifically, the different versions of the KL loss terms are:</span>
</span><span id=__span-0-1850><a id=__codelineno-0-1850 name=__codelineno-0-1850></a><span class=sd>            - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].</span>
</span><span id=__span-0-1851><a id=__codelineno-0-1851 name=__codelineno-0-1851></a><span class=sd>            - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].</span>
</span><span id=__span-0-1852><a id=__codelineno-0-1852 name=__codelineno-0-1852></a><span class=sd>            - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is</span>
</span><span id=__span-0-1853><a id=__codelineno-0-1853 name=__codelineno-0-1853></a><span class=sd>            used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].</span>
</span><span id=__span-0-1854><a id=__codelineno-0-1854 name=__codelineno-0-1854></a><span class=sd>            - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].</span>
</span><span id=__span-0-1855><a id=__codelineno-0-1855 name=__codelineno-0-1855></a><span class=sd>            - `kl_spatial`: # KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1856><a id=__codelineno-0-1856 name=__codelineno-0-1856></a>
</span><span id=__span-0-1857><a id=__codelineno-0-1857 name=__codelineno-0-1857></a><span class=sd>        NOTE: in this class all the KL metrics are set to `None`.</span>
</span><span id=__span-0-1858><a id=__codelineno-0-1858 name=__codelineno-0-1858></a>
</span><span id=__span-0-1859><a id=__codelineno-0-1859 name=__codelineno-0-1859></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1860><a id=__codelineno-0-1860 name=__codelineno-0-1860></a><span class=sd>        ----------</span>
</span><span id=__span-0-1861><a id=__codelineno-0-1861 name=__codelineno-0-1861></a><span class=sd>        p: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1862><a id=__codelineno-0-1862 name=__codelineno-0-1862></a><span class=sd>            The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1863><a id=__codelineno-0-1863 name=__codelineno-0-1863></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1864><a id=__codelineno-0-1864 name=__codelineno-0-1864></a><span class=sd>            The parameters of the prior generative distribution.</span>
</span><span id=__span-0-1865><a id=__codelineno-0-1865 name=__codelineno-0-1865></a><span class=sd>        q: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1866><a id=__codelineno-0-1866 name=__codelineno-0-1866></a><span class=sd>            The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</span>
</span><span id=__span-0-1867><a id=__codelineno-0-1867 name=__codelineno-0-1867></a><span class=sd>        q_params: torch.Tensor</span>
</span><span id=__span-0-1868><a id=__codelineno-0-1868 name=__codelineno-0-1868></a><span class=sd>            The parameters of the inference distribution.</span>
</span><span id=__span-0-1869><a id=__codelineno-0-1869 name=__codelineno-0-1869></a><span class=sd>        mode_pred: bool</span>
</span><span id=__span-0-1870><a id=__codelineno-0-1870 name=__codelineno-0-1870></a><span class=sd>            Whether the model is in prediction mode.</span>
</span><span id=__span-0-1871><a id=__codelineno-0-1871 name=__codelineno-0-1871></a><span class=sd>        analytical_kl: bool</span>
</span><span id=__span-0-1872><a id=__codelineno-0-1872 name=__codelineno-0-1872></a><span class=sd>            Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1873><a id=__codelineno-0-1873 name=__codelineno-0-1873></a><span class=sd>        z: torch.Tensor</span>
</span><span id=__span-0-1874><a id=__codelineno-0-1874 name=__codelineno-0-1874></a><span class=sd>            The sampled latent tensor.</span>
</span><span id=__span-0-1875><a id=__codelineno-0-1875 name=__codelineno-0-1875></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1876><a id=__codelineno-0-1876 name=__codelineno-0-1876></a>        <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-0-1877><a id=__codelineno-0-1877 name=__codelineno-0-1877></a>            <span class=s2>&quot;kl_elementwise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1878><a id=__codelineno-0-1878 name=__codelineno-0-1878></a>            <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1879><a id=__codelineno-0-1879 name=__codelineno-0-1879></a>            <span class=s2>&quot;kl_spatial&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, h, w)</span>
</span><span id=__span-0-1880><a id=__codelineno-0-1880 name=__codelineno-0-1880></a>            <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, ch)</span>
</span><span id=__span-0-1881><a id=__codelineno-0-1881 name=__codelineno-0-1881></a>        <span class=p>}</span>
</span><span id=__span-0-1882><a id=__codelineno-0-1882 name=__codelineno-0-1882></a>        <span class=k>return</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1883><a id=__codelineno-0-1883 name=__codelineno-0-1883></a>
</span><span id=__span-0-1884><a id=__codelineno-0-1884 name=__codelineno-0-1884></a>    <span class=k>def</span> <span class=nf>process_p_params</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>):</span>
</span><span id=__span-0-1885><a id=__codelineno-0-1885 name=__codelineno-0-1885></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1886><a id=__codelineno-0-1886 name=__codelineno-0-1886></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span><span class=p>(</span><span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1887><a id=__codelineno-0-1887 name=__codelineno-0-1887></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1888><a id=__codelineno-0-1888 name=__codelineno-0-1888></a>
</span><span id=__span-0-1889><a id=__codelineno-0-1889 name=__codelineno-0-1889></a>            <span class=k>assert</span> <span class=p>(</span>
</span><span id=__span-0-1890><a id=__codelineno-0-1890 name=__codelineno-0-1890></a>                <span class=n>p_params</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span>
</span><span id=__span-0-1891><a id=__codelineno-0-1891 name=__codelineno-0-1891></a>            <span class=p>),</span> <span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span><span class=si>}</span><span class=s2>&quot;</span>
</span><span id=__span-0-1892><a id=__codelineno-0-1892 name=__codelineno-0-1892></a>
</span><span id=__span-0-1893><a id=__codelineno-0-1893 name=__codelineno-0-1893></a>        <span class=c1># Define p(z)</span>
</span><span id=__span-0-1894><a id=__codelineno-0-1894 name=__codelineno-0-1894></a>        <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span> <span class=o>=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1895><a id=__codelineno-0-1895 name=__codelineno-0-1895></a>        <span class=k>return</span> <span class=n>p_mu</span><span class=p>,</span> <span class=kc>None</span>
</span><span id=__span-0-1896><a id=__codelineno-0-1896 name=__codelineno-0-1896></a>
</span><span id=__span-0-1897><a id=__codelineno-0-1897 name=__codelineno-0-1897></a>    <span class=k>def</span> <span class=nf>process_q_params</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span><span id=__span-0-1898><a id=__codelineno-0-1898 name=__codelineno-0-1898></a>        <span class=c1># Define q(z)</span>
</span><span id=__span-0-1899><a id=__codelineno-0-1899 name=__codelineno-0-1899></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>)</span>
</span><span id=__span-0-1900><a id=__codelineno-0-1900 name=__codelineno-0-1900></a>        <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span> <span class=o>=</span> <span class=n>q_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1901><a id=__codelineno-0-1901 name=__codelineno-0-1901></a>
</span><span id=__span-0-1902><a id=__codelineno-0-1902 name=__codelineno-0-1902></a>        <span class=k>if</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>and</span> <span class=n>allow_oddsizes</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1903><a id=__codelineno-0-1903 name=__codelineno-0-1903></a>            <span class=n>q_mu</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1904><a id=__codelineno-0-1904 name=__codelineno-0-1904></a>
</span><span id=__span-0-1905><a id=__codelineno-0-1905 name=__codelineno-0-1905></a>        <span class=k>return</span> <span class=n>q_mu</span><span class=p>,</span> <span class=kc>None</span>
</span><span id=__span-0-1906><a id=__codelineno-0-1906 name=__codelineno-0-1906></a>
</span><span id=__span-0-1907><a id=__codelineno-0-1907 name=__codelineno-0-1907></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1908><a id=__codelineno-0-1908 name=__codelineno-0-1908></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1909><a id=__codelineno-0-1909 name=__codelineno-0-1909></a>        <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1910><a id=__codelineno-0-1910 name=__codelineno-0-1910></a>        <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1911><a id=__codelineno-0-1911 name=__codelineno-0-1911></a>        <span class=n>forced_latent</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1912><a id=__codelineno-0-1912 name=__codelineno-0-1912></a>        <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1913><a id=__codelineno-0-1913 name=__codelineno-0-1913></a>        <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1914><a id=__codelineno-0-1914 name=__codelineno-0-1914></a>        <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1915><a id=__codelineno-0-1915 name=__codelineno-0-1915></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1916><a id=__codelineno-0-1916 name=__codelineno-0-1916></a>        <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1917><a id=__codelineno-0-1917 name=__codelineno-0-1917></a>        <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1918><a id=__codelineno-0-1918 name=__codelineno-0-1918></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1919><a id=__codelineno-0-1919 name=__codelineno-0-1919></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1920><a id=__codelineno-0-1920 name=__codelineno-0-1920></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1921><a id=__codelineno-0-1921 name=__codelineno-0-1921></a><span class=sd>        ----------</span>
</span><span id=__span-0-1922><a id=__codelineno-0-1922 name=__codelineno-0-1922></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1923><a id=__codelineno-0-1923 name=__codelineno-0-1923></a><span class=sd>            The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</span>
</span><span id=__span-0-1924><a id=__codelineno-0-1924 name=__codelineno-0-1924></a><span class=sd>        q_params: torch.Tensor, optional</span>
</span><span id=__span-0-1925><a id=__codelineno-0-1925 name=__codelineno-0-1925></a><span class=sd>            The tensor resulting from merging the bu_value tensor at the same hierarchical level</span>
</span><span id=__span-0-1926><a id=__codelineno-0-1926 name=__codelineno-0-1926></a><span class=sd>            from the bottom-up pass and the `p_params` tensor. Default is `None`.</span>
</span><span id=__span-0-1927><a id=__codelineno-0-1927 name=__codelineno-0-1927></a><span class=sd>        forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1928><a id=__codelineno-0-1928 name=__codelineno-0-1928></a><span class=sd>            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent</span>
</span><span id=__span-0-1929><a id=__codelineno-0-1929 name=__codelineno-0-1929></a><span class=sd>            tensor and, hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1930><a id=__codelineno-0-1930 name=__codelineno-0-1930></a><span class=sd>        use_mode: bool, optional</span>
</span><span id=__span-0-1931><a id=__codelineno-0-1931 name=__codelineno-0-1931></a><span class=sd>            Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1932><a id=__codelineno-0-1932 name=__codelineno-0-1932></a><span class=sd>            In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1933><a id=__codelineno-0-1933 name=__codelineno-0-1933></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1934><a id=__codelineno-0-1934 name=__codelineno-0-1934></a><span class=sd>        force_constant_output: bool, optional</span>
</span><span id=__span-0-1935><a id=__codelineno-0-1935 name=__codelineno-0-1935></a><span class=sd>            Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1936><a id=__codelineno-0-1936 name=__codelineno-0-1936></a><span class=sd>            This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1937><a id=__codelineno-0-1937 name=__codelineno-0-1937></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1938><a id=__codelineno-0-1938 name=__codelineno-0-1938></a><span class=sd>        analytical_kl: bool, optional</span>
</span><span id=__span-0-1939><a id=__codelineno-0-1939 name=__codelineno-0-1939></a><span class=sd>            Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1940><a id=__codelineno-0-1940 name=__codelineno-0-1940></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1941><a id=__codelineno-0-1941 name=__codelineno-0-1941></a><span class=sd>        mode_pred: bool, optional</span>
</span><span id=__span-0-1942><a id=__codelineno-0-1942 name=__codelineno-0-1942></a><span class=sd>            Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1943><a id=__codelineno-0-1943 name=__codelineno-0-1943></a><span class=sd>        use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1944><a id=__codelineno-0-1944 name=__codelineno-0-1944></a><span class=sd>            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1945><a id=__codelineno-0-1945 name=__codelineno-0-1945></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1946><a id=__codelineno-0-1946 name=__codelineno-0-1946></a><span class=sd>        var_clip_max: float, optional</span>
</span><span id=__span-0-1947><a id=__codelineno-0-1947 name=__codelineno-0-1947></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1948><a id=__codelineno-0-1948 name=__codelineno-0-1948></a><span class=sd>            Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1949><a id=__codelineno-0-1949 name=__codelineno-0-1949></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1950><a id=__codelineno-0-1950 name=__codelineno-0-1950></a>        <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-1951><a id=__codelineno-0-1951 name=__codelineno-0-1951></a>        <span class=k>assert</span> <span class=p>(</span><span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>)</span> <span class=ow>or</span> <span class=p>(</span><span class=ow>not</span> <span class=n>use_mode</span><span class=p>)</span>
</span><span id=__span-0-1952><a id=__codelineno-0-1952 name=__codelineno-0-1952></a>
</span><span id=__span-0-1953><a id=__codelineno-0-1953 name=__codelineno-0-1953></a>        <span class=n>p_mu</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_p_params</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1954><a id=__codelineno-0-1954 name=__codelineno-0-1954></a>
</span><span id=__span-0-1955><a id=__codelineno-0-1955 name=__codelineno-0-1955></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_mu</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span><span id=__span-0-1956><a id=__codelineno-0-1956 name=__codelineno-0-1956></a>
</span><span id=__span-0-1957><a id=__codelineno-0-1957 name=__codelineno-0-1957></a>        <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1958><a id=__codelineno-0-1958 name=__codelineno-0-1958></a>            <span class=c1># At inference time, just don&#39;t centercrop the q_params even if they are odd in size.</span>
</span><span id=__span-0-1959><a id=__codelineno-0-1959 name=__codelineno-0-1959></a>            <span class=n>q_mu</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1960><a id=__codelineno-0-1960 name=__codelineno-0-1960></a>                <span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1961><a id=__codelineno-0-1961 name=__codelineno-0-1961></a>            <span class=p>)</span>
</span><span id=__span-0-1962><a id=__codelineno-0-1962 name=__codelineno-0-1962></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span><span id=__span-0-1963><a id=__codelineno-0-1963 name=__codelineno-0-1963></a>            <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>q_mu</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-0-1964><a id=__codelineno-0-1964 name=__codelineno-0-1964></a>            <span class=c1># Sample from q(z)</span>
</span><span id=__span-0-1965><a id=__codelineno-0-1965 name=__codelineno-0-1965></a>            <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>q_mu</span>
</span><span id=__span-0-1966><a id=__codelineno-0-1966 name=__codelineno-0-1966></a>            <span class=n>q_size</span> <span class=o>=</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1967><a id=__codelineno-0-1967 name=__codelineno-0-1967></a>            <span class=k>if</span> <span class=n>p_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>!=</span> <span class=n>q_size</span> <span class=ow>and</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1968><a id=__codelineno-0-1968 name=__codelineno-0-1968></a>                <span class=n>p_mu</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1969><a id=__codelineno-0-1969 name=__codelineno-0-1969></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1970><a id=__codelineno-0-1970 name=__codelineno-0-1970></a>            <span class=c1># Sample from p(z)</span>
</span><span id=__span-0-1971><a id=__codelineno-0-1971 name=__codelineno-0-1971></a>            <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>p_mu</span>
</span><span id=__span-0-1972><a id=__codelineno-0-1972 name=__codelineno-0-1972></a>
</span><span id=__span-0-1973><a id=__codelineno-0-1973 name=__codelineno-0-1973></a>        <span class=c1># Generate latent variable (typically by sampling)</span>
</span><span id=__span-0-1974><a id=__codelineno-0-1974 name=__codelineno-0-1974></a>        <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span>
</span><span id=__span-0-1975><a id=__codelineno-0-1975 name=__codelineno-0-1975></a>
</span><span id=__span-0-1976><a id=__codelineno-0-1976 name=__codelineno-0-1976></a>        <span class=c1># Copy one sample (and distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1977><a id=__codelineno-0-1977 name=__codelineno-0-1977></a>        <span class=c1># This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1978><a id=__codelineno-0-1978 name=__codelineno-0-1978></a>        <span class=k>if</span> <span class=n>force_constant_output</span><span class=p>:</span>
</span><span id=__span-0-1979><a id=__codelineno-0-1979 name=__codelineno-0-1979></a>            <span class=n>z</span> <span class=o>=</span> <span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span><span id=__span-0-1980><a id=__codelineno-0-1980 name=__codelineno-0-1980></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1981><a id=__codelineno-0-1981 name=__codelineno-0-1981></a>                <span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1982><a id=__codelineno-0-1982 name=__codelineno-0-1982></a>                <span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1983><a id=__codelineno-0-1983 name=__codelineno-0-1983></a>            <span class=p>)</span>
</span><span id=__span-0-1984><a id=__codelineno-0-1984 name=__codelineno-0-1984></a>
</span><span id=__span-0-1985><a id=__codelineno-0-1985 name=__codelineno-0-1985></a>        <span class=c1># Output of stochastic layer</span>
</span><span id=__span-0-1986><a id=__codelineno-0-1986 name=__codelineno-0-1986></a>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-0-1987><a id=__codelineno-0-1987 name=__codelineno-0-1987></a>
</span><span id=__span-0-1988><a id=__codelineno-0-1988 name=__codelineno-0-1988></a>        <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-0-1989><a id=__codelineno-0-1989 name=__codelineno-0-1989></a>        <span class=n>logprob_q</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1990><a id=__codelineno-0-1990 name=__codelineno-0-1990></a>
</span><span id=__span-0-1991><a id=__codelineno-0-1991 name=__codelineno-0-1991></a>        <span class=n>data</span> <span class=o>=</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1992><a id=__codelineno-0-1992 name=__codelineno-0-1992></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;z&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>z</span>  <span class=c1># sampled variable at this layer (batch, ch, h, w)</span>
</span><span id=__span-0-1993><a id=__codelineno-0-1993 name=__codelineno-0-1993></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;p_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>p_params</span>  <span class=c1># (b, ch, h, w) where b is 1 or batch size</span>
</span><span id=__span-0-1994><a id=__codelineno-0-1994 name=__codelineno-0-1994></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_params</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1995><a id=__codelineno-0-1995 name=__codelineno-0-1995></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;logprob_q&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>logprob_q</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1996><a id=__codelineno-0-1996 name=__codelineno-0-1996></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;qvar_max&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>debug_qvar_max</span>
</span><span id=__span-0-1997><a id=__codelineno-0-1997 name=__codelineno-0-1997></a>
</span><span id=__span-0-1998><a id=__codelineno-0-1998 name=__codelineno-0-1998></a>        <span class=k>return</span> <span class=n>out</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NonStochasticBlock2d.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>c_vars</span><span class=p>,</span> <span class=n>c_in</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform_p_params</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>c_vars</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels of the latent space tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_in</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels of the input tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_out</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The output of the stochastic layer. Note that this is different from the sampled latent z.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>kernel</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The size of the kernel used in convolutional layers. Default is 3.</p> </div> </td> <td> <code>3</code> </td> </tr> <tr class=doc-section-item> <td><code>groups</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of groups to consider in the convolutions of this layer. Default is 1.</p> </div> </td> <td> <code>1</code> </td> </tr> <tr class=doc-section-item> <td><code>conv2d_bias</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use bias term is the convolutional blocks of this layer. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>transform_p_params</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether a transformation should be applied to the <code>p_params</code> tensor. The transformation consists in a 2D convolution ()<code>conv_in_p()</code>) that maps the input to a larger number of channels. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1781>1781</a></span>
<span class=normal><a href=#__codelineno-0-1782>1782</a></span>
<span class=normal><a href=#__codelineno-0-1783>1783</a></span>
<span class=normal><a href=#__codelineno-0-1784>1784</a></span>
<span class=normal><a href=#__codelineno-0-1785>1785</a></span>
<span class=normal><a href=#__codelineno-0-1786>1786</a></span>
<span class=normal><a href=#__codelineno-0-1787>1787</a></span>
<span class=normal><a href=#__codelineno-0-1788>1788</a></span>
<span class=normal><a href=#__codelineno-0-1789>1789</a></span>
<span class=normal><a href=#__codelineno-0-1790>1790</a></span>
<span class=normal><a href=#__codelineno-0-1791>1791</a></span>
<span class=normal><a href=#__codelineno-0-1792>1792</a></span>
<span class=normal><a href=#__codelineno-0-1793>1793</a></span>
<span class=normal><a href=#__codelineno-0-1794>1794</a></span>
<span class=normal><a href=#__codelineno-0-1795>1795</a></span>
<span class=normal><a href=#__codelineno-0-1796>1796</a></span>
<span class=normal><a href=#__codelineno-0-1797>1797</a></span>
<span class=normal><a href=#__codelineno-0-1798>1798</a></span>
<span class=normal><a href=#__codelineno-0-1799>1799</a></span>
<span class=normal><a href=#__codelineno-0-1800>1800</a></span>
<span class=normal><a href=#__codelineno-0-1801>1801</a></span>
<span class=normal><a href=#__codelineno-0-1802>1802</a></span>
<span class=normal><a href=#__codelineno-0-1803>1803</a></span>
<span class=normal><a href=#__codelineno-0-1804>1804</a></span>
<span class=normal><a href=#__codelineno-0-1805>1805</a></span>
<span class=normal><a href=#__codelineno-0-1806>1806</a></span>
<span class=normal><a href=#__codelineno-0-1807>1807</a></span>
<span class=normal><a href=#__codelineno-0-1808>1808</a></span>
<span class=normal><a href=#__codelineno-0-1809>1809</a></span>
<span class=normal><a href=#__codelineno-0-1810>1810</a></span>
<span class=normal><a href=#__codelineno-0-1811>1811</a></span>
<span class=normal><a href=#__codelineno-0-1812>1812</a></span>
<span class=normal><a href=#__codelineno-0-1813>1813</a></span>
<span class=normal><a href=#__codelineno-0-1814>1814</a></span>
<span class=normal><a href=#__codelineno-0-1815>1815</a></span>
<span class=normal><a href=#__codelineno-0-1816>1816</a></span>
<span class=normal><a href=#__codelineno-0-1817>1817</a></span>
<span class=normal><a href=#__codelineno-0-1818>1818</a></span>
<span class=normal><a href=#__codelineno-0-1819>1819</a></span>
<span class=normal><a href=#__codelineno-0-1820>1820</a></span>
<span class=normal><a href=#__codelineno-0-1821>1821</a></span>
<span class=normal><a href=#__codelineno-0-1822>1822</a></span>
<span class=normal><a href=#__codelineno-0-1823>1823</a></span>
<span class=normal><a href=#__codelineno-0-1824>1824</a></span>
<span class=normal><a href=#__codelineno-0-1825>1825</a></span>
<span class=normal><a href=#__codelineno-0-1826>1826</a></span>
<span class=normal><a href=#__codelineno-0-1827>1827</a></span>
<span class=normal><a href=#__codelineno-0-1828>1828</a></span>
<span class=normal><a href=#__codelineno-0-1829>1829</a></span>
<span class=normal><a href=#__codelineno-0-1830>1830</a></span>
<span class=normal><a href=#__codelineno-0-1831>1831</a></span>
<span class=normal><a href=#__codelineno-0-1832>1832</a></span>
<span class=normal><a href=#__codelineno-0-1833>1833</a></span>
<span class=normal><a href=#__codelineno-0-1834>1834</a></span>
<span class=normal><a href=#__codelineno-0-1835>1835</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1781><a id=__codelineno-0-1781 name=__codelineno-0-1781></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-1782><a id=__codelineno-0-1782 name=__codelineno-0-1782></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1783><a id=__codelineno-0-1783 name=__codelineno-0-1783></a>    <span class=n>c_vars</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1784><a id=__codelineno-0-1784 name=__codelineno-0-1784></a>    <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1785><a id=__codelineno-0-1785 name=__codelineno-0-1785></a>    <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1786><a id=__codelineno-0-1786 name=__codelineno-0-1786></a>    <span class=n>kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-1787><a id=__codelineno-0-1787 name=__codelineno-0-1787></a>    <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-1788><a id=__codelineno-0-1788 name=__codelineno-0-1788></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1789><a id=__codelineno-0-1789 name=__codelineno-0-1789></a>    <span class=n>transform_p_params</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1790><a id=__codelineno-0-1790 name=__codelineno-0-1790></a><span class=p>):</span>
</span><span id=__span-0-1791><a id=__codelineno-0-1791 name=__codelineno-0-1791></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1792><a id=__codelineno-0-1792 name=__codelineno-0-1792></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-1793><a id=__codelineno-0-1793 name=__codelineno-0-1793></a>
</span><span id=__span-0-1794><a id=__codelineno-0-1794 name=__codelineno-0-1794></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1795><a id=__codelineno-0-1795 name=__codelineno-0-1795></a><span class=sd>    ----------</span>
</span><span id=__span-0-1796><a id=__codelineno-0-1796 name=__codelineno-0-1796></a><span class=sd>    c_vars: int</span>
</span><span id=__span-0-1797><a id=__codelineno-0-1797 name=__codelineno-0-1797></a><span class=sd>        The number of channels of the latent space tensor.</span>
</span><span id=__span-0-1798><a id=__codelineno-0-1798 name=__codelineno-0-1798></a><span class=sd>    c_in: int</span>
</span><span id=__span-0-1799><a id=__codelineno-0-1799 name=__codelineno-0-1799></a><span class=sd>        The number of channels of the input tensor.</span>
</span><span id=__span-0-1800><a id=__codelineno-0-1800 name=__codelineno-0-1800></a><span class=sd>    c_out:  int</span>
</span><span id=__span-0-1801><a id=__codelineno-0-1801 name=__codelineno-0-1801></a><span class=sd>        The output of the stochastic layer.</span>
</span><span id=__span-0-1802><a id=__codelineno-0-1802 name=__codelineno-0-1802></a><span class=sd>        Note that this is different from the sampled latent z.</span>
</span><span id=__span-0-1803><a id=__codelineno-0-1803 name=__codelineno-0-1803></a><span class=sd>    kernel: int, optional</span>
</span><span id=__span-0-1804><a id=__codelineno-0-1804 name=__codelineno-0-1804></a><span class=sd>        The size of the kernel used in convolutional layers.</span>
</span><span id=__span-0-1805><a id=__codelineno-0-1805 name=__codelineno-0-1805></a><span class=sd>        Default is 3.</span>
</span><span id=__span-0-1806><a id=__codelineno-0-1806 name=__codelineno-0-1806></a><span class=sd>    groups: int, optional</span>
</span><span id=__span-0-1807><a id=__codelineno-0-1807 name=__codelineno-0-1807></a><span class=sd>        The number of groups to consider in the convolutions of this layer.</span>
</span><span id=__span-0-1808><a id=__codelineno-0-1808 name=__codelineno-0-1808></a><span class=sd>        Default is 1.</span>
</span><span id=__span-0-1809><a id=__codelineno-0-1809 name=__codelineno-0-1809></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-1810><a id=__codelineno-0-1810 name=__codelineno-0-1810></a><span class=sd>        Whether to use bias term is the convolutional blocks of this layer.</span>
</span><span id=__span-0-1811><a id=__codelineno-0-1811 name=__codelineno-0-1811></a><span class=sd>        Default is `True`.</span>
</span><span id=__span-0-1812><a id=__codelineno-0-1812 name=__codelineno-0-1812></a><span class=sd>    transform_p_params: bool, optional</span>
</span><span id=__span-0-1813><a id=__codelineno-0-1813 name=__codelineno-0-1813></a><span class=sd>        Whether a transformation should be applied to the `p_params` tensor.</span>
</span><span id=__span-0-1814><a id=__codelineno-0-1814 name=__codelineno-0-1814></a><span class=sd>        The transformation consists in a 2D convolution ()`conv_in_p()`) that</span>
</span><span id=__span-0-1815><a id=__codelineno-0-1815 name=__codelineno-0-1815></a><span class=sd>        maps the input to a larger number of channels.</span>
</span><span id=__span-0-1816><a id=__codelineno-0-1816 name=__codelineno-0-1816></a><span class=sd>        Default is `True`.</span>
</span><span id=__span-0-1817><a id=__codelineno-0-1817 name=__codelineno-0-1817></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1818><a id=__codelineno-0-1818 name=__codelineno-0-1818></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-1819><a id=__codelineno-0-1819 name=__codelineno-0-1819></a>    <span class=k>assert</span> <span class=n>kernel</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span>
</span><span id=__span-0-1820><a id=__codelineno-0-1820 name=__codelineno-0-1820></a>    <span class=n>pad</span> <span class=o>=</span> <span class=n>kernel</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1821><a id=__codelineno-0-1821 name=__codelineno-0-1821></a>    <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span> <span class=o>=</span> <span class=n>transform_p_params</span>
</span><span id=__span-0-1822><a id=__codelineno-0-1822 name=__codelineno-0-1822></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_in</span> <span class=o>=</span> <span class=n>c_in</span>
</span><span id=__span-0-1823><a id=__codelineno-0-1823 name=__codelineno-0-1823></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_out</span> <span class=o>=</span> <span class=n>c_out</span>
</span><span id=__span-0-1824><a id=__codelineno-0-1824 name=__codelineno-0-1824></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span> <span class=o>=</span> <span class=n>c_vars</span>
</span><span id=__span-0-1825><a id=__codelineno-0-1825 name=__codelineno-0-1825></a>
</span><span id=__span-0-1826><a id=__codelineno-0-1826 name=__codelineno-0-1826></a>    <span class=k>if</span> <span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1827><a id=__codelineno-0-1827 name=__codelineno-0-1827></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1828><a id=__codelineno-0-1828 name=__codelineno-0-1828></a>            <span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1829><a id=__codelineno-0-1829 name=__codelineno-0-1829></a>        <span class=p>)</span>
</span><span id=__span-0-1830><a id=__codelineno-0-1830 name=__codelineno-0-1830></a>    <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1831><a id=__codelineno-0-1831 name=__codelineno-0-1831></a>        <span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1832><a id=__codelineno-0-1832 name=__codelineno-0-1832></a>    <span class=p>)</span>
</span><span id=__span-0-1833><a id=__codelineno-0-1833 name=__codelineno-0-1833></a>    <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-1834><a id=__codelineno-0-1834 name=__codelineno-0-1834></a>        <span class=n>c_vars</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span>
</span><span id=__span-0-1835><a id=__codelineno-0-1835 name=__codelineno-0-1835></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NonStochasticBlock2d.compute_kl_metrics class="doc doc-heading"> <code class="highlight language-python"><span class=n>compute_kl_metrics</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>q_params</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=p>,</span> <span class=n>z</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.compute_kl_metrics class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric. Specifically, the different versions of the KL loss terms are: - <code>kl_elementwise</code>: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)]. - <code>kl_samplewise</code>: KL term associated to each sample in the batch [Shape: (batch, )]. - <code>kl_samplewise_restricted</code>: KL term only associated to the portion of the latent tensor that is used for prediction and summed over channel and spatial dimensions [Shape: (batch, )]. - <code>kl_channelwise</code>: KL term associated to each sample and each channel [Shape: (batch, ch, )]. - <code>kl_spatial</code>: # KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</p> <p>NOTE: in this class all the KL metrics are set to <code>None</code>.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p</code></td> <td> <code><span title="torch.distributions.normal.Normal">Normal</span></code> </td> <td> <div class=doc-md-description> <p>The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The parameters of the prior generative distribution.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q</code></td> <td> <code><span title="torch.distributions.normal.Normal">Normal</span></code> </td> <td> <div class=doc-md-description> <p>The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The parameters of the inference distribution.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is in prediction mode.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>analytical_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>z</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The sampled latent tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1837>1837</a></span>
<span class=normal><a href=#__codelineno-0-1838>1838</a></span>
<span class=normal><a href=#__codelineno-0-1839>1839</a></span>
<span class=normal><a href=#__codelineno-0-1840>1840</a></span>
<span class=normal><a href=#__codelineno-0-1841>1841</a></span>
<span class=normal><a href=#__codelineno-0-1842>1842</a></span>
<span class=normal><a href=#__codelineno-0-1843>1843</a></span>
<span class=normal><a href=#__codelineno-0-1844>1844</a></span>
<span class=normal><a href=#__codelineno-0-1845>1845</a></span>
<span class=normal><a href=#__codelineno-0-1846>1846</a></span>
<span class=normal><a href=#__codelineno-0-1847>1847</a></span>
<span class=normal><a href=#__codelineno-0-1848>1848</a></span>
<span class=normal><a href=#__codelineno-0-1849>1849</a></span>
<span class=normal><a href=#__codelineno-0-1850>1850</a></span>
<span class=normal><a href=#__codelineno-0-1851>1851</a></span>
<span class=normal><a href=#__codelineno-0-1852>1852</a></span>
<span class=normal><a href=#__codelineno-0-1853>1853</a></span>
<span class=normal><a href=#__codelineno-0-1854>1854</a></span>
<span class=normal><a href=#__codelineno-0-1855>1855</a></span>
<span class=normal><a href=#__codelineno-0-1856>1856</a></span>
<span class=normal><a href=#__codelineno-0-1857>1857</a></span>
<span class=normal><a href=#__codelineno-0-1858>1858</a></span>
<span class=normal><a href=#__codelineno-0-1859>1859</a></span>
<span class=normal><a href=#__codelineno-0-1860>1860</a></span>
<span class=normal><a href=#__codelineno-0-1861>1861</a></span>
<span class=normal><a href=#__codelineno-0-1862>1862</a></span>
<span class=normal><a href=#__codelineno-0-1863>1863</a></span>
<span class=normal><a href=#__codelineno-0-1864>1864</a></span>
<span class=normal><a href=#__codelineno-0-1865>1865</a></span>
<span class=normal><a href=#__codelineno-0-1866>1866</a></span>
<span class=normal><a href=#__codelineno-0-1867>1867</a></span>
<span class=normal><a href=#__codelineno-0-1868>1868</a></span>
<span class=normal><a href=#__codelineno-0-1869>1869</a></span>
<span class=normal><a href=#__codelineno-0-1870>1870</a></span>
<span class=normal><a href=#__codelineno-0-1871>1871</a></span>
<span class=normal><a href=#__codelineno-0-1872>1872</a></span>
<span class=normal><a href=#__codelineno-0-1873>1873</a></span>
<span class=normal><a href=#__codelineno-0-1874>1874</a></span>
<span class=normal><a href=#__codelineno-0-1875>1875</a></span>
<span class=normal><a href=#__codelineno-0-1876>1876</a></span>
<span class=normal><a href=#__codelineno-0-1877>1877</a></span>
<span class=normal><a href=#__codelineno-0-1878>1878</a></span>
<span class=normal><a href=#__codelineno-0-1879>1879</a></span>
<span class=normal><a href=#__codelineno-0-1880>1880</a></span>
<span class=normal><a href=#__codelineno-0-1881>1881</a></span>
<span class=normal><a href=#__codelineno-0-1882>1882</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1837><a id=__codelineno-0-1837 name=__codelineno-0-1837></a><span class=k>def</span> <span class=nf>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1838><a id=__codelineno-0-1838 name=__codelineno-0-1838></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1839><a id=__codelineno-0-1839 name=__codelineno-0-1839></a>    <span class=n>p</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1840><a id=__codelineno-0-1840 name=__codelineno-0-1840></a>    <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1841><a id=__codelineno-0-1841 name=__codelineno-0-1841></a>    <span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1842><a id=__codelineno-0-1842 name=__codelineno-0-1842></a>    <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1843><a id=__codelineno-0-1843 name=__codelineno-0-1843></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1844><a id=__codelineno-0-1844 name=__codelineno-0-1844></a>    <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1845><a id=__codelineno-0-1845 name=__codelineno-0-1845></a>    <span class=n>z</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1846><a id=__codelineno-0-1846 name=__codelineno-0-1846></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=kc>None</span><span class=p>]:</span>
</span><span id=__span-0-1847><a id=__codelineno-0-1847 name=__codelineno-0-1847></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1848><a id=__codelineno-0-1848 name=__codelineno-0-1848></a><span class=sd>    Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.</span>
</span><span id=__span-0-1849><a id=__codelineno-0-1849 name=__codelineno-0-1849></a><span class=sd>    Specifically, the different versions of the KL loss terms are:</span>
</span><span id=__span-0-1850><a id=__codelineno-0-1850 name=__codelineno-0-1850></a><span class=sd>        - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].</span>
</span><span id=__span-0-1851><a id=__codelineno-0-1851 name=__codelineno-0-1851></a><span class=sd>        - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].</span>
</span><span id=__span-0-1852><a id=__codelineno-0-1852 name=__codelineno-0-1852></a><span class=sd>        - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is</span>
</span><span id=__span-0-1853><a id=__codelineno-0-1853 name=__codelineno-0-1853></a><span class=sd>        used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].</span>
</span><span id=__span-0-1854><a id=__codelineno-0-1854 name=__codelineno-0-1854></a><span class=sd>        - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].</span>
</span><span id=__span-0-1855><a id=__codelineno-0-1855 name=__codelineno-0-1855></a><span class=sd>        - `kl_spatial`: # KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1856><a id=__codelineno-0-1856 name=__codelineno-0-1856></a>
</span><span id=__span-0-1857><a id=__codelineno-0-1857 name=__codelineno-0-1857></a><span class=sd>    NOTE: in this class all the KL metrics are set to `None`.</span>
</span><span id=__span-0-1858><a id=__codelineno-0-1858 name=__codelineno-0-1858></a>
</span><span id=__span-0-1859><a id=__codelineno-0-1859 name=__codelineno-0-1859></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1860><a id=__codelineno-0-1860 name=__codelineno-0-1860></a><span class=sd>    ----------</span>
</span><span id=__span-0-1861><a id=__codelineno-0-1861 name=__codelineno-0-1861></a><span class=sd>    p: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1862><a id=__codelineno-0-1862 name=__codelineno-0-1862></a><span class=sd>        The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1863><a id=__codelineno-0-1863 name=__codelineno-0-1863></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1864><a id=__codelineno-0-1864 name=__codelineno-0-1864></a><span class=sd>        The parameters of the prior generative distribution.</span>
</span><span id=__span-0-1865><a id=__codelineno-0-1865 name=__codelineno-0-1865></a><span class=sd>    q: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1866><a id=__codelineno-0-1866 name=__codelineno-0-1866></a><span class=sd>        The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</span>
</span><span id=__span-0-1867><a id=__codelineno-0-1867 name=__codelineno-0-1867></a><span class=sd>    q_params: torch.Tensor</span>
</span><span id=__span-0-1868><a id=__codelineno-0-1868 name=__codelineno-0-1868></a><span class=sd>        The parameters of the inference distribution.</span>
</span><span id=__span-0-1869><a id=__codelineno-0-1869 name=__codelineno-0-1869></a><span class=sd>    mode_pred: bool</span>
</span><span id=__span-0-1870><a id=__codelineno-0-1870 name=__codelineno-0-1870></a><span class=sd>        Whether the model is in prediction mode.</span>
</span><span id=__span-0-1871><a id=__codelineno-0-1871 name=__codelineno-0-1871></a><span class=sd>    analytical_kl: bool</span>
</span><span id=__span-0-1872><a id=__codelineno-0-1872 name=__codelineno-0-1872></a><span class=sd>        Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1873><a id=__codelineno-0-1873 name=__codelineno-0-1873></a><span class=sd>    z: torch.Tensor</span>
</span><span id=__span-0-1874><a id=__codelineno-0-1874 name=__codelineno-0-1874></a><span class=sd>        The sampled latent tensor.</span>
</span><span id=__span-0-1875><a id=__codelineno-0-1875 name=__codelineno-0-1875></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1876><a id=__codelineno-0-1876 name=__codelineno-0-1876></a>    <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-0-1877><a id=__codelineno-0-1877 name=__codelineno-0-1877></a>        <span class=s2>&quot;kl_elementwise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1878><a id=__codelineno-0-1878 name=__codelineno-0-1878></a>        <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1879><a id=__codelineno-0-1879 name=__codelineno-0-1879></a>        <span class=s2>&quot;kl_spatial&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, h, w)</span>
</span><span id=__span-0-1880><a id=__codelineno-0-1880 name=__codelineno-0-1880></a>        <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>:</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># (batch, ch)</span>
</span><span id=__span-0-1881><a id=__codelineno-0-1881 name=__codelineno-0-1881></a>    <span class=p>}</span>
</span><span id=__span-0-1882><a id=__codelineno-0-1882 name=__codelineno-0-1882></a>    <span class=k>return</span> <span class=n>kl_dict</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NonStochasticBlock2d.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>q_params</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>forced_latent</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>use_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>force_constant_output</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>mode_pred</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>use_uncond_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NonStochasticBlock2d.forward class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor resulting from merging the bu_value tensor at the same hierarchical level from the bottom-up pass and the <code>p_params</code> tensor. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>forced_latent</code></td> <td> <code><span title="typing.Union">Union</span>[None, <span title="torch.Tensor">Tensor</span>]</code> </td> <td> <div class=doc-md-description> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>use_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Wheteher the latent tensor should be set as the latent distribution mode. In the case of Gaussian, the mode coincides with the mean of the distribution. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>force_constant_output</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>analytical_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>use_uncond_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1907>1907</a></span>
<span class=normal><a href=#__codelineno-0-1908>1908</a></span>
<span class=normal><a href=#__codelineno-0-1909>1909</a></span>
<span class=normal><a href=#__codelineno-0-1910>1910</a></span>
<span class=normal><a href=#__codelineno-0-1911>1911</a></span>
<span class=normal><a href=#__codelineno-0-1912>1912</a></span>
<span class=normal><a href=#__codelineno-0-1913>1913</a></span>
<span class=normal><a href=#__codelineno-0-1914>1914</a></span>
<span class=normal><a href=#__codelineno-0-1915>1915</a></span>
<span class=normal><a href=#__codelineno-0-1916>1916</a></span>
<span class=normal><a href=#__codelineno-0-1917>1917</a></span>
<span class=normal><a href=#__codelineno-0-1918>1918</a></span>
<span class=normal><a href=#__codelineno-0-1919>1919</a></span>
<span class=normal><a href=#__codelineno-0-1920>1920</a></span>
<span class=normal><a href=#__codelineno-0-1921>1921</a></span>
<span class=normal><a href=#__codelineno-0-1922>1922</a></span>
<span class=normal><a href=#__codelineno-0-1923>1923</a></span>
<span class=normal><a href=#__codelineno-0-1924>1924</a></span>
<span class=normal><a href=#__codelineno-0-1925>1925</a></span>
<span class=normal><a href=#__codelineno-0-1926>1926</a></span>
<span class=normal><a href=#__codelineno-0-1927>1927</a></span>
<span class=normal><a href=#__codelineno-0-1928>1928</a></span>
<span class=normal><a href=#__codelineno-0-1929>1929</a></span>
<span class=normal><a href=#__codelineno-0-1930>1930</a></span>
<span class=normal><a href=#__codelineno-0-1931>1931</a></span>
<span class=normal><a href=#__codelineno-0-1932>1932</a></span>
<span class=normal><a href=#__codelineno-0-1933>1933</a></span>
<span class=normal><a href=#__codelineno-0-1934>1934</a></span>
<span class=normal><a href=#__codelineno-0-1935>1935</a></span>
<span class=normal><a href=#__codelineno-0-1936>1936</a></span>
<span class=normal><a href=#__codelineno-0-1937>1937</a></span>
<span class=normal><a href=#__codelineno-0-1938>1938</a></span>
<span class=normal><a href=#__codelineno-0-1939>1939</a></span>
<span class=normal><a href=#__codelineno-0-1940>1940</a></span>
<span class=normal><a href=#__codelineno-0-1941>1941</a></span>
<span class=normal><a href=#__codelineno-0-1942>1942</a></span>
<span class=normal><a href=#__codelineno-0-1943>1943</a></span>
<span class=normal><a href=#__codelineno-0-1944>1944</a></span>
<span class=normal><a href=#__codelineno-0-1945>1945</a></span>
<span class=normal><a href=#__codelineno-0-1946>1946</a></span>
<span class=normal><a href=#__codelineno-0-1947>1947</a></span>
<span class=normal><a href=#__codelineno-0-1948>1948</a></span>
<span class=normal><a href=#__codelineno-0-1949>1949</a></span>
<span class=normal><a href=#__codelineno-0-1950>1950</a></span>
<span class=normal><a href=#__codelineno-0-1951>1951</a></span>
<span class=normal><a href=#__codelineno-0-1952>1952</a></span>
<span class=normal><a href=#__codelineno-0-1953>1953</a></span>
<span class=normal><a href=#__codelineno-0-1954>1954</a></span>
<span class=normal><a href=#__codelineno-0-1955>1955</a></span>
<span class=normal><a href=#__codelineno-0-1956>1956</a></span>
<span class=normal><a href=#__codelineno-0-1957>1957</a></span>
<span class=normal><a href=#__codelineno-0-1958>1958</a></span>
<span class=normal><a href=#__codelineno-0-1959>1959</a></span>
<span class=normal><a href=#__codelineno-0-1960>1960</a></span>
<span class=normal><a href=#__codelineno-0-1961>1961</a></span>
<span class=normal><a href=#__codelineno-0-1962>1962</a></span>
<span class=normal><a href=#__codelineno-0-1963>1963</a></span>
<span class=normal><a href=#__codelineno-0-1964>1964</a></span>
<span class=normal><a href=#__codelineno-0-1965>1965</a></span>
<span class=normal><a href=#__codelineno-0-1966>1966</a></span>
<span class=normal><a href=#__codelineno-0-1967>1967</a></span>
<span class=normal><a href=#__codelineno-0-1968>1968</a></span>
<span class=normal><a href=#__codelineno-0-1969>1969</a></span>
<span class=normal><a href=#__codelineno-0-1970>1970</a></span>
<span class=normal><a href=#__codelineno-0-1971>1971</a></span>
<span class=normal><a href=#__codelineno-0-1972>1972</a></span>
<span class=normal><a href=#__codelineno-0-1973>1973</a></span>
<span class=normal><a href=#__codelineno-0-1974>1974</a></span>
<span class=normal><a href=#__codelineno-0-1975>1975</a></span>
<span class=normal><a href=#__codelineno-0-1976>1976</a></span>
<span class=normal><a href=#__codelineno-0-1977>1977</a></span>
<span class=normal><a href=#__codelineno-0-1978>1978</a></span>
<span class=normal><a href=#__codelineno-0-1979>1979</a></span>
<span class=normal><a href=#__codelineno-0-1980>1980</a></span>
<span class=normal><a href=#__codelineno-0-1981>1981</a></span>
<span class=normal><a href=#__codelineno-0-1982>1982</a></span>
<span class=normal><a href=#__codelineno-0-1983>1983</a></span>
<span class=normal><a href=#__codelineno-0-1984>1984</a></span>
<span class=normal><a href=#__codelineno-0-1985>1985</a></span>
<span class=normal><a href=#__codelineno-0-1986>1986</a></span>
<span class=normal><a href=#__codelineno-0-1987>1987</a></span>
<span class=normal><a href=#__codelineno-0-1988>1988</a></span>
<span class=normal><a href=#__codelineno-0-1989>1989</a></span>
<span class=normal><a href=#__codelineno-0-1990>1990</a></span>
<span class=normal><a href=#__codelineno-0-1991>1991</a></span>
<span class=normal><a href=#__codelineno-0-1992>1992</a></span>
<span class=normal><a href=#__codelineno-0-1993>1993</a></span>
<span class=normal><a href=#__codelineno-0-1994>1994</a></span>
<span class=normal><a href=#__codelineno-0-1995>1995</a></span>
<span class=normal><a href=#__codelineno-0-1996>1996</a></span>
<span class=normal><a href=#__codelineno-0-1997>1997</a></span>
<span class=normal><a href=#__codelineno-0-1998>1998</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1907><a id=__codelineno-0-1907 name=__codelineno-0-1907></a><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1908><a id=__codelineno-0-1908 name=__codelineno-0-1908></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1909><a id=__codelineno-0-1909 name=__codelineno-0-1909></a>    <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1910><a id=__codelineno-0-1910 name=__codelineno-0-1910></a>    <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1911><a id=__codelineno-0-1911 name=__codelineno-0-1911></a>    <span class=n>forced_latent</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1912><a id=__codelineno-0-1912 name=__codelineno-0-1912></a>    <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1913><a id=__codelineno-0-1913 name=__codelineno-0-1913></a>    <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1914><a id=__codelineno-0-1914 name=__codelineno-0-1914></a>    <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1915><a id=__codelineno-0-1915 name=__codelineno-0-1915></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1916><a id=__codelineno-0-1916 name=__codelineno-0-1916></a>    <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1917><a id=__codelineno-0-1917 name=__codelineno-0-1917></a>    <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1918><a id=__codelineno-0-1918 name=__codelineno-0-1918></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1919><a id=__codelineno-0-1919 name=__codelineno-0-1919></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1920><a id=__codelineno-0-1920 name=__codelineno-0-1920></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1921><a id=__codelineno-0-1921 name=__codelineno-0-1921></a><span class=sd>    ----------</span>
</span><span id=__span-0-1922><a id=__codelineno-0-1922 name=__codelineno-0-1922></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1923><a id=__codelineno-0-1923 name=__codelineno-0-1923></a><span class=sd>        The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</span>
</span><span id=__span-0-1924><a id=__codelineno-0-1924 name=__codelineno-0-1924></a><span class=sd>    q_params: torch.Tensor, optional</span>
</span><span id=__span-0-1925><a id=__codelineno-0-1925 name=__codelineno-0-1925></a><span class=sd>        The tensor resulting from merging the bu_value tensor at the same hierarchical level</span>
</span><span id=__span-0-1926><a id=__codelineno-0-1926 name=__codelineno-0-1926></a><span class=sd>        from the bottom-up pass and the `p_params` tensor. Default is `None`.</span>
</span><span id=__span-0-1927><a id=__codelineno-0-1927 name=__codelineno-0-1927></a><span class=sd>    forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1928><a id=__codelineno-0-1928 name=__codelineno-0-1928></a><span class=sd>        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent</span>
</span><span id=__span-0-1929><a id=__codelineno-0-1929 name=__codelineno-0-1929></a><span class=sd>        tensor and, hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1930><a id=__codelineno-0-1930 name=__codelineno-0-1930></a><span class=sd>    use_mode: bool, optional</span>
</span><span id=__span-0-1931><a id=__codelineno-0-1931 name=__codelineno-0-1931></a><span class=sd>        Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1932><a id=__codelineno-0-1932 name=__codelineno-0-1932></a><span class=sd>        In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1933><a id=__codelineno-0-1933 name=__codelineno-0-1933></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1934><a id=__codelineno-0-1934 name=__codelineno-0-1934></a><span class=sd>    force_constant_output: bool, optional</span>
</span><span id=__span-0-1935><a id=__codelineno-0-1935 name=__codelineno-0-1935></a><span class=sd>        Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1936><a id=__codelineno-0-1936 name=__codelineno-0-1936></a><span class=sd>        This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1937><a id=__codelineno-0-1937 name=__codelineno-0-1937></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1938><a id=__codelineno-0-1938 name=__codelineno-0-1938></a><span class=sd>    analytical_kl: bool, optional</span>
</span><span id=__span-0-1939><a id=__codelineno-0-1939 name=__codelineno-0-1939></a><span class=sd>        Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1940><a id=__codelineno-0-1940 name=__codelineno-0-1940></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1941><a id=__codelineno-0-1941 name=__codelineno-0-1941></a><span class=sd>    mode_pred: bool, optional</span>
</span><span id=__span-0-1942><a id=__codelineno-0-1942 name=__codelineno-0-1942></a><span class=sd>        Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1943><a id=__codelineno-0-1943 name=__codelineno-0-1943></a><span class=sd>    use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1944><a id=__codelineno-0-1944 name=__codelineno-0-1944></a><span class=sd>        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1945><a id=__codelineno-0-1945 name=__codelineno-0-1945></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1946><a id=__codelineno-0-1946 name=__codelineno-0-1946></a><span class=sd>    var_clip_max: float, optional</span>
</span><span id=__span-0-1947><a id=__codelineno-0-1947 name=__codelineno-0-1947></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1948><a id=__codelineno-0-1948 name=__codelineno-0-1948></a><span class=sd>        Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1949><a id=__codelineno-0-1949 name=__codelineno-0-1949></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1950><a id=__codelineno-0-1950 name=__codelineno-0-1950></a>    <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-1951><a id=__codelineno-0-1951 name=__codelineno-0-1951></a>    <span class=k>assert</span> <span class=p>(</span><span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>)</span> <span class=ow>or</span> <span class=p>(</span><span class=ow>not</span> <span class=n>use_mode</span><span class=p>)</span>
</span><span id=__span-0-1952><a id=__codelineno-0-1952 name=__codelineno-0-1952></a>
</span><span id=__span-0-1953><a id=__codelineno-0-1953 name=__codelineno-0-1953></a>    <span class=n>p_mu</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_p_params</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1954><a id=__codelineno-0-1954 name=__codelineno-0-1954></a>
</span><span id=__span-0-1955><a id=__codelineno-0-1955 name=__codelineno-0-1955></a>    <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_mu</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span><span id=__span-0-1956><a id=__codelineno-0-1956 name=__codelineno-0-1956></a>
</span><span id=__span-0-1957><a id=__codelineno-0-1957 name=__codelineno-0-1957></a>    <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1958><a id=__codelineno-0-1958 name=__codelineno-0-1958></a>        <span class=c1># At inference time, just don&#39;t centercrop the q_params even if they are odd in size.</span>
</span><span id=__span-0-1959><a id=__codelineno-0-1959 name=__codelineno-0-1959></a>        <span class=n>q_mu</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1960><a id=__codelineno-0-1960 name=__codelineno-0-1960></a>            <span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1961><a id=__codelineno-0-1961 name=__codelineno-0-1961></a>        <span class=p>)</span>
</span><span id=__span-0-1962><a id=__codelineno-0-1962 name=__codelineno-0-1962></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span><span id=__span-0-1963><a id=__codelineno-0-1963 name=__codelineno-0-1963></a>        <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>([</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>q_mu</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span><span id=__span-0-1964><a id=__codelineno-0-1964 name=__codelineno-0-1964></a>        <span class=c1># Sample from q(z)</span>
</span><span id=__span-0-1965><a id=__codelineno-0-1965 name=__codelineno-0-1965></a>        <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>q_mu</span>
</span><span id=__span-0-1966><a id=__codelineno-0-1966 name=__codelineno-0-1966></a>        <span class=n>q_size</span> <span class=o>=</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1967><a id=__codelineno-0-1967 name=__codelineno-0-1967></a>        <span class=k>if</span> <span class=n>p_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>!=</span> <span class=n>q_size</span> <span class=ow>and</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1968><a id=__codelineno-0-1968 name=__codelineno-0-1968></a>            <span class=n>p_mu</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1969><a id=__codelineno-0-1969 name=__codelineno-0-1969></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1970><a id=__codelineno-0-1970 name=__codelineno-0-1970></a>        <span class=c1># Sample from p(z)</span>
</span><span id=__span-0-1971><a id=__codelineno-0-1971 name=__codelineno-0-1971></a>        <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>p_mu</span>
</span><span id=__span-0-1972><a id=__codelineno-0-1972 name=__codelineno-0-1972></a>
</span><span id=__span-0-1973><a id=__codelineno-0-1973 name=__codelineno-0-1973></a>    <span class=c1># Generate latent variable (typically by sampling)</span>
</span><span id=__span-0-1974><a id=__codelineno-0-1974 name=__codelineno-0-1974></a>    <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span>
</span><span id=__span-0-1975><a id=__codelineno-0-1975 name=__codelineno-0-1975></a>
</span><span id=__span-0-1976><a id=__codelineno-0-1976 name=__codelineno-0-1976></a>    <span class=c1># Copy one sample (and distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1977><a id=__codelineno-0-1977 name=__codelineno-0-1977></a>    <span class=c1># This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1978><a id=__codelineno-0-1978 name=__codelineno-0-1978></a>    <span class=k>if</span> <span class=n>force_constant_output</span><span class=p>:</span>
</span><span id=__span-0-1979><a id=__codelineno-0-1979 name=__codelineno-0-1979></a>        <span class=n>z</span> <span class=o>=</span> <span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span><span id=__span-0-1980><a id=__codelineno-0-1980 name=__codelineno-0-1980></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1981><a id=__codelineno-0-1981 name=__codelineno-0-1981></a>            <span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1982><a id=__codelineno-0-1982 name=__codelineno-0-1982></a>            <span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1983><a id=__codelineno-0-1983 name=__codelineno-0-1983></a>        <span class=p>)</span>
</span><span id=__span-0-1984><a id=__codelineno-0-1984 name=__codelineno-0-1984></a>
</span><span id=__span-0-1985><a id=__codelineno-0-1985 name=__codelineno-0-1985></a>    <span class=c1># Output of stochastic layer</span>
</span><span id=__span-0-1986><a id=__codelineno-0-1986 name=__codelineno-0-1986></a>    <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-0-1987><a id=__codelineno-0-1987 name=__codelineno-0-1987></a>
</span><span id=__span-0-1988><a id=__codelineno-0-1988 name=__codelineno-0-1988></a>    <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-0-1989><a id=__codelineno-0-1989 name=__codelineno-0-1989></a>    <span class=n>logprob_q</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1990><a id=__codelineno-0-1990 name=__codelineno-0-1990></a>
</span><span id=__span-0-1991><a id=__codelineno-0-1991 name=__codelineno-0-1991></a>    <span class=n>data</span> <span class=o>=</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1992><a id=__codelineno-0-1992 name=__codelineno-0-1992></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;z&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>z</span>  <span class=c1># sampled variable at this layer (batch, ch, h, w)</span>
</span><span id=__span-0-1993><a id=__codelineno-0-1993 name=__codelineno-0-1993></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;p_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>p_params</span>  <span class=c1># (b, ch, h, w) where b is 1 or batch size</span>
</span><span id=__span-0-1994><a id=__codelineno-0-1994 name=__codelineno-0-1994></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_params</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1995><a id=__codelineno-0-1995 name=__codelineno-0-1995></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;logprob_q&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>logprob_q</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1996><a id=__codelineno-0-1996 name=__codelineno-0-1996></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;qvar_max&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>debug_qvar_max</span>
</span><span id=__span-0-1997><a id=__codelineno-0-1997 name=__codelineno-0-1997></a>
</span><span id=__span-0-1998><a id=__codelineno-0-1998 name=__codelineno-0-1998></a>    <span class=k>return</span> <span class=n>out</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.NormalStochasticBlock2d class="doc doc-heading"> <code>NormalStochasticBlock2d</code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Stochastic block used in the Top-Down inference pass.</p> <p>Algorithm: - map input parameters to q(z) and (optionally) p(z) via convolution - sample a latent tensor z ~ q(z) - feed z to convolution and return.</p> <p>NOTE 1: If parameters for q are not given, sampling is done from p(z).</p> <p>NOTE 2: The restricted KL divergence is obtained by first computing the element-wise KL divergence (i.e., the KL computed for each element of the latent tensors). Then, the restricted version is computed by summing over the channels and the spatial dimensions associated only to the portion of the latent tensor that is used for prediction.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1340>1340</a></span>
<span class=normal><a href=#__codelineno-0-1341>1341</a></span>
<span class=normal><a href=#__codelineno-0-1342>1342</a></span>
<span class=normal><a href=#__codelineno-0-1343>1343</a></span>
<span class=normal><a href=#__codelineno-0-1344>1344</a></span>
<span class=normal><a href=#__codelineno-0-1345>1345</a></span>
<span class=normal><a href=#__codelineno-0-1346>1346</a></span>
<span class=normal><a href=#__codelineno-0-1347>1347</a></span>
<span class=normal><a href=#__codelineno-0-1348>1348</a></span>
<span class=normal><a href=#__codelineno-0-1349>1349</a></span>
<span class=normal><a href=#__codelineno-0-1350>1350</a></span>
<span class=normal><a href=#__codelineno-0-1351>1351</a></span>
<span class=normal><a href=#__codelineno-0-1352>1352</a></span>
<span class=normal><a href=#__codelineno-0-1353>1353</a></span>
<span class=normal><a href=#__codelineno-0-1354>1354</a></span>
<span class=normal><a href=#__codelineno-0-1355>1355</a></span>
<span class=normal><a href=#__codelineno-0-1356>1356</a></span>
<span class=normal><a href=#__codelineno-0-1357>1357</a></span>
<span class=normal><a href=#__codelineno-0-1358>1358</a></span>
<span class=normal><a href=#__codelineno-0-1359>1359</a></span>
<span class=normal><a href=#__codelineno-0-1360>1360</a></span>
<span class=normal><a href=#__codelineno-0-1361>1361</a></span>
<span class=normal><a href=#__codelineno-0-1362>1362</a></span>
<span class=normal><a href=#__codelineno-0-1363>1363</a></span>
<span class=normal><a href=#__codelineno-0-1364>1364</a></span>
<span class=normal><a href=#__codelineno-0-1365>1365</a></span>
<span class=normal><a href=#__codelineno-0-1366>1366</a></span>
<span class=normal><a href=#__codelineno-0-1367>1367</a></span>
<span class=normal><a href=#__codelineno-0-1368>1368</a></span>
<span class=normal><a href=#__codelineno-0-1369>1369</a></span>
<span class=normal><a href=#__codelineno-0-1370>1370</a></span>
<span class=normal><a href=#__codelineno-0-1371>1371</a></span>
<span class=normal><a href=#__codelineno-0-1372>1372</a></span>
<span class=normal><a href=#__codelineno-0-1373>1373</a></span>
<span class=normal><a href=#__codelineno-0-1374>1374</a></span>
<span class=normal><a href=#__codelineno-0-1375>1375</a></span>
<span class=normal><a href=#__codelineno-0-1376>1376</a></span>
<span class=normal><a href=#__codelineno-0-1377>1377</a></span>
<span class=normal><a href=#__codelineno-0-1378>1378</a></span>
<span class=normal><a href=#__codelineno-0-1379>1379</a></span>
<span class=normal><a href=#__codelineno-0-1380>1380</a></span>
<span class=normal><a href=#__codelineno-0-1381>1381</a></span>
<span class=normal><a href=#__codelineno-0-1382>1382</a></span>
<span class=normal><a href=#__codelineno-0-1383>1383</a></span>
<span class=normal><a href=#__codelineno-0-1384>1384</a></span>
<span class=normal><a href=#__codelineno-0-1385>1385</a></span>
<span class=normal><a href=#__codelineno-0-1386>1386</a></span>
<span class=normal><a href=#__codelineno-0-1387>1387</a></span>
<span class=normal><a href=#__codelineno-0-1388>1388</a></span>
<span class=normal><a href=#__codelineno-0-1389>1389</a></span>
<span class=normal><a href=#__codelineno-0-1390>1390</a></span>
<span class=normal><a href=#__codelineno-0-1391>1391</a></span>
<span class=normal><a href=#__codelineno-0-1392>1392</a></span>
<span class=normal><a href=#__codelineno-0-1393>1393</a></span>
<span class=normal><a href=#__codelineno-0-1394>1394</a></span>
<span class=normal><a href=#__codelineno-0-1395>1395</a></span>
<span class=normal><a href=#__codelineno-0-1396>1396</a></span>
<span class=normal><a href=#__codelineno-0-1397>1397</a></span>
<span class=normal><a href=#__codelineno-0-1398>1398</a></span>
<span class=normal><a href=#__codelineno-0-1399>1399</a></span>
<span class=normal><a href=#__codelineno-0-1400>1400</a></span>
<span class=normal><a href=#__codelineno-0-1401>1401</a></span>
<span class=normal><a href=#__codelineno-0-1402>1402</a></span>
<span class=normal><a href=#__codelineno-0-1403>1403</a></span>
<span class=normal><a href=#__codelineno-0-1404>1404</a></span>
<span class=normal><a href=#__codelineno-0-1405>1405</a></span>
<span class=normal><a href=#__codelineno-0-1406>1406</a></span>
<span class=normal><a href=#__codelineno-0-1407>1407</a></span>
<span class=normal><a href=#__codelineno-0-1408>1408</a></span>
<span class=normal><a href=#__codelineno-0-1409>1409</a></span>
<span class=normal><a href=#__codelineno-0-1410>1410</a></span>
<span class=normal><a href=#__codelineno-0-1411>1411</a></span>
<span class=normal><a href=#__codelineno-0-1412>1412</a></span>
<span class=normal><a href=#__codelineno-0-1413>1413</a></span>
<span class=normal><a href=#__codelineno-0-1414>1414</a></span>
<span class=normal><a href=#__codelineno-0-1415>1415</a></span>
<span class=normal><a href=#__codelineno-0-1416>1416</a></span>
<span class=normal><a href=#__codelineno-0-1417>1417</a></span>
<span class=normal><a href=#__codelineno-0-1418>1418</a></span>
<span class=normal><a href=#__codelineno-0-1419>1419</a></span>
<span class=normal><a href=#__codelineno-0-1420>1420</a></span>
<span class=normal><a href=#__codelineno-0-1421>1421</a></span>
<span class=normal><a href=#__codelineno-0-1422>1422</a></span>
<span class=normal><a href=#__codelineno-0-1423>1423</a></span>
<span class=normal><a href=#__codelineno-0-1424>1424</a></span>
<span class=normal><a href=#__codelineno-0-1425>1425</a></span>
<span class=normal><a href=#__codelineno-0-1426>1426</a></span>
<span class=normal><a href=#__codelineno-0-1427>1427</a></span>
<span class=normal><a href=#__codelineno-0-1428>1428</a></span>
<span class=normal><a href=#__codelineno-0-1429>1429</a></span>
<span class=normal><a href=#__codelineno-0-1430>1430</a></span>
<span class=normal><a href=#__codelineno-0-1431>1431</a></span>
<span class=normal><a href=#__codelineno-0-1432>1432</a></span>
<span class=normal><a href=#__codelineno-0-1433>1433</a></span>
<span class=normal><a href=#__codelineno-0-1434>1434</a></span>
<span class=normal><a href=#__codelineno-0-1435>1435</a></span>
<span class=normal><a href=#__codelineno-0-1436>1436</a></span>
<span class=normal><a href=#__codelineno-0-1437>1437</a></span>
<span class=normal><a href=#__codelineno-0-1438>1438</a></span>
<span class=normal><a href=#__codelineno-0-1439>1439</a></span>
<span class=normal><a href=#__codelineno-0-1440>1440</a></span>
<span class=normal><a href=#__codelineno-0-1441>1441</a></span>
<span class=normal><a href=#__codelineno-0-1442>1442</a></span>
<span class=normal><a href=#__codelineno-0-1443>1443</a></span>
<span class=normal><a href=#__codelineno-0-1444>1444</a></span>
<span class=normal><a href=#__codelineno-0-1445>1445</a></span>
<span class=normal><a href=#__codelineno-0-1446>1446</a></span>
<span class=normal><a href=#__codelineno-0-1447>1447</a></span>
<span class=normal><a href=#__codelineno-0-1448>1448</a></span>
<span class=normal><a href=#__codelineno-0-1449>1449</a></span>
<span class=normal><a href=#__codelineno-0-1450>1450</a></span>
<span class=normal><a href=#__codelineno-0-1451>1451</a></span>
<span class=normal><a href=#__codelineno-0-1452>1452</a></span>
<span class=normal><a href=#__codelineno-0-1453>1453</a></span>
<span class=normal><a href=#__codelineno-0-1454>1454</a></span>
<span class=normal><a href=#__codelineno-0-1455>1455</a></span>
<span class=normal><a href=#__codelineno-0-1456>1456</a></span>
<span class=normal><a href=#__codelineno-0-1457>1457</a></span>
<span class=normal><a href=#__codelineno-0-1458>1458</a></span>
<span class=normal><a href=#__codelineno-0-1459>1459</a></span>
<span class=normal><a href=#__codelineno-0-1460>1460</a></span>
<span class=normal><a href=#__codelineno-0-1461>1461</a></span>
<span class=normal><a href=#__codelineno-0-1462>1462</a></span>
<span class=normal><a href=#__codelineno-0-1463>1463</a></span>
<span class=normal><a href=#__codelineno-0-1464>1464</a></span>
<span class=normal><a href=#__codelineno-0-1465>1465</a></span>
<span class=normal><a href=#__codelineno-0-1466>1466</a></span>
<span class=normal><a href=#__codelineno-0-1467>1467</a></span>
<span class=normal><a href=#__codelineno-0-1468>1468</a></span>
<span class=normal><a href=#__codelineno-0-1469>1469</a></span>
<span class=normal><a href=#__codelineno-0-1470>1470</a></span>
<span class=normal><a href=#__codelineno-0-1471>1471</a></span>
<span class=normal><a href=#__codelineno-0-1472>1472</a></span>
<span class=normal><a href=#__codelineno-0-1473>1473</a></span>
<span class=normal><a href=#__codelineno-0-1474>1474</a></span>
<span class=normal><a href=#__codelineno-0-1475>1475</a></span>
<span class=normal><a href=#__codelineno-0-1476>1476</a></span>
<span class=normal><a href=#__codelineno-0-1477>1477</a></span>
<span class=normal><a href=#__codelineno-0-1478>1478</a></span>
<span class=normal><a href=#__codelineno-0-1479>1479</a></span>
<span class=normal><a href=#__codelineno-0-1480>1480</a></span>
<span class=normal><a href=#__codelineno-0-1481>1481</a></span>
<span class=normal><a href=#__codelineno-0-1482>1482</a></span>
<span class=normal><a href=#__codelineno-0-1483>1483</a></span>
<span class=normal><a href=#__codelineno-0-1484>1484</a></span>
<span class=normal><a href=#__codelineno-0-1485>1485</a></span>
<span class=normal><a href=#__codelineno-0-1486>1486</a></span>
<span class=normal><a href=#__codelineno-0-1487>1487</a></span>
<span class=normal><a href=#__codelineno-0-1488>1488</a></span>
<span class=normal><a href=#__codelineno-0-1489>1489</a></span>
<span class=normal><a href=#__codelineno-0-1490>1490</a></span>
<span class=normal><a href=#__codelineno-0-1491>1491</a></span>
<span class=normal><a href=#__codelineno-0-1492>1492</a></span>
<span class=normal><a href=#__codelineno-0-1493>1493</a></span>
<span class=normal><a href=#__codelineno-0-1494>1494</a></span>
<span class=normal><a href=#__codelineno-0-1495>1495</a></span>
<span class=normal><a href=#__codelineno-0-1496>1496</a></span>
<span class=normal><a href=#__codelineno-0-1497>1497</a></span>
<span class=normal><a href=#__codelineno-0-1498>1498</a></span>
<span class=normal><a href=#__codelineno-0-1499>1499</a></span>
<span class=normal><a href=#__codelineno-0-1500>1500</a></span>
<span class=normal><a href=#__codelineno-0-1501>1501</a></span>
<span class=normal><a href=#__codelineno-0-1502>1502</a></span>
<span class=normal><a href=#__codelineno-0-1503>1503</a></span>
<span class=normal><a href=#__codelineno-0-1504>1504</a></span>
<span class=normal><a href=#__codelineno-0-1505>1505</a></span>
<span class=normal><a href=#__codelineno-0-1506>1506</a></span>
<span class=normal><a href=#__codelineno-0-1507>1507</a></span>
<span class=normal><a href=#__codelineno-0-1508>1508</a></span>
<span class=normal><a href=#__codelineno-0-1509>1509</a></span>
<span class=normal><a href=#__codelineno-0-1510>1510</a></span>
<span class=normal><a href=#__codelineno-0-1511>1511</a></span>
<span class=normal><a href=#__codelineno-0-1512>1512</a></span>
<span class=normal><a href=#__codelineno-0-1513>1513</a></span>
<span class=normal><a href=#__codelineno-0-1514>1514</a></span>
<span class=normal><a href=#__codelineno-0-1515>1515</a></span>
<span class=normal><a href=#__codelineno-0-1516>1516</a></span>
<span class=normal><a href=#__codelineno-0-1517>1517</a></span>
<span class=normal><a href=#__codelineno-0-1518>1518</a></span>
<span class=normal><a href=#__codelineno-0-1519>1519</a></span>
<span class=normal><a href=#__codelineno-0-1520>1520</a></span>
<span class=normal><a href=#__codelineno-0-1521>1521</a></span>
<span class=normal><a href=#__codelineno-0-1522>1522</a></span>
<span class=normal><a href=#__codelineno-0-1523>1523</a></span>
<span class=normal><a href=#__codelineno-0-1524>1524</a></span>
<span class=normal><a href=#__codelineno-0-1525>1525</a></span>
<span class=normal><a href=#__codelineno-0-1526>1526</a></span>
<span class=normal><a href=#__codelineno-0-1527>1527</a></span>
<span class=normal><a href=#__codelineno-0-1528>1528</a></span>
<span class=normal><a href=#__codelineno-0-1529>1529</a></span>
<span class=normal><a href=#__codelineno-0-1530>1530</a></span>
<span class=normal><a href=#__codelineno-0-1531>1531</a></span>
<span class=normal><a href=#__codelineno-0-1532>1532</a></span>
<span class=normal><a href=#__codelineno-0-1533>1533</a></span>
<span class=normal><a href=#__codelineno-0-1534>1534</a></span>
<span class=normal><a href=#__codelineno-0-1535>1535</a></span>
<span class=normal><a href=#__codelineno-0-1536>1536</a></span>
<span class=normal><a href=#__codelineno-0-1537>1537</a></span>
<span class=normal><a href=#__codelineno-0-1538>1538</a></span>
<span class=normal><a href=#__codelineno-0-1539>1539</a></span>
<span class=normal><a href=#__codelineno-0-1540>1540</a></span>
<span class=normal><a href=#__codelineno-0-1541>1541</a></span>
<span class=normal><a href=#__codelineno-0-1542>1542</a></span>
<span class=normal><a href=#__codelineno-0-1543>1543</a></span>
<span class=normal><a href=#__codelineno-0-1544>1544</a></span>
<span class=normal><a href=#__codelineno-0-1545>1545</a></span>
<span class=normal><a href=#__codelineno-0-1546>1546</a></span>
<span class=normal><a href=#__codelineno-0-1547>1547</a></span>
<span class=normal><a href=#__codelineno-0-1548>1548</a></span>
<span class=normal><a href=#__codelineno-0-1549>1549</a></span>
<span class=normal><a href=#__codelineno-0-1550>1550</a></span>
<span class=normal><a href=#__codelineno-0-1551>1551</a></span>
<span class=normal><a href=#__codelineno-0-1552>1552</a></span>
<span class=normal><a href=#__codelineno-0-1553>1553</a></span>
<span class=normal><a href=#__codelineno-0-1554>1554</a></span>
<span class=normal><a href=#__codelineno-0-1555>1555</a></span>
<span class=normal><a href=#__codelineno-0-1556>1556</a></span>
<span class=normal><a href=#__codelineno-0-1557>1557</a></span>
<span class=normal><a href=#__codelineno-0-1558>1558</a></span>
<span class=normal><a href=#__codelineno-0-1559>1559</a></span>
<span class=normal><a href=#__codelineno-0-1560>1560</a></span>
<span class=normal><a href=#__codelineno-0-1561>1561</a></span>
<span class=normal><a href=#__codelineno-0-1562>1562</a></span>
<span class=normal><a href=#__codelineno-0-1563>1563</a></span>
<span class=normal><a href=#__codelineno-0-1564>1564</a></span>
<span class=normal><a href=#__codelineno-0-1565>1565</a></span>
<span class=normal><a href=#__codelineno-0-1566>1566</a></span>
<span class=normal><a href=#__codelineno-0-1567>1567</a></span>
<span class=normal><a href=#__codelineno-0-1568>1568</a></span>
<span class=normal><a href=#__codelineno-0-1569>1569</a></span>
<span class=normal><a href=#__codelineno-0-1570>1570</a></span>
<span class=normal><a href=#__codelineno-0-1571>1571</a></span>
<span class=normal><a href=#__codelineno-0-1572>1572</a></span>
<span class=normal><a href=#__codelineno-0-1573>1573</a></span>
<span class=normal><a href=#__codelineno-0-1574>1574</a></span>
<span class=normal><a href=#__codelineno-0-1575>1575</a></span>
<span class=normal><a href=#__codelineno-0-1576>1576</a></span>
<span class=normal><a href=#__codelineno-0-1577>1577</a></span>
<span class=normal><a href=#__codelineno-0-1578>1578</a></span>
<span class=normal><a href=#__codelineno-0-1579>1579</a></span>
<span class=normal><a href=#__codelineno-0-1580>1580</a></span>
<span class=normal><a href=#__codelineno-0-1581>1581</a></span>
<span class=normal><a href=#__codelineno-0-1582>1582</a></span>
<span class=normal><a href=#__codelineno-0-1583>1583</a></span>
<span class=normal><a href=#__codelineno-0-1584>1584</a></span>
<span class=normal><a href=#__codelineno-0-1585>1585</a></span>
<span class=normal><a href=#__codelineno-0-1586>1586</a></span>
<span class=normal><a href=#__codelineno-0-1587>1587</a></span>
<span class=normal><a href=#__codelineno-0-1588>1588</a></span>
<span class=normal><a href=#__codelineno-0-1589>1589</a></span>
<span class=normal><a href=#__codelineno-0-1590>1590</a></span>
<span class=normal><a href=#__codelineno-0-1591>1591</a></span>
<span class=normal><a href=#__codelineno-0-1592>1592</a></span>
<span class=normal><a href=#__codelineno-0-1593>1593</a></span>
<span class=normal><a href=#__codelineno-0-1594>1594</a></span>
<span class=normal><a href=#__codelineno-0-1595>1595</a></span>
<span class=normal><a href=#__codelineno-0-1596>1596</a></span>
<span class=normal><a href=#__codelineno-0-1597>1597</a></span>
<span class=normal><a href=#__codelineno-0-1598>1598</a></span>
<span class=normal><a href=#__codelineno-0-1599>1599</a></span>
<span class=normal><a href=#__codelineno-0-1600>1600</a></span>
<span class=normal><a href=#__codelineno-0-1601>1601</a></span>
<span class=normal><a href=#__codelineno-0-1602>1602</a></span>
<span class=normal><a href=#__codelineno-0-1603>1603</a></span>
<span class=normal><a href=#__codelineno-0-1604>1604</a></span>
<span class=normal><a href=#__codelineno-0-1605>1605</a></span>
<span class=normal><a href=#__codelineno-0-1606>1606</a></span>
<span class=normal><a href=#__codelineno-0-1607>1607</a></span>
<span class=normal><a href=#__codelineno-0-1608>1608</a></span>
<span class=normal><a href=#__codelineno-0-1609>1609</a></span>
<span class=normal><a href=#__codelineno-0-1610>1610</a></span>
<span class=normal><a href=#__codelineno-0-1611>1611</a></span>
<span class=normal><a href=#__codelineno-0-1612>1612</a></span>
<span class=normal><a href=#__codelineno-0-1613>1613</a></span>
<span class=normal><a href=#__codelineno-0-1614>1614</a></span>
<span class=normal><a href=#__codelineno-0-1615>1615</a></span>
<span class=normal><a href=#__codelineno-0-1616>1616</a></span>
<span class=normal><a href=#__codelineno-0-1617>1617</a></span>
<span class=normal><a href=#__codelineno-0-1618>1618</a></span>
<span class=normal><a href=#__codelineno-0-1619>1619</a></span>
<span class=normal><a href=#__codelineno-0-1620>1620</a></span>
<span class=normal><a href=#__codelineno-0-1621>1621</a></span>
<span class=normal><a href=#__codelineno-0-1622>1622</a></span>
<span class=normal><a href=#__codelineno-0-1623>1623</a></span>
<span class=normal><a href=#__codelineno-0-1624>1624</a></span>
<span class=normal><a href=#__codelineno-0-1625>1625</a></span>
<span class=normal><a href=#__codelineno-0-1626>1626</a></span>
<span class=normal><a href=#__codelineno-0-1627>1627</a></span>
<span class=normal><a href=#__codelineno-0-1628>1628</a></span>
<span class=normal><a href=#__codelineno-0-1629>1629</a></span>
<span class=normal><a href=#__codelineno-0-1630>1630</a></span>
<span class=normal><a href=#__codelineno-0-1631>1631</a></span>
<span class=normal><a href=#__codelineno-0-1632>1632</a></span>
<span class=normal><a href=#__codelineno-0-1633>1633</a></span>
<span class=normal><a href=#__codelineno-0-1634>1634</a></span>
<span class=normal><a href=#__codelineno-0-1635>1635</a></span>
<span class=normal><a href=#__codelineno-0-1636>1636</a></span>
<span class=normal><a href=#__codelineno-0-1637>1637</a></span>
<span class=normal><a href=#__codelineno-0-1638>1638</a></span>
<span class=normal><a href=#__codelineno-0-1639>1639</a></span>
<span class=normal><a href=#__codelineno-0-1640>1640</a></span>
<span class=normal><a href=#__codelineno-0-1641>1641</a></span>
<span class=normal><a href=#__codelineno-0-1642>1642</a></span>
<span class=normal><a href=#__codelineno-0-1643>1643</a></span>
<span class=normal><a href=#__codelineno-0-1644>1644</a></span>
<span class=normal><a href=#__codelineno-0-1645>1645</a></span>
<span class=normal><a href=#__codelineno-0-1646>1646</a></span>
<span class=normal><a href=#__codelineno-0-1647>1647</a></span>
<span class=normal><a href=#__codelineno-0-1648>1648</a></span>
<span class=normal><a href=#__codelineno-0-1649>1649</a></span>
<span class=normal><a href=#__codelineno-0-1650>1650</a></span>
<span class=normal><a href=#__codelineno-0-1651>1651</a></span>
<span class=normal><a href=#__codelineno-0-1652>1652</a></span>
<span class=normal><a href=#__codelineno-0-1653>1653</a></span>
<span class=normal><a href=#__codelineno-0-1654>1654</a></span>
<span class=normal><a href=#__codelineno-0-1655>1655</a></span>
<span class=normal><a href=#__codelineno-0-1656>1656</a></span>
<span class=normal><a href=#__codelineno-0-1657>1657</a></span>
<span class=normal><a href=#__codelineno-0-1658>1658</a></span>
<span class=normal><a href=#__codelineno-0-1659>1659</a></span>
<span class=normal><a href=#__codelineno-0-1660>1660</a></span>
<span class=normal><a href=#__codelineno-0-1661>1661</a></span>
<span class=normal><a href=#__codelineno-0-1662>1662</a></span>
<span class=normal><a href=#__codelineno-0-1663>1663</a></span>
<span class=normal><a href=#__codelineno-0-1664>1664</a></span>
<span class=normal><a href=#__codelineno-0-1665>1665</a></span>
<span class=normal><a href=#__codelineno-0-1666>1666</a></span>
<span class=normal><a href=#__codelineno-0-1667>1667</a></span>
<span class=normal><a href=#__codelineno-0-1668>1668</a></span>
<span class=normal><a href=#__codelineno-0-1669>1669</a></span>
<span class=normal><a href=#__codelineno-0-1670>1670</a></span>
<span class=normal><a href=#__codelineno-0-1671>1671</a></span>
<span class=normal><a href=#__codelineno-0-1672>1672</a></span>
<span class=normal><a href=#__codelineno-0-1673>1673</a></span>
<span class=normal><a href=#__codelineno-0-1674>1674</a></span>
<span class=normal><a href=#__codelineno-0-1675>1675</a></span>
<span class=normal><a href=#__codelineno-0-1676>1676</a></span>
<span class=normal><a href=#__codelineno-0-1677>1677</a></span>
<span class=normal><a href=#__codelineno-0-1678>1678</a></span>
<span class=normal><a href=#__codelineno-0-1679>1679</a></span>
<span class=normal><a href=#__codelineno-0-1680>1680</a></span>
<span class=normal><a href=#__codelineno-0-1681>1681</a></span>
<span class=normal><a href=#__codelineno-0-1682>1682</a></span>
<span class=normal><a href=#__codelineno-0-1683>1683</a></span>
<span class=normal><a href=#__codelineno-0-1684>1684</a></span>
<span class=normal><a href=#__codelineno-0-1685>1685</a></span>
<span class=normal><a href=#__codelineno-0-1686>1686</a></span>
<span class=normal><a href=#__codelineno-0-1687>1687</a></span>
<span class=normal><a href=#__codelineno-0-1688>1688</a></span>
<span class=normal><a href=#__codelineno-0-1689>1689</a></span>
<span class=normal><a href=#__codelineno-0-1690>1690</a></span>
<span class=normal><a href=#__codelineno-0-1691>1691</a></span>
<span class=normal><a href=#__codelineno-0-1692>1692</a></span>
<span class=normal><a href=#__codelineno-0-1693>1693</a></span>
<span class=normal><a href=#__codelineno-0-1694>1694</a></span>
<span class=normal><a href=#__codelineno-0-1695>1695</a></span>
<span class=normal><a href=#__codelineno-0-1696>1696</a></span>
<span class=normal><a href=#__codelineno-0-1697>1697</a></span>
<span class=normal><a href=#__codelineno-0-1698>1698</a></span>
<span class=normal><a href=#__codelineno-0-1699>1699</a></span>
<span class=normal><a href=#__codelineno-0-1700>1700</a></span>
<span class=normal><a href=#__codelineno-0-1701>1701</a></span>
<span class=normal><a href=#__codelineno-0-1702>1702</a></span>
<span class=normal><a href=#__codelineno-0-1703>1703</a></span>
<span class=normal><a href=#__codelineno-0-1704>1704</a></span>
<span class=normal><a href=#__codelineno-0-1705>1705</a></span>
<span class=normal><a href=#__codelineno-0-1706>1706</a></span>
<span class=normal><a href=#__codelineno-0-1707>1707</a></span>
<span class=normal><a href=#__codelineno-0-1708>1708</a></span>
<span class=normal><a href=#__codelineno-0-1709>1709</a></span>
<span class=normal><a href=#__codelineno-0-1710>1710</a></span>
<span class=normal><a href=#__codelineno-0-1711>1711</a></span>
<span class=normal><a href=#__codelineno-0-1712>1712</a></span>
<span class=normal><a href=#__codelineno-0-1713>1713</a></span>
<span class=normal><a href=#__codelineno-0-1714>1714</a></span>
<span class=normal><a href=#__codelineno-0-1715>1715</a></span>
<span class=normal><a href=#__codelineno-0-1716>1716</a></span>
<span class=normal><a href=#__codelineno-0-1717>1717</a></span>
<span class=normal><a href=#__codelineno-0-1718>1718</a></span>
<span class=normal><a href=#__codelineno-0-1719>1719</a></span>
<span class=normal><a href=#__codelineno-0-1720>1720</a></span>
<span class=normal><a href=#__codelineno-0-1721>1721</a></span>
<span class=normal><a href=#__codelineno-0-1722>1722</a></span>
<span class=normal><a href=#__codelineno-0-1723>1723</a></span>
<span class=normal><a href=#__codelineno-0-1724>1724</a></span>
<span class=normal><a href=#__codelineno-0-1725>1725</a></span>
<span class=normal><a href=#__codelineno-0-1726>1726</a></span>
<span class=normal><a href=#__codelineno-0-1727>1727</a></span>
<span class=normal><a href=#__codelineno-0-1728>1728</a></span>
<span class=normal><a href=#__codelineno-0-1729>1729</a></span>
<span class=normal><a href=#__codelineno-0-1730>1730</a></span>
<span class=normal><a href=#__codelineno-0-1731>1731</a></span>
<span class=normal><a href=#__codelineno-0-1732>1732</a></span>
<span class=normal><a href=#__codelineno-0-1733>1733</a></span>
<span class=normal><a href=#__codelineno-0-1734>1734</a></span>
<span class=normal><a href=#__codelineno-0-1735>1735</a></span>
<span class=normal><a href=#__codelineno-0-1736>1736</a></span>
<span class=normal><a href=#__codelineno-0-1737>1737</a></span>
<span class=normal><a href=#__codelineno-0-1738>1738</a></span>
<span class=normal><a href=#__codelineno-0-1739>1739</a></span>
<span class=normal><a href=#__codelineno-0-1740>1740</a></span>
<span class=normal><a href=#__codelineno-0-1741>1741</a></span>
<span class=normal><a href=#__codelineno-0-1742>1742</a></span>
<span class=normal><a href=#__codelineno-0-1743>1743</a></span>
<span class=normal><a href=#__codelineno-0-1744>1744</a></span>
<span class=normal><a href=#__codelineno-0-1745>1745</a></span>
<span class=normal><a href=#__codelineno-0-1746>1746</a></span>
<span class=normal><a href=#__codelineno-0-1747>1747</a></span>
<span class=normal><a href=#__codelineno-0-1748>1748</a></span>
<span class=normal><a href=#__codelineno-0-1749>1749</a></span>
<span class=normal><a href=#__codelineno-0-1750>1750</a></span>
<span class=normal><a href=#__codelineno-0-1751>1751</a></span>
<span class=normal><a href=#__codelineno-0-1752>1752</a></span>
<span class=normal><a href=#__codelineno-0-1753>1753</a></span>
<span class=normal><a href=#__codelineno-0-1754>1754</a></span>
<span class=normal><a href=#__codelineno-0-1755>1755</a></span>
<span class=normal><a href=#__codelineno-0-1756>1756</a></span>
<span class=normal><a href=#__codelineno-0-1757>1757</a></span>
<span class=normal><a href=#__codelineno-0-1758>1758</a></span>
<span class=normal><a href=#__codelineno-0-1759>1759</a></span>
<span class=normal><a href=#__codelineno-0-1760>1760</a></span>
<span class=normal><a href=#__codelineno-0-1761>1761</a></span>
<span class=normal><a href=#__codelineno-0-1762>1762</a></span>
<span class=normal><a href=#__codelineno-0-1763>1763</a></span>
<span class=normal><a href=#__codelineno-0-1764>1764</a></span>
<span class=normal><a href=#__codelineno-0-1765>1765</a></span>
<span class=normal><a href=#__codelineno-0-1766>1766</a></span>
<span class=normal><a href=#__codelineno-0-1767>1767</a></span>
<span class=normal><a href=#__codelineno-0-1768>1768</a></span>
<span class=normal><a href=#__codelineno-0-1769>1769</a></span>
<span class=normal><a href=#__codelineno-0-1770>1770</a></span>
<span class=normal><a href=#__codelineno-0-1771>1771</a></span>
<span class=normal><a href=#__codelineno-0-1772>1772</a></span>
<span class=normal><a href=#__codelineno-0-1773>1773</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1340><a id=__codelineno-0-1340 name=__codelineno-0-1340></a><span class=k>class</span> <span class=nc>NormalStochasticBlock2d</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-1341><a id=__codelineno-0-1341 name=__codelineno-0-1341></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1342><a id=__codelineno-0-1342 name=__codelineno-0-1342></a><span class=sd>    Stochastic block used in the Top-Down inference pass.</span>
</span><span id=__span-0-1343><a id=__codelineno-0-1343 name=__codelineno-0-1343></a>
</span><span id=__span-0-1344><a id=__codelineno-0-1344 name=__codelineno-0-1344></a><span class=sd>    Algorithm:</span>
</span><span id=__span-0-1345><a id=__codelineno-0-1345 name=__codelineno-0-1345></a><span class=sd>        - map input parameters to q(z) and (optionally) p(z) via convolution</span>
</span><span id=__span-0-1346><a id=__codelineno-0-1346 name=__codelineno-0-1346></a><span class=sd>        - sample a latent tensor z ~ q(z)</span>
</span><span id=__span-0-1347><a id=__codelineno-0-1347 name=__codelineno-0-1347></a><span class=sd>        - feed z to convolution and return.</span>
</span><span id=__span-0-1348><a id=__codelineno-0-1348 name=__codelineno-0-1348></a>
</span><span id=__span-0-1349><a id=__codelineno-0-1349 name=__codelineno-0-1349></a><span class=sd>    NOTE 1:</span>
</span><span id=__span-0-1350><a id=__codelineno-0-1350 name=__codelineno-0-1350></a><span class=sd>        If parameters for q are not given, sampling is done from p(z).</span>
</span><span id=__span-0-1351><a id=__codelineno-0-1351 name=__codelineno-0-1351></a>
</span><span id=__span-0-1352><a id=__codelineno-0-1352 name=__codelineno-0-1352></a><span class=sd>    NOTE 2:</span>
</span><span id=__span-0-1353><a id=__codelineno-0-1353 name=__codelineno-0-1353></a><span class=sd>        The restricted KL divergence is obtained by first computing the element-wise KL divergence</span>
</span><span id=__span-0-1354><a id=__codelineno-0-1354 name=__codelineno-0-1354></a><span class=sd>        (i.e., the KL computed for each element of the latent tensors). Then, the restricted version</span>
</span><span id=__span-0-1355><a id=__codelineno-0-1355 name=__codelineno-0-1355></a><span class=sd>        is computed by summing over the channels and the spatial dimensions associated only to the</span>
</span><span id=__span-0-1356><a id=__codelineno-0-1356 name=__codelineno-0-1356></a><span class=sd>        portion of the latent tensor that is used for prediction.</span>
</span><span id=__span-0-1357><a id=__codelineno-0-1357 name=__codelineno-0-1357></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1358><a id=__codelineno-0-1358 name=__codelineno-0-1358></a>
</span><span id=__span-0-1359><a id=__codelineno-0-1359 name=__codelineno-0-1359></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-1360><a id=__codelineno-0-1360 name=__codelineno-0-1360></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1361><a id=__codelineno-0-1361 name=__codelineno-0-1361></a>        <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1362><a id=__codelineno-0-1362 name=__codelineno-0-1362></a>        <span class=n>c_vars</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1363><a id=__codelineno-0-1363 name=__codelineno-0-1363></a>        <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1364><a id=__codelineno-0-1364 name=__codelineno-0-1364></a>        <span class=n>kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-1365><a id=__codelineno-0-1365 name=__codelineno-0-1365></a>        <span class=n>transform_p_params</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1366><a id=__codelineno-0-1366 name=__codelineno-0-1366></a>        <span class=n>vanilla_latent_hw</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1367><a id=__codelineno-0-1367 name=__codelineno-0-1367></a>        <span class=n>restricted_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1368><a id=__codelineno-0-1368 name=__codelineno-0-1368></a>        <span class=n>use_naive_exponential</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1369><a id=__codelineno-0-1369 name=__codelineno-0-1369></a>    <span class=p>):</span>
</span><span id=__span-0-1370><a id=__codelineno-0-1370 name=__codelineno-0-1370></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1371><a id=__codelineno-0-1371 name=__codelineno-0-1371></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1372><a id=__codelineno-0-1372 name=__codelineno-0-1372></a><span class=sd>        ----------</span>
</span><span id=__span-0-1373><a id=__codelineno-0-1373 name=__codelineno-0-1373></a><span class=sd>        c_in: int</span>
</span><span id=__span-0-1374><a id=__codelineno-0-1374 name=__codelineno-0-1374></a><span class=sd>            The number of channels of the input tensor.</span>
</span><span id=__span-0-1375><a id=__codelineno-0-1375 name=__codelineno-0-1375></a><span class=sd>        c_vars: int</span>
</span><span id=__span-0-1376><a id=__codelineno-0-1376 name=__codelineno-0-1376></a><span class=sd>            The number of channels of the latent space tensor.</span>
</span><span id=__span-0-1377><a id=__codelineno-0-1377 name=__codelineno-0-1377></a><span class=sd>        c_out:  int</span>
</span><span id=__span-0-1378><a id=__codelineno-0-1378 name=__codelineno-0-1378></a><span class=sd>            The output of the stochastic layer.</span>
</span><span id=__span-0-1379><a id=__codelineno-0-1379 name=__codelineno-0-1379></a><span class=sd>            Note that this is different from the sampled latent z.</span>
</span><span id=__span-0-1380><a id=__codelineno-0-1380 name=__codelineno-0-1380></a><span class=sd>        kernel: int, optional</span>
</span><span id=__span-0-1381><a id=__codelineno-0-1381 name=__codelineno-0-1381></a><span class=sd>            The size of the kernel used in convolutional layers.</span>
</span><span id=__span-0-1382><a id=__codelineno-0-1382 name=__codelineno-0-1382></a><span class=sd>            Default is 3.</span>
</span><span id=__span-0-1383><a id=__codelineno-0-1383 name=__codelineno-0-1383></a><span class=sd>        transform_p_params: bool, optional</span>
</span><span id=__span-0-1384><a id=__codelineno-0-1384 name=__codelineno-0-1384></a><span class=sd>            Whether a transformation should be applied to the `p_params` tensor.</span>
</span><span id=__span-0-1385><a id=__codelineno-0-1385 name=__codelineno-0-1385></a><span class=sd>            The transformation consists in a 2D convolution ()`conv_in_p()`) that</span>
</span><span id=__span-0-1386><a id=__codelineno-0-1386 name=__codelineno-0-1386></a><span class=sd>            maps the input to a larger number of channels.</span>
</span><span id=__span-0-1387><a id=__codelineno-0-1387 name=__codelineno-0-1387></a><span class=sd>            Default is `True`.</span>
</span><span id=__span-0-1388><a id=__codelineno-0-1388 name=__codelineno-0-1388></a><span class=sd>        vanilla_latent_hw: int, optional</span>
</span><span id=__span-0-1389><a id=__codelineno-0-1389 name=__codelineno-0-1389></a><span class=sd>            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).</span>
</span><span id=__span-0-1390><a id=__codelineno-0-1390 name=__codelineno-0-1390></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-1391><a id=__codelineno-0-1391 name=__codelineno-0-1391></a><span class=sd>        restricted_kl: bool, optional</span>
</span><span id=__span-0-1392><a id=__codelineno-0-1392 name=__codelineno-0-1392></a><span class=sd>            Whether to compute the restricted version of KL Divergence.</span>
</span><span id=__span-0-1393><a id=__codelineno-0-1393 name=__codelineno-0-1393></a><span class=sd>            See NOTE 2 for more information about its computation.</span>
</span><span id=__span-0-1394><a id=__codelineno-0-1394 name=__codelineno-0-1394></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1395><a id=__codelineno-0-1395 name=__codelineno-0-1395></a><span class=sd>        use_naive_exponential: bool, optional</span>
</span><span id=__span-0-1396><a id=__codelineno-0-1396 name=__codelineno-0-1396></a><span class=sd>            If `False`, exponentials are computed according to the alternative definition</span>
</span><span id=__span-0-1397><a id=__codelineno-0-1397 name=__codelineno-0-1397></a><span class=sd>            provided by `StableExponential` class. This should improve numerical stability</span>
</span><span id=__span-0-1398><a id=__codelineno-0-1398 name=__codelineno-0-1398></a><span class=sd>            in the training process. Default is `False`.</span>
</span><span id=__span-0-1399><a id=__codelineno-0-1399 name=__codelineno-0-1399></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1400><a id=__codelineno-0-1400 name=__codelineno-0-1400></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-1401><a id=__codelineno-0-1401 name=__codelineno-0-1401></a>        <span class=k>assert</span> <span class=n>kernel</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span>
</span><span id=__span-0-1402><a id=__codelineno-0-1402 name=__codelineno-0-1402></a>        <span class=n>pad</span> <span class=o>=</span> <span class=n>kernel</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1403><a id=__codelineno-0-1403 name=__codelineno-0-1403></a>        <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span> <span class=o>=</span> <span class=n>transform_p_params</span>
</span><span id=__span-0-1404><a id=__codelineno-0-1404 name=__codelineno-0-1404></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_in</span> <span class=o>=</span> <span class=n>c_in</span>
</span><span id=__span-0-1405><a id=__codelineno-0-1405 name=__codelineno-0-1405></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_out</span> <span class=o>=</span> <span class=n>c_out</span>
</span><span id=__span-0-1406><a id=__codelineno-0-1406 name=__codelineno-0-1406></a>        <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span> <span class=o>=</span> <span class=n>c_vars</span>
</span><span id=__span-0-1407><a id=__codelineno-0-1407 name=__codelineno-0-1407></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span> <span class=o>=</span> <span class=n>use_naive_exponential</span>
</span><span id=__span-0-1408><a id=__codelineno-0-1408 name=__codelineno-0-1408></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span> <span class=o>=</span> <span class=n>vanilla_latent_hw</span>
</span><span id=__span-0-1409><a id=__codelineno-0-1409 name=__codelineno-0-1409></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_restricted_kl</span> <span class=o>=</span> <span class=n>restricted_kl</span>
</span><span id=__span-0-1410><a id=__codelineno-0-1410 name=__codelineno-0-1410></a>
</span><span id=__span-0-1411><a id=__codelineno-0-1411 name=__codelineno-0-1411></a>        <span class=k>if</span> <span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1412><a id=__codelineno-0-1412 name=__codelineno-0-1412></a>            <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-1413><a id=__codelineno-0-1413 name=__codelineno-0-1413></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-1414><a id=__codelineno-0-1414 name=__codelineno-0-1414></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_vars</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-1415><a id=__codelineno-0-1415 name=__codelineno-0-1415></a>
</span><span id=__span-0-1416><a id=__codelineno-0-1416 name=__codelineno-0-1416></a>    <span class=c1># def forward_swapped(self, p_params, q_mu, q_lv):</span>
</span><span id=__span-0-1417><a id=__codelineno-0-1417 name=__codelineno-0-1417></a>    <span class=c1>#</span>
</span><span id=__span-0-1418><a id=__codelineno-0-1418 name=__codelineno-0-1418></a>    <span class=c1>#     if self.transform_p_params:</span>
</span><span id=__span-0-1419><a id=__codelineno-0-1419 name=__codelineno-0-1419></a>    <span class=c1>#         p_params = self.conv_in_p(p_params)</span>
</span><span id=__span-0-1420><a id=__codelineno-0-1420 name=__codelineno-0-1420></a>    <span class=c1>#     else:</span>
</span><span id=__span-0-1421><a id=__codelineno-0-1421 name=__codelineno-0-1421></a>    <span class=c1>#         assert p_params.size(1) == 2 * self.c_vars</span>
</span><span id=__span-0-1422><a id=__codelineno-0-1422 name=__codelineno-0-1422></a>    <span class=c1>#</span>
</span><span id=__span-0-1423><a id=__codelineno-0-1423 name=__codelineno-0-1423></a>    <span class=c1>#     # Define p(z)</span>
</span><span id=__span-0-1424><a id=__codelineno-0-1424 name=__codelineno-0-1424></a>    <span class=c1>#     p_mu, p_lv = p_params.chunk(2, dim=1)</span>
</span><span id=__span-0-1425><a id=__codelineno-0-1425 name=__codelineno-0-1425></a>    <span class=c1>#     p = Normal(p_mu, (p_lv / 2).exp())</span>
</span><span id=__span-0-1426><a id=__codelineno-0-1426 name=__codelineno-0-1426></a>    <span class=c1>#</span>
</span><span id=__span-0-1427><a id=__codelineno-0-1427 name=__codelineno-0-1427></a>    <span class=c1>#     # Define q(z)</span>
</span><span id=__span-0-1428><a id=__codelineno-0-1428 name=__codelineno-0-1428></a>    <span class=c1>#     q = Normal(q_mu, (q_lv / 2).exp())</span>
</span><span id=__span-0-1429><a id=__codelineno-0-1429 name=__codelineno-0-1429></a>    <span class=c1>#     # Sample from q(z)</span>
</span><span id=__span-0-1430><a id=__codelineno-0-1430 name=__codelineno-0-1430></a>    <span class=c1>#     sampling_distrib = q</span>
</span><span id=__span-0-1431><a id=__codelineno-0-1431 name=__codelineno-0-1431></a>    <span class=c1>#</span>
</span><span id=__span-0-1432><a id=__codelineno-0-1432 name=__codelineno-0-1432></a>    <span class=c1>#     # Generate latent variable (typically by sampling)</span>
</span><span id=__span-0-1433><a id=__codelineno-0-1433 name=__codelineno-0-1433></a>    <span class=c1>#     z = sampling_distrib.rsample()</span>
</span><span id=__span-0-1434><a id=__codelineno-0-1434 name=__codelineno-0-1434></a>    <span class=c1>#</span>
</span><span id=__span-0-1435><a id=__codelineno-0-1435 name=__codelineno-0-1435></a>    <span class=c1>#     # Output of stochastic layer</span>
</span><span id=__span-0-1436><a id=__codelineno-0-1436 name=__codelineno-0-1436></a>    <span class=c1>#     out = self.conv_out(z)</span>
</span><span id=__span-0-1437><a id=__codelineno-0-1437 name=__codelineno-0-1437></a>    <span class=c1>#</span>
</span><span id=__span-0-1438><a id=__codelineno-0-1438 name=__codelineno-0-1438></a>    <span class=c1>#     data = {</span>
</span><span id=__span-0-1439><a id=__codelineno-0-1439 name=__codelineno-0-1439></a>    <span class=c1>#         &#39;z&#39;: z,  # sampled variable at this layer (batch, ch, h, w)</span>
</span><span id=__span-0-1440><a id=__codelineno-0-1440 name=__codelineno-0-1440></a>    <span class=c1>#         &#39;p_params&#39;: p_params,  # (b, ch, h, w) where b is 1 or batch size</span>
</span><span id=__span-0-1441><a id=__codelineno-0-1441 name=__codelineno-0-1441></a>    <span class=c1>#     }</span>
</span><span id=__span-0-1442><a id=__codelineno-0-1442 name=__codelineno-0-1442></a>    <span class=c1>#     return out, data</span>
</span><span id=__span-0-1443><a id=__codelineno-0-1443 name=__codelineno-0-1443></a>
</span><span id=__span-0-1444><a id=__codelineno-0-1444 name=__codelineno-0-1444></a>    <span class=k>def</span> <span class=nf>get_z</span><span class=p>(</span>
</span><span id=__span-0-1445><a id=__codelineno-0-1445 name=__codelineno-0-1445></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1446><a id=__codelineno-0-1446 name=__codelineno-0-1446></a>        <span class=n>sampling_distrib</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1447><a id=__codelineno-0-1447 name=__codelineno-0-1447></a>        <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1448><a id=__codelineno-0-1448 name=__codelineno-0-1448></a>        <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1449><a id=__codelineno-0-1449 name=__codelineno-0-1449></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1450><a id=__codelineno-0-1450 name=__codelineno-0-1450></a>        <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1451><a id=__codelineno-0-1451 name=__codelineno-0-1451></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1452><a id=__codelineno-0-1452 name=__codelineno-0-1452></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1453><a id=__codelineno-0-1453 name=__codelineno-0-1453></a><span class=sd>        This method enables to sample a latent tensor given the distribution to sample from.</span>
</span><span id=__span-0-1454><a id=__codelineno-0-1454 name=__codelineno-0-1454></a>
</span><span id=__span-0-1455><a id=__codelineno-0-1455 name=__codelineno-0-1455></a><span class=sd>        Latent variable can be obtained is several ways:</span>
</span><span id=__span-0-1456><a id=__codelineno-0-1456 name=__codelineno-0-1456></a><span class=sd>            - Sampled from the (Gaussian) latent distribution.</span>
</span><span id=__span-0-1457><a id=__codelineno-0-1457 name=__codelineno-0-1457></a><span class=sd>            - Taken as a pre-defined forced latent.</span>
</span><span id=__span-0-1458><a id=__codelineno-0-1458 name=__codelineno-0-1458></a><span class=sd>            - Taken as the mode (mean) of the latent distribution.</span>
</span><span id=__span-0-1459><a id=__codelineno-0-1459 name=__codelineno-0-1459></a><span class=sd>            - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.</span>
</span><span id=__span-0-1460><a id=__codelineno-0-1460 name=__codelineno-0-1460></a>
</span><span id=__span-0-1461><a id=__codelineno-0-1461 name=__codelineno-0-1461></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1462><a id=__codelineno-0-1462 name=__codelineno-0-1462></a><span class=sd>        ----------</span>
</span><span id=__span-0-1463><a id=__codelineno-0-1463 name=__codelineno-0-1463></a><span class=sd>        sampling_distrib: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1464><a id=__codelineno-0-1464 name=__codelineno-0-1464></a><span class=sd>            The Gaussian distribution from which latent tensor is sampled.</span>
</span><span id=__span-0-1465><a id=__codelineno-0-1465 name=__codelineno-0-1465></a><span class=sd>        forced_latent: torch.Tensor</span>
</span><span id=__span-0-1466><a id=__codelineno-0-1466 name=__codelineno-0-1466></a><span class=sd>            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,</span>
</span><span id=__span-0-1467><a id=__codelineno-0-1467 name=__codelineno-0-1467></a><span class=sd>            hence, sampling does not happen.</span>
</span><span id=__span-0-1468><a id=__codelineno-0-1468 name=__codelineno-0-1468></a><span class=sd>        use_mode: bool</span>
</span><span id=__span-0-1469><a id=__codelineno-0-1469 name=__codelineno-0-1469></a><span class=sd>            Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1470><a id=__codelineno-0-1470 name=__codelineno-0-1470></a><span class=sd>            In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1471><a id=__codelineno-0-1471 name=__codelineno-0-1471></a><span class=sd>        mode_pred: bool</span>
</span><span id=__span-0-1472><a id=__codelineno-0-1472 name=__codelineno-0-1472></a><span class=sd>            Whether the model is prediction mode.</span>
</span><span id=__span-0-1473><a id=__codelineno-0-1473 name=__codelineno-0-1473></a><span class=sd>        use_uncond_mode: bool</span>
</span><span id=__span-0-1474><a id=__codelineno-0-1474 name=__codelineno-0-1474></a><span class=sd>            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1475><a id=__codelineno-0-1475 name=__codelineno-0-1475></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1476><a id=__codelineno-0-1476 name=__codelineno-0-1476></a>        <span class=k>if</span> <span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1477><a id=__codelineno-0-1477 name=__codelineno-0-1477></a>            <span class=k>if</span> <span class=n>use_mode</span><span class=p>:</span>
</span><span id=__span-0-1478><a id=__codelineno-0-1478 name=__codelineno-0-1478></a>                <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>mean</span>
</span><span id=__span-0-1479><a id=__codelineno-0-1479 name=__codelineno-0-1479></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1480><a id=__codelineno-0-1480 name=__codelineno-0-1480></a>                <span class=k>if</span> <span class=n>mode_pred</span><span class=p>:</span>
</span><span id=__span-0-1481><a id=__codelineno-0-1481 name=__codelineno-0-1481></a>                    <span class=k>if</span> <span class=n>use_uncond_mode</span><span class=p>:</span>
</span><span id=__span-0-1482><a id=__codelineno-0-1482 name=__codelineno-0-1482></a>                        <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>mean</span>
</span><span id=__span-0-1483><a id=__codelineno-0-1483 name=__codelineno-0-1483></a>                    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1484><a id=__codelineno-0-1484 name=__codelineno-0-1484></a>                        <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span><span id=__span-0-1485><a id=__codelineno-0-1485 name=__codelineno-0-1485></a>                <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1486><a id=__codelineno-0-1486 name=__codelineno-0-1486></a>                    <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span><span id=__span-0-1487><a id=__codelineno-0-1487 name=__codelineno-0-1487></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1488><a id=__codelineno-0-1488 name=__codelineno-0-1488></a>            <span class=n>z</span> <span class=o>=</span> <span class=n>forced_latent</span>
</span><span id=__span-0-1489><a id=__codelineno-0-1489 name=__codelineno-0-1489></a>        <span class=k>return</span> <span class=n>z</span>
</span><span id=__span-0-1490><a id=__codelineno-0-1490 name=__codelineno-0-1490></a>
</span><span id=__span-0-1491><a id=__codelineno-0-1491 name=__codelineno-0-1491></a>    <span class=k>def</span> <span class=nf>sample_from_q</span><span class=p>(</span>
</span><span id=__span-0-1492><a id=__codelineno-0-1492 name=__codelineno-0-1492></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span>
</span><span id=__span-0-1493><a id=__codelineno-0-1493 name=__codelineno-0-1493></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1494><a id=__codelineno-0-1494 name=__codelineno-0-1494></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1495><a id=__codelineno-0-1495 name=__codelineno-0-1495></a><span class=sd>        Given an input parameter tensor defining q(z),</span>
</span><span id=__span-0-1496><a id=__codelineno-0-1496 name=__codelineno-0-1496></a><span class=sd>        it processes it by calling `process_q_params()` method and</span>
</span><span id=__span-0-1497><a id=__codelineno-0-1497 name=__codelineno-0-1497></a><span class=sd>        sample a latent tensor from the resulting distribution.</span>
</span><span id=__span-0-1498><a id=__codelineno-0-1498 name=__codelineno-0-1498></a>
</span><span id=__span-0-1499><a id=__codelineno-0-1499 name=__codelineno-0-1499></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1500><a id=__codelineno-0-1500 name=__codelineno-0-1500></a><span class=sd>        ----------</span>
</span><span id=__span-0-1501><a id=__codelineno-0-1501 name=__codelineno-0-1501></a><span class=sd>        q_params: torch.Tensor</span>
</span><span id=__span-0-1502><a id=__codelineno-0-1502 name=__codelineno-0-1502></a><span class=sd>            The input tensor to be processed.</span>
</span><span id=__span-0-1503><a id=__codelineno-0-1503 name=__codelineno-0-1503></a><span class=sd>        var_clip_max: float</span>
</span><span id=__span-0-1504><a id=__codelineno-0-1504 name=__codelineno-0-1504></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1505><a id=__codelineno-0-1505 name=__codelineno-0-1505></a><span class=sd>            Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1506><a id=__codelineno-0-1506 name=__codelineno-0-1506></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1507><a id=__codelineno-0-1507 name=__codelineno-0-1507></a>        <span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1508><a id=__codelineno-0-1508 name=__codelineno-0-1508></a>        <span class=k>return</span> <span class=n>q</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span><span id=__span-0-1509><a id=__codelineno-0-1509 name=__codelineno-0-1509></a>
</span><span id=__span-0-1510><a id=__codelineno-0-1510 name=__codelineno-0-1510></a>    <span class=k>def</span> <span class=nf>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1511><a id=__codelineno-0-1511 name=__codelineno-0-1511></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1512><a id=__codelineno-0-1512 name=__codelineno-0-1512></a>        <span class=n>p</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1513><a id=__codelineno-0-1513 name=__codelineno-0-1513></a>        <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1514><a id=__codelineno-0-1514 name=__codelineno-0-1514></a>        <span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1515><a id=__codelineno-0-1515 name=__codelineno-0-1515></a>        <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1516><a id=__codelineno-0-1516 name=__codelineno-0-1516></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1517><a id=__codelineno-0-1517 name=__codelineno-0-1517></a>        <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1518><a id=__codelineno-0-1518 name=__codelineno-0-1518></a>        <span class=n>z</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1519><a id=__codelineno-0-1519 name=__codelineno-0-1519></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-1520><a id=__codelineno-0-1520 name=__codelineno-0-1520></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1521><a id=__codelineno-0-1521 name=__codelineno-0-1521></a><span class=sd>        Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.</span>
</span><span id=__span-0-1522><a id=__codelineno-0-1522 name=__codelineno-0-1522></a><span class=sd>        Specifically, the different versions of the KL loss terms are:</span>
</span><span id=__span-0-1523><a id=__codelineno-0-1523 name=__codelineno-0-1523></a><span class=sd>            - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].</span>
</span><span id=__span-0-1524><a id=__codelineno-0-1524 name=__codelineno-0-1524></a><span class=sd>            - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].</span>
</span><span id=__span-0-1525><a id=__codelineno-0-1525 name=__codelineno-0-1525></a><span class=sd>            - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is</span>
</span><span id=__span-0-1526><a id=__codelineno-0-1526 name=__codelineno-0-1526></a><span class=sd>            used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].</span>
</span><span id=__span-0-1527><a id=__codelineno-0-1527 name=__codelineno-0-1527></a><span class=sd>            - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].</span>
</span><span id=__span-0-1528><a id=__codelineno-0-1528 name=__codelineno-0-1528></a><span class=sd>            - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1529><a id=__codelineno-0-1529 name=__codelineno-0-1529></a>
</span><span id=__span-0-1530><a id=__codelineno-0-1530 name=__codelineno-0-1530></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1531><a id=__codelineno-0-1531 name=__codelineno-0-1531></a><span class=sd>        ----------</span>
</span><span id=__span-0-1532><a id=__codelineno-0-1532 name=__codelineno-0-1532></a><span class=sd>        p: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1533><a id=__codelineno-0-1533 name=__codelineno-0-1533></a><span class=sd>            The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1534><a id=__codelineno-0-1534 name=__codelineno-0-1534></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1535><a id=__codelineno-0-1535 name=__codelineno-0-1535></a><span class=sd>            The parameters of the prior generative distribution.</span>
</span><span id=__span-0-1536><a id=__codelineno-0-1536 name=__codelineno-0-1536></a><span class=sd>        q: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1537><a id=__codelineno-0-1537 name=__codelineno-0-1537></a><span class=sd>            The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</span>
</span><span id=__span-0-1538><a id=__codelineno-0-1538 name=__codelineno-0-1538></a><span class=sd>        q_params: torch.Tensor</span>
</span><span id=__span-0-1539><a id=__codelineno-0-1539 name=__codelineno-0-1539></a><span class=sd>            The parameters of the inference distribution.</span>
</span><span id=__span-0-1540><a id=__codelineno-0-1540 name=__codelineno-0-1540></a><span class=sd>        mode_pred: bool</span>
</span><span id=__span-0-1541><a id=__codelineno-0-1541 name=__codelineno-0-1541></a><span class=sd>            Whether the model is in prediction mode.</span>
</span><span id=__span-0-1542><a id=__codelineno-0-1542 name=__codelineno-0-1542></a><span class=sd>        analytical_kl: bool</span>
</span><span id=__span-0-1543><a id=__codelineno-0-1543 name=__codelineno-0-1543></a><span class=sd>            Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1544><a id=__codelineno-0-1544 name=__codelineno-0-1544></a><span class=sd>        z: torch.Tensor</span>
</span><span id=__span-0-1545><a id=__codelineno-0-1545 name=__codelineno-0-1545></a><span class=sd>            The sampled latent tensor.</span>
</span><span id=__span-0-1546><a id=__codelineno-0-1546 name=__codelineno-0-1546></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1547><a id=__codelineno-0-1547 name=__codelineno-0-1547></a>        <span class=n>kl_samplewise_restricted</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1548><a id=__codelineno-0-1548 name=__codelineno-0-1548></a>
</span><span id=__span-0-1549><a id=__codelineno-0-1549 name=__codelineno-0-1549></a>        <span class=k>if</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>  <span class=c1># if not in prediction mode</span>
</span><span id=__span-0-1550><a id=__codelineno-0-1550 name=__codelineno-0-1550></a>            <span class=c1># KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)]</span>
</span><span id=__span-0-1551><a id=__codelineno-0-1551 name=__codelineno-0-1551></a>            <span class=k>if</span> <span class=n>analytical_kl</span><span class=p>:</span>
</span><span id=__span-0-1552><a id=__codelineno-0-1552 name=__codelineno-0-1552></a>                <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_divergence</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>p</span><span class=p>)</span>
</span><span id=__span-0-1553><a id=__codelineno-0-1553 name=__codelineno-0-1553></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1554><a id=__codelineno-0-1554 name=__codelineno-0-1554></a>                <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_normal_mc</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q_params</span><span class=p>)</span>
</span><span id=__span-0-1555><a id=__codelineno-0-1555 name=__codelineno-0-1555></a>
</span><span id=__span-0-1556><a id=__codelineno-0-1556 name=__codelineno-0-1556></a>            <span class=c1># KL term only associated to the portion of the latent tensor that is used for prediction and</span>
</span><span id=__span-0-1557><a id=__codelineno-0-1557 name=__codelineno-0-1557></a>            <span class=c1># summed over channel and spatial dimensions. [Shape: (batch, )]</span>
</span><span id=__span-0-1558><a id=__codelineno-0-1558 name=__codelineno-0-1558></a>            <span class=c1># NOTE: vanilla_latent_hw is the shape of the latent tensor used for prediction, hence</span>
</span><span id=__span-0-1559><a id=__codelineno-0-1559 name=__codelineno-0-1559></a>            <span class=c1># the restriction has shape [Shape: (batch, ch, vanilla_latent_hw[0], vanilla_latent_hw[1])]</span>
</span><span id=__span-0-1560><a id=__codelineno-0-1560 name=__codelineno-0-1560></a>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_restricted_kl</span><span class=p>:</span>
</span><span id=__span-0-1561><a id=__codelineno-0-1561 name=__codelineno-0-1561></a>                <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>kl_elementwise</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1562><a id=__codelineno-0-1562 name=__codelineno-0-1562></a>                <span class=k>assert</span> <span class=n>pad</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&quot;Disable restricted kl since there is no restriction.&quot;</span>
</span><span id=__span-0-1563><a id=__codelineno-0-1563 name=__codelineno-0-1563></a>                <span class=n>tmp</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>pad</span><span class=p>:</span><span class=o>-</span><span class=n>pad</span><span class=p>,</span> <span class=n>pad</span><span class=p>:</span><span class=o>-</span><span class=n>pad</span><span class=p>]</span>
</span><span id=__span-0-1564><a id=__codelineno-0-1564 name=__codelineno-0-1564></a>                <span class=n>kl_samplewise_restricted</span> <span class=o>=</span> <span class=n>tmp</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1565><a id=__codelineno-0-1565 name=__codelineno-0-1565></a>
</span><span id=__span-0-1566><a id=__codelineno-0-1566 name=__codelineno-0-1566></a>            <span class=c1># KL term associated to each sample in the batch [Shape: (batch, )]</span>
</span><span id=__span-0-1567><a id=__codelineno-0-1567 name=__codelineno-0-1567></a>            <span class=n>kl_samplewise</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1568><a id=__codelineno-0-1568 name=__codelineno-0-1568></a>
</span><span id=__span-0-1569><a id=__codelineno-0-1569 name=__codelineno-0-1569></a>            <span class=c1># KL term associated to each sample and each channel [Shape: (batch, ch, )]</span>
</span><span id=__span-0-1570><a id=__codelineno-0-1570 name=__codelineno-0-1570></a>            <span class=n>kl_channelwise</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1571><a id=__codelineno-0-1571 name=__codelineno-0-1571></a>
</span><span id=__span-0-1572><a id=__codelineno-0-1572 name=__codelineno-0-1572></a>            <span class=c1># KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1573><a id=__codelineno-0-1573 name=__codelineno-0-1573></a>            <span class=n>kl_spatial</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1574><a id=__codelineno-0-1574 name=__codelineno-0-1574></a>        <span class=k>else</span><span class=p>:</span>  <span class=c1># if predicting, no need to compute KL</span>
</span><span id=__span-0-1575><a id=__codelineno-0-1575 name=__codelineno-0-1575></a>            <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_samplewise</span> <span class=o>=</span> <span class=n>kl_spatial</span> <span class=o>=</span> <span class=n>kl_channelwise</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1576><a id=__codelineno-0-1576 name=__codelineno-0-1576></a>
</span><span id=__span-0-1577><a id=__codelineno-0-1577 name=__codelineno-0-1577></a>        <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-0-1578><a id=__codelineno-0-1578 name=__codelineno-0-1578></a>            <span class=s2>&quot;kl_elementwise&quot;</span><span class=p>:</span> <span class=n>kl_elementwise</span><span class=p>,</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1579><a id=__codelineno-0-1579 name=__codelineno-0-1579></a>            <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>:</span> <span class=n>kl_samplewise</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1580><a id=__codelineno-0-1580 name=__codelineno-0-1580></a>            <span class=s2>&quot;kl_samplewise_restricted&quot;</span><span class=p>:</span> <span class=n>kl_samplewise_restricted</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1581><a id=__codelineno-0-1581 name=__codelineno-0-1581></a>            <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>:</span> <span class=n>kl_channelwise</span><span class=p>,</span>  <span class=c1># (batch, ch)</span>
</span><span id=__span-0-1582><a id=__codelineno-0-1582 name=__codelineno-0-1582></a>            <span class=s2>&quot;kl_spatial&quot;</span><span class=p>:</span> <span class=n>kl_spatial</span><span class=p>,</span>  <span class=c1># (batch, h, w)</span>
</span><span id=__span-0-1583><a id=__codelineno-0-1583 name=__codelineno-0-1583></a>        <span class=p>}</span>
</span><span id=__span-0-1584><a id=__codelineno-0-1584 name=__codelineno-0-1584></a>        <span class=k>return</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1585><a id=__codelineno-0-1585 name=__codelineno-0-1585></a>
</span><span id=__span-0-1586><a id=__codelineno-0-1586 name=__codelineno-0-1586></a>    <span class=k>def</span> <span class=nf>process_p_params</span><span class=p>(</span>
</span><span id=__span-0-1587><a id=__codelineno-0-1587 name=__codelineno-0-1587></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span>
</span><span id=__span-0-1588><a id=__codelineno-0-1588 name=__codelineno-0-1588></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>]:</span>
</span><span id=__span-0-1589><a id=__codelineno-0-1589 name=__codelineno-0-1589></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1590><a id=__codelineno-0-1590 name=__codelineno-0-1590></a><span class=sd>        Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1591><a id=__codelineno-0-1591 name=__codelineno-0-1591></a>
</span><span id=__span-0-1592><a id=__codelineno-0-1592 name=__codelineno-0-1592></a><span class=sd>        Processing consists in:</span>
</span><span id=__span-0-1593><a id=__codelineno-0-1593 name=__codelineno-0-1593></a><span class=sd>            - (optionally) 2D convolution on the input tensor to increase number of channels.</span>
</span><span id=__span-0-1594><a id=__codelineno-0-1594 name=__codelineno-0-1594></a><span class=sd>            - split the resulting tensor into two chunks, the mean and the log-variance.</span>
</span><span id=__span-0-1595><a id=__codelineno-0-1595 name=__codelineno-0-1595></a><span class=sd>            - (optionally) clip the log-variance to an upper threshold.</span>
</span><span id=__span-0-1596><a id=__codelineno-0-1596 name=__codelineno-0-1596></a><span class=sd>            - define the normal distribution p(z) given the parameter tensors above.</span>
</span><span id=__span-0-1597><a id=__codelineno-0-1597 name=__codelineno-0-1597></a>
</span><span id=__span-0-1598><a id=__codelineno-0-1598 name=__codelineno-0-1598></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1599><a id=__codelineno-0-1599 name=__codelineno-0-1599></a><span class=sd>        ----------</span>
</span><span id=__span-0-1600><a id=__codelineno-0-1600 name=__codelineno-0-1600></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1601><a id=__codelineno-0-1601 name=__codelineno-0-1601></a><span class=sd>            The input tensor to be processed.</span>
</span><span id=__span-0-1602><a id=__codelineno-0-1602 name=__codelineno-0-1602></a><span class=sd>        var_clip_max: float</span>
</span><span id=__span-0-1603><a id=__codelineno-0-1603 name=__codelineno-0-1603></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1604><a id=__codelineno-0-1604 name=__codelineno-0-1604></a><span class=sd>            Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1605><a id=__codelineno-0-1605 name=__codelineno-0-1605></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1606><a id=__codelineno-0-1606 name=__codelineno-0-1606></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1607><a id=__codelineno-0-1607 name=__codelineno-0-1607></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span><span class=p>(</span><span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1608><a id=__codelineno-0-1608 name=__codelineno-0-1608></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1609><a id=__codelineno-0-1609 name=__codelineno-0-1609></a>            <span class=k>assert</span> <span class=n>p_params</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span>
</span><span id=__span-0-1610><a id=__codelineno-0-1610 name=__codelineno-0-1610></a>
</span><span id=__span-0-1611><a id=__codelineno-0-1611 name=__codelineno-0-1611></a>        <span class=c1># Define p(z)</span>
</span><span id=__span-0-1612><a id=__codelineno-0-1612 name=__codelineno-0-1612></a>        <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span> <span class=o>=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1613><a id=__codelineno-0-1613 name=__codelineno-0-1613></a>        <span class=k>if</span> <span class=n>var_clip_max</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1614><a id=__codelineno-0-1614 name=__codelineno-0-1614></a>            <span class=n>p_lv</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>p_lv</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1615><a id=__codelineno-0-1615 name=__codelineno-0-1615></a>
</span><span id=__span-0-1616><a id=__codelineno-0-1616 name=__codelineno-0-1616></a>        <span class=n>p_mu</span> <span class=o>=</span> <span class=n>StableMean</span><span class=p>(</span><span class=n>p_mu</span><span class=p>)</span>
</span><span id=__span-0-1617><a id=__codelineno-0-1617 name=__codelineno-0-1617></a>        <span class=n>p_lv</span> <span class=o>=</span> <span class=n>StableLogVar</span><span class=p>(</span><span class=n>p_lv</span><span class=p>,</span> <span class=n>enable_stable</span><span class=o>=</span><span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span><span class=p>)</span>
</span><span id=__span-0-1618><a id=__codelineno-0-1618 name=__codelineno-0-1618></a>        <span class=n>p</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=n>p_mu</span><span class=o>.</span><span class=n>get</span><span class=p>(),</span> <span class=n>p_lv</span><span class=o>.</span><span class=n>get_std</span><span class=p>())</span>
</span><span id=__span-0-1619><a id=__codelineno-0-1619 name=__codelineno-0-1619></a>        <span class=k>return</span> <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>,</span> <span class=n>p</span>
</span><span id=__span-0-1620><a id=__codelineno-0-1620 name=__codelineno-0-1620></a>
</span><span id=__span-0-1621><a id=__codelineno-0-1621 name=__codelineno-0-1621></a>    <span class=k>def</span> <span class=nf>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1622><a id=__codelineno-0-1622 name=__codelineno-0-1622></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-1623><a id=__codelineno-0-1623 name=__codelineno-0-1623></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>]:</span>
</span><span id=__span-0-1624><a id=__codelineno-0-1624 name=__codelineno-0-1624></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1625><a id=__codelineno-0-1625 name=__codelineno-0-1625></a><span class=sd>        Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).</span>
</span><span id=__span-0-1626><a id=__codelineno-0-1626 name=__codelineno-0-1626></a>
</span><span id=__span-0-1627><a id=__codelineno-0-1627 name=__codelineno-0-1627></a><span class=sd>        Processing consists in:</span>
</span><span id=__span-0-1628><a id=__codelineno-0-1628 name=__codelineno-0-1628></a><span class=sd>            - 2D convolution on the input tensor to increase number of channels.</span>
</span><span id=__span-0-1629><a id=__codelineno-0-1629 name=__codelineno-0-1629></a><span class=sd>            - split the resulting tensor into two chunks, the mean and the log-variance.</span>
</span><span id=__span-0-1630><a id=__codelineno-0-1630 name=__codelineno-0-1630></a><span class=sd>            - (optionally) clip the log-variance to an upper threshold.</span>
</span><span id=__span-0-1631><a id=__codelineno-0-1631 name=__codelineno-0-1631></a><span class=sd>            - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.</span>
</span><span id=__span-0-1632><a id=__codelineno-0-1632 name=__codelineno-0-1632></a><span class=sd>            - define the normal distribution q(z) given the parameter tensors above.</span>
</span><span id=__span-0-1633><a id=__codelineno-0-1633 name=__codelineno-0-1633></a>
</span><span id=__span-0-1634><a id=__codelineno-0-1634 name=__codelineno-0-1634></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1635><a id=__codelineno-0-1635 name=__codelineno-0-1635></a><span class=sd>        ----------</span>
</span><span id=__span-0-1636><a id=__codelineno-0-1636 name=__codelineno-0-1636></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1637><a id=__codelineno-0-1637 name=__codelineno-0-1637></a><span class=sd>            The input tensor to be processed.</span>
</span><span id=__span-0-1638><a id=__codelineno-0-1638 name=__codelineno-0-1638></a><span class=sd>        var_clip_max: float</span>
</span><span id=__span-0-1639><a id=__codelineno-0-1639 name=__codelineno-0-1639></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1640><a id=__codelineno-0-1640 name=__codelineno-0-1640></a><span class=sd>            Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1641><a id=__codelineno-0-1641 name=__codelineno-0-1641></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1642><a id=__codelineno-0-1642 name=__codelineno-0-1642></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>)</span>
</span><span id=__span-0-1643><a id=__codelineno-0-1643 name=__codelineno-0-1643></a>
</span><span id=__span-0-1644><a id=__codelineno-0-1644 name=__codelineno-0-1644></a>        <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span> <span class=o>=</span> <span class=n>q_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1645><a id=__codelineno-0-1645 name=__codelineno-0-1645></a>        <span class=k>if</span> <span class=n>var_clip_max</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1646><a id=__codelineno-0-1646 name=__codelineno-0-1646></a>            <span class=n>q_lv</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1647><a id=__codelineno-0-1647 name=__codelineno-0-1647></a>
</span><span id=__span-0-1648><a id=__codelineno-0-1648 name=__codelineno-0-1648></a>        <span class=k>if</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>and</span> <span class=n>allow_oddsizes</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1649><a id=__codelineno-0-1649 name=__codelineno-0-1649></a>            <span class=n>q_mu</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1650><a id=__codelineno-0-1650 name=__codelineno-0-1650></a>            <span class=n>q_lv</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=n>q_lv</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1651><a id=__codelineno-0-1651 name=__codelineno-0-1651></a>            <span class=c1># clip_start = np.random.rand() &gt; 0.5</span>
</span><span id=__span-0-1652><a id=__codelineno-0-1652 name=__codelineno-0-1652></a>            <span class=c1># q_mu = q_mu[:, :, 1:, 1:] if clip_start else q_mu[:, :, :-1, :-1]</span>
</span><span id=__span-0-1653><a id=__codelineno-0-1653 name=__codelineno-0-1653></a>            <span class=c1># q_lv = q_lv[:, :, 1:, 1:] if clip_start else q_lv[:, :, :-1, :-1]</span>
</span><span id=__span-0-1654><a id=__codelineno-0-1654 name=__codelineno-0-1654></a>
</span><span id=__span-0-1655><a id=__codelineno-0-1655 name=__codelineno-0-1655></a>        <span class=n>q_mu</span> <span class=o>=</span> <span class=n>StableMean</span><span class=p>(</span><span class=n>q_mu</span><span class=p>)</span>
</span><span id=__span-0-1656><a id=__codelineno-0-1656 name=__codelineno-0-1656></a>        <span class=n>q_lv</span> <span class=o>=</span> <span class=n>StableLogVar</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=n>enable_stable</span><span class=o>=</span><span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span><span class=p>)</span>
</span><span id=__span-0-1657><a id=__codelineno-0-1657 name=__codelineno-0-1657></a>        <span class=n>q</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=n>q_mu</span><span class=o>.</span><span class=n>get</span><span class=p>(),</span> <span class=n>q_lv</span><span class=o>.</span><span class=n>get_std</span><span class=p>())</span>
</span><span id=__span-0-1658><a id=__codelineno-0-1658 name=__codelineno-0-1658></a>        <span class=k>return</span> <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>,</span> <span class=n>q</span>
</span><span id=__span-0-1659><a id=__codelineno-0-1659 name=__codelineno-0-1659></a>
</span><span id=__span-0-1660><a id=__codelineno-0-1660 name=__codelineno-0-1660></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1661><a id=__codelineno-0-1661 name=__codelineno-0-1661></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1662><a id=__codelineno-0-1662 name=__codelineno-0-1662></a>        <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1663><a id=__codelineno-0-1663 name=__codelineno-0-1663></a>        <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1664><a id=__codelineno-0-1664 name=__codelineno-0-1664></a>        <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1665><a id=__codelineno-0-1665 name=__codelineno-0-1665></a>        <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1666><a id=__codelineno-0-1666 name=__codelineno-0-1666></a>        <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1667><a id=__codelineno-0-1667 name=__codelineno-0-1667></a>        <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1668><a id=__codelineno-0-1668 name=__codelineno-0-1668></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1669><a id=__codelineno-0-1669 name=__codelineno-0-1669></a>        <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1670><a id=__codelineno-0-1670 name=__codelineno-0-1670></a>        <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1671><a id=__codelineno-0-1671 name=__codelineno-0-1671></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1672><a id=__codelineno-0-1672 name=__codelineno-0-1672></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1673><a id=__codelineno-0-1673 name=__codelineno-0-1673></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1674><a id=__codelineno-0-1674 name=__codelineno-0-1674></a><span class=sd>        ----------</span>
</span><span id=__span-0-1675><a id=__codelineno-0-1675 name=__codelineno-0-1675></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1676><a id=__codelineno-0-1676 name=__codelineno-0-1676></a><span class=sd>            The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</span>
</span><span id=__span-0-1677><a id=__codelineno-0-1677 name=__codelineno-0-1677></a><span class=sd>        q_params: torch.Tensor, optional</span>
</span><span id=__span-0-1678><a id=__codelineno-0-1678 name=__codelineno-0-1678></a><span class=sd>            The tensor resulting from merging the bu_value tensor at the same hierarchical level</span>
</span><span id=__span-0-1679><a id=__codelineno-0-1679 name=__codelineno-0-1679></a><span class=sd>            from the bottom-up pass and the `p_params` tensor. Default is `None`.</span>
</span><span id=__span-0-1680><a id=__codelineno-0-1680 name=__codelineno-0-1680></a><span class=sd>        forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1681><a id=__codelineno-0-1681 name=__codelineno-0-1681></a><span class=sd>            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent</span>
</span><span id=__span-0-1682><a id=__codelineno-0-1682 name=__codelineno-0-1682></a><span class=sd>            tensor and, hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1683><a id=__codelineno-0-1683 name=__codelineno-0-1683></a><span class=sd>        use_mode: bool, optional</span>
</span><span id=__span-0-1684><a id=__codelineno-0-1684 name=__codelineno-0-1684></a><span class=sd>            Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1685><a id=__codelineno-0-1685 name=__codelineno-0-1685></a><span class=sd>            In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1686><a id=__codelineno-0-1686 name=__codelineno-0-1686></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1687><a id=__codelineno-0-1687 name=__codelineno-0-1687></a><span class=sd>        force_constant_output: bool, optional</span>
</span><span id=__span-0-1688><a id=__codelineno-0-1688 name=__codelineno-0-1688></a><span class=sd>            Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1689><a id=__codelineno-0-1689 name=__codelineno-0-1689></a><span class=sd>            This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1690><a id=__codelineno-0-1690 name=__codelineno-0-1690></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1691><a id=__codelineno-0-1691 name=__codelineno-0-1691></a><span class=sd>        analytical_kl: bool, optional</span>
</span><span id=__span-0-1692><a id=__codelineno-0-1692 name=__codelineno-0-1692></a><span class=sd>            Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1693><a id=__codelineno-0-1693 name=__codelineno-0-1693></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1694><a id=__codelineno-0-1694 name=__codelineno-0-1694></a><span class=sd>        mode_pred: bool, optional</span>
</span><span id=__span-0-1695><a id=__codelineno-0-1695 name=__codelineno-0-1695></a><span class=sd>            Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1696><a id=__codelineno-0-1696 name=__codelineno-0-1696></a><span class=sd>        use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1697><a id=__codelineno-0-1697 name=__codelineno-0-1697></a><span class=sd>            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1698><a id=__codelineno-0-1698 name=__codelineno-0-1698></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1699><a id=__codelineno-0-1699 name=__codelineno-0-1699></a><span class=sd>        var_clip_max: float, optional</span>
</span><span id=__span-0-1700><a id=__codelineno-0-1700 name=__codelineno-0-1700></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1701><a id=__codelineno-0-1701 name=__codelineno-0-1701></a><span class=sd>            Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1702><a id=__codelineno-0-1702 name=__codelineno-0-1702></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1703><a id=__codelineno-0-1703 name=__codelineno-0-1703></a>        <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-1704><a id=__codelineno-0-1704 name=__codelineno-0-1704></a>
</span><span id=__span-0-1705><a id=__codelineno-0-1705 name=__codelineno-0-1705></a>        <span class=c1># Check sampling options consistency</span>
</span><span id=__span-0-1706><a id=__codelineno-0-1706 name=__codelineno-0-1706></a>        <span class=k>assert</span> <span class=p>(</span><span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>)</span> <span class=ow>or</span> <span class=p>(</span><span class=ow>not</span> <span class=n>use_mode</span><span class=p>)</span>
</span><span id=__span-0-1707><a id=__codelineno-0-1707 name=__codelineno-0-1707></a>
</span><span id=__span-0-1708><a id=__codelineno-0-1708 name=__codelineno-0-1708></a>        <span class=c1># Get generative distribution p(z_i|z_{i+1})</span>
</span><span id=__span-0-1709><a id=__codelineno-0-1709 name=__codelineno-0-1709></a>        <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>,</span> <span class=n>p</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_p_params</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1710><a id=__codelineno-0-1710 name=__codelineno-0-1710></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>)</span>
</span><span id=__span-0-1711><a id=__codelineno-0-1711 name=__codelineno-0-1711></a>
</span><span id=__span-0-1712><a id=__codelineno-0-1712 name=__codelineno-0-1712></a>        <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1713><a id=__codelineno-0-1713 name=__codelineno-0-1713></a>            <span class=c1># Get inference distribution q(z_i|z_{i+1})</span>
</span><span id=__span-0-1714><a id=__codelineno-0-1714 name=__codelineno-0-1714></a>            <span class=c1># NOTE: At inference time, don&#39;t centercrop the q_params even if they are odd in size.</span>
</span><span id=__span-0-1715><a id=__codelineno-0-1715 name=__codelineno-0-1715></a>            <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>,</span> <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1716><a id=__codelineno-0-1716 name=__codelineno-0-1716></a>                <span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1717><a id=__codelineno-0-1717 name=__codelineno-0-1717></a>            <span class=p>)</span>
</span><span id=__span-0-1718><a id=__codelineno-0-1718 name=__codelineno-0-1718></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>)</span>
</span><span id=__span-0-1719><a id=__codelineno-0-1719 name=__codelineno-0-1719></a>            <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>q</span>
</span><span id=__span-0-1720><a id=__codelineno-0-1720 name=__codelineno-0-1720></a>            <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>q_lv</span><span class=o>.</span><span class=n>get</span><span class=p>())</span>
</span><span id=__span-0-1721><a id=__codelineno-0-1721 name=__codelineno-0-1721></a>
</span><span id=__span-0-1722><a id=__codelineno-0-1722 name=__codelineno-0-1722></a>            <span class=c1># Centercrop p_params so that their size matches the one of q_params</span>
</span><span id=__span-0-1723><a id=__codelineno-0-1723 name=__codelineno-0-1723></a>            <span class=n>q_size</span> <span class=o>=</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>get</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1724><a id=__codelineno-0-1724 name=__codelineno-0-1724></a>            <span class=k>if</span> <span class=n>p_mu</span><span class=o>.</span><span class=n>get</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>!=</span> <span class=n>q_size</span> <span class=ow>and</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1725><a id=__codelineno-0-1725 name=__codelineno-0-1725></a>                <span class=n>p_mu</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1726><a id=__codelineno-0-1726 name=__codelineno-0-1726></a>                <span class=n>p_lv</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1727><a id=__codelineno-0-1727 name=__codelineno-0-1727></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1728><a id=__codelineno-0-1728 name=__codelineno-0-1728></a>            <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>p</span>
</span><span id=__span-0-1729><a id=__codelineno-0-1729 name=__codelineno-0-1729></a>
</span><span id=__span-0-1730><a id=__codelineno-0-1730 name=__codelineno-0-1730></a>        <span class=c1># Sample latent variable</span>
</span><span id=__span-0-1731><a id=__codelineno-0-1731 name=__codelineno-0-1731></a>        <span class=n>z</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_z</span><span class=p>(</span>
</span><span id=__span-0-1732><a id=__codelineno-0-1732 name=__codelineno-0-1732></a>            <span class=n>sampling_distrib</span><span class=p>,</span> <span class=n>forced_latent</span><span class=p>,</span> <span class=n>use_mode</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>use_uncond_mode</span>
</span><span id=__span-0-1733><a id=__codelineno-0-1733 name=__codelineno-0-1733></a>        <span class=p>)</span>
</span><span id=__span-0-1734><a id=__codelineno-0-1734 name=__codelineno-0-1734></a>
</span><span id=__span-0-1735><a id=__codelineno-0-1735 name=__codelineno-0-1735></a>        <span class=c1># Copy one sample (and distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1736><a id=__codelineno-0-1736 name=__codelineno-0-1736></a>        <span class=c1># This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1737><a id=__codelineno-0-1737 name=__codelineno-0-1737></a>        <span class=k>if</span> <span class=n>force_constant_output</span><span class=p>:</span>
</span><span id=__span-0-1738><a id=__codelineno-0-1738 name=__codelineno-0-1738></a>            <span class=n>z</span> <span class=o>=</span> <span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span><span id=__span-0-1739><a id=__codelineno-0-1739 name=__codelineno-0-1739></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1740><a id=__codelineno-0-1740 name=__codelineno-0-1740></a>                <span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1741><a id=__codelineno-0-1741 name=__codelineno-0-1741></a>                <span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1742><a id=__codelineno-0-1742 name=__codelineno-0-1742></a>            <span class=p>)</span>
</span><span id=__span-0-1743><a id=__codelineno-0-1743 name=__codelineno-0-1743></a>
</span><span id=__span-0-1744><a id=__codelineno-0-1744 name=__codelineno-0-1744></a>        <span class=c1># Pass the sampled latent througn the output convolutional layer of stochastic block</span>
</span><span id=__span-0-1745><a id=__codelineno-0-1745 name=__codelineno-0-1745></a>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-0-1746><a id=__codelineno-0-1746 name=__codelineno-0-1746></a>
</span><span id=__span-0-1747><a id=__codelineno-0-1747 name=__codelineno-0-1747></a>        <span class=c1># Compute log p(z)# NOTE: disabling its computation.</span>
</span><span id=__span-0-1748><a id=__codelineno-0-1748 name=__codelineno-0-1748></a>        <span class=c1># if mode_pred is False:</span>
</span><span id=__span-0-1749><a id=__codelineno-0-1749 name=__codelineno-0-1749></a>        <span class=c1>#     logprob_p =  p.log_prob(z).sum((1, 2, 3))</span>
</span><span id=__span-0-1750><a id=__codelineno-0-1750 name=__codelineno-0-1750></a>        <span class=c1># else:</span>
</span><span id=__span-0-1751><a id=__codelineno-0-1751 name=__codelineno-0-1751></a>        <span class=c1>#     logprob_p = None</span>
</span><span id=__span-0-1752><a id=__codelineno-0-1752 name=__codelineno-0-1752></a>
</span><span id=__span-0-1753><a id=__codelineno-0-1753 name=__codelineno-0-1753></a>        <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1754><a id=__codelineno-0-1754 name=__codelineno-0-1754></a>            <span class=c1># Compute log q(z)</span>
</span><span id=__span-0-1755><a id=__codelineno-0-1755 name=__codelineno-0-1755></a>            <span class=n>logprob_q</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>log_prob</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1756><a id=__codelineno-0-1756 name=__codelineno-0-1756></a>            <span class=c1># Compute KL divergence metrics</span>
</span><span id=__span-0-1757><a id=__codelineno-0-1757 name=__codelineno-0-1757></a>            <span class=n>kl_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1758><a id=__codelineno-0-1758 name=__codelineno-0-1758></a>                <span class=n>p</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>q_params</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=p>,</span> <span class=n>z</span>
</span><span id=__span-0-1759><a id=__codelineno-0-1759 name=__codelineno-0-1759></a>            <span class=p>)</span>
</span><span id=__span-0-1760><a id=__codelineno-0-1760 name=__codelineno-0-1760></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1761><a id=__codelineno-0-1761 name=__codelineno-0-1761></a>            <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-0-1762><a id=__codelineno-0-1762 name=__codelineno-0-1762></a>            <span class=n>logprob_q</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1763><a id=__codelineno-0-1763 name=__codelineno-0-1763></a>
</span><span id=__span-0-1764><a id=__codelineno-0-1764 name=__codelineno-0-1764></a>        <span class=c1># Store meaningful quantities to use them in following layers</span>
</span><span id=__span-0-1765><a id=__codelineno-0-1765 name=__codelineno-0-1765></a>        <span class=n>data</span> <span class=o>=</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1766><a id=__codelineno-0-1766 name=__codelineno-0-1766></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;z&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>z</span>  <span class=c1># sampled variable at this layer (batch, ch, h, w)</span>
</span><span id=__span-0-1767><a id=__codelineno-0-1767 name=__codelineno-0-1767></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;p_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>p_params</span>  <span class=c1># (b, ch, h, w) where b is 1 or batch size</span>
</span><span id=__span-0-1768><a id=__codelineno-0-1768 name=__codelineno-0-1768></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_params</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1769><a id=__codelineno-0-1769 name=__codelineno-0-1769></a>        <span class=c1># data[&#39;logprob_p&#39;] = logprob_p  # (batch, )</span>
</span><span id=__span-0-1770><a id=__codelineno-0-1770 name=__codelineno-0-1770></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;logprob_q&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>logprob_q</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1771><a id=__codelineno-0-1771 name=__codelineno-0-1771></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;qvar_max&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>debug_qvar_max</span>
</span><span id=__span-0-1772><a id=__codelineno-0-1772 name=__codelineno-0-1772></a>
</span><span id=__span-0-1773><a id=__codelineno-0-1773 name=__codelineno-0-1773></a>        <span class=k>return</span> <span class=n>out</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>c_in</span><span class=p>,</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>transform_p_params</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>vanilla_latent_hw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>restricted_kl</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>use_naive_exponential</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>c_in</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels of the input tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_vars</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels of the latent space tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_out</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The output of the stochastic layer. Note that this is different from the sampled latent z.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>kernel</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The size of the kernel used in convolutional layers. Default is 3.</p> </div> </td> <td> <code>3</code> </td> </tr> <tr class=doc-section-item> <td><code>transform_p_params</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether a transformation should be applied to the <code>p_params</code> tensor. The transformation consists in a 2D convolution ()<code>conv_in_p()</code>) that maps the input to a larger number of channels. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>vanilla_latent_hw</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>restricted_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the restricted version of KL Divergence. See NOTE 2 for more information about its computation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>use_naive_exponential</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>If <code>False</code>, exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1359>1359</a></span>
<span class=normal><a href=#__codelineno-0-1360>1360</a></span>
<span class=normal><a href=#__codelineno-0-1361>1361</a></span>
<span class=normal><a href=#__codelineno-0-1362>1362</a></span>
<span class=normal><a href=#__codelineno-0-1363>1363</a></span>
<span class=normal><a href=#__codelineno-0-1364>1364</a></span>
<span class=normal><a href=#__codelineno-0-1365>1365</a></span>
<span class=normal><a href=#__codelineno-0-1366>1366</a></span>
<span class=normal><a href=#__codelineno-0-1367>1367</a></span>
<span class=normal><a href=#__codelineno-0-1368>1368</a></span>
<span class=normal><a href=#__codelineno-0-1369>1369</a></span>
<span class=normal><a href=#__codelineno-0-1370>1370</a></span>
<span class=normal><a href=#__codelineno-0-1371>1371</a></span>
<span class=normal><a href=#__codelineno-0-1372>1372</a></span>
<span class=normal><a href=#__codelineno-0-1373>1373</a></span>
<span class=normal><a href=#__codelineno-0-1374>1374</a></span>
<span class=normal><a href=#__codelineno-0-1375>1375</a></span>
<span class=normal><a href=#__codelineno-0-1376>1376</a></span>
<span class=normal><a href=#__codelineno-0-1377>1377</a></span>
<span class=normal><a href=#__codelineno-0-1378>1378</a></span>
<span class=normal><a href=#__codelineno-0-1379>1379</a></span>
<span class=normal><a href=#__codelineno-0-1380>1380</a></span>
<span class=normal><a href=#__codelineno-0-1381>1381</a></span>
<span class=normal><a href=#__codelineno-0-1382>1382</a></span>
<span class=normal><a href=#__codelineno-0-1383>1383</a></span>
<span class=normal><a href=#__codelineno-0-1384>1384</a></span>
<span class=normal><a href=#__codelineno-0-1385>1385</a></span>
<span class=normal><a href=#__codelineno-0-1386>1386</a></span>
<span class=normal><a href=#__codelineno-0-1387>1387</a></span>
<span class=normal><a href=#__codelineno-0-1388>1388</a></span>
<span class=normal><a href=#__codelineno-0-1389>1389</a></span>
<span class=normal><a href=#__codelineno-0-1390>1390</a></span>
<span class=normal><a href=#__codelineno-0-1391>1391</a></span>
<span class=normal><a href=#__codelineno-0-1392>1392</a></span>
<span class=normal><a href=#__codelineno-0-1393>1393</a></span>
<span class=normal><a href=#__codelineno-0-1394>1394</a></span>
<span class=normal><a href=#__codelineno-0-1395>1395</a></span>
<span class=normal><a href=#__codelineno-0-1396>1396</a></span>
<span class=normal><a href=#__codelineno-0-1397>1397</a></span>
<span class=normal><a href=#__codelineno-0-1398>1398</a></span>
<span class=normal><a href=#__codelineno-0-1399>1399</a></span>
<span class=normal><a href=#__codelineno-0-1400>1400</a></span>
<span class=normal><a href=#__codelineno-0-1401>1401</a></span>
<span class=normal><a href=#__codelineno-0-1402>1402</a></span>
<span class=normal><a href=#__codelineno-0-1403>1403</a></span>
<span class=normal><a href=#__codelineno-0-1404>1404</a></span>
<span class=normal><a href=#__codelineno-0-1405>1405</a></span>
<span class=normal><a href=#__codelineno-0-1406>1406</a></span>
<span class=normal><a href=#__codelineno-0-1407>1407</a></span>
<span class=normal><a href=#__codelineno-0-1408>1408</a></span>
<span class=normal><a href=#__codelineno-0-1409>1409</a></span>
<span class=normal><a href=#__codelineno-0-1410>1410</a></span>
<span class=normal><a href=#__codelineno-0-1411>1411</a></span>
<span class=normal><a href=#__codelineno-0-1412>1412</a></span>
<span class=normal><a href=#__codelineno-0-1413>1413</a></span>
<span class=normal><a href=#__codelineno-0-1414>1414</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1359><a id=__codelineno-0-1359 name=__codelineno-0-1359></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-1360><a id=__codelineno-0-1360 name=__codelineno-0-1360></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1361><a id=__codelineno-0-1361 name=__codelineno-0-1361></a>    <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1362><a id=__codelineno-0-1362 name=__codelineno-0-1362></a>    <span class=n>c_vars</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1363><a id=__codelineno-0-1363 name=__codelineno-0-1363></a>    <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1364><a id=__codelineno-0-1364 name=__codelineno-0-1364></a>    <span class=n>kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-1365><a id=__codelineno-0-1365 name=__codelineno-0-1365></a>    <span class=n>transform_p_params</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-1366><a id=__codelineno-0-1366 name=__codelineno-0-1366></a>    <span class=n>vanilla_latent_hw</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1367><a id=__codelineno-0-1367 name=__codelineno-0-1367></a>    <span class=n>restricted_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1368><a id=__codelineno-0-1368 name=__codelineno-0-1368></a>    <span class=n>use_naive_exponential</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1369><a id=__codelineno-0-1369 name=__codelineno-0-1369></a><span class=p>):</span>
</span><span id=__span-0-1370><a id=__codelineno-0-1370 name=__codelineno-0-1370></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1371><a id=__codelineno-0-1371 name=__codelineno-0-1371></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1372><a id=__codelineno-0-1372 name=__codelineno-0-1372></a><span class=sd>    ----------</span>
</span><span id=__span-0-1373><a id=__codelineno-0-1373 name=__codelineno-0-1373></a><span class=sd>    c_in: int</span>
</span><span id=__span-0-1374><a id=__codelineno-0-1374 name=__codelineno-0-1374></a><span class=sd>        The number of channels of the input tensor.</span>
</span><span id=__span-0-1375><a id=__codelineno-0-1375 name=__codelineno-0-1375></a><span class=sd>    c_vars: int</span>
</span><span id=__span-0-1376><a id=__codelineno-0-1376 name=__codelineno-0-1376></a><span class=sd>        The number of channels of the latent space tensor.</span>
</span><span id=__span-0-1377><a id=__codelineno-0-1377 name=__codelineno-0-1377></a><span class=sd>    c_out:  int</span>
</span><span id=__span-0-1378><a id=__codelineno-0-1378 name=__codelineno-0-1378></a><span class=sd>        The output of the stochastic layer.</span>
</span><span id=__span-0-1379><a id=__codelineno-0-1379 name=__codelineno-0-1379></a><span class=sd>        Note that this is different from the sampled latent z.</span>
</span><span id=__span-0-1380><a id=__codelineno-0-1380 name=__codelineno-0-1380></a><span class=sd>    kernel: int, optional</span>
</span><span id=__span-0-1381><a id=__codelineno-0-1381 name=__codelineno-0-1381></a><span class=sd>        The size of the kernel used in convolutional layers.</span>
</span><span id=__span-0-1382><a id=__codelineno-0-1382 name=__codelineno-0-1382></a><span class=sd>        Default is 3.</span>
</span><span id=__span-0-1383><a id=__codelineno-0-1383 name=__codelineno-0-1383></a><span class=sd>    transform_p_params: bool, optional</span>
</span><span id=__span-0-1384><a id=__codelineno-0-1384 name=__codelineno-0-1384></a><span class=sd>        Whether a transformation should be applied to the `p_params` tensor.</span>
</span><span id=__span-0-1385><a id=__codelineno-0-1385 name=__codelineno-0-1385></a><span class=sd>        The transformation consists in a 2D convolution ()`conv_in_p()`) that</span>
</span><span id=__span-0-1386><a id=__codelineno-0-1386 name=__codelineno-0-1386></a><span class=sd>        maps the input to a larger number of channels.</span>
</span><span id=__span-0-1387><a id=__codelineno-0-1387 name=__codelineno-0-1387></a><span class=sd>        Default is `True`.</span>
</span><span id=__span-0-1388><a id=__codelineno-0-1388 name=__codelineno-0-1388></a><span class=sd>    vanilla_latent_hw: int, optional</span>
</span><span id=__span-0-1389><a id=__codelineno-0-1389 name=__codelineno-0-1389></a><span class=sd>        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).</span>
</span><span id=__span-0-1390><a id=__codelineno-0-1390 name=__codelineno-0-1390></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-1391><a id=__codelineno-0-1391 name=__codelineno-0-1391></a><span class=sd>    restricted_kl: bool, optional</span>
</span><span id=__span-0-1392><a id=__codelineno-0-1392 name=__codelineno-0-1392></a><span class=sd>        Whether to compute the restricted version of KL Divergence.</span>
</span><span id=__span-0-1393><a id=__codelineno-0-1393 name=__codelineno-0-1393></a><span class=sd>        See NOTE 2 for more information about its computation.</span>
</span><span id=__span-0-1394><a id=__codelineno-0-1394 name=__codelineno-0-1394></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1395><a id=__codelineno-0-1395 name=__codelineno-0-1395></a><span class=sd>    use_naive_exponential: bool, optional</span>
</span><span id=__span-0-1396><a id=__codelineno-0-1396 name=__codelineno-0-1396></a><span class=sd>        If `False`, exponentials are computed according to the alternative definition</span>
</span><span id=__span-0-1397><a id=__codelineno-0-1397 name=__codelineno-0-1397></a><span class=sd>        provided by `StableExponential` class. This should improve numerical stability</span>
</span><span id=__span-0-1398><a id=__codelineno-0-1398 name=__codelineno-0-1398></a><span class=sd>        in the training process. Default is `False`.</span>
</span><span id=__span-0-1399><a id=__codelineno-0-1399 name=__codelineno-0-1399></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1400><a id=__codelineno-0-1400 name=__codelineno-0-1400></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-1401><a id=__codelineno-0-1401 name=__codelineno-0-1401></a>    <span class=k>assert</span> <span class=n>kernel</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span>
</span><span id=__span-0-1402><a id=__codelineno-0-1402 name=__codelineno-0-1402></a>    <span class=n>pad</span> <span class=o>=</span> <span class=n>kernel</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1403><a id=__codelineno-0-1403 name=__codelineno-0-1403></a>    <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span> <span class=o>=</span> <span class=n>transform_p_params</span>
</span><span id=__span-0-1404><a id=__codelineno-0-1404 name=__codelineno-0-1404></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_in</span> <span class=o>=</span> <span class=n>c_in</span>
</span><span id=__span-0-1405><a id=__codelineno-0-1405 name=__codelineno-0-1405></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_out</span> <span class=o>=</span> <span class=n>c_out</span>
</span><span id=__span-0-1406><a id=__codelineno-0-1406 name=__codelineno-0-1406></a>    <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span> <span class=o>=</span> <span class=n>c_vars</span>
</span><span id=__span-0-1407><a id=__codelineno-0-1407 name=__codelineno-0-1407></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span> <span class=o>=</span> <span class=n>use_naive_exponential</span>
</span><span id=__span-0-1408><a id=__codelineno-0-1408 name=__codelineno-0-1408></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span> <span class=o>=</span> <span class=n>vanilla_latent_hw</span>
</span><span id=__span-0-1409><a id=__codelineno-0-1409 name=__codelineno-0-1409></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_restricted_kl</span> <span class=o>=</span> <span class=n>restricted_kl</span>
</span><span id=__span-0-1410><a id=__codelineno-0-1410 name=__codelineno-0-1410></a>
</span><span id=__span-0-1411><a id=__codelineno-0-1411 name=__codelineno-0-1411></a>    <span class=k>if</span> <span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1412><a id=__codelineno-0-1412 name=__codelineno-0-1412></a>        <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-1413><a id=__codelineno-0-1413 name=__codelineno-0-1413></a>    <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_in</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>c_vars</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span><span id=__span-0-1414><a id=__codelineno-0-1414 name=__codelineno-0-1414></a>    <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>c_vars</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>kernel</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.compute_kl_metrics class="doc doc-heading"> <code class="highlight language-python"><span class=n>compute_kl_metrics</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>q_params</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=p>,</span> <span class=n>z</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.compute_kl_metrics class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric. Specifically, the different versions of the KL loss terms are: - <code>kl_elementwise</code>: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)]. - <code>kl_samplewise</code>: KL term associated to each sample in the batch [Shape: (batch, )]. - <code>kl_samplewise_restricted</code>: KL term only associated to the portion of the latent tensor that is used for prediction and summed over channel and spatial dimensions [Shape: (batch, )]. - <code>kl_channelwise</code>: KL term associated to each sample and each channel [Shape: (batch, ch, )]. - <code>kl_spatial</code>: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p</code></td> <td> <code><span title="torch.distributions.normal.Normal">Normal</span></code> </td> <td> <div class=doc-md-description> <p>The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The parameters of the prior generative distribution.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q</code></td> <td> <code><span title="torch.distributions.normal.Normal">Normal</span></code> </td> <td> <div class=doc-md-description> <p>The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The parameters of the inference distribution.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is in prediction mode.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>analytical_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>z</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The sampled latent tensor.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1510>1510</a></span>
<span class=normal><a href=#__codelineno-0-1511>1511</a></span>
<span class=normal><a href=#__codelineno-0-1512>1512</a></span>
<span class=normal><a href=#__codelineno-0-1513>1513</a></span>
<span class=normal><a href=#__codelineno-0-1514>1514</a></span>
<span class=normal><a href=#__codelineno-0-1515>1515</a></span>
<span class=normal><a href=#__codelineno-0-1516>1516</a></span>
<span class=normal><a href=#__codelineno-0-1517>1517</a></span>
<span class=normal><a href=#__codelineno-0-1518>1518</a></span>
<span class=normal><a href=#__codelineno-0-1519>1519</a></span>
<span class=normal><a href=#__codelineno-0-1520>1520</a></span>
<span class=normal><a href=#__codelineno-0-1521>1521</a></span>
<span class=normal><a href=#__codelineno-0-1522>1522</a></span>
<span class=normal><a href=#__codelineno-0-1523>1523</a></span>
<span class=normal><a href=#__codelineno-0-1524>1524</a></span>
<span class=normal><a href=#__codelineno-0-1525>1525</a></span>
<span class=normal><a href=#__codelineno-0-1526>1526</a></span>
<span class=normal><a href=#__codelineno-0-1527>1527</a></span>
<span class=normal><a href=#__codelineno-0-1528>1528</a></span>
<span class=normal><a href=#__codelineno-0-1529>1529</a></span>
<span class=normal><a href=#__codelineno-0-1530>1530</a></span>
<span class=normal><a href=#__codelineno-0-1531>1531</a></span>
<span class=normal><a href=#__codelineno-0-1532>1532</a></span>
<span class=normal><a href=#__codelineno-0-1533>1533</a></span>
<span class=normal><a href=#__codelineno-0-1534>1534</a></span>
<span class=normal><a href=#__codelineno-0-1535>1535</a></span>
<span class=normal><a href=#__codelineno-0-1536>1536</a></span>
<span class=normal><a href=#__codelineno-0-1537>1537</a></span>
<span class=normal><a href=#__codelineno-0-1538>1538</a></span>
<span class=normal><a href=#__codelineno-0-1539>1539</a></span>
<span class=normal><a href=#__codelineno-0-1540>1540</a></span>
<span class=normal><a href=#__codelineno-0-1541>1541</a></span>
<span class=normal><a href=#__codelineno-0-1542>1542</a></span>
<span class=normal><a href=#__codelineno-0-1543>1543</a></span>
<span class=normal><a href=#__codelineno-0-1544>1544</a></span>
<span class=normal><a href=#__codelineno-0-1545>1545</a></span>
<span class=normal><a href=#__codelineno-0-1546>1546</a></span>
<span class=normal><a href=#__codelineno-0-1547>1547</a></span>
<span class=normal><a href=#__codelineno-0-1548>1548</a></span>
<span class=normal><a href=#__codelineno-0-1549>1549</a></span>
<span class=normal><a href=#__codelineno-0-1550>1550</a></span>
<span class=normal><a href=#__codelineno-0-1551>1551</a></span>
<span class=normal><a href=#__codelineno-0-1552>1552</a></span>
<span class=normal><a href=#__codelineno-0-1553>1553</a></span>
<span class=normal><a href=#__codelineno-0-1554>1554</a></span>
<span class=normal><a href=#__codelineno-0-1555>1555</a></span>
<span class=normal><a href=#__codelineno-0-1556>1556</a></span>
<span class=normal><a href=#__codelineno-0-1557>1557</a></span>
<span class=normal><a href=#__codelineno-0-1558>1558</a></span>
<span class=normal><a href=#__codelineno-0-1559>1559</a></span>
<span class=normal><a href=#__codelineno-0-1560>1560</a></span>
<span class=normal><a href=#__codelineno-0-1561>1561</a></span>
<span class=normal><a href=#__codelineno-0-1562>1562</a></span>
<span class=normal><a href=#__codelineno-0-1563>1563</a></span>
<span class=normal><a href=#__codelineno-0-1564>1564</a></span>
<span class=normal><a href=#__codelineno-0-1565>1565</a></span>
<span class=normal><a href=#__codelineno-0-1566>1566</a></span>
<span class=normal><a href=#__codelineno-0-1567>1567</a></span>
<span class=normal><a href=#__codelineno-0-1568>1568</a></span>
<span class=normal><a href=#__codelineno-0-1569>1569</a></span>
<span class=normal><a href=#__codelineno-0-1570>1570</a></span>
<span class=normal><a href=#__codelineno-0-1571>1571</a></span>
<span class=normal><a href=#__codelineno-0-1572>1572</a></span>
<span class=normal><a href=#__codelineno-0-1573>1573</a></span>
<span class=normal><a href=#__codelineno-0-1574>1574</a></span>
<span class=normal><a href=#__codelineno-0-1575>1575</a></span>
<span class=normal><a href=#__codelineno-0-1576>1576</a></span>
<span class=normal><a href=#__codelineno-0-1577>1577</a></span>
<span class=normal><a href=#__codelineno-0-1578>1578</a></span>
<span class=normal><a href=#__codelineno-0-1579>1579</a></span>
<span class=normal><a href=#__codelineno-0-1580>1580</a></span>
<span class=normal><a href=#__codelineno-0-1581>1581</a></span>
<span class=normal><a href=#__codelineno-0-1582>1582</a></span>
<span class=normal><a href=#__codelineno-0-1583>1583</a></span>
<span class=normal><a href=#__codelineno-0-1584>1584</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1510><a id=__codelineno-0-1510 name=__codelineno-0-1510></a><span class=k>def</span> <span class=nf>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1511><a id=__codelineno-0-1511 name=__codelineno-0-1511></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1512><a id=__codelineno-0-1512 name=__codelineno-0-1512></a>    <span class=n>p</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1513><a id=__codelineno-0-1513 name=__codelineno-0-1513></a>    <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1514><a id=__codelineno-0-1514 name=__codelineno-0-1514></a>    <span class=n>q</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1515><a id=__codelineno-0-1515 name=__codelineno-0-1515></a>    <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1516><a id=__codelineno-0-1516 name=__codelineno-0-1516></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1517><a id=__codelineno-0-1517 name=__codelineno-0-1517></a>    <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1518><a id=__codelineno-0-1518 name=__codelineno-0-1518></a>    <span class=n>z</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1519><a id=__codelineno-0-1519 name=__codelineno-0-1519></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-1520><a id=__codelineno-0-1520 name=__codelineno-0-1520></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1521><a id=__codelineno-0-1521 name=__codelineno-0-1521></a><span class=sd>    Compute KL (analytical or MC estimate) and then process it, extracting composed versions of the metric.</span>
</span><span id=__span-0-1522><a id=__codelineno-0-1522 name=__codelineno-0-1522></a><span class=sd>    Specifically, the different versions of the KL loss terms are:</span>
</span><span id=__span-0-1523><a id=__codelineno-0-1523 name=__codelineno-0-1523></a><span class=sd>        - `kl_elementwise`: KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)].</span>
</span><span id=__span-0-1524><a id=__codelineno-0-1524 name=__codelineno-0-1524></a><span class=sd>        - `kl_samplewise`: KL term associated to each sample in the batch [Shape: (batch, )].</span>
</span><span id=__span-0-1525><a id=__codelineno-0-1525 name=__codelineno-0-1525></a><span class=sd>        - `kl_samplewise_restricted`: KL term only associated to the portion of the latent tensor that is</span>
</span><span id=__span-0-1526><a id=__codelineno-0-1526 name=__codelineno-0-1526></a><span class=sd>        used for prediction and summed over channel and spatial dimensions [Shape: (batch, )].</span>
</span><span id=__span-0-1527><a id=__codelineno-0-1527 name=__codelineno-0-1527></a><span class=sd>        - `kl_channelwise`: KL term associated to each sample and each channel [Shape: (batch, ch, )].</span>
</span><span id=__span-0-1528><a id=__codelineno-0-1528 name=__codelineno-0-1528></a><span class=sd>        - `kl_spatial`: KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1529><a id=__codelineno-0-1529 name=__codelineno-0-1529></a>
</span><span id=__span-0-1530><a id=__codelineno-0-1530 name=__codelineno-0-1530></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1531><a id=__codelineno-0-1531 name=__codelineno-0-1531></a><span class=sd>    ----------</span>
</span><span id=__span-0-1532><a id=__codelineno-0-1532 name=__codelineno-0-1532></a><span class=sd>    p: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1533><a id=__codelineno-0-1533 name=__codelineno-0-1533></a><span class=sd>        The prior generative distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1534><a id=__codelineno-0-1534 name=__codelineno-0-1534></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1535><a id=__codelineno-0-1535 name=__codelineno-0-1535></a><span class=sd>        The parameters of the prior generative distribution.</span>
</span><span id=__span-0-1536><a id=__codelineno-0-1536 name=__codelineno-0-1536></a><span class=sd>    q: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1537><a id=__codelineno-0-1537 name=__codelineno-0-1537></a><span class=sd>        The inference distribution q(z_i|z_{i+1}) (or q(z_L|x)).</span>
</span><span id=__span-0-1538><a id=__codelineno-0-1538 name=__codelineno-0-1538></a><span class=sd>    q_params: torch.Tensor</span>
</span><span id=__span-0-1539><a id=__codelineno-0-1539 name=__codelineno-0-1539></a><span class=sd>        The parameters of the inference distribution.</span>
</span><span id=__span-0-1540><a id=__codelineno-0-1540 name=__codelineno-0-1540></a><span class=sd>    mode_pred: bool</span>
</span><span id=__span-0-1541><a id=__codelineno-0-1541 name=__codelineno-0-1541></a><span class=sd>        Whether the model is in prediction mode.</span>
</span><span id=__span-0-1542><a id=__codelineno-0-1542 name=__codelineno-0-1542></a><span class=sd>    analytical_kl: bool</span>
</span><span id=__span-0-1543><a id=__codelineno-0-1543 name=__codelineno-0-1543></a><span class=sd>        Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1544><a id=__codelineno-0-1544 name=__codelineno-0-1544></a><span class=sd>    z: torch.Tensor</span>
</span><span id=__span-0-1545><a id=__codelineno-0-1545 name=__codelineno-0-1545></a><span class=sd>        The sampled latent tensor.</span>
</span><span id=__span-0-1546><a id=__codelineno-0-1546 name=__codelineno-0-1546></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1547><a id=__codelineno-0-1547 name=__codelineno-0-1547></a>    <span class=n>kl_samplewise_restricted</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1548><a id=__codelineno-0-1548 name=__codelineno-0-1548></a>
</span><span id=__span-0-1549><a id=__codelineno-0-1549 name=__codelineno-0-1549></a>    <span class=k>if</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>  <span class=c1># if not in prediction mode</span>
</span><span id=__span-0-1550><a id=__codelineno-0-1550 name=__codelineno-0-1550></a>        <span class=c1># KL term for each single element of the latent tensor [Shape: (batch, ch, h, w)]</span>
</span><span id=__span-0-1551><a id=__codelineno-0-1551 name=__codelineno-0-1551></a>        <span class=k>if</span> <span class=n>analytical_kl</span><span class=p>:</span>
</span><span id=__span-0-1552><a id=__codelineno-0-1552 name=__codelineno-0-1552></a>            <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_divergence</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>p</span><span class=p>)</span>
</span><span id=__span-0-1553><a id=__codelineno-0-1553 name=__codelineno-0-1553></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1554><a id=__codelineno-0-1554 name=__codelineno-0-1554></a>            <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_normal_mc</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q_params</span><span class=p>)</span>
</span><span id=__span-0-1555><a id=__codelineno-0-1555 name=__codelineno-0-1555></a>
</span><span id=__span-0-1556><a id=__codelineno-0-1556 name=__codelineno-0-1556></a>        <span class=c1># KL term only associated to the portion of the latent tensor that is used for prediction and</span>
</span><span id=__span-0-1557><a id=__codelineno-0-1557 name=__codelineno-0-1557></a>        <span class=c1># summed over channel and spatial dimensions. [Shape: (batch, )]</span>
</span><span id=__span-0-1558><a id=__codelineno-0-1558 name=__codelineno-0-1558></a>        <span class=c1># NOTE: vanilla_latent_hw is the shape of the latent tensor used for prediction, hence</span>
</span><span id=__span-0-1559><a id=__codelineno-0-1559 name=__codelineno-0-1559></a>        <span class=c1># the restriction has shape [Shape: (batch, ch, vanilla_latent_hw[0], vanilla_latent_hw[1])]</span>
</span><span id=__span-0-1560><a id=__codelineno-0-1560 name=__codelineno-0-1560></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_restricted_kl</span><span class=p>:</span>
</span><span id=__span-0-1561><a id=__codelineno-0-1561 name=__codelineno-0-1561></a>            <span class=n>pad</span> <span class=o>=</span> <span class=p>(</span><span class=n>kl_elementwise</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span>
</span><span id=__span-0-1562><a id=__codelineno-0-1562 name=__codelineno-0-1562></a>            <span class=k>assert</span> <span class=n>pad</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&quot;Disable restricted kl since there is no restriction.&quot;</span>
</span><span id=__span-0-1563><a id=__codelineno-0-1563 name=__codelineno-0-1563></a>            <span class=n>tmp</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=n>pad</span><span class=p>:</span><span class=o>-</span><span class=n>pad</span><span class=p>,</span> <span class=n>pad</span><span class=p>:</span><span class=o>-</span><span class=n>pad</span><span class=p>]</span>
</span><span id=__span-0-1564><a id=__codelineno-0-1564 name=__codelineno-0-1564></a>            <span class=n>kl_samplewise_restricted</span> <span class=o>=</span> <span class=n>tmp</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1565><a id=__codelineno-0-1565 name=__codelineno-0-1565></a>
</span><span id=__span-0-1566><a id=__codelineno-0-1566 name=__codelineno-0-1566></a>        <span class=c1># KL term associated to each sample in the batch [Shape: (batch, )]</span>
</span><span id=__span-0-1567><a id=__codelineno-0-1567 name=__codelineno-0-1567></a>        <span class=n>kl_samplewise</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1568><a id=__codelineno-0-1568 name=__codelineno-0-1568></a>
</span><span id=__span-0-1569><a id=__codelineno-0-1569 name=__codelineno-0-1569></a>        <span class=c1># KL term associated to each sample and each channel [Shape: (batch, ch, )]</span>
</span><span id=__span-0-1570><a id=__codelineno-0-1570 name=__codelineno-0-1570></a>        <span class=n>kl_channelwise</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1571><a id=__codelineno-0-1571 name=__codelineno-0-1571></a>
</span><span id=__span-0-1572><a id=__codelineno-0-1572 name=__codelineno-0-1572></a>        <span class=c1># KL term summed over the channels, i.e., retaining the spatial dimensions [Shape: (batch, h, w)]</span>
</span><span id=__span-0-1573><a id=__codelineno-0-1573 name=__codelineno-0-1573></a>        <span class=n>kl_spatial</span> <span class=o>=</span> <span class=n>kl_elementwise</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1574><a id=__codelineno-0-1574 name=__codelineno-0-1574></a>    <span class=k>else</span><span class=p>:</span>  <span class=c1># if predicting, no need to compute KL</span>
</span><span id=__span-0-1575><a id=__codelineno-0-1575 name=__codelineno-0-1575></a>        <span class=n>kl_elementwise</span> <span class=o>=</span> <span class=n>kl_samplewise</span> <span class=o>=</span> <span class=n>kl_spatial</span> <span class=o>=</span> <span class=n>kl_channelwise</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1576><a id=__codelineno-0-1576 name=__codelineno-0-1576></a>
</span><span id=__span-0-1577><a id=__codelineno-0-1577 name=__codelineno-0-1577></a>    <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{</span>
</span><span id=__span-0-1578><a id=__codelineno-0-1578 name=__codelineno-0-1578></a>        <span class=s2>&quot;kl_elementwise&quot;</span><span class=p>:</span> <span class=n>kl_elementwise</span><span class=p>,</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1579><a id=__codelineno-0-1579 name=__codelineno-0-1579></a>        <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>:</span> <span class=n>kl_samplewise</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1580><a id=__codelineno-0-1580 name=__codelineno-0-1580></a>        <span class=s2>&quot;kl_samplewise_restricted&quot;</span><span class=p>:</span> <span class=n>kl_samplewise_restricted</span><span class=p>,</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1581><a id=__codelineno-0-1581 name=__codelineno-0-1581></a>        <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>:</span> <span class=n>kl_channelwise</span><span class=p>,</span>  <span class=c1># (batch, ch)</span>
</span><span id=__span-0-1582><a id=__codelineno-0-1582 name=__codelineno-0-1582></a>        <span class=s2>&quot;kl_spatial&quot;</span><span class=p>:</span> <span class=n>kl_spatial</span><span class=p>,</span>  <span class=c1># (batch, h, w)</span>
</span><span id=__span-0-1583><a id=__codelineno-0-1583 name=__codelineno-0-1583></a>    <span class=p>}</span>
</span><span id=__span-0-1584><a id=__codelineno-0-1584 name=__codelineno-0-1584></a>    <span class=k>return</span> <span class=n>kl_dict</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>q_params</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>forced_latent</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>use_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>force_constant_output</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>mode_pred</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>use_uncond_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.forward class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>q_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor resulting from merging the bu_value tensor at the same hierarchical level from the bottom-up pass and the <code>p_params</code> tensor. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>forced_latent</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>use_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Wheteher the latent tensor should be set as the latent distribution mode. In the case of Gaussian, the mode coincides with the mean of the distribution. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>force_constant_output</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>analytical_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the KL divergence analytically or using Monte Carlo estimation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>use_uncond_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1660>1660</a></span>
<span class=normal><a href=#__codelineno-0-1661>1661</a></span>
<span class=normal><a href=#__codelineno-0-1662>1662</a></span>
<span class=normal><a href=#__codelineno-0-1663>1663</a></span>
<span class=normal><a href=#__codelineno-0-1664>1664</a></span>
<span class=normal><a href=#__codelineno-0-1665>1665</a></span>
<span class=normal><a href=#__codelineno-0-1666>1666</a></span>
<span class=normal><a href=#__codelineno-0-1667>1667</a></span>
<span class=normal><a href=#__codelineno-0-1668>1668</a></span>
<span class=normal><a href=#__codelineno-0-1669>1669</a></span>
<span class=normal><a href=#__codelineno-0-1670>1670</a></span>
<span class=normal><a href=#__codelineno-0-1671>1671</a></span>
<span class=normal><a href=#__codelineno-0-1672>1672</a></span>
<span class=normal><a href=#__codelineno-0-1673>1673</a></span>
<span class=normal><a href=#__codelineno-0-1674>1674</a></span>
<span class=normal><a href=#__codelineno-0-1675>1675</a></span>
<span class=normal><a href=#__codelineno-0-1676>1676</a></span>
<span class=normal><a href=#__codelineno-0-1677>1677</a></span>
<span class=normal><a href=#__codelineno-0-1678>1678</a></span>
<span class=normal><a href=#__codelineno-0-1679>1679</a></span>
<span class=normal><a href=#__codelineno-0-1680>1680</a></span>
<span class=normal><a href=#__codelineno-0-1681>1681</a></span>
<span class=normal><a href=#__codelineno-0-1682>1682</a></span>
<span class=normal><a href=#__codelineno-0-1683>1683</a></span>
<span class=normal><a href=#__codelineno-0-1684>1684</a></span>
<span class=normal><a href=#__codelineno-0-1685>1685</a></span>
<span class=normal><a href=#__codelineno-0-1686>1686</a></span>
<span class=normal><a href=#__codelineno-0-1687>1687</a></span>
<span class=normal><a href=#__codelineno-0-1688>1688</a></span>
<span class=normal><a href=#__codelineno-0-1689>1689</a></span>
<span class=normal><a href=#__codelineno-0-1690>1690</a></span>
<span class=normal><a href=#__codelineno-0-1691>1691</a></span>
<span class=normal><a href=#__codelineno-0-1692>1692</a></span>
<span class=normal><a href=#__codelineno-0-1693>1693</a></span>
<span class=normal><a href=#__codelineno-0-1694>1694</a></span>
<span class=normal><a href=#__codelineno-0-1695>1695</a></span>
<span class=normal><a href=#__codelineno-0-1696>1696</a></span>
<span class=normal><a href=#__codelineno-0-1697>1697</a></span>
<span class=normal><a href=#__codelineno-0-1698>1698</a></span>
<span class=normal><a href=#__codelineno-0-1699>1699</a></span>
<span class=normal><a href=#__codelineno-0-1700>1700</a></span>
<span class=normal><a href=#__codelineno-0-1701>1701</a></span>
<span class=normal><a href=#__codelineno-0-1702>1702</a></span>
<span class=normal><a href=#__codelineno-0-1703>1703</a></span>
<span class=normal><a href=#__codelineno-0-1704>1704</a></span>
<span class=normal><a href=#__codelineno-0-1705>1705</a></span>
<span class=normal><a href=#__codelineno-0-1706>1706</a></span>
<span class=normal><a href=#__codelineno-0-1707>1707</a></span>
<span class=normal><a href=#__codelineno-0-1708>1708</a></span>
<span class=normal><a href=#__codelineno-0-1709>1709</a></span>
<span class=normal><a href=#__codelineno-0-1710>1710</a></span>
<span class=normal><a href=#__codelineno-0-1711>1711</a></span>
<span class=normal><a href=#__codelineno-0-1712>1712</a></span>
<span class=normal><a href=#__codelineno-0-1713>1713</a></span>
<span class=normal><a href=#__codelineno-0-1714>1714</a></span>
<span class=normal><a href=#__codelineno-0-1715>1715</a></span>
<span class=normal><a href=#__codelineno-0-1716>1716</a></span>
<span class=normal><a href=#__codelineno-0-1717>1717</a></span>
<span class=normal><a href=#__codelineno-0-1718>1718</a></span>
<span class=normal><a href=#__codelineno-0-1719>1719</a></span>
<span class=normal><a href=#__codelineno-0-1720>1720</a></span>
<span class=normal><a href=#__codelineno-0-1721>1721</a></span>
<span class=normal><a href=#__codelineno-0-1722>1722</a></span>
<span class=normal><a href=#__codelineno-0-1723>1723</a></span>
<span class=normal><a href=#__codelineno-0-1724>1724</a></span>
<span class=normal><a href=#__codelineno-0-1725>1725</a></span>
<span class=normal><a href=#__codelineno-0-1726>1726</a></span>
<span class=normal><a href=#__codelineno-0-1727>1727</a></span>
<span class=normal><a href=#__codelineno-0-1728>1728</a></span>
<span class=normal><a href=#__codelineno-0-1729>1729</a></span>
<span class=normal><a href=#__codelineno-0-1730>1730</a></span>
<span class=normal><a href=#__codelineno-0-1731>1731</a></span>
<span class=normal><a href=#__codelineno-0-1732>1732</a></span>
<span class=normal><a href=#__codelineno-0-1733>1733</a></span>
<span class=normal><a href=#__codelineno-0-1734>1734</a></span>
<span class=normal><a href=#__codelineno-0-1735>1735</a></span>
<span class=normal><a href=#__codelineno-0-1736>1736</a></span>
<span class=normal><a href=#__codelineno-0-1737>1737</a></span>
<span class=normal><a href=#__codelineno-0-1738>1738</a></span>
<span class=normal><a href=#__codelineno-0-1739>1739</a></span>
<span class=normal><a href=#__codelineno-0-1740>1740</a></span>
<span class=normal><a href=#__codelineno-0-1741>1741</a></span>
<span class=normal><a href=#__codelineno-0-1742>1742</a></span>
<span class=normal><a href=#__codelineno-0-1743>1743</a></span>
<span class=normal><a href=#__codelineno-0-1744>1744</a></span>
<span class=normal><a href=#__codelineno-0-1745>1745</a></span>
<span class=normal><a href=#__codelineno-0-1746>1746</a></span>
<span class=normal><a href=#__codelineno-0-1747>1747</a></span>
<span class=normal><a href=#__codelineno-0-1748>1748</a></span>
<span class=normal><a href=#__codelineno-0-1749>1749</a></span>
<span class=normal><a href=#__codelineno-0-1750>1750</a></span>
<span class=normal><a href=#__codelineno-0-1751>1751</a></span>
<span class=normal><a href=#__codelineno-0-1752>1752</a></span>
<span class=normal><a href=#__codelineno-0-1753>1753</a></span>
<span class=normal><a href=#__codelineno-0-1754>1754</a></span>
<span class=normal><a href=#__codelineno-0-1755>1755</a></span>
<span class=normal><a href=#__codelineno-0-1756>1756</a></span>
<span class=normal><a href=#__codelineno-0-1757>1757</a></span>
<span class=normal><a href=#__codelineno-0-1758>1758</a></span>
<span class=normal><a href=#__codelineno-0-1759>1759</a></span>
<span class=normal><a href=#__codelineno-0-1760>1760</a></span>
<span class=normal><a href=#__codelineno-0-1761>1761</a></span>
<span class=normal><a href=#__codelineno-0-1762>1762</a></span>
<span class=normal><a href=#__codelineno-0-1763>1763</a></span>
<span class=normal><a href=#__codelineno-0-1764>1764</a></span>
<span class=normal><a href=#__codelineno-0-1765>1765</a></span>
<span class=normal><a href=#__codelineno-0-1766>1766</a></span>
<span class=normal><a href=#__codelineno-0-1767>1767</a></span>
<span class=normal><a href=#__codelineno-0-1768>1768</a></span>
<span class=normal><a href=#__codelineno-0-1769>1769</a></span>
<span class=normal><a href=#__codelineno-0-1770>1770</a></span>
<span class=normal><a href=#__codelineno-0-1771>1771</a></span>
<span class=normal><a href=#__codelineno-0-1772>1772</a></span>
<span class=normal><a href=#__codelineno-0-1773>1773</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1660><a id=__codelineno-0-1660 name=__codelineno-0-1660></a><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1661><a id=__codelineno-0-1661 name=__codelineno-0-1661></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1662><a id=__codelineno-0-1662 name=__codelineno-0-1662></a>    <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1663><a id=__codelineno-0-1663 name=__codelineno-0-1663></a>    <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1664><a id=__codelineno-0-1664 name=__codelineno-0-1664></a>    <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1665><a id=__codelineno-0-1665 name=__codelineno-0-1665></a>    <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1666><a id=__codelineno-0-1666 name=__codelineno-0-1666></a>    <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1667><a id=__codelineno-0-1667 name=__codelineno-0-1667></a>    <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1668><a id=__codelineno-0-1668 name=__codelineno-0-1668></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1669><a id=__codelineno-0-1669 name=__codelineno-0-1669></a>    <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1670><a id=__codelineno-0-1670 name=__codelineno-0-1670></a>    <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1671><a id=__codelineno-0-1671 name=__codelineno-0-1671></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1672><a id=__codelineno-0-1672 name=__codelineno-0-1672></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1673><a id=__codelineno-0-1673 name=__codelineno-0-1673></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1674><a id=__codelineno-0-1674 name=__codelineno-0-1674></a><span class=sd>    ----------</span>
</span><span id=__span-0-1675><a id=__codelineno-0-1675 name=__codelineno-0-1675></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1676><a id=__codelineno-0-1676 name=__codelineno-0-1676></a><span class=sd>        The output tensor of the top-down layer above (i.e., mu_{p,i+1}, sigma_{p,i+1}).</span>
</span><span id=__span-0-1677><a id=__codelineno-0-1677 name=__codelineno-0-1677></a><span class=sd>    q_params: torch.Tensor, optional</span>
</span><span id=__span-0-1678><a id=__codelineno-0-1678 name=__codelineno-0-1678></a><span class=sd>        The tensor resulting from merging the bu_value tensor at the same hierarchical level</span>
</span><span id=__span-0-1679><a id=__codelineno-0-1679 name=__codelineno-0-1679></a><span class=sd>        from the bottom-up pass and the `p_params` tensor. Default is `None`.</span>
</span><span id=__span-0-1680><a id=__codelineno-0-1680 name=__codelineno-0-1680></a><span class=sd>    forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1681><a id=__codelineno-0-1681 name=__codelineno-0-1681></a><span class=sd>        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent</span>
</span><span id=__span-0-1682><a id=__codelineno-0-1682 name=__codelineno-0-1682></a><span class=sd>        tensor and, hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1683><a id=__codelineno-0-1683 name=__codelineno-0-1683></a><span class=sd>    use_mode: bool, optional</span>
</span><span id=__span-0-1684><a id=__codelineno-0-1684 name=__codelineno-0-1684></a><span class=sd>        Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1685><a id=__codelineno-0-1685 name=__codelineno-0-1685></a><span class=sd>        In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1686><a id=__codelineno-0-1686 name=__codelineno-0-1686></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1687><a id=__codelineno-0-1687 name=__codelineno-0-1687></a><span class=sd>    force_constant_output: bool, optional</span>
</span><span id=__span-0-1688><a id=__codelineno-0-1688 name=__codelineno-0-1688></a><span class=sd>        Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1689><a id=__codelineno-0-1689 name=__codelineno-0-1689></a><span class=sd>        This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1690><a id=__codelineno-0-1690 name=__codelineno-0-1690></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1691><a id=__codelineno-0-1691 name=__codelineno-0-1691></a><span class=sd>    analytical_kl: bool, optional</span>
</span><span id=__span-0-1692><a id=__codelineno-0-1692 name=__codelineno-0-1692></a><span class=sd>        Whether to compute the KL divergence analytically or using Monte Carlo estimation.</span>
</span><span id=__span-0-1693><a id=__codelineno-0-1693 name=__codelineno-0-1693></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1694><a id=__codelineno-0-1694 name=__codelineno-0-1694></a><span class=sd>    mode_pred: bool, optional</span>
</span><span id=__span-0-1695><a id=__codelineno-0-1695 name=__codelineno-0-1695></a><span class=sd>        Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1696><a id=__codelineno-0-1696 name=__codelineno-0-1696></a><span class=sd>    use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1697><a id=__codelineno-0-1697 name=__codelineno-0-1697></a><span class=sd>        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1698><a id=__codelineno-0-1698 name=__codelineno-0-1698></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1699><a id=__codelineno-0-1699 name=__codelineno-0-1699></a><span class=sd>    var_clip_max: float, optional</span>
</span><span id=__span-0-1700><a id=__codelineno-0-1700 name=__codelineno-0-1700></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1701><a id=__codelineno-0-1701 name=__codelineno-0-1701></a><span class=sd>        Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1702><a id=__codelineno-0-1702 name=__codelineno-0-1702></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1703><a id=__codelineno-0-1703 name=__codelineno-0-1703></a>    <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-1704><a id=__codelineno-0-1704 name=__codelineno-0-1704></a>
</span><span id=__span-0-1705><a id=__codelineno-0-1705 name=__codelineno-0-1705></a>    <span class=c1># Check sampling options consistency</span>
</span><span id=__span-0-1706><a id=__codelineno-0-1706 name=__codelineno-0-1706></a>    <span class=k>assert</span> <span class=p>(</span><span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>)</span> <span class=ow>or</span> <span class=p>(</span><span class=ow>not</span> <span class=n>use_mode</span><span class=p>)</span>
</span><span id=__span-0-1707><a id=__codelineno-0-1707 name=__codelineno-0-1707></a>
</span><span id=__span-0-1708><a id=__codelineno-0-1708 name=__codelineno-0-1708></a>    <span class=c1># Get generative distribution p(z_i|z_{i+1})</span>
</span><span id=__span-0-1709><a id=__codelineno-0-1709 name=__codelineno-0-1709></a>    <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>,</span> <span class=n>p</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_p_params</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1710><a id=__codelineno-0-1710 name=__codelineno-0-1710></a>    <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>)</span>
</span><span id=__span-0-1711><a id=__codelineno-0-1711 name=__codelineno-0-1711></a>
</span><span id=__span-0-1712><a id=__codelineno-0-1712 name=__codelineno-0-1712></a>    <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1713><a id=__codelineno-0-1713 name=__codelineno-0-1713></a>        <span class=c1># Get inference distribution q(z_i|z_{i+1})</span>
</span><span id=__span-0-1714><a id=__codelineno-0-1714 name=__codelineno-0-1714></a>        <span class=c1># NOTE: At inference time, don&#39;t centercrop the q_params even if they are odd in size.</span>
</span><span id=__span-0-1715><a id=__codelineno-0-1715 name=__codelineno-0-1715></a>        <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>,</span> <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1716><a id=__codelineno-0-1716 name=__codelineno-0-1716></a>            <span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1717><a id=__codelineno-0-1717 name=__codelineno-0-1717></a>        <span class=p>)</span>
</span><span id=__span-0-1718><a id=__codelineno-0-1718 name=__codelineno-0-1718></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>)</span>
</span><span id=__span-0-1719><a id=__codelineno-0-1719 name=__codelineno-0-1719></a>        <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>q</span>
</span><span id=__span-0-1720><a id=__codelineno-0-1720 name=__codelineno-0-1720></a>        <span class=n>debug_qvar_max</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>q_lv</span><span class=o>.</span><span class=n>get</span><span class=p>())</span>
</span><span id=__span-0-1721><a id=__codelineno-0-1721 name=__codelineno-0-1721></a>
</span><span id=__span-0-1722><a id=__codelineno-0-1722 name=__codelineno-0-1722></a>        <span class=c1># Centercrop p_params so that their size matches the one of q_params</span>
</span><span id=__span-0-1723><a id=__codelineno-0-1723 name=__codelineno-0-1723></a>        <span class=n>q_size</span> <span class=o>=</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>get</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1724><a id=__codelineno-0-1724 name=__codelineno-0-1724></a>        <span class=k>if</span> <span class=n>p_mu</span><span class=o>.</span><span class=n>get</span><span class=p>()</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>!=</span> <span class=n>q_size</span> <span class=ow>and</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1725><a id=__codelineno-0-1725 name=__codelineno-0-1725></a>            <span class=n>p_mu</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1726><a id=__codelineno-0-1726 name=__codelineno-0-1726></a>            <span class=n>p_lv</span><span class=o>.</span><span class=n>centercrop_to_size</span><span class=p>(</span><span class=n>q_size</span><span class=p>)</span>
</span><span id=__span-0-1727><a id=__codelineno-0-1727 name=__codelineno-0-1727></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1728><a id=__codelineno-0-1728 name=__codelineno-0-1728></a>        <span class=n>sampling_distrib</span> <span class=o>=</span> <span class=n>p</span>
</span><span id=__span-0-1729><a id=__codelineno-0-1729 name=__codelineno-0-1729></a>
</span><span id=__span-0-1730><a id=__codelineno-0-1730 name=__codelineno-0-1730></a>    <span class=c1># Sample latent variable</span>
</span><span id=__span-0-1731><a id=__codelineno-0-1731 name=__codelineno-0-1731></a>    <span class=n>z</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_z</span><span class=p>(</span>
</span><span id=__span-0-1732><a id=__codelineno-0-1732 name=__codelineno-0-1732></a>        <span class=n>sampling_distrib</span><span class=p>,</span> <span class=n>forced_latent</span><span class=p>,</span> <span class=n>use_mode</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>use_uncond_mode</span>
</span><span id=__span-0-1733><a id=__codelineno-0-1733 name=__codelineno-0-1733></a>    <span class=p>)</span>
</span><span id=__span-0-1734><a id=__codelineno-0-1734 name=__codelineno-0-1734></a>
</span><span id=__span-0-1735><a id=__codelineno-0-1735 name=__codelineno-0-1735></a>    <span class=c1># Copy one sample (and distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1736><a id=__codelineno-0-1736 name=__codelineno-0-1736></a>    <span class=c1># This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1737><a id=__codelineno-0-1737 name=__codelineno-0-1737></a>    <span class=k>if</span> <span class=n>force_constant_output</span><span class=p>:</span>
</span><span id=__span-0-1738><a id=__codelineno-0-1738 name=__codelineno-0-1738></a>        <span class=n>z</span> <span class=o>=</span> <span class=n>z</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span><span id=__span-0-1739><a id=__codelineno-0-1739 name=__codelineno-0-1739></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1740><a id=__codelineno-0-1740 name=__codelineno-0-1740></a>            <span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1741><a id=__codelineno-0-1741 name=__codelineno-0-1741></a>            <span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>:</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>p_params</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span><span class=o>.</span><span class=n>clone</span><span class=p>(),</span>
</span><span id=__span-0-1742><a id=__codelineno-0-1742 name=__codelineno-0-1742></a>        <span class=p>)</span>
</span><span id=__span-0-1743><a id=__codelineno-0-1743 name=__codelineno-0-1743></a>
</span><span id=__span-0-1744><a id=__codelineno-0-1744 name=__codelineno-0-1744></a>    <span class=c1># Pass the sampled latent througn the output convolutional layer of stochastic block</span>
</span><span id=__span-0-1745><a id=__codelineno-0-1745 name=__codelineno-0-1745></a>    <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_out</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span><span id=__span-0-1746><a id=__codelineno-0-1746 name=__codelineno-0-1746></a>
</span><span id=__span-0-1747><a id=__codelineno-0-1747 name=__codelineno-0-1747></a>    <span class=c1># Compute log p(z)# NOTE: disabling its computation.</span>
</span><span id=__span-0-1748><a id=__codelineno-0-1748 name=__codelineno-0-1748></a>    <span class=c1># if mode_pred is False:</span>
</span><span id=__span-0-1749><a id=__codelineno-0-1749 name=__codelineno-0-1749></a>    <span class=c1>#     logprob_p =  p.log_prob(z).sum((1, 2, 3))</span>
</span><span id=__span-0-1750><a id=__codelineno-0-1750 name=__codelineno-0-1750></a>    <span class=c1># else:</span>
</span><span id=__span-0-1751><a id=__codelineno-0-1751 name=__codelineno-0-1751></a>    <span class=c1>#     logprob_p = None</span>
</span><span id=__span-0-1752><a id=__codelineno-0-1752 name=__codelineno-0-1752></a>
</span><span id=__span-0-1753><a id=__codelineno-0-1753 name=__codelineno-0-1753></a>    <span class=k>if</span> <span class=n>q_params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1754><a id=__codelineno-0-1754 name=__codelineno-0-1754></a>        <span class=c1># Compute log q(z)</span>
</span><span id=__span-0-1755><a id=__codelineno-0-1755 name=__codelineno-0-1755></a>        <span class=n>logprob_q</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>log_prob</span><span class=p>(</span><span class=n>z</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span><span id=__span-0-1756><a id=__codelineno-0-1756 name=__codelineno-0-1756></a>        <span class=c1># Compute KL divergence metrics</span>
</span><span id=__span-0-1757><a id=__codelineno-0-1757 name=__codelineno-0-1757></a>        <span class=n>kl_dict</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>compute_kl_metrics</span><span class=p>(</span>
</span><span id=__span-0-1758><a id=__codelineno-0-1758 name=__codelineno-0-1758></a>            <span class=n>p</span><span class=p>,</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>q</span><span class=p>,</span> <span class=n>q_params</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=p>,</span> <span class=n>z</span>
</span><span id=__span-0-1759><a id=__codelineno-0-1759 name=__codelineno-0-1759></a>        <span class=p>)</span>
</span><span id=__span-0-1760><a id=__codelineno-0-1760 name=__codelineno-0-1760></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1761><a id=__codelineno-0-1761 name=__codelineno-0-1761></a>        <span class=n>kl_dict</span> <span class=o>=</span> <span class=p>{}</span>
</span><span id=__span-0-1762><a id=__codelineno-0-1762 name=__codelineno-0-1762></a>        <span class=n>logprob_q</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1763><a id=__codelineno-0-1763 name=__codelineno-0-1763></a>
</span><span id=__span-0-1764><a id=__codelineno-0-1764 name=__codelineno-0-1764></a>    <span class=c1># Store meaningful quantities to use them in following layers</span>
</span><span id=__span-0-1765><a id=__codelineno-0-1765 name=__codelineno-0-1765></a>    <span class=n>data</span> <span class=o>=</span> <span class=n>kl_dict</span>
</span><span id=__span-0-1766><a id=__codelineno-0-1766 name=__codelineno-0-1766></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;z&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>z</span>  <span class=c1># sampled variable at this layer (batch, ch, h, w)</span>
</span><span id=__span-0-1767><a id=__codelineno-0-1767 name=__codelineno-0-1767></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;p_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>p_params</span>  <span class=c1># (b, ch, h, w) where b is 1 or batch size</span>
</span><span id=__span-0-1768><a id=__codelineno-0-1768 name=__codelineno-0-1768></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_params</span>  <span class=c1># (batch, ch, h, w)</span>
</span><span id=__span-0-1769><a id=__codelineno-0-1769 name=__codelineno-0-1769></a>    <span class=c1># data[&#39;logprob_p&#39;] = logprob_p  # (batch, )</span>
</span><span id=__span-0-1770><a id=__codelineno-0-1770 name=__codelineno-0-1770></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;logprob_q&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>logprob_q</span>  <span class=c1># (batch, )</span>
</span><span id=__span-0-1771><a id=__codelineno-0-1771 name=__codelineno-0-1771></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;qvar_max&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>debug_qvar_max</span>
</span><span id=__span-0-1772><a id=__codelineno-0-1772 name=__codelineno-0-1772></a>
</span><span id=__span-0-1773><a id=__codelineno-0-1773 name=__codelineno-0-1773></a>    <span class=k>return</span> <span class=n>out</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.get_z class="doc doc-heading"> <code class="highlight language-python"><span class=n>get_z</span><span class=p>(</span><span class=n>sampling_distrib</span><span class=p>,</span> <span class=n>forced_latent</span><span class=p>,</span> <span class=n>use_mode</span><span class=p>,</span> <span class=n>mode_pred</span><span class=p>,</span> <span class=n>use_uncond_mode</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.get_z class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>This method enables to sample a latent tensor given the distribution to sample from.</p> <p>Latent variable can be obtained is several ways: - Sampled from the (Gaussian) latent distribution. - Taken as a pre-defined forced latent. - Taken as the mode (mean) of the latent distribution. - In prediction mode (<code>mode_pred==True</code>), can be either sample or taken as the distribution mode.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>sampling_distrib</code></td> <td> <code><span title="torch.distributions.normal.Normal">Normal</span></code> </td> <td> <div class=doc-md-description> <p>The Gaussian distribution from which latent tensor is sampled.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>forced_latent</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>use_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Wheteher the latent tensor should be set as the latent distribution mode. In the case of Gaussian, the mode coincides with the mean of the distribution.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is prediction mode.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>use_uncond_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1444>1444</a></span>
<span class=normal><a href=#__codelineno-0-1445>1445</a></span>
<span class=normal><a href=#__codelineno-0-1446>1446</a></span>
<span class=normal><a href=#__codelineno-0-1447>1447</a></span>
<span class=normal><a href=#__codelineno-0-1448>1448</a></span>
<span class=normal><a href=#__codelineno-0-1449>1449</a></span>
<span class=normal><a href=#__codelineno-0-1450>1450</a></span>
<span class=normal><a href=#__codelineno-0-1451>1451</a></span>
<span class=normal><a href=#__codelineno-0-1452>1452</a></span>
<span class=normal><a href=#__codelineno-0-1453>1453</a></span>
<span class=normal><a href=#__codelineno-0-1454>1454</a></span>
<span class=normal><a href=#__codelineno-0-1455>1455</a></span>
<span class=normal><a href=#__codelineno-0-1456>1456</a></span>
<span class=normal><a href=#__codelineno-0-1457>1457</a></span>
<span class=normal><a href=#__codelineno-0-1458>1458</a></span>
<span class=normal><a href=#__codelineno-0-1459>1459</a></span>
<span class=normal><a href=#__codelineno-0-1460>1460</a></span>
<span class=normal><a href=#__codelineno-0-1461>1461</a></span>
<span class=normal><a href=#__codelineno-0-1462>1462</a></span>
<span class=normal><a href=#__codelineno-0-1463>1463</a></span>
<span class=normal><a href=#__codelineno-0-1464>1464</a></span>
<span class=normal><a href=#__codelineno-0-1465>1465</a></span>
<span class=normal><a href=#__codelineno-0-1466>1466</a></span>
<span class=normal><a href=#__codelineno-0-1467>1467</a></span>
<span class=normal><a href=#__codelineno-0-1468>1468</a></span>
<span class=normal><a href=#__codelineno-0-1469>1469</a></span>
<span class=normal><a href=#__codelineno-0-1470>1470</a></span>
<span class=normal><a href=#__codelineno-0-1471>1471</a></span>
<span class=normal><a href=#__codelineno-0-1472>1472</a></span>
<span class=normal><a href=#__codelineno-0-1473>1473</a></span>
<span class=normal><a href=#__codelineno-0-1474>1474</a></span>
<span class=normal><a href=#__codelineno-0-1475>1475</a></span>
<span class=normal><a href=#__codelineno-0-1476>1476</a></span>
<span class=normal><a href=#__codelineno-0-1477>1477</a></span>
<span class=normal><a href=#__codelineno-0-1478>1478</a></span>
<span class=normal><a href=#__codelineno-0-1479>1479</a></span>
<span class=normal><a href=#__codelineno-0-1480>1480</a></span>
<span class=normal><a href=#__codelineno-0-1481>1481</a></span>
<span class=normal><a href=#__codelineno-0-1482>1482</a></span>
<span class=normal><a href=#__codelineno-0-1483>1483</a></span>
<span class=normal><a href=#__codelineno-0-1484>1484</a></span>
<span class=normal><a href=#__codelineno-0-1485>1485</a></span>
<span class=normal><a href=#__codelineno-0-1486>1486</a></span>
<span class=normal><a href=#__codelineno-0-1487>1487</a></span>
<span class=normal><a href=#__codelineno-0-1488>1488</a></span>
<span class=normal><a href=#__codelineno-0-1489>1489</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1444><a id=__codelineno-0-1444 name=__codelineno-0-1444></a><span class=k>def</span> <span class=nf>get_z</span><span class=p>(</span>
</span><span id=__span-0-1445><a id=__codelineno-0-1445 name=__codelineno-0-1445></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1446><a id=__codelineno-0-1446 name=__codelineno-0-1446></a>    <span class=n>sampling_distrib</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>,</span>
</span><span id=__span-0-1447><a id=__codelineno-0-1447 name=__codelineno-0-1447></a>    <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1448><a id=__codelineno-0-1448 name=__codelineno-0-1448></a>    <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1449><a id=__codelineno-0-1449 name=__codelineno-0-1449></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1450><a id=__codelineno-0-1450 name=__codelineno-0-1450></a>    <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-1451><a id=__codelineno-0-1451 name=__codelineno-0-1451></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1452><a id=__codelineno-0-1452 name=__codelineno-0-1452></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1453><a id=__codelineno-0-1453 name=__codelineno-0-1453></a><span class=sd>    This method enables to sample a latent tensor given the distribution to sample from.</span>
</span><span id=__span-0-1454><a id=__codelineno-0-1454 name=__codelineno-0-1454></a>
</span><span id=__span-0-1455><a id=__codelineno-0-1455 name=__codelineno-0-1455></a><span class=sd>    Latent variable can be obtained is several ways:</span>
</span><span id=__span-0-1456><a id=__codelineno-0-1456 name=__codelineno-0-1456></a><span class=sd>        - Sampled from the (Gaussian) latent distribution.</span>
</span><span id=__span-0-1457><a id=__codelineno-0-1457 name=__codelineno-0-1457></a><span class=sd>        - Taken as a pre-defined forced latent.</span>
</span><span id=__span-0-1458><a id=__codelineno-0-1458 name=__codelineno-0-1458></a><span class=sd>        - Taken as the mode (mean) of the latent distribution.</span>
</span><span id=__span-0-1459><a id=__codelineno-0-1459 name=__codelineno-0-1459></a><span class=sd>        - In prediction mode (`mode_pred==True`), can be either sample or taken as the distribution mode.</span>
</span><span id=__span-0-1460><a id=__codelineno-0-1460 name=__codelineno-0-1460></a>
</span><span id=__span-0-1461><a id=__codelineno-0-1461 name=__codelineno-0-1461></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1462><a id=__codelineno-0-1462 name=__codelineno-0-1462></a><span class=sd>    ----------</span>
</span><span id=__span-0-1463><a id=__codelineno-0-1463 name=__codelineno-0-1463></a><span class=sd>    sampling_distrib: torch.distributions.normal.Normal</span>
</span><span id=__span-0-1464><a id=__codelineno-0-1464 name=__codelineno-0-1464></a><span class=sd>        The Gaussian distribution from which latent tensor is sampled.</span>
</span><span id=__span-0-1465><a id=__codelineno-0-1465 name=__codelineno-0-1465></a><span class=sd>    forced_latent: torch.Tensor</span>
</span><span id=__span-0-1466><a id=__codelineno-0-1466 name=__codelineno-0-1466></a><span class=sd>        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,</span>
</span><span id=__span-0-1467><a id=__codelineno-0-1467 name=__codelineno-0-1467></a><span class=sd>        hence, sampling does not happen.</span>
</span><span id=__span-0-1468><a id=__codelineno-0-1468 name=__codelineno-0-1468></a><span class=sd>    use_mode: bool</span>
</span><span id=__span-0-1469><a id=__codelineno-0-1469 name=__codelineno-0-1469></a><span class=sd>        Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1470><a id=__codelineno-0-1470 name=__codelineno-0-1470></a><span class=sd>        In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1471><a id=__codelineno-0-1471 name=__codelineno-0-1471></a><span class=sd>    mode_pred: bool</span>
</span><span id=__span-0-1472><a id=__codelineno-0-1472 name=__codelineno-0-1472></a><span class=sd>        Whether the model is prediction mode.</span>
</span><span id=__span-0-1473><a id=__codelineno-0-1473 name=__codelineno-0-1473></a><span class=sd>    use_uncond_mode: bool</span>
</span><span id=__span-0-1474><a id=__codelineno-0-1474 name=__codelineno-0-1474></a><span class=sd>        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1475><a id=__codelineno-0-1475 name=__codelineno-0-1475></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1476><a id=__codelineno-0-1476 name=__codelineno-0-1476></a>    <span class=k>if</span> <span class=n>forced_latent</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1477><a id=__codelineno-0-1477 name=__codelineno-0-1477></a>        <span class=k>if</span> <span class=n>use_mode</span><span class=p>:</span>
</span><span id=__span-0-1478><a id=__codelineno-0-1478 name=__codelineno-0-1478></a>            <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>mean</span>
</span><span id=__span-0-1479><a id=__codelineno-0-1479 name=__codelineno-0-1479></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1480><a id=__codelineno-0-1480 name=__codelineno-0-1480></a>            <span class=k>if</span> <span class=n>mode_pred</span><span class=p>:</span>
</span><span id=__span-0-1481><a id=__codelineno-0-1481 name=__codelineno-0-1481></a>                <span class=k>if</span> <span class=n>use_uncond_mode</span><span class=p>:</span>
</span><span id=__span-0-1482><a id=__codelineno-0-1482 name=__codelineno-0-1482></a>                    <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>mean</span>
</span><span id=__span-0-1483><a id=__codelineno-0-1483 name=__codelineno-0-1483></a>                <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1484><a id=__codelineno-0-1484 name=__codelineno-0-1484></a>                    <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span><span id=__span-0-1485><a id=__codelineno-0-1485 name=__codelineno-0-1485></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1486><a id=__codelineno-0-1486 name=__codelineno-0-1486></a>                <span class=n>z</span> <span class=o>=</span> <span class=n>sampling_distrib</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span><span id=__span-0-1487><a id=__codelineno-0-1487 name=__codelineno-0-1487></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1488><a id=__codelineno-0-1488 name=__codelineno-0-1488></a>        <span class=n>z</span> <span class=o>=</span> <span class=n>forced_latent</span>
</span><span id=__span-0-1489><a id=__codelineno-0-1489 name=__codelineno-0-1489></a>    <span class=k>return</span> <span class=n>z</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.process_p_params class="doc doc-heading"> <code class="highlight language-python"><span class=n>process_p_params</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_p_params class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).</p> <p>Processing consists in: - (optionally) 2D convolution on the input tensor to increase number of channels. - split the resulting tensor into two chunks, the mean and the log-variance. - (optionally) clip the log-variance to an upper threshold. - define the normal distribution p(z) given the parameter tensors above.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input tensor to be processed.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1586>1586</a></span>
<span class=normal><a href=#__codelineno-0-1587>1587</a></span>
<span class=normal><a href=#__codelineno-0-1588>1588</a></span>
<span class=normal><a href=#__codelineno-0-1589>1589</a></span>
<span class=normal><a href=#__codelineno-0-1590>1590</a></span>
<span class=normal><a href=#__codelineno-0-1591>1591</a></span>
<span class=normal><a href=#__codelineno-0-1592>1592</a></span>
<span class=normal><a href=#__codelineno-0-1593>1593</a></span>
<span class=normal><a href=#__codelineno-0-1594>1594</a></span>
<span class=normal><a href=#__codelineno-0-1595>1595</a></span>
<span class=normal><a href=#__codelineno-0-1596>1596</a></span>
<span class=normal><a href=#__codelineno-0-1597>1597</a></span>
<span class=normal><a href=#__codelineno-0-1598>1598</a></span>
<span class=normal><a href=#__codelineno-0-1599>1599</a></span>
<span class=normal><a href=#__codelineno-0-1600>1600</a></span>
<span class=normal><a href=#__codelineno-0-1601>1601</a></span>
<span class=normal><a href=#__codelineno-0-1602>1602</a></span>
<span class=normal><a href=#__codelineno-0-1603>1603</a></span>
<span class=normal><a href=#__codelineno-0-1604>1604</a></span>
<span class=normal><a href=#__codelineno-0-1605>1605</a></span>
<span class=normal><a href=#__codelineno-0-1606>1606</a></span>
<span class=normal><a href=#__codelineno-0-1607>1607</a></span>
<span class=normal><a href=#__codelineno-0-1608>1608</a></span>
<span class=normal><a href=#__codelineno-0-1609>1609</a></span>
<span class=normal><a href=#__codelineno-0-1610>1610</a></span>
<span class=normal><a href=#__codelineno-0-1611>1611</a></span>
<span class=normal><a href=#__codelineno-0-1612>1612</a></span>
<span class=normal><a href=#__codelineno-0-1613>1613</a></span>
<span class=normal><a href=#__codelineno-0-1614>1614</a></span>
<span class=normal><a href=#__codelineno-0-1615>1615</a></span>
<span class=normal><a href=#__codelineno-0-1616>1616</a></span>
<span class=normal><a href=#__codelineno-0-1617>1617</a></span>
<span class=normal><a href=#__codelineno-0-1618>1618</a></span>
<span class=normal><a href=#__codelineno-0-1619>1619</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1586><a id=__codelineno-0-1586 name=__codelineno-0-1586></a><span class=k>def</span> <span class=nf>process_p_params</span><span class=p>(</span>
</span><span id=__span-0-1587><a id=__codelineno-0-1587 name=__codelineno-0-1587></a>    <span class=bp>self</span><span class=p>,</span> <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span>
</span><span id=__span-0-1588><a id=__codelineno-0-1588 name=__codelineno-0-1588></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>]:</span>
</span><span id=__span-0-1589><a id=__codelineno-0-1589 name=__codelineno-0-1589></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1590><a id=__codelineno-0-1590 name=__codelineno-0-1590></a><span class=sd>    Process the input parameters to get the prior distribution p(z_i|z_{i+1}) (or p(z_L)).</span>
</span><span id=__span-0-1591><a id=__codelineno-0-1591 name=__codelineno-0-1591></a>
</span><span id=__span-0-1592><a id=__codelineno-0-1592 name=__codelineno-0-1592></a><span class=sd>    Processing consists in:</span>
</span><span id=__span-0-1593><a id=__codelineno-0-1593 name=__codelineno-0-1593></a><span class=sd>        - (optionally) 2D convolution on the input tensor to increase number of channels.</span>
</span><span id=__span-0-1594><a id=__codelineno-0-1594 name=__codelineno-0-1594></a><span class=sd>        - split the resulting tensor into two chunks, the mean and the log-variance.</span>
</span><span id=__span-0-1595><a id=__codelineno-0-1595 name=__codelineno-0-1595></a><span class=sd>        - (optionally) clip the log-variance to an upper threshold.</span>
</span><span id=__span-0-1596><a id=__codelineno-0-1596 name=__codelineno-0-1596></a><span class=sd>        - define the normal distribution p(z) given the parameter tensors above.</span>
</span><span id=__span-0-1597><a id=__codelineno-0-1597 name=__codelineno-0-1597></a>
</span><span id=__span-0-1598><a id=__codelineno-0-1598 name=__codelineno-0-1598></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1599><a id=__codelineno-0-1599 name=__codelineno-0-1599></a><span class=sd>    ----------</span>
</span><span id=__span-0-1600><a id=__codelineno-0-1600 name=__codelineno-0-1600></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1601><a id=__codelineno-0-1601 name=__codelineno-0-1601></a><span class=sd>        The input tensor to be processed.</span>
</span><span id=__span-0-1602><a id=__codelineno-0-1602 name=__codelineno-0-1602></a><span class=sd>    var_clip_max: float</span>
</span><span id=__span-0-1603><a id=__codelineno-0-1603 name=__codelineno-0-1603></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1604><a id=__codelineno-0-1604 name=__codelineno-0-1604></a><span class=sd>        Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1605><a id=__codelineno-0-1605 name=__codelineno-0-1605></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1606><a id=__codelineno-0-1606 name=__codelineno-0-1606></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>transform_p_params</span><span class=p>:</span>
</span><span id=__span-0-1607><a id=__codelineno-0-1607 name=__codelineno-0-1607></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_p</span><span class=p>(</span><span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1608><a id=__codelineno-0-1608 name=__codelineno-0-1608></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1609><a id=__codelineno-0-1609 name=__codelineno-0-1609></a>        <span class=k>assert</span> <span class=n>p_params</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=mi>2</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_vars</span>
</span><span id=__span-0-1610><a id=__codelineno-0-1610 name=__codelineno-0-1610></a>
</span><span id=__span-0-1611><a id=__codelineno-0-1611 name=__codelineno-0-1611></a>    <span class=c1># Define p(z)</span>
</span><span id=__span-0-1612><a id=__codelineno-0-1612 name=__codelineno-0-1612></a>    <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span> <span class=o>=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1613><a id=__codelineno-0-1613 name=__codelineno-0-1613></a>    <span class=k>if</span> <span class=n>var_clip_max</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1614><a id=__codelineno-0-1614 name=__codelineno-0-1614></a>        <span class=n>p_lv</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>p_lv</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1615><a id=__codelineno-0-1615 name=__codelineno-0-1615></a>
</span><span id=__span-0-1616><a id=__codelineno-0-1616 name=__codelineno-0-1616></a>    <span class=n>p_mu</span> <span class=o>=</span> <span class=n>StableMean</span><span class=p>(</span><span class=n>p_mu</span><span class=p>)</span>
</span><span id=__span-0-1617><a id=__codelineno-0-1617 name=__codelineno-0-1617></a>    <span class=n>p_lv</span> <span class=o>=</span> <span class=n>StableLogVar</span><span class=p>(</span><span class=n>p_lv</span><span class=p>,</span> <span class=n>enable_stable</span><span class=o>=</span><span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span><span class=p>)</span>
</span><span id=__span-0-1618><a id=__codelineno-0-1618 name=__codelineno-0-1618></a>    <span class=n>p</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=n>p_mu</span><span class=o>.</span><span class=n>get</span><span class=p>(),</span> <span class=n>p_lv</span><span class=o>.</span><span class=n>get_std</span><span class=p>())</span>
</span><span id=__span-0-1619><a id=__codelineno-0-1619 name=__codelineno-0-1619></a>    <span class=k>return</span> <span class=n>p_mu</span><span class=p>,</span> <span class=n>p_lv</span><span class=p>,</span> <span class=n>p</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.process_q_params class="doc doc-heading"> <code class="highlight language-python"><span class=n>process_q_params</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.process_q_params class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).</p> <p>Processing consists in: - 2D convolution on the input tensor to increase number of channels. - split the resulting tensor into two chunks, the mean and the log-variance. - (optionally) clip the log-variance to an upper threshold. - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even. - define the normal distribution q(z) given the parameter tensors above.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p_params</code></td> <td> </td> <td> <div class=doc-md-description> <p>The input tensor to be processed.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1621>1621</a></span>
<span class=normal><a href=#__codelineno-0-1622>1622</a></span>
<span class=normal><a href=#__codelineno-0-1623>1623</a></span>
<span class=normal><a href=#__codelineno-0-1624>1624</a></span>
<span class=normal><a href=#__codelineno-0-1625>1625</a></span>
<span class=normal><a href=#__codelineno-0-1626>1626</a></span>
<span class=normal><a href=#__codelineno-0-1627>1627</a></span>
<span class=normal><a href=#__codelineno-0-1628>1628</a></span>
<span class=normal><a href=#__codelineno-0-1629>1629</a></span>
<span class=normal><a href=#__codelineno-0-1630>1630</a></span>
<span class=normal><a href=#__codelineno-0-1631>1631</a></span>
<span class=normal><a href=#__codelineno-0-1632>1632</a></span>
<span class=normal><a href=#__codelineno-0-1633>1633</a></span>
<span class=normal><a href=#__codelineno-0-1634>1634</a></span>
<span class=normal><a href=#__codelineno-0-1635>1635</a></span>
<span class=normal><a href=#__codelineno-0-1636>1636</a></span>
<span class=normal><a href=#__codelineno-0-1637>1637</a></span>
<span class=normal><a href=#__codelineno-0-1638>1638</a></span>
<span class=normal><a href=#__codelineno-0-1639>1639</a></span>
<span class=normal><a href=#__codelineno-0-1640>1640</a></span>
<span class=normal><a href=#__codelineno-0-1641>1641</a></span>
<span class=normal><a href=#__codelineno-0-1642>1642</a></span>
<span class=normal><a href=#__codelineno-0-1643>1643</a></span>
<span class=normal><a href=#__codelineno-0-1644>1644</a></span>
<span class=normal><a href=#__codelineno-0-1645>1645</a></span>
<span class=normal><a href=#__codelineno-0-1646>1646</a></span>
<span class=normal><a href=#__codelineno-0-1647>1647</a></span>
<span class=normal><a href=#__codelineno-0-1648>1648</a></span>
<span class=normal><a href=#__codelineno-0-1649>1649</a></span>
<span class=normal><a href=#__codelineno-0-1650>1650</a></span>
<span class=normal><a href=#__codelineno-0-1651>1651</a></span>
<span class=normal><a href=#__codelineno-0-1652>1652</a></span>
<span class=normal><a href=#__codelineno-0-1653>1653</a></span>
<span class=normal><a href=#__codelineno-0-1654>1654</a></span>
<span class=normal><a href=#__codelineno-0-1655>1655</a></span>
<span class=normal><a href=#__codelineno-0-1656>1656</a></span>
<span class=normal><a href=#__codelineno-0-1657>1657</a></span>
<span class=normal><a href=#__codelineno-0-1658>1658</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1621><a id=__codelineno-0-1621 name=__codelineno-0-1621></a><span class=k>def</span> <span class=nf>process_q_params</span><span class=p>(</span>
</span><span id=__span-0-1622><a id=__codelineno-0-1622 name=__codelineno-0-1622></a>    <span class=bp>self</span><span class=p>,</span> <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>allow_oddsizes</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-1623><a id=__codelineno-0-1623 name=__codelineno-0-1623></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>distributions</span><span class=o>.</span><span class=n>normal</span><span class=o>.</span><span class=n>Normal</span><span class=p>]:</span>
</span><span id=__span-0-1624><a id=__codelineno-0-1624 name=__codelineno-0-1624></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1625><a id=__codelineno-0-1625 name=__codelineno-0-1625></a><span class=sd>    Process the input parameters to get the inference distribution q(z_i|z_{i+1}) (or q(z|x)).</span>
</span><span id=__span-0-1626><a id=__codelineno-0-1626 name=__codelineno-0-1626></a>
</span><span id=__span-0-1627><a id=__codelineno-0-1627 name=__codelineno-0-1627></a><span class=sd>    Processing consists in:</span>
</span><span id=__span-0-1628><a id=__codelineno-0-1628 name=__codelineno-0-1628></a><span class=sd>        - 2D convolution on the input tensor to increase number of channels.</span>
</span><span id=__span-0-1629><a id=__codelineno-0-1629 name=__codelineno-0-1629></a><span class=sd>        - split the resulting tensor into two chunks, the mean and the log-variance.</span>
</span><span id=__span-0-1630><a id=__codelineno-0-1630 name=__codelineno-0-1630></a><span class=sd>        - (optionally) clip the log-variance to an upper threshold.</span>
</span><span id=__span-0-1631><a id=__codelineno-0-1631 name=__codelineno-0-1631></a><span class=sd>        - (optionally) crop the resulting tensors to ensure that the last spatial dimension is even.</span>
</span><span id=__span-0-1632><a id=__codelineno-0-1632 name=__codelineno-0-1632></a><span class=sd>        - define the normal distribution q(z) given the parameter tensors above.</span>
</span><span id=__span-0-1633><a id=__codelineno-0-1633 name=__codelineno-0-1633></a>
</span><span id=__span-0-1634><a id=__codelineno-0-1634 name=__codelineno-0-1634></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1635><a id=__codelineno-0-1635 name=__codelineno-0-1635></a><span class=sd>    ----------</span>
</span><span id=__span-0-1636><a id=__codelineno-0-1636 name=__codelineno-0-1636></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1637><a id=__codelineno-0-1637 name=__codelineno-0-1637></a><span class=sd>        The input tensor to be processed.</span>
</span><span id=__span-0-1638><a id=__codelineno-0-1638 name=__codelineno-0-1638></a><span class=sd>    var_clip_max: float</span>
</span><span id=__span-0-1639><a id=__codelineno-0-1639 name=__codelineno-0-1639></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1640><a id=__codelineno-0-1640 name=__codelineno-0-1640></a><span class=sd>        Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1641><a id=__codelineno-0-1641 name=__codelineno-0-1641></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1642><a id=__codelineno-0-1642 name=__codelineno-0-1642></a>    <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>conv_in_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>)</span>
</span><span id=__span-0-1643><a id=__codelineno-0-1643 name=__codelineno-0-1643></a>
</span><span id=__span-0-1644><a id=__codelineno-0-1644 name=__codelineno-0-1644></a>    <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span> <span class=o>=</span> <span class=n>q_params</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1645><a id=__codelineno-0-1645 name=__codelineno-0-1645></a>    <span class=k>if</span> <span class=n>var_clip_max</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1646><a id=__codelineno-0-1646 name=__codelineno-0-1646></a>        <span class=n>q_lv</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clip</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1647><a id=__codelineno-0-1647 name=__codelineno-0-1647></a>
</span><span id=__span-0-1648><a id=__codelineno-0-1648 name=__codelineno-0-1648></a>    <span class=k>if</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>and</span> <span class=n>allow_oddsizes</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1649><a id=__codelineno-0-1649 name=__codelineno-0-1649></a>        <span class=n>q_mu</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>q_mu</span><span class=p>,</span> <span class=n>q_mu</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1650><a id=__codelineno-0-1650 name=__codelineno-0-1650></a>        <span class=n>q_lv</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=n>q_lv</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1651><a id=__codelineno-0-1651 name=__codelineno-0-1651></a>        <span class=c1># clip_start = np.random.rand() &gt; 0.5</span>
</span><span id=__span-0-1652><a id=__codelineno-0-1652 name=__codelineno-0-1652></a>        <span class=c1># q_mu = q_mu[:, :, 1:, 1:] if clip_start else q_mu[:, :, :-1, :-1]</span>
</span><span id=__span-0-1653><a id=__codelineno-0-1653 name=__codelineno-0-1653></a>        <span class=c1># q_lv = q_lv[:, :, 1:, 1:] if clip_start else q_lv[:, :, :-1, :-1]</span>
</span><span id=__span-0-1654><a id=__codelineno-0-1654 name=__codelineno-0-1654></a>
</span><span id=__span-0-1655><a id=__codelineno-0-1655 name=__codelineno-0-1655></a>    <span class=n>q_mu</span> <span class=o>=</span> <span class=n>StableMean</span><span class=p>(</span><span class=n>q_mu</span><span class=p>)</span>
</span><span id=__span-0-1656><a id=__codelineno-0-1656 name=__codelineno-0-1656></a>    <span class=n>q_lv</span> <span class=o>=</span> <span class=n>StableLogVar</span><span class=p>(</span><span class=n>q_lv</span><span class=p>,</span> <span class=n>enable_stable</span><span class=o>=</span><span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>_use_naive_exponential</span><span class=p>)</span>
</span><span id=__span-0-1657><a id=__codelineno-0-1657 name=__codelineno-0-1657></a>    <span class=n>q</span> <span class=o>=</span> <span class=n>Normal</span><span class=p>(</span><span class=n>q_mu</span><span class=o>.</span><span class=n>get</span><span class=p>(),</span> <span class=n>q_lv</span><span class=o>.</span><span class=n>get_std</span><span class=p>())</span>
</span><span id=__span-0-1658><a id=__codelineno-0-1658 name=__codelineno-0-1658></a>    <span class=k>return</span> <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span><span class=p>,</span> <span class=n>q</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.NormalStochasticBlock2d.sample_from_q class="doc doc-heading"> <code class="highlight language-python"><span class=n>sample_from_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.NormalStochasticBlock2d.sample_from_q class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Given an input parameter tensor defining q(z), it processes it by calling <code>process_q_params()</code> method and sample a latent tensor from the resulting distribution.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>q_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input tensor to be processed.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1491>1491</a></span>
<span class=normal><a href=#__codelineno-0-1492>1492</a></span>
<span class=normal><a href=#__codelineno-0-1493>1493</a></span>
<span class=normal><a href=#__codelineno-0-1494>1494</a></span>
<span class=normal><a href=#__codelineno-0-1495>1495</a></span>
<span class=normal><a href=#__codelineno-0-1496>1496</a></span>
<span class=normal><a href=#__codelineno-0-1497>1497</a></span>
<span class=normal><a href=#__codelineno-0-1498>1498</a></span>
<span class=normal><a href=#__codelineno-0-1499>1499</a></span>
<span class=normal><a href=#__codelineno-0-1500>1500</a></span>
<span class=normal><a href=#__codelineno-0-1501>1501</a></span>
<span class=normal><a href=#__codelineno-0-1502>1502</a></span>
<span class=normal><a href=#__codelineno-0-1503>1503</a></span>
<span class=normal><a href=#__codelineno-0-1504>1504</a></span>
<span class=normal><a href=#__codelineno-0-1505>1505</a></span>
<span class=normal><a href=#__codelineno-0-1506>1506</a></span>
<span class=normal><a href=#__codelineno-0-1507>1507</a></span>
<span class=normal><a href=#__codelineno-0-1508>1508</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1491><a id=__codelineno-0-1491 name=__codelineno-0-1491></a><span class=k>def</span> <span class=nf>sample_from_q</span><span class=p>(</span>
</span><span id=__span-0-1492><a id=__codelineno-0-1492 name=__codelineno-0-1492></a>    <span class=bp>self</span><span class=p>,</span> <span class=n>q_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span>
</span><span id=__span-0-1493><a id=__codelineno-0-1493 name=__codelineno-0-1493></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1494><a id=__codelineno-0-1494 name=__codelineno-0-1494></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1495><a id=__codelineno-0-1495 name=__codelineno-0-1495></a><span class=sd>    Given an input parameter tensor defining q(z),</span>
</span><span id=__span-0-1496><a id=__codelineno-0-1496 name=__codelineno-0-1496></a><span class=sd>    it processes it by calling `process_q_params()` method and</span>
</span><span id=__span-0-1497><a id=__codelineno-0-1497 name=__codelineno-0-1497></a><span class=sd>    sample a latent tensor from the resulting distribution.</span>
</span><span id=__span-0-1498><a id=__codelineno-0-1498 name=__codelineno-0-1498></a>
</span><span id=__span-0-1499><a id=__codelineno-0-1499 name=__codelineno-0-1499></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1500><a id=__codelineno-0-1500 name=__codelineno-0-1500></a><span class=sd>    ----------</span>
</span><span id=__span-0-1501><a id=__codelineno-0-1501 name=__codelineno-0-1501></a><span class=sd>    q_params: torch.Tensor</span>
</span><span id=__span-0-1502><a id=__codelineno-0-1502 name=__codelineno-0-1502></a><span class=sd>        The input tensor to be processed.</span>
</span><span id=__span-0-1503><a id=__codelineno-0-1503 name=__codelineno-0-1503></a><span class=sd>    var_clip_max: float</span>
</span><span id=__span-0-1504><a id=__codelineno-0-1504 name=__codelineno-0-1504></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1505><a id=__codelineno-0-1505 name=__codelineno-0-1505></a><span class=sd>        Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1506><a id=__codelineno-0-1506 name=__codelineno-0-1506></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1507><a id=__codelineno-0-1507 name=__codelineno-0-1507></a>    <span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>process_q_params</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1508><a id=__codelineno-0-1508 name=__codelineno-0-1508></a>    <span class=k>return</span> <span class=n>q</span><span class=o>.</span><span class=n>rsample</span><span class=p>()</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.ResBlockWithResampling class="doc doc-heading"> <code>ResBlockWithResampling</code> <a href=#careamics.models.lvae.layers.ResBlockWithResampling class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2). It is structured as follows: 1. <code>pre_conv</code>: a downsampling or upsampling strided convolutional layer in case of resampling, or a 1x1 convolutional layer that maps the number of channels of the input to <code>inner_channels</code>. 2. <code>ResidualBlock</code> 3. <code>post_conv</code>: a 1x1 convolutional layer that maps the number of channels to <code>c_out</code>.</p> <p>Some implementation notes: - Resampling is performed through a strided convolution layer at the beginning of the block. - The strided convolution block has fixed kernel size of 3x3 and 1 layer of zero-padding. - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers. - The number of internal channels is by default the same as the number of output channels, but min_inner_channels can override the behaviour.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-202>202</a></span>
<span class=normal><a href=#__codelineno-0-203>203</a></span>
<span class=normal><a href=#__codelineno-0-204>204</a></span>
<span class=normal><a href=#__codelineno-0-205>205</a></span>
<span class=normal><a href=#__codelineno-0-206>206</a></span>
<span class=normal><a href=#__codelineno-0-207>207</a></span>
<span class=normal><a href=#__codelineno-0-208>208</a></span>
<span class=normal><a href=#__codelineno-0-209>209</a></span>
<span class=normal><a href=#__codelineno-0-210>210</a></span>
<span class=normal><a href=#__codelineno-0-211>211</a></span>
<span class=normal><a href=#__codelineno-0-212>212</a></span>
<span class=normal><a href=#__codelineno-0-213>213</a></span>
<span class=normal><a href=#__codelineno-0-214>214</a></span>
<span class=normal><a href=#__codelineno-0-215>215</a></span>
<span class=normal><a href=#__codelineno-0-216>216</a></span>
<span class=normal><a href=#__codelineno-0-217>217</a></span>
<span class=normal><a href=#__codelineno-0-218>218</a></span>
<span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span>
<span class=normal><a href=#__codelineno-0-270>270</a></span>
<span class=normal><a href=#__codelineno-0-271>271</a></span>
<span class=normal><a href=#__codelineno-0-272>272</a></span>
<span class=normal><a href=#__codelineno-0-273>273</a></span>
<span class=normal><a href=#__codelineno-0-274>274</a></span>
<span class=normal><a href=#__codelineno-0-275>275</a></span>
<span class=normal><a href=#__codelineno-0-276>276</a></span>
<span class=normal><a href=#__codelineno-0-277>277</a></span>
<span class=normal><a href=#__codelineno-0-278>278</a></span>
<span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span>
<span class=normal><a href=#__codelineno-0-310>310</a></span>
<span class=normal><a href=#__codelineno-0-311>311</a></span>
<span class=normal><a href=#__codelineno-0-312>312</a></span>
<span class=normal><a href=#__codelineno-0-313>313</a></span>
<span class=normal><a href=#__codelineno-0-314>314</a></span>
<span class=normal><a href=#__codelineno-0-315>315</a></span>
<span class=normal><a href=#__codelineno-0-316>316</a></span>
<span class=normal><a href=#__codelineno-0-317>317</a></span>
<span class=normal><a href=#__codelineno-0-318>318</a></span>
<span class=normal><a href=#__codelineno-0-319>319</a></span>
<span class=normal><a href=#__codelineno-0-320>320</a></span>
<span class=normal><a href=#__codelineno-0-321>321</a></span>
<span class=normal><a href=#__codelineno-0-322>322</a></span>
<span class=normal><a href=#__codelineno-0-323>323</a></span>
<span class=normal><a href=#__codelineno-0-324>324</a></span>
<span class=normal><a href=#__codelineno-0-325>325</a></span>
<span class=normal><a href=#__codelineno-0-326>326</a></span>
<span class=normal><a href=#__codelineno-0-327>327</a></span>
<span class=normal><a href=#__codelineno-0-328>328</a></span>
<span class=normal><a href=#__codelineno-0-329>329</a></span>
<span class=normal><a href=#__codelineno-0-330>330</a></span>
<span class=normal><a href=#__codelineno-0-331>331</a></span>
<span class=normal><a href=#__codelineno-0-332>332</a></span>
<span class=normal><a href=#__codelineno-0-333>333</a></span>
<span class=normal><a href=#__codelineno-0-334>334</a></span>
<span class=normal><a href=#__codelineno-0-335>335</a></span>
<span class=normal><a href=#__codelineno-0-336>336</a></span>
<span class=normal><a href=#__codelineno-0-337>337</a></span>
<span class=normal><a href=#__codelineno-0-338>338</a></span>
<span class=normal><a href=#__codelineno-0-339>339</a></span>
<span class=normal><a href=#__codelineno-0-340>340</a></span>
<span class=normal><a href=#__codelineno-0-341>341</a></span>
<span class=normal><a href=#__codelineno-0-342>342</a></span>
<span class=normal><a href=#__codelineno-0-343>343</a></span>
<span class=normal><a href=#__codelineno-0-344>344</a></span>
<span class=normal><a href=#__codelineno-0-345>345</a></span>
<span class=normal><a href=#__codelineno-0-346>346</a></span>
<span class=normal><a href=#__codelineno-0-347>347</a></span>
<span class=normal><a href=#__codelineno-0-348>348</a></span>
<span class=normal><a href=#__codelineno-0-349>349</a></span>
<span class=normal><a href=#__codelineno-0-350>350</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-202><a id=__codelineno-0-202 name=__codelineno-0-202></a><span class=k>class</span> <span class=nc>ResBlockWithResampling</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-203><a id=__codelineno-0-203 name=__codelineno-0-203></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-204><a id=__codelineno-0-204 name=__codelineno-0-204></a><span class=sd>    Residual block that takes care of resampling (i.e. downsampling or upsampling) steps (by a factor 2).</span>
</span><span id=__span-0-205><a id=__codelineno-0-205 name=__codelineno-0-205></a><span class=sd>    It is structured as follows:</span>
</span><span id=__span-0-206><a id=__codelineno-0-206 name=__codelineno-0-206></a><span class=sd>        1. `pre_conv`: a downsampling or upsampling strided convolutional layer in case of resampling, or</span>
</span><span id=__span-0-207><a id=__codelineno-0-207 name=__codelineno-0-207></a><span class=sd>            a 1x1 convolutional layer that maps the number of channels of the input to `inner_channels`.</span>
</span><span id=__span-0-208><a id=__codelineno-0-208 name=__codelineno-0-208></a><span class=sd>        2. `ResidualBlock`</span>
</span><span id=__span-0-209><a id=__codelineno-0-209 name=__codelineno-0-209></a><span class=sd>        3. `post_conv`: a 1x1 convolutional layer that maps the number of channels to `c_out`.</span>
</span><span id=__span-0-210><a id=__codelineno-0-210 name=__codelineno-0-210></a>
</span><span id=__span-0-211><a id=__codelineno-0-211 name=__codelineno-0-211></a><span class=sd>    Some implementation notes:</span>
</span><span id=__span-0-212><a id=__codelineno-0-212 name=__codelineno-0-212></a><span class=sd>    - Resampling is performed through a strided convolution layer at the beginning of the block.</span>
</span><span id=__span-0-213><a id=__codelineno-0-213 name=__codelineno-0-213></a><span class=sd>    - The strided convolution block has fixed kernel size of 3x3 and 1 layer of zero-padding.</span>
</span><span id=__span-0-214><a id=__codelineno-0-214 name=__codelineno-0-214></a><span class=sd>    - The number of channels is adjusted at the beginning and end of the block through 1x1 convolutional layers.</span>
</span><span id=__span-0-215><a id=__codelineno-0-215 name=__codelineno-0-215></a><span class=sd>    - The number of internal channels is by default the same as the number of output channels, but</span>
</span><span id=__span-0-216><a id=__codelineno-0-216 name=__codelineno-0-216></a><span class=sd>      min_inner_channels can override the behaviour.</span>
</span><span id=__span-0-217><a id=__codelineno-0-217 name=__codelineno-0-217></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-218><a id=__codelineno-0-218 name=__codelineno-0-218></a>
</span><span id=__span-0-219><a id=__codelineno-0-219 name=__codelineno-0-219></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-220><a id=__codelineno-0-220 name=__codelineno-0-220></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-221><a id=__codelineno-0-221 name=__codelineno-0-221></a>        <span class=n>mode</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;top-down&quot;</span><span class=p>,</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>],</span>
</span><span id=__span-0-222><a id=__codelineno-0-222 name=__codelineno-0-222></a>        <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-223><a id=__codelineno-0-223 name=__codelineno-0-223></a>        <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-224><a id=__codelineno-0-224 name=__codelineno-0-224></a>        <span class=n>min_inner_channels</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-225><a id=__codelineno-0-225 name=__codelineno-0-225></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span>
</span><span id=__span-0-226><a id=__codelineno-0-226 name=__codelineno-0-226></a>        <span class=n>resample</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-227><a id=__codelineno-0-227 name=__codelineno-0-227></a>        <span class=n>res_block_kernel</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-228><a id=__codelineno-0-228 name=__codelineno-0-228></a>        <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-229><a id=__codelineno-0-229 name=__codelineno-0-229></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-230><a id=__codelineno-0-230 name=__codelineno-0-230></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-231><a id=__codelineno-0-231 name=__codelineno-0-231></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-232><a id=__codelineno-0-232 name=__codelineno-0-232></a>        <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-233><a id=__codelineno-0-233 name=__codelineno-0-233></a>        <span class=n>skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-234><a id=__codelineno-0-234 name=__codelineno-0-234></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-235><a id=__codelineno-0-235 name=__codelineno-0-235></a>        <span class=c1># lowres_input: bool = False,</span>
</span><span id=__span-0-236><a id=__codelineno-0-236 name=__codelineno-0-236></a>    <span class=p>):</span>
</span><span id=__span-0-237><a id=__codelineno-0-237 name=__codelineno-0-237></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-238><a id=__codelineno-0-238 name=__codelineno-0-238></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-239><a id=__codelineno-0-239 name=__codelineno-0-239></a>
</span><span id=__span-0-240><a id=__codelineno-0-240 name=__codelineno-0-240></a><span class=sd>        Parameters</span>
</span><span id=__span-0-241><a id=__codelineno-0-241 name=__codelineno-0-241></a><span class=sd>        ----------</span>
</span><span id=__span-0-242><a id=__codelineno-0-242 name=__codelineno-0-242></a><span class=sd>        mode: Literal[&quot;top-down&quot;, &quot;bottom-up&quot;]</span>
</span><span id=__span-0-243><a id=__codelineno-0-243 name=__codelineno-0-243></a><span class=sd>            The type of resampling performed in the initial strided convolution of the block.</span>
</span><span id=__span-0-244><a id=__codelineno-0-244 name=__codelineno-0-244></a><span class=sd>            If &quot;bottom-up&quot; downsampling of a factor 2 is done.</span>
</span><span id=__span-0-245><a id=__codelineno-0-245 name=__codelineno-0-245></a><span class=sd>            If &quot;top-down&quot; upsampling of a factor 2 is done.</span>
</span><span id=__span-0-246><a id=__codelineno-0-246 name=__codelineno-0-246></a><span class=sd>        c_in: int</span>
</span><span id=__span-0-247><a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=sd>            The number of input channels.</span>
</span><span id=__span-0-248><a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=sd>        c_out: int</span>
</span><span id=__span-0-249><a id=__codelineno-0-249 name=__codelineno-0-249></a><span class=sd>            The number of output channels.</span>
</span><span id=__span-0-250><a id=__codelineno-0-250 name=__codelineno-0-250></a><span class=sd>        min_inner_channels: int, optional</span>
</span><span id=__span-0-251><a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>            The number of channels used in the inner layer of this module.</span>
</span><span id=__span-0-252><a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>            Default is `None`, meaning that the number of inner channels is set to `c_out`.</span>
</span><span id=__span-0-253><a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-254><a id=__codelineno-0-254 name=__codelineno-0-254></a><span class=sd>            The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-255><a id=__codelineno-0-255 name=__codelineno-0-255></a><span class=sd>        resample: bool, optional</span>
</span><span id=__span-0-256><a id=__codelineno-0-256 name=__codelineno-0-256></a><span class=sd>            Whether to perform resampling in the first convolutional layer.</span>
</span><span id=__span-0-257><a id=__codelineno-0-257 name=__codelineno-0-257></a><span class=sd>            If `False`, the first convolutional layer just maps the input to a tensor with</span>
</span><span id=__span-0-258><a id=__codelineno-0-258 name=__codelineno-0-258></a><span class=sd>            `inner_channels` channels through 1x1 convolution. Deafult is `False`.</span>
</span><span id=__span-0-259><a id=__codelineno-0-259 name=__codelineno-0-259></a><span class=sd>        res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-260><a id=__codelineno-0-260 name=__codelineno-0-260></a><span class=sd>            The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-261><a id=__codelineno-0-261 name=__codelineno-0-261></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-262><a id=__codelineno-0-262 name=__codelineno-0-262></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-263><a id=__codelineno-0-263 name=__codelineno-0-263></a><span class=sd>        groups: int, optional</span>
</span><span id=__span-0-264><a id=__codelineno-0-264 name=__codelineno-0-264></a><span class=sd>            The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-265><a id=__codelineno-0-265 name=__codelineno-0-265></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-266><a id=__codelineno-0-266 name=__codelineno-0-266></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-267><a id=__codelineno-0-267 name=__codelineno-0-267></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-268><a id=__codelineno-0-268 name=__codelineno-0-268></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-269><a id=__codelineno-0-269 name=__codelineno-0-269></a><span class=sd>            Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-270><a id=__codelineno-0-270 name=__codelineno-0-270></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-271><a id=__codelineno-0-271 name=__codelineno-0-271></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-272><a id=__codelineno-0-272 name=__codelineno-0-272></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-273><a id=__codelineno-0-273 name=__codelineno-0-273></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-274><a id=__codelineno-0-274 name=__codelineno-0-274></a><span class=sd>        gated: bool, optional</span>
</span><span id=__span-0-275><a id=__codelineno-0-275 name=__codelineno-0-275></a><span class=sd>            Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-276><a id=__codelineno-0-276 name=__codelineno-0-276></a><span class=sd>        skip_padding: bool, optional</span>
</span><span id=__span-0-277><a id=__codelineno-0-277 name=__codelineno-0-277></a><span class=sd>            Whether to skip padding in convolutions. Default is `False`.</span>
</span><span id=__span-0-278><a id=__codelineno-0-278 name=__codelineno-0-278></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-279><a id=__codelineno-0-279 name=__codelineno-0-279></a><span class=sd>            Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-280><a id=__codelineno-0-280 name=__codelineno-0-280></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-281><a id=__codelineno-0-281 name=__codelineno-0-281></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-282><a id=__codelineno-0-282 name=__codelineno-0-282></a>        <span class=k>assert</span> <span class=n>mode</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&quot;top-down&quot;</span><span class=p>,</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>]</span>
</span><span id=__span-0-283><a id=__codelineno-0-283 name=__codelineno-0-283></a>
</span><span id=__span-0-284><a id=__codelineno-0-284 name=__codelineno-0-284></a>        <span class=k>if</span> <span class=n>min_inner_channels</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-285><a id=__codelineno-0-285 name=__codelineno-0-285></a>            <span class=n>min_inner_channels</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-286><a id=__codelineno-0-286 name=__codelineno-0-286></a>        <span class=c1># inner_channels is the number of channels used in the inner layers</span>
</span><span id=__span-0-287><a id=__codelineno-0-287 name=__codelineno-0-287></a>        <span class=c1># of ResBlockWithResampling</span>
</span><span id=__span-0-288><a id=__codelineno-0-288 name=__codelineno-0-288></a>        <span class=n>inner_channels</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>c_out</span><span class=p>,</span> <span class=n>min_inner_channels</span><span class=p>)</span>
</span><span id=__span-0-289><a id=__codelineno-0-289 name=__codelineno-0-289></a>
</span><span id=__span-0-290><a id=__codelineno-0-290 name=__codelineno-0-290></a>        <span class=c1># Define first conv layer to change num channels and/or up/downsample</span>
</span><span id=__span-0-291><a id=__codelineno-0-291 name=__codelineno-0-291></a>        <span class=k>if</span> <span class=n>resample</span><span class=p>:</span>
</span><span id=__span-0-292><a id=__codelineno-0-292 name=__codelineno-0-292></a>            <span class=k>if</span> <span class=n>mode</span> <span class=o>==</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>:</span>  <span class=c1># downsample</span>
</span><span id=__span-0-293><a id=__codelineno-0-293 name=__codelineno-0-293></a>                <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-294><a id=__codelineno-0-294 name=__codelineno-0-294></a>                    <span class=n>in_channels</span><span class=o>=</span><span class=n>c_in</span><span class=p>,</span>
</span><span id=__span-0-295><a id=__codelineno-0-295 name=__codelineno-0-295></a>                    <span class=n>out_channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-296><a id=__codelineno-0-296 name=__codelineno-0-296></a>                    <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-297><a id=__codelineno-0-297 name=__codelineno-0-297></a>                    <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-298><a id=__codelineno-0-298 name=__codelineno-0-298></a>                    <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-0-299><a id=__codelineno-0-299 name=__codelineno-0-299></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-300><a id=__codelineno-0-300 name=__codelineno-0-300></a>                    <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-301><a id=__codelineno-0-301 name=__codelineno-0-301></a>                <span class=p>)</span>
</span><span id=__span-0-302><a id=__codelineno-0-302 name=__codelineno-0-302></a>            <span class=k>elif</span> <span class=n>mode</span> <span class=o>==</span> <span class=s2>&quot;top-down&quot;</span><span class=p>:</span>  <span class=c1># upsample</span>
</span><span id=__span-0-303><a id=__codelineno-0-303 name=__codelineno-0-303></a>                <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span>
</span><span id=__span-0-304><a id=__codelineno-0-304 name=__codelineno-0-304></a>                    <span class=n>in_channels</span><span class=o>=</span><span class=n>c_in</span><span class=p>,</span>
</span><span id=__span-0-305><a id=__codelineno-0-305 name=__codelineno-0-305></a>                    <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-306><a id=__codelineno-0-306 name=__codelineno-0-306></a>                    <span class=n>out_channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-307><a id=__codelineno-0-307 name=__codelineno-0-307></a>                    <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-308><a id=__codelineno-0-308 name=__codelineno-0-308></a>                    <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-0-309><a id=__codelineno-0-309 name=__codelineno-0-309></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-310><a id=__codelineno-0-310 name=__codelineno-0-310></a>                    <span class=n>output_padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-311><a id=__codelineno-0-311 name=__codelineno-0-311></a>                    <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-312><a id=__codelineno-0-312 name=__codelineno-0-312></a>                <span class=p>)</span>
</span><span id=__span-0-313><a id=__codelineno-0-313 name=__codelineno-0-313></a>        <span class=k>elif</span> <span class=n>c_in</span> <span class=o>!=</span> <span class=n>inner_channels</span><span class=p>:</span>
</span><span id=__span-0-314><a id=__codelineno-0-314 name=__codelineno-0-314></a>            <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-315><a id=__codelineno-0-315 name=__codelineno-0-315></a>                <span class=n>c_in</span><span class=p>,</span> <span class=n>inner_channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-316><a id=__codelineno-0-316 name=__codelineno-0-316></a>            <span class=p>)</span>
</span><span id=__span-0-317><a id=__codelineno-0-317 name=__codelineno-0-317></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-318><a id=__codelineno-0-318 name=__codelineno-0-318></a>            <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-319><a id=__codelineno-0-319 name=__codelineno-0-319></a>
</span><span id=__span-0-320><a id=__codelineno-0-320 name=__codelineno-0-320></a>        <span class=c1># Residual block</span>
</span><span id=__span-0-321><a id=__codelineno-0-321 name=__codelineno-0-321></a>        <span class=bp>self</span><span class=o>.</span><span class=n>res</span> <span class=o>=</span> <span class=n>ResidualBlock</span><span class=p>(</span>
</span><span id=__span-0-322><a id=__codelineno-0-322 name=__codelineno-0-322></a>            <span class=n>channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-323><a id=__codelineno-0-323 name=__codelineno-0-323></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-324><a id=__codelineno-0-324 name=__codelineno-0-324></a>            <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-325><a id=__codelineno-0-325 name=__codelineno-0-325></a>            <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-326><a id=__codelineno-0-326 name=__codelineno-0-326></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-327><a id=__codelineno-0-327 name=__codelineno-0-327></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-328><a id=__codelineno-0-328 name=__codelineno-0-328></a>            <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-329><a id=__codelineno-0-329 name=__codelineno-0-329></a>            <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-330><a id=__codelineno-0-330 name=__codelineno-0-330></a>            <span class=n>skip_padding</span><span class=o>=</span><span class=n>skip_padding</span><span class=p>,</span>
</span><span id=__span-0-331><a id=__codelineno-0-331 name=__codelineno-0-331></a>            <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-332><a id=__codelineno-0-332 name=__codelineno-0-332></a>        <span class=p>)</span>
</span><span id=__span-0-333><a id=__codelineno-0-333 name=__codelineno-0-333></a>
</span><span id=__span-0-334><a id=__codelineno-0-334 name=__codelineno-0-334></a>        <span class=c1># Define last conv layer to get correct num output channels</span>
</span><span id=__span-0-335><a id=__codelineno-0-335 name=__codelineno-0-335></a>        <span class=k>if</span> <span class=n>inner_channels</span> <span class=o>!=</span> <span class=n>c_out</span><span class=p>:</span>
</span><span id=__span-0-336><a id=__codelineno-0-336 name=__codelineno-0-336></a>            <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-337><a id=__codelineno-0-337 name=__codelineno-0-337></a>                <span class=n>inner_channels</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-338><a id=__codelineno-0-338 name=__codelineno-0-338></a>            <span class=p>)</span>
</span><span id=__span-0-339><a id=__codelineno-0-339 name=__codelineno-0-339></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-340><a id=__codelineno-0-340 name=__codelineno-0-340></a>            <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-341><a id=__codelineno-0-341 name=__codelineno-0-341></a>
</span><span id=__span-0-342><a id=__codelineno-0-342 name=__codelineno-0-342></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-0-343><a id=__codelineno-0-343 name=__codelineno-0-343></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-344><a id=__codelineno-0-344 name=__codelineno-0-344></a>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-345><a id=__codelineno-0-345 name=__codelineno-0-345></a>
</span><span id=__span-0-346><a id=__codelineno-0-346 name=__codelineno-0-346></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>res</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-347><a id=__codelineno-0-347 name=__codelineno-0-347></a>
</span><span id=__span-0-348><a id=__codelineno-0-348 name=__codelineno-0-348></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-349><a id=__codelineno-0-349 name=__codelineno-0-349></a>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-350><a id=__codelineno-0-350 name=__codelineno-0-350></a>        <span class=k>return</span> <span class=n>x</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.ResBlockWithResampling.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>mode</span><span class=p>,</span> <span class=n>c_in</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=n>min_inner_channels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>nonlin</span><span class=o>=</span><span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span> <span class=n>resample</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>res_block_kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>batchnorm</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>res_block_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>gated</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>skip_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.ResBlockWithResampling.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>mode</code></td> <td> <code><span title="typing.Literal">Literal</span>[&#39;top-down&#39;, &#39;bottom-up&#39;]</code> </td> <td> <div class=doc-md-description> <p>The type of resampling performed in the initial strided convolution of the block. If "bottom-up" downsampling of a factor 2 is done. If "top-down" upsampling of a factor 2 is done.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_in</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of input channels.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>c_out</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of output channels.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>min_inner_channels</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels used in the inner layer of this module. Default is <code>None</code>, meaning that the number of inner channels is set to <code>c_out</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>nonlin</code></td> <td> <code><span title="typing.Callable">Callable</span></code> </td> <td> <div class=doc-md-description> <p>The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>.</p> </div> </td> <td> <code><span title="torch.nn.LeakyReLU">LeakyReLU</span></code> </td> </tr> <tr class=doc-section-item> <td><code>resample</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to perform resampling in the first convolutional layer. If <code>False</code>, the first convolutional layer just maps the input to a tensor with <code>inner_channels</code> channels through 1x1 convolution. Deafult is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_kernel</code></td> <td> <code><span title="typing.Union">Union</span>[int, <span title="typing.Iterable">Iterable</span>[int]]</code> </td> <td> <div class=doc-md-description> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>groups</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of groups to consider in the convolutions. Default is 1.</p> </div> </td> <td> <code>1</code> </td> </tr> <tr class=doc-section-item> <td><code>batchnorm</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_type</code></td> <td> <code>str</code> </td> <td> <div class=doc-md-description> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>dropout</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>gated</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use gated layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>skip_padding</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to skip padding in convolutions. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>conv2d_bias</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-219>219</a></span>
<span class=normal><a href=#__codelineno-0-220>220</a></span>
<span class=normal><a href=#__codelineno-0-221>221</a></span>
<span class=normal><a href=#__codelineno-0-222>222</a></span>
<span class=normal><a href=#__codelineno-0-223>223</a></span>
<span class=normal><a href=#__codelineno-0-224>224</a></span>
<span class=normal><a href=#__codelineno-0-225>225</a></span>
<span class=normal><a href=#__codelineno-0-226>226</a></span>
<span class=normal><a href=#__codelineno-0-227>227</a></span>
<span class=normal><a href=#__codelineno-0-228>228</a></span>
<span class=normal><a href=#__codelineno-0-229>229</a></span>
<span class=normal><a href=#__codelineno-0-230>230</a></span>
<span class=normal><a href=#__codelineno-0-231>231</a></span>
<span class=normal><a href=#__codelineno-0-232>232</a></span>
<span class=normal><a href=#__codelineno-0-233>233</a></span>
<span class=normal><a href=#__codelineno-0-234>234</a></span>
<span class=normal><a href=#__codelineno-0-235>235</a></span>
<span class=normal><a href=#__codelineno-0-236>236</a></span>
<span class=normal><a href=#__codelineno-0-237>237</a></span>
<span class=normal><a href=#__codelineno-0-238>238</a></span>
<span class=normal><a href=#__codelineno-0-239>239</a></span>
<span class=normal><a href=#__codelineno-0-240>240</a></span>
<span class=normal><a href=#__codelineno-0-241>241</a></span>
<span class=normal><a href=#__codelineno-0-242>242</a></span>
<span class=normal><a href=#__codelineno-0-243>243</a></span>
<span class=normal><a href=#__codelineno-0-244>244</a></span>
<span class=normal><a href=#__codelineno-0-245>245</a></span>
<span class=normal><a href=#__codelineno-0-246>246</a></span>
<span class=normal><a href=#__codelineno-0-247>247</a></span>
<span class=normal><a href=#__codelineno-0-248>248</a></span>
<span class=normal><a href=#__codelineno-0-249>249</a></span>
<span class=normal><a href=#__codelineno-0-250>250</a></span>
<span class=normal><a href=#__codelineno-0-251>251</a></span>
<span class=normal><a href=#__codelineno-0-252>252</a></span>
<span class=normal><a href=#__codelineno-0-253>253</a></span>
<span class=normal><a href=#__codelineno-0-254>254</a></span>
<span class=normal><a href=#__codelineno-0-255>255</a></span>
<span class=normal><a href=#__codelineno-0-256>256</a></span>
<span class=normal><a href=#__codelineno-0-257>257</a></span>
<span class=normal><a href=#__codelineno-0-258>258</a></span>
<span class=normal><a href=#__codelineno-0-259>259</a></span>
<span class=normal><a href=#__codelineno-0-260>260</a></span>
<span class=normal><a href=#__codelineno-0-261>261</a></span>
<span class=normal><a href=#__codelineno-0-262>262</a></span>
<span class=normal><a href=#__codelineno-0-263>263</a></span>
<span class=normal><a href=#__codelineno-0-264>264</a></span>
<span class=normal><a href=#__codelineno-0-265>265</a></span>
<span class=normal><a href=#__codelineno-0-266>266</a></span>
<span class=normal><a href=#__codelineno-0-267>267</a></span>
<span class=normal><a href=#__codelineno-0-268>268</a></span>
<span class=normal><a href=#__codelineno-0-269>269</a></span>
<span class=normal><a href=#__codelineno-0-270>270</a></span>
<span class=normal><a href=#__codelineno-0-271>271</a></span>
<span class=normal><a href=#__codelineno-0-272>272</a></span>
<span class=normal><a href=#__codelineno-0-273>273</a></span>
<span class=normal><a href=#__codelineno-0-274>274</a></span>
<span class=normal><a href=#__codelineno-0-275>275</a></span>
<span class=normal><a href=#__codelineno-0-276>276</a></span>
<span class=normal><a href=#__codelineno-0-277>277</a></span>
<span class=normal><a href=#__codelineno-0-278>278</a></span>
<span class=normal><a href=#__codelineno-0-279>279</a></span>
<span class=normal><a href=#__codelineno-0-280>280</a></span>
<span class=normal><a href=#__codelineno-0-281>281</a></span>
<span class=normal><a href=#__codelineno-0-282>282</a></span>
<span class=normal><a href=#__codelineno-0-283>283</a></span>
<span class=normal><a href=#__codelineno-0-284>284</a></span>
<span class=normal><a href=#__codelineno-0-285>285</a></span>
<span class=normal><a href=#__codelineno-0-286>286</a></span>
<span class=normal><a href=#__codelineno-0-287>287</a></span>
<span class=normal><a href=#__codelineno-0-288>288</a></span>
<span class=normal><a href=#__codelineno-0-289>289</a></span>
<span class=normal><a href=#__codelineno-0-290>290</a></span>
<span class=normal><a href=#__codelineno-0-291>291</a></span>
<span class=normal><a href=#__codelineno-0-292>292</a></span>
<span class=normal><a href=#__codelineno-0-293>293</a></span>
<span class=normal><a href=#__codelineno-0-294>294</a></span>
<span class=normal><a href=#__codelineno-0-295>295</a></span>
<span class=normal><a href=#__codelineno-0-296>296</a></span>
<span class=normal><a href=#__codelineno-0-297>297</a></span>
<span class=normal><a href=#__codelineno-0-298>298</a></span>
<span class=normal><a href=#__codelineno-0-299>299</a></span>
<span class=normal><a href=#__codelineno-0-300>300</a></span>
<span class=normal><a href=#__codelineno-0-301>301</a></span>
<span class=normal><a href=#__codelineno-0-302>302</a></span>
<span class=normal><a href=#__codelineno-0-303>303</a></span>
<span class=normal><a href=#__codelineno-0-304>304</a></span>
<span class=normal><a href=#__codelineno-0-305>305</a></span>
<span class=normal><a href=#__codelineno-0-306>306</a></span>
<span class=normal><a href=#__codelineno-0-307>307</a></span>
<span class=normal><a href=#__codelineno-0-308>308</a></span>
<span class=normal><a href=#__codelineno-0-309>309</a></span>
<span class=normal><a href=#__codelineno-0-310>310</a></span>
<span class=normal><a href=#__codelineno-0-311>311</a></span>
<span class=normal><a href=#__codelineno-0-312>312</a></span>
<span class=normal><a href=#__codelineno-0-313>313</a></span>
<span class=normal><a href=#__codelineno-0-314>314</a></span>
<span class=normal><a href=#__codelineno-0-315>315</a></span>
<span class=normal><a href=#__codelineno-0-316>316</a></span>
<span class=normal><a href=#__codelineno-0-317>317</a></span>
<span class=normal><a href=#__codelineno-0-318>318</a></span>
<span class=normal><a href=#__codelineno-0-319>319</a></span>
<span class=normal><a href=#__codelineno-0-320>320</a></span>
<span class=normal><a href=#__codelineno-0-321>321</a></span>
<span class=normal><a href=#__codelineno-0-322>322</a></span>
<span class=normal><a href=#__codelineno-0-323>323</a></span>
<span class=normal><a href=#__codelineno-0-324>324</a></span>
<span class=normal><a href=#__codelineno-0-325>325</a></span>
<span class=normal><a href=#__codelineno-0-326>326</a></span>
<span class=normal><a href=#__codelineno-0-327>327</a></span>
<span class=normal><a href=#__codelineno-0-328>328</a></span>
<span class=normal><a href=#__codelineno-0-329>329</a></span>
<span class=normal><a href=#__codelineno-0-330>330</a></span>
<span class=normal><a href=#__codelineno-0-331>331</a></span>
<span class=normal><a href=#__codelineno-0-332>332</a></span>
<span class=normal><a href=#__codelineno-0-333>333</a></span>
<span class=normal><a href=#__codelineno-0-334>334</a></span>
<span class=normal><a href=#__codelineno-0-335>335</a></span>
<span class=normal><a href=#__codelineno-0-336>336</a></span>
<span class=normal><a href=#__codelineno-0-337>337</a></span>
<span class=normal><a href=#__codelineno-0-338>338</a></span>
<span class=normal><a href=#__codelineno-0-339>339</a></span>
<span class=normal><a href=#__codelineno-0-340>340</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-219><a id=__codelineno-0-219 name=__codelineno-0-219></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-220><a id=__codelineno-0-220 name=__codelineno-0-220></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-221><a id=__codelineno-0-221 name=__codelineno-0-221></a>    <span class=n>mode</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;top-down&quot;</span><span class=p>,</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>],</span>
</span><span id=__span-0-222><a id=__codelineno-0-222 name=__codelineno-0-222></a>    <span class=n>c_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-223><a id=__codelineno-0-223 name=__codelineno-0-223></a>    <span class=n>c_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-224><a id=__codelineno-0-224 name=__codelineno-0-224></a>    <span class=n>min_inner_channels</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-225><a id=__codelineno-0-225 name=__codelineno-0-225></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>,</span>
</span><span id=__span-0-226><a id=__codelineno-0-226 name=__codelineno-0-226></a>    <span class=n>resample</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-227><a id=__codelineno-0-227 name=__codelineno-0-227></a>    <span class=n>res_block_kernel</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-228><a id=__codelineno-0-228 name=__codelineno-0-228></a>    <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-229><a id=__codelineno-0-229 name=__codelineno-0-229></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-230><a id=__codelineno-0-230 name=__codelineno-0-230></a>    <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-231><a id=__codelineno-0-231 name=__codelineno-0-231></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-232><a id=__codelineno-0-232 name=__codelineno-0-232></a>    <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-233><a id=__codelineno-0-233 name=__codelineno-0-233></a>    <span class=n>skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-234><a id=__codelineno-0-234 name=__codelineno-0-234></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-235><a id=__codelineno-0-235 name=__codelineno-0-235></a>    <span class=c1># lowres_input: bool = False,</span>
</span><span id=__span-0-236><a id=__codelineno-0-236 name=__codelineno-0-236></a><span class=p>):</span>
</span><span id=__span-0-237><a id=__codelineno-0-237 name=__codelineno-0-237></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-238><a id=__codelineno-0-238 name=__codelineno-0-238></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-239><a id=__codelineno-0-239 name=__codelineno-0-239></a>
</span><span id=__span-0-240><a id=__codelineno-0-240 name=__codelineno-0-240></a><span class=sd>    Parameters</span>
</span><span id=__span-0-241><a id=__codelineno-0-241 name=__codelineno-0-241></a><span class=sd>    ----------</span>
</span><span id=__span-0-242><a id=__codelineno-0-242 name=__codelineno-0-242></a><span class=sd>    mode: Literal[&quot;top-down&quot;, &quot;bottom-up&quot;]</span>
</span><span id=__span-0-243><a id=__codelineno-0-243 name=__codelineno-0-243></a><span class=sd>        The type of resampling performed in the initial strided convolution of the block.</span>
</span><span id=__span-0-244><a id=__codelineno-0-244 name=__codelineno-0-244></a><span class=sd>        If &quot;bottom-up&quot; downsampling of a factor 2 is done.</span>
</span><span id=__span-0-245><a id=__codelineno-0-245 name=__codelineno-0-245></a><span class=sd>        If &quot;top-down&quot; upsampling of a factor 2 is done.</span>
</span><span id=__span-0-246><a id=__codelineno-0-246 name=__codelineno-0-246></a><span class=sd>    c_in: int</span>
</span><span id=__span-0-247><a id=__codelineno-0-247 name=__codelineno-0-247></a><span class=sd>        The number of input channels.</span>
</span><span id=__span-0-248><a id=__codelineno-0-248 name=__codelineno-0-248></a><span class=sd>    c_out: int</span>
</span><span id=__span-0-249><a id=__codelineno-0-249 name=__codelineno-0-249></a><span class=sd>        The number of output channels.</span>
</span><span id=__span-0-250><a id=__codelineno-0-250 name=__codelineno-0-250></a><span class=sd>    min_inner_channels: int, optional</span>
</span><span id=__span-0-251><a id=__codelineno-0-251 name=__codelineno-0-251></a><span class=sd>        The number of channels used in the inner layer of this module.</span>
</span><span id=__span-0-252><a id=__codelineno-0-252 name=__codelineno-0-252></a><span class=sd>        Default is `None`, meaning that the number of inner channels is set to `c_out`.</span>
</span><span id=__span-0-253><a id=__codelineno-0-253 name=__codelineno-0-253></a><span class=sd>    nonlin: Callable, optional</span>
</span><span id=__span-0-254><a id=__codelineno-0-254 name=__codelineno-0-254></a><span class=sd>        The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-255><a id=__codelineno-0-255 name=__codelineno-0-255></a><span class=sd>    resample: bool, optional</span>
</span><span id=__span-0-256><a id=__codelineno-0-256 name=__codelineno-0-256></a><span class=sd>        Whether to perform resampling in the first convolutional layer.</span>
</span><span id=__span-0-257><a id=__codelineno-0-257 name=__codelineno-0-257></a><span class=sd>        If `False`, the first convolutional layer just maps the input to a tensor with</span>
</span><span id=__span-0-258><a id=__codelineno-0-258 name=__codelineno-0-258></a><span class=sd>        `inner_channels` channels through 1x1 convolution. Deafult is `False`.</span>
</span><span id=__span-0-259><a id=__codelineno-0-259 name=__codelineno-0-259></a><span class=sd>    res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-260><a id=__codelineno-0-260 name=__codelineno-0-260></a><span class=sd>        The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-261><a id=__codelineno-0-261 name=__codelineno-0-261></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-262><a id=__codelineno-0-262 name=__codelineno-0-262></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-263><a id=__codelineno-0-263 name=__codelineno-0-263></a><span class=sd>    groups: int, optional</span>
</span><span id=__span-0-264><a id=__codelineno-0-264 name=__codelineno-0-264></a><span class=sd>        The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-265><a id=__codelineno-0-265 name=__codelineno-0-265></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-266><a id=__codelineno-0-266 name=__codelineno-0-266></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-267><a id=__codelineno-0-267 name=__codelineno-0-267></a><span class=sd>    res_block_type: str, optional</span>
</span><span id=__span-0-268><a id=__codelineno-0-268 name=__codelineno-0-268></a><span class=sd>        A string specifying the structure of residual block.</span>
</span><span id=__span-0-269><a id=__codelineno-0-269 name=__codelineno-0-269></a><span class=sd>        Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-270><a id=__codelineno-0-270 name=__codelineno-0-270></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-271><a id=__codelineno-0-271 name=__codelineno-0-271></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-272><a id=__codelineno-0-272 name=__codelineno-0-272></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-273><a id=__codelineno-0-273 name=__codelineno-0-273></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-274><a id=__codelineno-0-274 name=__codelineno-0-274></a><span class=sd>    gated: bool, optional</span>
</span><span id=__span-0-275><a id=__codelineno-0-275 name=__codelineno-0-275></a><span class=sd>        Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-276><a id=__codelineno-0-276 name=__codelineno-0-276></a><span class=sd>    skip_padding: bool, optional</span>
</span><span id=__span-0-277><a id=__codelineno-0-277 name=__codelineno-0-277></a><span class=sd>        Whether to skip padding in convolutions. Default is `False`.</span>
</span><span id=__span-0-278><a id=__codelineno-0-278 name=__codelineno-0-278></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-279><a id=__codelineno-0-279 name=__codelineno-0-279></a><span class=sd>        Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-280><a id=__codelineno-0-280 name=__codelineno-0-280></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-281><a id=__codelineno-0-281 name=__codelineno-0-281></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-282><a id=__codelineno-0-282 name=__codelineno-0-282></a>    <span class=k>assert</span> <span class=n>mode</span> <span class=ow>in</span> <span class=p>[</span><span class=s2>&quot;top-down&quot;</span><span class=p>,</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>]</span>
</span><span id=__span-0-283><a id=__codelineno-0-283 name=__codelineno-0-283></a>
</span><span id=__span-0-284><a id=__codelineno-0-284 name=__codelineno-0-284></a>    <span class=k>if</span> <span class=n>min_inner_channels</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-285><a id=__codelineno-0-285 name=__codelineno-0-285></a>        <span class=n>min_inner_channels</span> <span class=o>=</span> <span class=mi>0</span>
</span><span id=__span-0-286><a id=__codelineno-0-286 name=__codelineno-0-286></a>    <span class=c1># inner_channels is the number of channels used in the inner layers</span>
</span><span id=__span-0-287><a id=__codelineno-0-287 name=__codelineno-0-287></a>    <span class=c1># of ResBlockWithResampling</span>
</span><span id=__span-0-288><a id=__codelineno-0-288 name=__codelineno-0-288></a>    <span class=n>inner_channels</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>c_out</span><span class=p>,</span> <span class=n>min_inner_channels</span><span class=p>)</span>
</span><span id=__span-0-289><a id=__codelineno-0-289 name=__codelineno-0-289></a>
</span><span id=__span-0-290><a id=__codelineno-0-290 name=__codelineno-0-290></a>    <span class=c1># Define first conv layer to change num channels and/or up/downsample</span>
</span><span id=__span-0-291><a id=__codelineno-0-291 name=__codelineno-0-291></a>    <span class=k>if</span> <span class=n>resample</span><span class=p>:</span>
</span><span id=__span-0-292><a id=__codelineno-0-292 name=__codelineno-0-292></a>        <span class=k>if</span> <span class=n>mode</span> <span class=o>==</span> <span class=s2>&quot;bottom-up&quot;</span><span class=p>:</span>  <span class=c1># downsample</span>
</span><span id=__span-0-293><a id=__codelineno-0-293 name=__codelineno-0-293></a>            <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-294><a id=__codelineno-0-294 name=__codelineno-0-294></a>                <span class=n>in_channels</span><span class=o>=</span><span class=n>c_in</span><span class=p>,</span>
</span><span id=__span-0-295><a id=__codelineno-0-295 name=__codelineno-0-295></a>                <span class=n>out_channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-296><a id=__codelineno-0-296 name=__codelineno-0-296></a>                <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-297><a id=__codelineno-0-297 name=__codelineno-0-297></a>                <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-298><a id=__codelineno-0-298 name=__codelineno-0-298></a>                <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-0-299><a id=__codelineno-0-299 name=__codelineno-0-299></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-300><a id=__codelineno-0-300 name=__codelineno-0-300></a>                <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-301><a id=__codelineno-0-301 name=__codelineno-0-301></a>            <span class=p>)</span>
</span><span id=__span-0-302><a id=__codelineno-0-302 name=__codelineno-0-302></a>        <span class=k>elif</span> <span class=n>mode</span> <span class=o>==</span> <span class=s2>&quot;top-down&quot;</span><span class=p>:</span>  <span class=c1># upsample</span>
</span><span id=__span-0-303><a id=__codelineno-0-303 name=__codelineno-0-303></a>            <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ConvTranspose2d</span><span class=p>(</span>
</span><span id=__span-0-304><a id=__codelineno-0-304 name=__codelineno-0-304></a>                <span class=n>in_channels</span><span class=o>=</span><span class=n>c_in</span><span class=p>,</span>
</span><span id=__span-0-305><a id=__codelineno-0-305 name=__codelineno-0-305></a>                <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span><span id=__span-0-306><a id=__codelineno-0-306 name=__codelineno-0-306></a>                <span class=n>out_channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-307><a id=__codelineno-0-307 name=__codelineno-0-307></a>                <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-308><a id=__codelineno-0-308 name=__codelineno-0-308></a>                <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span><span id=__span-0-309><a id=__codelineno-0-309 name=__codelineno-0-309></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-310><a id=__codelineno-0-310 name=__codelineno-0-310></a>                <span class=n>output_padding</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-311><a id=__codelineno-0-311 name=__codelineno-0-311></a>                <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-312><a id=__codelineno-0-312 name=__codelineno-0-312></a>            <span class=p>)</span>
</span><span id=__span-0-313><a id=__codelineno-0-313 name=__codelineno-0-313></a>    <span class=k>elif</span> <span class=n>c_in</span> <span class=o>!=</span> <span class=n>inner_channels</span><span class=p>:</span>
</span><span id=__span-0-314><a id=__codelineno-0-314 name=__codelineno-0-314></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-315><a id=__codelineno-0-315 name=__codelineno-0-315></a>            <span class=n>c_in</span><span class=p>,</span> <span class=n>inner_channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-316><a id=__codelineno-0-316 name=__codelineno-0-316></a>        <span class=p>)</span>
</span><span id=__span-0-317><a id=__codelineno-0-317 name=__codelineno-0-317></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-318><a id=__codelineno-0-318 name=__codelineno-0-318></a>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_conv</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-319><a id=__codelineno-0-319 name=__codelineno-0-319></a>
</span><span id=__span-0-320><a id=__codelineno-0-320 name=__codelineno-0-320></a>    <span class=c1># Residual block</span>
</span><span id=__span-0-321><a id=__codelineno-0-321 name=__codelineno-0-321></a>    <span class=bp>self</span><span class=o>.</span><span class=n>res</span> <span class=o>=</span> <span class=n>ResidualBlock</span><span class=p>(</span>
</span><span id=__span-0-322><a id=__codelineno-0-322 name=__codelineno-0-322></a>        <span class=n>channels</span><span class=o>=</span><span class=n>inner_channels</span><span class=p>,</span>
</span><span id=__span-0-323><a id=__codelineno-0-323 name=__codelineno-0-323></a>        <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-324><a id=__codelineno-0-324 name=__codelineno-0-324></a>        <span class=n>kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-325><a id=__codelineno-0-325 name=__codelineno-0-325></a>        <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-326><a id=__codelineno-0-326 name=__codelineno-0-326></a>        <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-327><a id=__codelineno-0-327 name=__codelineno-0-327></a>        <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-328><a id=__codelineno-0-328 name=__codelineno-0-328></a>        <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-329><a id=__codelineno-0-329 name=__codelineno-0-329></a>        <span class=n>block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-330><a id=__codelineno-0-330 name=__codelineno-0-330></a>        <span class=n>skip_padding</span><span class=o>=</span><span class=n>skip_padding</span><span class=p>,</span>
</span><span id=__span-0-331><a id=__codelineno-0-331 name=__codelineno-0-331></a>        <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-332><a id=__codelineno-0-332 name=__codelineno-0-332></a>    <span class=p>)</span>
</span><span id=__span-0-333><a id=__codelineno-0-333 name=__codelineno-0-333></a>
</span><span id=__span-0-334><a id=__codelineno-0-334 name=__codelineno-0-334></a>    <span class=c1># Define last conv layer to get correct num output channels</span>
</span><span id=__span-0-335><a id=__codelineno-0-335 name=__codelineno-0-335></a>    <span class=k>if</span> <span class=n>inner_channels</span> <span class=o>!=</span> <span class=n>c_out</span><span class=p>:</span>
</span><span id=__span-0-336><a id=__codelineno-0-336 name=__codelineno-0-336></a>        <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-337><a id=__codelineno-0-337 name=__codelineno-0-337></a>            <span class=n>inner_channels</span><span class=p>,</span> <span class=n>c_out</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span>
</span><span id=__span-0-338><a id=__codelineno-0-338 name=__codelineno-0-338></a>        <span class=p>)</span>
</span><span id=__span-0-339><a id=__codelineno-0-339 name=__codelineno-0-339></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-340><a id=__codelineno-0-340 name=__codelineno-0-340></a>        <span class=bp>self</span><span class=o>.</span><span class=n>post_conv</span> <span class=o>=</span> <span class=kc>None</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.ResidualBlock class="doc doc-heading"> <code>ResidualBlock</code> <a href=#careamics.models.lvae.layers.ResidualBlock class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Residual block with 2 convolutional layers.</p> <p>Some architectural notes: - The number of input, intermediate, and output channels is the same, - Padding is always 'same', - The 2 convolutional layers have the same groups, - No stride allowed, - Kernel sizes must be odd.</p> <p>The output isgiven by: <code>out = gate(f(x)) + x</code>. The presence of the gating mechanism is optional, and f(x) has different structures depending on the <code>block_type</code> argument. Specifically, <code>block_type</code> is a string specifying the block's structure, with: a = activation b = batch norm c = conv layer d = dropout. For example, "bacdbacd" defines a block with 2x[batchnorm, activation, conv, dropout].</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-26> 26</a></span>
<span class=normal><a href=#__codelineno-0-27> 27</a></span>
<span class=normal><a href=#__codelineno-0-28> 28</a></span>
<span class=normal><a href=#__codelineno-0-29> 29</a></span>
<span class=normal><a href=#__codelineno-0-30> 30</a></span>
<span class=normal><a href=#__codelineno-0-31> 31</a></span>
<span class=normal><a href=#__codelineno-0-32> 32</a></span>
<span class=normal><a href=#__codelineno-0-33> 33</a></span>
<span class=normal><a href=#__codelineno-0-34> 34</a></span>
<span class=normal><a href=#__codelineno-0-35> 35</a></span>
<span class=normal><a href=#__codelineno-0-36> 36</a></span>
<span class=normal><a href=#__codelineno-0-37> 37</a></span>
<span class=normal><a href=#__codelineno-0-38> 38</a></span>
<span class=normal><a href=#__codelineno-0-39> 39</a></span>
<span class=normal><a href=#__codelineno-0-40> 40</a></span>
<span class=normal><a href=#__codelineno-0-41> 41</a></span>
<span class=normal><a href=#__codelineno-0-42> 42</a></span>
<span class=normal><a href=#__codelineno-0-43> 43</a></span>
<span class=normal><a href=#__codelineno-0-44> 44</a></span>
<span class=normal><a href=#__codelineno-0-45> 45</a></span>
<span class=normal><a href=#__codelineno-0-46> 46</a></span>
<span class=normal><a href=#__codelineno-0-47> 47</a></span>
<span class=normal><a href=#__codelineno-0-48> 48</a></span>
<span class=normal><a href=#__codelineno-0-49> 49</a></span>
<span class=normal><a href=#__codelineno-0-50> 50</a></span>
<span class=normal><a href=#__codelineno-0-51> 51</a></span>
<span class=normal><a href=#__codelineno-0-52> 52</a></span>
<span class=normal><a href=#__codelineno-0-53> 53</a></span>
<span class=normal><a href=#__codelineno-0-54> 54</a></span>
<span class=normal><a href=#__codelineno-0-55> 55</a></span>
<span class=normal><a href=#__codelineno-0-56> 56</a></span>
<span class=normal><a href=#__codelineno-0-57> 57</a></span>
<span class=normal><a href=#__codelineno-0-58> 58</a></span>
<span class=normal><a href=#__codelineno-0-59> 59</a></span>
<span class=normal><a href=#__codelineno-0-60> 60</a></span>
<span class=normal><a href=#__codelineno-0-61> 61</a></span>
<span class=normal><a href=#__codelineno-0-62> 62</a></span>
<span class=normal><a href=#__codelineno-0-63> 63</a></span>
<span class=normal><a href=#__codelineno-0-64> 64</a></span>
<span class=normal><a href=#__codelineno-0-65> 65</a></span>
<span class=normal><a href=#__codelineno-0-66> 66</a></span>
<span class=normal><a href=#__codelineno-0-67> 67</a></span>
<span class=normal><a href=#__codelineno-0-68> 68</a></span>
<span class=normal><a href=#__codelineno-0-69> 69</a></span>
<span class=normal><a href=#__codelineno-0-70> 70</a></span>
<span class=normal><a href=#__codelineno-0-71> 71</a></span>
<span class=normal><a href=#__codelineno-0-72> 72</a></span>
<span class=normal><a href=#__codelineno-0-73> 73</a></span>
<span class=normal><a href=#__codelineno-0-74> 74</a></span>
<span class=normal><a href=#__codelineno-0-75> 75</a></span>
<span class=normal><a href=#__codelineno-0-76> 76</a></span>
<span class=normal><a href=#__codelineno-0-77> 77</a></span>
<span class=normal><a href=#__codelineno-0-78> 78</a></span>
<span class=normal><a href=#__codelineno-0-79> 79</a></span>
<span class=normal><a href=#__codelineno-0-80> 80</a></span>
<span class=normal><a href=#__codelineno-0-81> 81</a></span>
<span class=normal><a href=#__codelineno-0-82> 82</a></span>
<span class=normal><a href=#__codelineno-0-83> 83</a></span>
<span class=normal><a href=#__codelineno-0-84> 84</a></span>
<span class=normal><a href=#__codelineno-0-85> 85</a></span>
<span class=normal><a href=#__codelineno-0-86> 86</a></span>
<span class=normal><a href=#__codelineno-0-87> 87</a></span>
<span class=normal><a href=#__codelineno-0-88> 88</a></span>
<span class=normal><a href=#__codelineno-0-89> 89</a></span>
<span class=normal><a href=#__codelineno-0-90> 90</a></span>
<span class=normal><a href=#__codelineno-0-91> 91</a></span>
<span class=normal><a href=#__codelineno-0-92> 92</a></span>
<span class=normal><a href=#__codelineno-0-93> 93</a></span>
<span class=normal><a href=#__codelineno-0-94> 94</a></span>
<span class=normal><a href=#__codelineno-0-95> 95</a></span>
<span class=normal><a href=#__codelineno-0-96> 96</a></span>
<span class=normal><a href=#__codelineno-0-97> 97</a></span>
<span class=normal><a href=#__codelineno-0-98> 98</a></span>
<span class=normal><a href=#__codelineno-0-99> 99</a></span>
<span class=normal><a href=#__codelineno-0-100>100</a></span>
<span class=normal><a href=#__codelineno-0-101>101</a></span>
<span class=normal><a href=#__codelineno-0-102>102</a></span>
<span class=normal><a href=#__codelineno-0-103>103</a></span>
<span class=normal><a href=#__codelineno-0-104>104</a></span>
<span class=normal><a href=#__codelineno-0-105>105</a></span>
<span class=normal><a href=#__codelineno-0-106>106</a></span>
<span class=normal><a href=#__codelineno-0-107>107</a></span>
<span class=normal><a href=#__codelineno-0-108>108</a></span>
<span class=normal><a href=#__codelineno-0-109>109</a></span>
<span class=normal><a href=#__codelineno-0-110>110</a></span>
<span class=normal><a href=#__codelineno-0-111>111</a></span>
<span class=normal><a href=#__codelineno-0-112>112</a></span>
<span class=normal><a href=#__codelineno-0-113>113</a></span>
<span class=normal><a href=#__codelineno-0-114>114</a></span>
<span class=normal><a href=#__codelineno-0-115>115</a></span>
<span class=normal><a href=#__codelineno-0-116>116</a></span>
<span class=normal><a href=#__codelineno-0-117>117</a></span>
<span class=normal><a href=#__codelineno-0-118>118</a></span>
<span class=normal><a href=#__codelineno-0-119>119</a></span>
<span class=normal><a href=#__codelineno-0-120>120</a></span>
<span class=normal><a href=#__codelineno-0-121>121</a></span>
<span class=normal><a href=#__codelineno-0-122>122</a></span>
<span class=normal><a href=#__codelineno-0-123>123</a></span>
<span class=normal><a href=#__codelineno-0-124>124</a></span>
<span class=normal><a href=#__codelineno-0-125>125</a></span>
<span class=normal><a href=#__codelineno-0-126>126</a></span>
<span class=normal><a href=#__codelineno-0-127>127</a></span>
<span class=normal><a href=#__codelineno-0-128>128</a></span>
<span class=normal><a href=#__codelineno-0-129>129</a></span>
<span class=normal><a href=#__codelineno-0-130>130</a></span>
<span class=normal><a href=#__codelineno-0-131>131</a></span>
<span class=normal><a href=#__codelineno-0-132>132</a></span>
<span class=normal><a href=#__codelineno-0-133>133</a></span>
<span class=normal><a href=#__codelineno-0-134>134</a></span>
<span class=normal><a href=#__codelineno-0-135>135</a></span>
<span class=normal><a href=#__codelineno-0-136>136</a></span>
<span class=normal><a href=#__codelineno-0-137>137</a></span>
<span class=normal><a href=#__codelineno-0-138>138</a></span>
<span class=normal><a href=#__codelineno-0-139>139</a></span>
<span class=normal><a href=#__codelineno-0-140>140</a></span>
<span class=normal><a href=#__codelineno-0-141>141</a></span>
<span class=normal><a href=#__codelineno-0-142>142</a></span>
<span class=normal><a href=#__codelineno-0-143>143</a></span>
<span class=normal><a href=#__codelineno-0-144>144</a></span>
<span class=normal><a href=#__codelineno-0-145>145</a></span>
<span class=normal><a href=#__codelineno-0-146>146</a></span>
<span class=normal><a href=#__codelineno-0-147>147</a></span>
<span class=normal><a href=#__codelineno-0-148>148</a></span>
<span class=normal><a href=#__codelineno-0-149>149</a></span>
<span class=normal><a href=#__codelineno-0-150>150</a></span>
<span class=normal><a href=#__codelineno-0-151>151</a></span>
<span class=normal><a href=#__codelineno-0-152>152</a></span>
<span class=normal><a href=#__codelineno-0-153>153</a></span>
<span class=normal><a href=#__codelineno-0-154>154</a></span>
<span class=normal><a href=#__codelineno-0-155>155</a></span>
<span class=normal><a href=#__codelineno-0-156>156</a></span>
<span class=normal><a href=#__codelineno-0-157>157</a></span>
<span class=normal><a href=#__codelineno-0-158>158</a></span>
<span class=normal><a href=#__codelineno-0-159>159</a></span>
<span class=normal><a href=#__codelineno-0-160>160</a></span>
<span class=normal><a href=#__codelineno-0-161>161</a></span>
<span class=normal><a href=#__codelineno-0-162>162</a></span>
<span class=normal><a href=#__codelineno-0-163>163</a></span>
<span class=normal><a href=#__codelineno-0-164>164</a></span>
<span class=normal><a href=#__codelineno-0-165>165</a></span>
<span class=normal><a href=#__codelineno-0-166>166</a></span>
<span class=normal><a href=#__codelineno-0-167>167</a></span>
<span class=normal><a href=#__codelineno-0-168>168</a></span>
<span class=normal><a href=#__codelineno-0-169>169</a></span>
<span class=normal><a href=#__codelineno-0-170>170</a></span>
<span class=normal><a href=#__codelineno-0-171>171</a></span>
<span class=normal><a href=#__codelineno-0-172>172</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-26><a id=__codelineno-0-26 name=__codelineno-0-26></a><span class=k>class</span> <span class=nc>ResidualBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-27><a id=__codelineno-0-27 name=__codelineno-0-27></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-28><a id=__codelineno-0-28 name=__codelineno-0-28></a><span class=sd>    Residual block with 2 convolutional layers.</span>
</span><span id=__span-0-29><a id=__codelineno-0-29 name=__codelineno-0-29></a>
</span><span id=__span-0-30><a id=__codelineno-0-30 name=__codelineno-0-30></a><span class=sd>    Some architectural notes:</span>
</span><span id=__span-0-31><a id=__codelineno-0-31 name=__codelineno-0-31></a><span class=sd>        - The number of input, intermediate, and output channels is the same,</span>
</span><span id=__span-0-32><a id=__codelineno-0-32 name=__codelineno-0-32></a><span class=sd>        - Padding is always &#39;same&#39;,</span>
</span><span id=__span-0-33><a id=__codelineno-0-33 name=__codelineno-0-33></a><span class=sd>        - The 2 convolutional layers have the same groups,</span>
</span><span id=__span-0-34><a id=__codelineno-0-34 name=__codelineno-0-34></a><span class=sd>        - No stride allowed,</span>
</span><span id=__span-0-35><a id=__codelineno-0-35 name=__codelineno-0-35></a><span class=sd>        - Kernel sizes must be odd.</span>
</span><span id=__span-0-36><a id=__codelineno-0-36 name=__codelineno-0-36></a>
</span><span id=__span-0-37><a id=__codelineno-0-37 name=__codelineno-0-37></a><span class=sd>    The output isgiven by: `out = gate(f(x)) + x`.</span>
</span><span id=__span-0-38><a id=__codelineno-0-38 name=__codelineno-0-38></a><span class=sd>    The presence of the gating mechanism is optional, and f(x) has different</span>
</span><span id=__span-0-39><a id=__codelineno-0-39 name=__codelineno-0-39></a><span class=sd>    structures depending on the `block_type` argument.</span>
</span><span id=__span-0-40><a id=__codelineno-0-40 name=__codelineno-0-40></a><span class=sd>    Specifically, `block_type` is a string specifying the block&#39;s structure, with:</span>
</span><span id=__span-0-41><a id=__codelineno-0-41 name=__codelineno-0-41></a><span class=sd>        a = activation</span>
</span><span id=__span-0-42><a id=__codelineno-0-42 name=__codelineno-0-42></a><span class=sd>        b = batch norm</span>
</span><span id=__span-0-43><a id=__codelineno-0-43 name=__codelineno-0-43></a><span class=sd>        c = conv layer</span>
</span><span id=__span-0-44><a id=__codelineno-0-44 name=__codelineno-0-44></a><span class=sd>        d = dropout.</span>
</span><span id=__span-0-45><a id=__codelineno-0-45 name=__codelineno-0-45></a><span class=sd>    For example, &quot;bacdbacd&quot; defines a block with 2x[batchnorm, activation, conv, dropout].</span>
</span><span id=__span-0-46><a id=__codelineno-0-46 name=__codelineno-0-46></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-47><a id=__codelineno-0-47 name=__codelineno-0-47></a>
</span><span id=__span-0-48><a id=__codelineno-0-48 name=__codelineno-0-48></a>    <span class=n>default_kernel_size</span> <span class=o>=</span> <span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span><span id=__span-0-49><a id=__codelineno-0-49 name=__codelineno-0-49></a>
</span><span id=__span-0-50><a id=__codelineno-0-50 name=__codelineno-0-50></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-51><a id=__codelineno-0-51 name=__codelineno-0-51></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-52><a id=__codelineno-0-52 name=__codelineno-0-52></a>        <span class=n>channels</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-53><a id=__codelineno-0-53 name=__codelineno-0-53></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span><span class=p>,</span>
</span><span id=__span-0-54><a id=__codelineno-0-54 name=__codelineno-0-54></a>        <span class=n>kernel</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-55><a id=__codelineno-0-55 name=__codelineno-0-55></a>        <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-56><a id=__codelineno-0-56 name=__codelineno-0-56></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-57><a id=__codelineno-0-57 name=__codelineno-0-57></a>        <span class=n>block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-58><a id=__codelineno-0-58 name=__codelineno-0-58></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-59><a id=__codelineno-0-59 name=__codelineno-0-59></a>        <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-60><a id=__codelineno-0-60 name=__codelineno-0-60></a>        <span class=n>skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-61><a id=__codelineno-0-61 name=__codelineno-0-61></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-62><a id=__codelineno-0-62 name=__codelineno-0-62></a>    <span class=p>):</span>
</span><span id=__span-0-63><a id=__codelineno-0-63 name=__codelineno-0-63></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-64><a id=__codelineno-0-64 name=__codelineno-0-64></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-65><a id=__codelineno-0-65 name=__codelineno-0-65></a>
</span><span id=__span-0-66><a id=__codelineno-0-66 name=__codelineno-0-66></a><span class=sd>        Parameters</span>
</span><span id=__span-0-67><a id=__codelineno-0-67 name=__codelineno-0-67></a><span class=sd>        ----------</span>
</span><span id=__span-0-68><a id=__codelineno-0-68 name=__codelineno-0-68></a><span class=sd>        channels: int</span>
</span><span id=__span-0-69><a id=__codelineno-0-69 name=__codelineno-0-69></a><span class=sd>            The number of input and output channels (they are the same).</span>
</span><span id=__span-0-70><a id=__codelineno-0-70 name=__codelineno-0-70></a><span class=sd>        nonlin: Callable</span>
</span><span id=__span-0-71><a id=__codelineno-0-71 name=__codelineno-0-71></a><span class=sd>            The non-linearity function used in the block (e.g., `nn.ReLU`).</span>
</span><span id=__span-0-72><a id=__codelineno-0-72 name=__codelineno-0-72></a><span class=sd>        kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-73><a id=__codelineno-0-73 name=__codelineno-0-73></a><span class=sd>            The kernel size used in the convolutions of the block.</span>
</span><span id=__span-0-74><a id=__codelineno-0-74 name=__codelineno-0-74></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-75><a id=__codelineno-0-75 name=__codelineno-0-75></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-76><a id=__codelineno-0-76 name=__codelineno-0-76></a><span class=sd>        groups: int, optional</span>
</span><span id=__span-0-77><a id=__codelineno-0-77 name=__codelineno-0-77></a><span class=sd>            The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-78><a id=__codelineno-0-78 name=__codelineno-0-78></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-79><a id=__codelineno-0-79 name=__codelineno-0-79></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-80><a id=__codelineno-0-80 name=__codelineno-0-80></a><span class=sd>        block_type: str, optional</span>
</span><span id=__span-0-81><a id=__codelineno-0-81 name=__codelineno-0-81></a><span class=sd>            A string specifying the block structure, check class docstring for more info.</span>
</span><span id=__span-0-82><a id=__codelineno-0-82 name=__codelineno-0-82></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-83><a id=__codelineno-0-83 name=__codelineno-0-83></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-84><a id=__codelineno-0-84 name=__codelineno-0-84></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-85><a id=__codelineno-0-85 name=__codelineno-0-85></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-86><a id=__codelineno-0-86 name=__codelineno-0-86></a><span class=sd>        gated: bool, optional</span>
</span><span id=__span-0-87><a id=__codelineno-0-87 name=__codelineno-0-87></a><span class=sd>            Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-88><a id=__codelineno-0-88 name=__codelineno-0-88></a><span class=sd>        skip_padding: bool, optional</span>
</span><span id=__span-0-89><a id=__codelineno-0-89 name=__codelineno-0-89></a><span class=sd>            Whether to skip padding in convolutions. Default is `False`.</span>
</span><span id=__span-0-90><a id=__codelineno-0-90 name=__codelineno-0-90></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-91><a id=__codelineno-0-91 name=__codelineno-0-91></a><span class=sd>            Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-92><a id=__codelineno-0-92 name=__codelineno-0-92></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-93><a id=__codelineno-0-93 name=__codelineno-0-93></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-94><a id=__codelineno-0-94 name=__codelineno-0-94></a>
</span><span id=__span-0-95><a id=__codelineno-0-95 name=__codelineno-0-95></a>        <span class=c1># Set kernel size &amp; padding</span>
</span><span id=__span-0-96><a id=__codelineno-0-96 name=__codelineno-0-96></a>        <span class=k>if</span> <span class=n>kernel</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-97><a id=__codelineno-0-97 name=__codelineno-0-97></a>            <span class=n>kernel</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>default_kernel_size</span>
</span><span id=__span-0-98><a id=__codelineno-0-98 name=__codelineno-0-98></a>        <span class=k>elif</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>kernel</span><span class=p>,</span> <span class=nb>int</span><span class=p>):</span>
</span><span id=__span-0-99><a id=__codelineno-0-99 name=__codelineno-0-99></a>            <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=n>kernel</span><span class=p>,</span> <span class=n>kernel</span><span class=p>)</span>
</span><span id=__span-0-100><a id=__codelineno-0-100 name=__codelineno-0-100></a>        <span class=k>elif</span> <span class=nb>len</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span> <span class=o>!=</span> <span class=mi>2</span><span class=p>:</span>
</span><span id=__span-0-101><a id=__codelineno-0-101 name=__codelineno-0-101></a>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;kernel has to be None, int, or an iterable of length 2&quot;</span><span class=p>)</span>
</span><span id=__span-0-102><a id=__codelineno-0-102 name=__codelineno-0-102></a>        <span class=k>assert</span> <span class=nb>all</span><span class=p>([</span><span class=n>k</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>kernel</span><span class=p>]),</span> <span class=s2>&quot;kernel sizes have to be odd&quot;</span>
</span><span id=__span-0-103><a id=__codelineno-0-103 name=__codelineno-0-103></a>        <span class=n>kernel</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span>
</span><span id=__span-0-104><a id=__codelineno-0-104 name=__codelineno-0-104></a>        <span class=bp>self</span><span class=o>.</span><span class=n>skip_padding</span> <span class=o>=</span> <span class=n>skip_padding</span>
</span><span id=__span-0-105><a id=__codelineno-0-105 name=__codelineno-0-105></a>        <span class=n>pad</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>skip_padding</span> <span class=k>else</span> <span class=p>[</span><span class=n>k</span> <span class=o>//</span> <span class=mi>2</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>kernel</span><span class=p>]</span>
</span><span id=__span-0-106><a id=__codelineno-0-106 name=__codelineno-0-106></a>        <span class=c1># print(kernel, pad)</span>
</span><span id=__span-0-107><a id=__codelineno-0-107 name=__codelineno-0-107></a>
</span><span id=__span-0-108><a id=__codelineno-0-108 name=__codelineno-0-108></a>        <span class=n>modules</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-109><a id=__codelineno-0-109 name=__codelineno-0-109></a>        <span class=k>if</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;cabdcabd&quot;</span><span class=p>:</span>
</span><span id=__span-0-110><a id=__codelineno-0-110 name=__codelineno-0-110></a>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-111><a id=__codelineno-0-111 name=__codelineno-0-111></a>                <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-112><a id=__codelineno-0-112 name=__codelineno-0-112></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-113><a id=__codelineno-0-113 name=__codelineno-0-113></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-114><a id=__codelineno-0-114 name=__codelineno-0-114></a>                    <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-115><a id=__codelineno-0-115 name=__codelineno-0-115></a>                    <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-116><a id=__codelineno-0-116 name=__codelineno-0-116></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-117><a id=__codelineno-0-117 name=__codelineno-0-117></a>                    <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-118><a id=__codelineno-0-118 name=__codelineno-0-118></a>                <span class=p>)</span>
</span><span id=__span-0-119><a id=__codelineno-0-119 name=__codelineno-0-119></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-120><a id=__codelineno-0-120 name=__codelineno-0-120></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-121><a id=__codelineno-0-121 name=__codelineno-0-121></a>                <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-122><a id=__codelineno-0-122 name=__codelineno-0-122></a>                    <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-123><a id=__codelineno-0-123 name=__codelineno-0-123></a>                <span class=k>if</span> <span class=n>dropout</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-124><a id=__codelineno-0-124 name=__codelineno-0-124></a>                    <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-125><a id=__codelineno-0-125 name=__codelineno-0-125></a>        <span class=k>elif</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;bacdbac&quot;</span><span class=p>:</span>
</span><span id=__span-0-126><a id=__codelineno-0-126 name=__codelineno-0-126></a>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-127><a id=__codelineno-0-127 name=__codelineno-0-127></a>                <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-128><a id=__codelineno-0-128 name=__codelineno-0-128></a>                    <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-129><a id=__codelineno-0-129 name=__codelineno-0-129></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-130><a id=__codelineno-0-130 name=__codelineno-0-130></a>                <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-131><a id=__codelineno-0-131 name=__codelineno-0-131></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-132><a id=__codelineno-0-132 name=__codelineno-0-132></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-133><a id=__codelineno-0-133 name=__codelineno-0-133></a>                    <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-134><a id=__codelineno-0-134 name=__codelineno-0-134></a>                    <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-135><a id=__codelineno-0-135 name=__codelineno-0-135></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-136><a id=__codelineno-0-136 name=__codelineno-0-136></a>                    <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-137><a id=__codelineno-0-137 name=__codelineno-0-137></a>                <span class=p>)</span>
</span><span id=__span-0-138><a id=__codelineno-0-138 name=__codelineno-0-138></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-139><a id=__codelineno-0-139 name=__codelineno-0-139></a>                <span class=k>if</span> <span class=n>dropout</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-140><a id=__codelineno-0-140 name=__codelineno-0-140></a>                    <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-141><a id=__codelineno-0-141 name=__codelineno-0-141></a>        <span class=k>elif</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;bacdbacd&quot;</span><span class=p>:</span>
</span><span id=__span-0-142><a id=__codelineno-0-142 name=__codelineno-0-142></a>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-143><a id=__codelineno-0-143 name=__codelineno-0-143></a>                <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-144><a id=__codelineno-0-144 name=__codelineno-0-144></a>                    <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-145><a id=__codelineno-0-145 name=__codelineno-0-145></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-146><a id=__codelineno-0-146 name=__codelineno-0-146></a>                <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-147><a id=__codelineno-0-147 name=__codelineno-0-147></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-148><a id=__codelineno-0-148 name=__codelineno-0-148></a>                    <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-149><a id=__codelineno-0-149 name=__codelineno-0-149></a>                    <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-150><a id=__codelineno-0-150 name=__codelineno-0-150></a>                    <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-151><a id=__codelineno-0-151 name=__codelineno-0-151></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-152><a id=__codelineno-0-152 name=__codelineno-0-152></a>                    <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-153><a id=__codelineno-0-153 name=__codelineno-0-153></a>                <span class=p>)</span>
</span><span id=__span-0-154><a id=__codelineno-0-154 name=__codelineno-0-154></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-155><a id=__codelineno-0-155 name=__codelineno-0-155></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-156><a id=__codelineno-0-156 name=__codelineno-0-156></a>
</span><span id=__span-0-157><a id=__codelineno-0-157 name=__codelineno-0-157></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-158><a id=__codelineno-0-158 name=__codelineno-0-158></a>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;unrecognized block type &#39;</span><span class=si>{</span><span class=n>block_type</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span><span id=__span-0-159><a id=__codelineno-0-159 name=__codelineno-0-159></a>
</span><span id=__span-0-160><a id=__codelineno-0-160 name=__codelineno-0-160></a>        <span class=bp>self</span><span class=o>.</span><span class=n>gated</span> <span class=o>=</span> <span class=n>gated</span>
</span><span id=__span-0-161><a id=__codelineno-0-161 name=__codelineno-0-161></a>        <span class=k>if</span> <span class=n>gated</span><span class=p>:</span>
</span><span id=__span-0-162><a id=__codelineno-0-162 name=__codelineno-0-162></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>GateLayer2d</span><span class=p>(</span><span class=n>channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>nonlin</span><span class=p>))</span>
</span><span id=__span-0-163><a id=__codelineno-0-163 name=__codelineno-0-163></a>
</span><span id=__span-0-164><a id=__codelineno-0-164 name=__codelineno-0-164></a>        <span class=bp>self</span><span class=o>.</span><span class=n>block</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>modules</span><span class=p>)</span>
</span><span id=__span-0-165><a id=__codelineno-0-165 name=__codelineno-0-165></a>
</span><span id=__span-0-166><a id=__codelineno-0-166 name=__codelineno-0-166></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span><span id=__span-0-167><a id=__codelineno-0-167 name=__codelineno-0-167></a>
</span><span id=__span-0-168><a id=__codelineno-0-168 name=__codelineno-0-168></a>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>block</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-169><a id=__codelineno-0-169 name=__codelineno-0-169></a>        <span class=k>if</span> <span class=n>out</span><span class=o>.</span><span class=n>shape</span> <span class=o>!=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>:</span>
</span><span id=__span-0-170><a id=__codelineno-0-170 name=__codelineno-0-170></a>            <span class=k>return</span> <span class=n>out</span> <span class=o>+</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-171><a id=__codelineno-0-171 name=__codelineno-0-171></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-172><a id=__codelineno-0-172 name=__codelineno-0-172></a>            <span class=k>return</span> <span class=n>out</span> <span class=o>+</span> <span class=n>x</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.ResidualBlock.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>channels</span><span class=p>,</span> <span class=n>nonlin</span><span class=p>,</span> <span class=n>kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>batchnorm</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>block_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>gated</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>skip_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.ResidualBlock.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>channels</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of input and output channels (they are the same).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>nonlin</code></td> <td> <code><span title="typing.Callable">Callable</span></code> </td> <td> <div class=doc-md-description> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>kernel</code></td> <td> <code><span title="typing.Union">Union</span>[int, <span title="typing.Iterable">Iterable</span>[int]]</code> </td> <td> <div class=doc-md-description> <p>The kernel size used in the convolutions of the block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>groups</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of groups to consider in the convolutions. Default is 1.</p> </div> </td> <td> <code>1</code> </td> </tr> <tr class=doc-section-item> <td><code>batchnorm</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>block_type</code></td> <td> <code>str</code> </td> <td> <div class=doc-md-description> <p>A string specifying the block structure, check class docstring for more info. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>dropout</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>gated</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use gated layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>skip_padding</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to skip padding in convolutions. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>conv2d_bias</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use bias term in convolutions. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-50> 50</a></span>
<span class=normal><a href=#__codelineno-0-51> 51</a></span>
<span class=normal><a href=#__codelineno-0-52> 52</a></span>
<span class=normal><a href=#__codelineno-0-53> 53</a></span>
<span class=normal><a href=#__codelineno-0-54> 54</a></span>
<span class=normal><a href=#__codelineno-0-55> 55</a></span>
<span class=normal><a href=#__codelineno-0-56> 56</a></span>
<span class=normal><a href=#__codelineno-0-57> 57</a></span>
<span class=normal><a href=#__codelineno-0-58> 58</a></span>
<span class=normal><a href=#__codelineno-0-59> 59</a></span>
<span class=normal><a href=#__codelineno-0-60> 60</a></span>
<span class=normal><a href=#__codelineno-0-61> 61</a></span>
<span class=normal><a href=#__codelineno-0-62> 62</a></span>
<span class=normal><a href=#__codelineno-0-63> 63</a></span>
<span class=normal><a href=#__codelineno-0-64> 64</a></span>
<span class=normal><a href=#__codelineno-0-65> 65</a></span>
<span class=normal><a href=#__codelineno-0-66> 66</a></span>
<span class=normal><a href=#__codelineno-0-67> 67</a></span>
<span class=normal><a href=#__codelineno-0-68> 68</a></span>
<span class=normal><a href=#__codelineno-0-69> 69</a></span>
<span class=normal><a href=#__codelineno-0-70> 70</a></span>
<span class=normal><a href=#__codelineno-0-71> 71</a></span>
<span class=normal><a href=#__codelineno-0-72> 72</a></span>
<span class=normal><a href=#__codelineno-0-73> 73</a></span>
<span class=normal><a href=#__codelineno-0-74> 74</a></span>
<span class=normal><a href=#__codelineno-0-75> 75</a></span>
<span class=normal><a href=#__codelineno-0-76> 76</a></span>
<span class=normal><a href=#__codelineno-0-77> 77</a></span>
<span class=normal><a href=#__codelineno-0-78> 78</a></span>
<span class=normal><a href=#__codelineno-0-79> 79</a></span>
<span class=normal><a href=#__codelineno-0-80> 80</a></span>
<span class=normal><a href=#__codelineno-0-81> 81</a></span>
<span class=normal><a href=#__codelineno-0-82> 82</a></span>
<span class=normal><a href=#__codelineno-0-83> 83</a></span>
<span class=normal><a href=#__codelineno-0-84> 84</a></span>
<span class=normal><a href=#__codelineno-0-85> 85</a></span>
<span class=normal><a href=#__codelineno-0-86> 86</a></span>
<span class=normal><a href=#__codelineno-0-87> 87</a></span>
<span class=normal><a href=#__codelineno-0-88> 88</a></span>
<span class=normal><a href=#__codelineno-0-89> 89</a></span>
<span class=normal><a href=#__codelineno-0-90> 90</a></span>
<span class=normal><a href=#__codelineno-0-91> 91</a></span>
<span class=normal><a href=#__codelineno-0-92> 92</a></span>
<span class=normal><a href=#__codelineno-0-93> 93</a></span>
<span class=normal><a href=#__codelineno-0-94> 94</a></span>
<span class=normal><a href=#__codelineno-0-95> 95</a></span>
<span class=normal><a href=#__codelineno-0-96> 96</a></span>
<span class=normal><a href=#__codelineno-0-97> 97</a></span>
<span class=normal><a href=#__codelineno-0-98> 98</a></span>
<span class=normal><a href=#__codelineno-0-99> 99</a></span>
<span class=normal><a href=#__codelineno-0-100>100</a></span>
<span class=normal><a href=#__codelineno-0-101>101</a></span>
<span class=normal><a href=#__codelineno-0-102>102</a></span>
<span class=normal><a href=#__codelineno-0-103>103</a></span>
<span class=normal><a href=#__codelineno-0-104>104</a></span>
<span class=normal><a href=#__codelineno-0-105>105</a></span>
<span class=normal><a href=#__codelineno-0-106>106</a></span>
<span class=normal><a href=#__codelineno-0-107>107</a></span>
<span class=normal><a href=#__codelineno-0-108>108</a></span>
<span class=normal><a href=#__codelineno-0-109>109</a></span>
<span class=normal><a href=#__codelineno-0-110>110</a></span>
<span class=normal><a href=#__codelineno-0-111>111</a></span>
<span class=normal><a href=#__codelineno-0-112>112</a></span>
<span class=normal><a href=#__codelineno-0-113>113</a></span>
<span class=normal><a href=#__codelineno-0-114>114</a></span>
<span class=normal><a href=#__codelineno-0-115>115</a></span>
<span class=normal><a href=#__codelineno-0-116>116</a></span>
<span class=normal><a href=#__codelineno-0-117>117</a></span>
<span class=normal><a href=#__codelineno-0-118>118</a></span>
<span class=normal><a href=#__codelineno-0-119>119</a></span>
<span class=normal><a href=#__codelineno-0-120>120</a></span>
<span class=normal><a href=#__codelineno-0-121>121</a></span>
<span class=normal><a href=#__codelineno-0-122>122</a></span>
<span class=normal><a href=#__codelineno-0-123>123</a></span>
<span class=normal><a href=#__codelineno-0-124>124</a></span>
<span class=normal><a href=#__codelineno-0-125>125</a></span>
<span class=normal><a href=#__codelineno-0-126>126</a></span>
<span class=normal><a href=#__codelineno-0-127>127</a></span>
<span class=normal><a href=#__codelineno-0-128>128</a></span>
<span class=normal><a href=#__codelineno-0-129>129</a></span>
<span class=normal><a href=#__codelineno-0-130>130</a></span>
<span class=normal><a href=#__codelineno-0-131>131</a></span>
<span class=normal><a href=#__codelineno-0-132>132</a></span>
<span class=normal><a href=#__codelineno-0-133>133</a></span>
<span class=normal><a href=#__codelineno-0-134>134</a></span>
<span class=normal><a href=#__codelineno-0-135>135</a></span>
<span class=normal><a href=#__codelineno-0-136>136</a></span>
<span class=normal><a href=#__codelineno-0-137>137</a></span>
<span class=normal><a href=#__codelineno-0-138>138</a></span>
<span class=normal><a href=#__codelineno-0-139>139</a></span>
<span class=normal><a href=#__codelineno-0-140>140</a></span>
<span class=normal><a href=#__codelineno-0-141>141</a></span>
<span class=normal><a href=#__codelineno-0-142>142</a></span>
<span class=normal><a href=#__codelineno-0-143>143</a></span>
<span class=normal><a href=#__codelineno-0-144>144</a></span>
<span class=normal><a href=#__codelineno-0-145>145</a></span>
<span class=normal><a href=#__codelineno-0-146>146</a></span>
<span class=normal><a href=#__codelineno-0-147>147</a></span>
<span class=normal><a href=#__codelineno-0-148>148</a></span>
<span class=normal><a href=#__codelineno-0-149>149</a></span>
<span class=normal><a href=#__codelineno-0-150>150</a></span>
<span class=normal><a href=#__codelineno-0-151>151</a></span>
<span class=normal><a href=#__codelineno-0-152>152</a></span>
<span class=normal><a href=#__codelineno-0-153>153</a></span>
<span class=normal><a href=#__codelineno-0-154>154</a></span>
<span class=normal><a href=#__codelineno-0-155>155</a></span>
<span class=normal><a href=#__codelineno-0-156>156</a></span>
<span class=normal><a href=#__codelineno-0-157>157</a></span>
<span class=normal><a href=#__codelineno-0-158>158</a></span>
<span class=normal><a href=#__codelineno-0-159>159</a></span>
<span class=normal><a href=#__codelineno-0-160>160</a></span>
<span class=normal><a href=#__codelineno-0-161>161</a></span>
<span class=normal><a href=#__codelineno-0-162>162</a></span>
<span class=normal><a href=#__codelineno-0-163>163</a></span>
<span class=normal><a href=#__codelineno-0-164>164</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-50><a id=__codelineno-0-50 name=__codelineno-0-50></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-51><a id=__codelineno-0-51 name=__codelineno-0-51></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-52><a id=__codelineno-0-52 name=__codelineno-0-52></a>    <span class=n>channels</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-53><a id=__codelineno-0-53 name=__codelineno-0-53></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span><span class=p>,</span>
</span><span id=__span-0-54><a id=__codelineno-0-54 name=__codelineno-0-54></a>    <span class=n>kernel</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-55><a id=__codelineno-0-55 name=__codelineno-0-55></a>    <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-56><a id=__codelineno-0-56 name=__codelineno-0-56></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-57><a id=__codelineno-0-57 name=__codelineno-0-57></a>    <span class=n>block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-58><a id=__codelineno-0-58 name=__codelineno-0-58></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-59><a id=__codelineno-0-59 name=__codelineno-0-59></a>    <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-60><a id=__codelineno-0-60 name=__codelineno-0-60></a>    <span class=n>skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-61><a id=__codelineno-0-61 name=__codelineno-0-61></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-62><a id=__codelineno-0-62 name=__codelineno-0-62></a><span class=p>):</span>
</span><span id=__span-0-63><a id=__codelineno-0-63 name=__codelineno-0-63></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-64><a id=__codelineno-0-64 name=__codelineno-0-64></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-65><a id=__codelineno-0-65 name=__codelineno-0-65></a>
</span><span id=__span-0-66><a id=__codelineno-0-66 name=__codelineno-0-66></a><span class=sd>    Parameters</span>
</span><span id=__span-0-67><a id=__codelineno-0-67 name=__codelineno-0-67></a><span class=sd>    ----------</span>
</span><span id=__span-0-68><a id=__codelineno-0-68 name=__codelineno-0-68></a><span class=sd>    channels: int</span>
</span><span id=__span-0-69><a id=__codelineno-0-69 name=__codelineno-0-69></a><span class=sd>        The number of input and output channels (they are the same).</span>
</span><span id=__span-0-70><a id=__codelineno-0-70 name=__codelineno-0-70></a><span class=sd>    nonlin: Callable</span>
</span><span id=__span-0-71><a id=__codelineno-0-71 name=__codelineno-0-71></a><span class=sd>        The non-linearity function used in the block (e.g., `nn.ReLU`).</span>
</span><span id=__span-0-72><a id=__codelineno-0-72 name=__codelineno-0-72></a><span class=sd>    kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-73><a id=__codelineno-0-73 name=__codelineno-0-73></a><span class=sd>        The kernel size used in the convolutions of the block.</span>
</span><span id=__span-0-74><a id=__codelineno-0-74 name=__codelineno-0-74></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-75><a id=__codelineno-0-75 name=__codelineno-0-75></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-76><a id=__codelineno-0-76 name=__codelineno-0-76></a><span class=sd>    groups: int, optional</span>
</span><span id=__span-0-77><a id=__codelineno-0-77 name=__codelineno-0-77></a><span class=sd>        The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-78><a id=__codelineno-0-78 name=__codelineno-0-78></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-79><a id=__codelineno-0-79 name=__codelineno-0-79></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-80><a id=__codelineno-0-80 name=__codelineno-0-80></a><span class=sd>    block_type: str, optional</span>
</span><span id=__span-0-81><a id=__codelineno-0-81 name=__codelineno-0-81></a><span class=sd>        A string specifying the block structure, check class docstring for more info.</span>
</span><span id=__span-0-82><a id=__codelineno-0-82 name=__codelineno-0-82></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-83><a id=__codelineno-0-83 name=__codelineno-0-83></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-84><a id=__codelineno-0-84 name=__codelineno-0-84></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-85><a id=__codelineno-0-85 name=__codelineno-0-85></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-86><a id=__codelineno-0-86 name=__codelineno-0-86></a><span class=sd>    gated: bool, optional</span>
</span><span id=__span-0-87><a id=__codelineno-0-87 name=__codelineno-0-87></a><span class=sd>        Whether to use gated layer. Default is `None`.</span>
</span><span id=__span-0-88><a id=__codelineno-0-88 name=__codelineno-0-88></a><span class=sd>    skip_padding: bool, optional</span>
</span><span id=__span-0-89><a id=__codelineno-0-89 name=__codelineno-0-89></a><span class=sd>        Whether to skip padding in convolutions. Default is `False`.</span>
</span><span id=__span-0-90><a id=__codelineno-0-90 name=__codelineno-0-90></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-91><a id=__codelineno-0-91 name=__codelineno-0-91></a><span class=sd>        Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-92><a id=__codelineno-0-92 name=__codelineno-0-92></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-93><a id=__codelineno-0-93 name=__codelineno-0-93></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-94><a id=__codelineno-0-94 name=__codelineno-0-94></a>
</span><span id=__span-0-95><a id=__codelineno-0-95 name=__codelineno-0-95></a>    <span class=c1># Set kernel size &amp; padding</span>
</span><span id=__span-0-96><a id=__codelineno-0-96 name=__codelineno-0-96></a>    <span class=k>if</span> <span class=n>kernel</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-97><a id=__codelineno-0-97 name=__codelineno-0-97></a>        <span class=n>kernel</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>default_kernel_size</span>
</span><span id=__span-0-98><a id=__codelineno-0-98 name=__codelineno-0-98></a>    <span class=k>elif</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>kernel</span><span class=p>,</span> <span class=nb>int</span><span class=p>):</span>
</span><span id=__span-0-99><a id=__codelineno-0-99 name=__codelineno-0-99></a>        <span class=n>kernel</span> <span class=o>=</span> <span class=p>(</span><span class=n>kernel</span><span class=p>,</span> <span class=n>kernel</span><span class=p>)</span>
</span><span id=__span-0-100><a id=__codelineno-0-100 name=__codelineno-0-100></a>    <span class=k>elif</span> <span class=nb>len</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span> <span class=o>!=</span> <span class=mi>2</span><span class=p>:</span>
</span><span id=__span-0-101><a id=__codelineno-0-101 name=__codelineno-0-101></a>        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;kernel has to be None, int, or an iterable of length 2&quot;</span><span class=p>)</span>
</span><span id=__span-0-102><a id=__codelineno-0-102 name=__codelineno-0-102></a>    <span class=k>assert</span> <span class=nb>all</span><span class=p>([</span><span class=n>k</span> <span class=o>%</span> <span class=mi>2</span> <span class=o>==</span> <span class=mi>1</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>kernel</span><span class=p>]),</span> <span class=s2>&quot;kernel sizes have to be odd&quot;</span>
</span><span id=__span-0-103><a id=__codelineno-0-103 name=__codelineno-0-103></a>    <span class=n>kernel</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span>
</span><span id=__span-0-104><a id=__codelineno-0-104 name=__codelineno-0-104></a>    <span class=bp>self</span><span class=o>.</span><span class=n>skip_padding</span> <span class=o>=</span> <span class=n>skip_padding</span>
</span><span id=__span-0-105><a id=__codelineno-0-105 name=__codelineno-0-105></a>    <span class=n>pad</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>kernel</span><span class=p>)</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>skip_padding</span> <span class=k>else</span> <span class=p>[</span><span class=n>k</span> <span class=o>//</span> <span class=mi>2</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>kernel</span><span class=p>]</span>
</span><span id=__span-0-106><a id=__codelineno-0-106 name=__codelineno-0-106></a>    <span class=c1># print(kernel, pad)</span>
</span><span id=__span-0-107><a id=__codelineno-0-107 name=__codelineno-0-107></a>
</span><span id=__span-0-108><a id=__codelineno-0-108 name=__codelineno-0-108></a>    <span class=n>modules</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-109><a id=__codelineno-0-109 name=__codelineno-0-109></a>    <span class=k>if</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;cabdcabd&quot;</span><span class=p>:</span>
</span><span id=__span-0-110><a id=__codelineno-0-110 name=__codelineno-0-110></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-111><a id=__codelineno-0-111 name=__codelineno-0-111></a>            <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-112><a id=__codelineno-0-112 name=__codelineno-0-112></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-113><a id=__codelineno-0-113 name=__codelineno-0-113></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-114><a id=__codelineno-0-114 name=__codelineno-0-114></a>                <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-115><a id=__codelineno-0-115 name=__codelineno-0-115></a>                <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-116><a id=__codelineno-0-116 name=__codelineno-0-116></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-117><a id=__codelineno-0-117 name=__codelineno-0-117></a>                <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-118><a id=__codelineno-0-118 name=__codelineno-0-118></a>            <span class=p>)</span>
</span><span id=__span-0-119><a id=__codelineno-0-119 name=__codelineno-0-119></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-120><a id=__codelineno-0-120 name=__codelineno-0-120></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-121><a id=__codelineno-0-121 name=__codelineno-0-121></a>            <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-122><a id=__codelineno-0-122 name=__codelineno-0-122></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-123><a id=__codelineno-0-123 name=__codelineno-0-123></a>            <span class=k>if</span> <span class=n>dropout</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-124><a id=__codelineno-0-124 name=__codelineno-0-124></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-125><a id=__codelineno-0-125 name=__codelineno-0-125></a>    <span class=k>elif</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;bacdbac&quot;</span><span class=p>:</span>
</span><span id=__span-0-126><a id=__codelineno-0-126 name=__codelineno-0-126></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-127><a id=__codelineno-0-127 name=__codelineno-0-127></a>            <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-128><a id=__codelineno-0-128 name=__codelineno-0-128></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-129><a id=__codelineno-0-129 name=__codelineno-0-129></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-130><a id=__codelineno-0-130 name=__codelineno-0-130></a>            <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-131><a id=__codelineno-0-131 name=__codelineno-0-131></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-132><a id=__codelineno-0-132 name=__codelineno-0-132></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-133><a id=__codelineno-0-133 name=__codelineno-0-133></a>                <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-134><a id=__codelineno-0-134 name=__codelineno-0-134></a>                <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-135><a id=__codelineno-0-135 name=__codelineno-0-135></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-136><a id=__codelineno-0-136 name=__codelineno-0-136></a>                <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-137><a id=__codelineno-0-137 name=__codelineno-0-137></a>            <span class=p>)</span>
</span><span id=__span-0-138><a id=__codelineno-0-138 name=__codelineno-0-138></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-139><a id=__codelineno-0-139 name=__codelineno-0-139></a>            <span class=k>if</span> <span class=n>dropout</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>i</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-140><a id=__codelineno-0-140 name=__codelineno-0-140></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-141><a id=__codelineno-0-141 name=__codelineno-0-141></a>    <span class=k>elif</span> <span class=n>block_type</span> <span class=o>==</span> <span class=s2>&quot;bacdbacd&quot;</span><span class=p>:</span>
</span><span id=__span-0-142><a id=__codelineno-0-142 name=__codelineno-0-142></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>2</span><span class=p>):</span>
</span><span id=__span-0-143><a id=__codelineno-0-143 name=__codelineno-0-143></a>            <span class=k>if</span> <span class=n>batchnorm</span><span class=p>:</span>
</span><span id=__span-0-144><a id=__codelineno-0-144 name=__codelineno-0-144></a>                <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>BatchNorm2d</span><span class=p>(</span><span class=n>channels</span><span class=p>))</span>
</span><span id=__span-0-145><a id=__codelineno-0-145 name=__codelineno-0-145></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nonlin</span><span class=p>())</span>
</span><span id=__span-0-146><a id=__codelineno-0-146 name=__codelineno-0-146></a>            <span class=n>conv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span><span id=__span-0-147><a id=__codelineno-0-147 name=__codelineno-0-147></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-148><a id=__codelineno-0-148 name=__codelineno-0-148></a>                <span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-149><a id=__codelineno-0-149 name=__codelineno-0-149></a>                <span class=n>kernel</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-150><a id=__codelineno-0-150 name=__codelineno-0-150></a>                <span class=n>padding</span><span class=o>=</span><span class=n>pad</span><span class=p>[</span><span class=n>i</span><span class=p>],</span>
</span><span id=__span-0-151><a id=__codelineno-0-151 name=__codelineno-0-151></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-152><a id=__codelineno-0-152 name=__codelineno-0-152></a>                <span class=n>bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-153><a id=__codelineno-0-153 name=__codelineno-0-153></a>            <span class=p>)</span>
</span><span id=__span-0-154><a id=__codelineno-0-154 name=__codelineno-0-154></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>conv</span><span class=p>)</span>
</span><span id=__span-0-155><a id=__codelineno-0-155 name=__codelineno-0-155></a>            <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout2d</span><span class=p>(</span><span class=n>dropout</span><span class=p>))</span>
</span><span id=__span-0-156><a id=__codelineno-0-156 name=__codelineno-0-156></a>
</span><span id=__span-0-157><a id=__codelineno-0-157 name=__codelineno-0-157></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-158><a id=__codelineno-0-158 name=__codelineno-0-158></a>        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;unrecognized block type &#39;</span><span class=si>{</span><span class=n>block_type</span><span class=si>}</span><span class=s2>&#39;&quot;</span><span class=p>)</span>
</span><span id=__span-0-159><a id=__codelineno-0-159 name=__codelineno-0-159></a>
</span><span id=__span-0-160><a id=__codelineno-0-160 name=__codelineno-0-160></a>    <span class=bp>self</span><span class=o>.</span><span class=n>gated</span> <span class=o>=</span> <span class=n>gated</span>
</span><span id=__span-0-161><a id=__codelineno-0-161 name=__codelineno-0-161></a>    <span class=k>if</span> <span class=n>gated</span><span class=p>:</span>
</span><span id=__span-0-162><a id=__codelineno-0-162 name=__codelineno-0-162></a>        <span class=n>modules</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>GateLayer2d</span><span class=p>(</span><span class=n>channels</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>nonlin</span><span class=p>))</span>
</span><span id=__span-0-163><a id=__codelineno-0-163 name=__codelineno-0-163></a>
</span><span id=__span-0-164><a id=__codelineno-0-164 name=__codelineno-0-164></a>    <span class=bp>self</span><span class=o>.</span><span class=n>block</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>modules</span><span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.SkipConnectionMerger class="doc doc-heading"> <code>SkipConnectionMerger</code> <a href=#careamics.models.lvae.layers.SkipConnectionMerger class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><a class="autorefs autorefs-internal" title="careamics.models.lvae.layers.MergeLayer" href="#careamics.models.lvae.layers.MergeLayer">MergeLayer</a></code></p> <p>A specialized <code>MergeLayer</code> module, designed to handle skip connections in the model.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-753>753</a></span>
<span class=normal><a href=#__codelineno-0-754>754</a></span>
<span class=normal><a href=#__codelineno-0-755>755</a></span>
<span class=normal><a href=#__codelineno-0-756>756</a></span>
<span class=normal><a href=#__codelineno-0-757>757</a></span>
<span class=normal><a href=#__codelineno-0-758>758</a></span>
<span class=normal><a href=#__codelineno-0-759>759</a></span>
<span class=normal><a href=#__codelineno-0-760>760</a></span>
<span class=normal><a href=#__codelineno-0-761>761</a></span>
<span class=normal><a href=#__codelineno-0-762>762</a></span>
<span class=normal><a href=#__codelineno-0-763>763</a></span>
<span class=normal><a href=#__codelineno-0-764>764</a></span>
<span class=normal><a href=#__codelineno-0-765>765</a></span>
<span class=normal><a href=#__codelineno-0-766>766</a></span>
<span class=normal><a href=#__codelineno-0-767>767</a></span>
<span class=normal><a href=#__codelineno-0-768>768</a></span>
<span class=normal><a href=#__codelineno-0-769>769</a></span>
<span class=normal><a href=#__codelineno-0-770>770</a></span>
<span class=normal><a href=#__codelineno-0-771>771</a></span>
<span class=normal><a href=#__codelineno-0-772>772</a></span>
<span class=normal><a href=#__codelineno-0-773>773</a></span>
<span class=normal><a href=#__codelineno-0-774>774</a></span>
<span class=normal><a href=#__codelineno-0-775>775</a></span>
<span class=normal><a href=#__codelineno-0-776>776</a></span>
<span class=normal><a href=#__codelineno-0-777>777</a></span>
<span class=normal><a href=#__codelineno-0-778>778</a></span>
<span class=normal><a href=#__codelineno-0-779>779</a></span>
<span class=normal><a href=#__codelineno-0-780>780</a></span>
<span class=normal><a href=#__codelineno-0-781>781</a></span>
<span class=normal><a href=#__codelineno-0-782>782</a></span>
<span class=normal><a href=#__codelineno-0-783>783</a></span>
<span class=normal><a href=#__codelineno-0-784>784</a></span>
<span class=normal><a href=#__codelineno-0-785>785</a></span>
<span class=normal><a href=#__codelineno-0-786>786</a></span>
<span class=normal><a href=#__codelineno-0-787>787</a></span>
<span class=normal><a href=#__codelineno-0-788>788</a></span>
<span class=normal><a href=#__codelineno-0-789>789</a></span>
<span class=normal><a href=#__codelineno-0-790>790</a></span>
<span class=normal><a href=#__codelineno-0-791>791</a></span>
<span class=normal><a href=#__codelineno-0-792>792</a></span>
<span class=normal><a href=#__codelineno-0-793>793</a></span>
<span class=normal><a href=#__codelineno-0-794>794</a></span>
<span class=normal><a href=#__codelineno-0-795>795</a></span>
<span class=normal><a href=#__codelineno-0-796>796</a></span>
<span class=normal><a href=#__codelineno-0-797>797</a></span>
<span class=normal><a href=#__codelineno-0-798>798</a></span>
<span class=normal><a href=#__codelineno-0-799>799</a></span>
<span class=normal><a href=#__codelineno-0-800>800</a></span>
<span class=normal><a href=#__codelineno-0-801>801</a></span>
<span class=normal><a href=#__codelineno-0-802>802</a></span>
<span class=normal><a href=#__codelineno-0-803>803</a></span>
<span class=normal><a href=#__codelineno-0-804>804</a></span>
<span class=normal><a href=#__codelineno-0-805>805</a></span>
<span class=normal><a href=#__codelineno-0-806>806</a></span>
<span class=normal><a href=#__codelineno-0-807>807</a></span>
<span class=normal><a href=#__codelineno-0-808>808</a></span>
<span class=normal><a href=#__codelineno-0-809>809</a></span>
<span class=normal><a href=#__codelineno-0-810>810</a></span>
<span class=normal><a href=#__codelineno-0-811>811</a></span>
<span class=normal><a href=#__codelineno-0-812>812</a></span>
<span class=normal><a href=#__codelineno-0-813>813</a></span>
<span class=normal><a href=#__codelineno-0-814>814</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-753><a id=__codelineno-0-753 name=__codelineno-0-753></a><span class=k>class</span> <span class=nc>SkipConnectionMerger</span><span class=p>(</span><span class=n>MergeLayer</span><span class=p>):</span>
</span><span id=__span-0-754><a id=__codelineno-0-754 name=__codelineno-0-754></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-755><a id=__codelineno-0-755 name=__codelineno-0-755></a><span class=sd>    A specialized `MergeLayer` module, designed to handle skip connections in the model.</span>
</span><span id=__span-0-756><a id=__codelineno-0-756 name=__codelineno-0-756></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-757><a id=__codelineno-0-757 name=__codelineno-0-757></a>
</span><span id=__span-0-758><a id=__codelineno-0-758 name=__codelineno-0-758></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-759><a id=__codelineno-0-759 name=__codelineno-0-759></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-760><a id=__codelineno-0-760 name=__codelineno-0-760></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span><span class=p>,</span>
</span><span id=__span-0-761><a id=__codelineno-0-761 name=__codelineno-0-761></a>        <span class=n>channels</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span>
</span><span id=__span-0-762><a id=__codelineno-0-762 name=__codelineno-0-762></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-763><a id=__codelineno-0-763 name=__codelineno-0-763></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span><span id=__span-0-764><a id=__codelineno-0-764 name=__codelineno-0-764></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span><span id=__span-0-765><a id=__codelineno-0-765 name=__codelineno-0-765></a>        <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span>
</span><span id=__span-0-766><a id=__codelineno-0-766 name=__codelineno-0-766></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-767><a id=__codelineno-0-767 name=__codelineno-0-767></a>        <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-768><a id=__codelineno-0-768 name=__codelineno-0-768></a>        <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-769><a id=__codelineno-0-769 name=__codelineno-0-769></a>    <span class=p>):</span>
</span><span id=__span-0-770><a id=__codelineno-0-770 name=__codelineno-0-770></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-771><a id=__codelineno-0-771 name=__codelineno-0-771></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-772><a id=__codelineno-0-772 name=__codelineno-0-772></a>
</span><span id=__span-0-773><a id=__codelineno-0-773 name=__codelineno-0-773></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-774><a id=__codelineno-0-774 name=__codelineno-0-774></a><span class=sd>            The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-775><a id=__codelineno-0-775 name=__codelineno-0-775></a><span class=sd>        channels: Union[int, Iterable[int]]</span>
</span><span id=__span-0-776><a id=__codelineno-0-776 name=__codelineno-0-776></a><span class=sd>            The number of channels used in the convolutional blocks of this layer.</span>
</span><span id=__span-0-777><a id=__codelineno-0-777 name=__codelineno-0-777></a><span class=sd>            If it is an `int`:</span>
</span><span id=__span-0-778><a id=__codelineno-0-778 name=__codelineno-0-778></a><span class=sd>                - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels</span>
</span><span id=__span-0-779><a id=__codelineno-0-779 name=__codelineno-0-779></a><span class=sd>                - (Optional) ResBlock: in_channels=channels, out_channels=channels</span>
</span><span id=__span-0-780><a id=__codelineno-0-780 name=__codelineno-0-780></a><span class=sd>            If it is an Iterable (must have `len(channels)==3`):</span>
</span><span id=__span-0-781><a id=__codelineno-0-781 name=__codelineno-0-781></a><span class=sd>                - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]</span>
</span><span id=__span-0-782><a id=__codelineno-0-782 name=__codelineno-0-782></a><span class=sd>                - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]</span>
</span><span id=__span-0-783><a id=__codelineno-0-783 name=__codelineno-0-783></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-784><a id=__codelineno-0-784 name=__codelineno-0-784></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-785><a id=__codelineno-0-785 name=__codelineno-0-785></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-786><a id=__codelineno-0-786 name=__codelineno-0-786></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-787><a id=__codelineno-0-787 name=__codelineno-0-787></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-788><a id=__codelineno-0-788 name=__codelineno-0-788></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-789><a id=__codelineno-0-789 name=__codelineno-0-789></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-790><a id=__codelineno-0-790 name=__codelineno-0-790></a><span class=sd>            Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-791><a id=__codelineno-0-791 name=__codelineno-0-791></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-792><a id=__codelineno-0-792 name=__codelineno-0-792></a><span class=sd>        merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;]</span>
</span><span id=__span-0-793><a id=__codelineno-0-793 name=__codelineno-0-793></a><span class=sd>            The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;, and &quot;residual_ungated&quot;.</span>
</span><span id=__span-0-794><a id=__codelineno-0-794 name=__codelineno-0-794></a><span class=sd>            Check the class docstring for more information about the behaviour of different merge modalities.</span>
</span><span id=__span-0-795><a id=__codelineno-0-795 name=__codelineno-0-795></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-796><a id=__codelineno-0-796 name=__codelineno-0-796></a><span class=sd>            Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-797><a id=__codelineno-0-797 name=__codelineno-0-797></a><span class=sd>        res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-798><a id=__codelineno-0-798 name=__codelineno-0-798></a><span class=sd>            The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-799><a id=__codelineno-0-799 name=__codelineno-0-799></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-800><a id=__codelineno-0-800 name=__codelineno-0-800></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-801><a id=__codelineno-0-801 name=__codelineno-0-801></a><span class=sd>        res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-802><a id=__codelineno-0-802 name=__codelineno-0-802></a><span class=sd>            Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-803><a id=__codelineno-0-803 name=__codelineno-0-803></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-804><a id=__codelineno-0-804 name=__codelineno-0-804></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-805><a id=__codelineno-0-805 name=__codelineno-0-805></a>            <span class=n>channels</span><span class=o>=</span><span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-806><a id=__codelineno-0-806 name=__codelineno-0-806></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-807><a id=__codelineno-0-807 name=__codelineno-0-807></a>            <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-808><a id=__codelineno-0-808 name=__codelineno-0-808></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-809><a id=__codelineno-0-809 name=__codelineno-0-809></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-810><a id=__codelineno-0-810 name=__codelineno-0-810></a>            <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-811><a id=__codelineno-0-811 name=__codelineno-0-811></a>            <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-812><a id=__codelineno-0-812 name=__codelineno-0-812></a>            <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-813><a id=__codelineno-0-813 name=__codelineno-0-813></a>            <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-814><a id=__codelineno-0-814 name=__codelineno-0-814></a>        <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.SkipConnectionMerger.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>nonlin</span><span class=p>,</span> <span class=n>channels</span><span class=p>,</span> <span class=n>batchnorm</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=n>res_block_type</span><span class=p>,</span> <span class=n>merge_type</span><span class=o>=</span><span class=s1>&#39;residual&#39;</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>res_block_kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.SkipConnectionMerger.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p>nonlin: Callable, optional The non-linearity function used in the block. Default is <code>nn.LeakyReLU</code>. channels: Union[int, Iterable[int]] The number of channels used in the convolutional blocks of this layer. If it is an <code>int</code>: - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels - (Optional) ResBlock: in_channels=channels, out_channels=channels If it is an Iterable (must have <code>len(channels)==3</code>): - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1] - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1] batchnorm: bool, optional Whether to use batchnorm layers. Default is <code>True</code>. dropout: float, optional The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>. res_block_type: str, optional A string specifying the structure of residual block. Check <code>ResidualBlock</code> doscstring for more information. Default is <code>None</code>. merge_type: Literal["linear", "residual", "residual_ungated"] The type of merge done in the layer. It can be chosen between "linear", "residual", and "residual_ungated". Check the class docstring for more information about the behaviour of different merge modalities. conv2d_bias: bool, optional Whether to use bias term in convolutions. Default is <code>True</code>. res_block_kernel: Union[int, Iterable[int]], optional The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>. res_block_skip_padding: bool, optional Whether to skip padding in convolutions in the Residual block. Default is <code>False</code>.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-758>758</a></span>
<span class=normal><a href=#__codelineno-0-759>759</a></span>
<span class=normal><a href=#__codelineno-0-760>760</a></span>
<span class=normal><a href=#__codelineno-0-761>761</a></span>
<span class=normal><a href=#__codelineno-0-762>762</a></span>
<span class=normal><a href=#__codelineno-0-763>763</a></span>
<span class=normal><a href=#__codelineno-0-764>764</a></span>
<span class=normal><a href=#__codelineno-0-765>765</a></span>
<span class=normal><a href=#__codelineno-0-766>766</a></span>
<span class=normal><a href=#__codelineno-0-767>767</a></span>
<span class=normal><a href=#__codelineno-0-768>768</a></span>
<span class=normal><a href=#__codelineno-0-769>769</a></span>
<span class=normal><a href=#__codelineno-0-770>770</a></span>
<span class=normal><a href=#__codelineno-0-771>771</a></span>
<span class=normal><a href=#__codelineno-0-772>772</a></span>
<span class=normal><a href=#__codelineno-0-773>773</a></span>
<span class=normal><a href=#__codelineno-0-774>774</a></span>
<span class=normal><a href=#__codelineno-0-775>775</a></span>
<span class=normal><a href=#__codelineno-0-776>776</a></span>
<span class=normal><a href=#__codelineno-0-777>777</a></span>
<span class=normal><a href=#__codelineno-0-778>778</a></span>
<span class=normal><a href=#__codelineno-0-779>779</a></span>
<span class=normal><a href=#__codelineno-0-780>780</a></span>
<span class=normal><a href=#__codelineno-0-781>781</a></span>
<span class=normal><a href=#__codelineno-0-782>782</a></span>
<span class=normal><a href=#__codelineno-0-783>783</a></span>
<span class=normal><a href=#__codelineno-0-784>784</a></span>
<span class=normal><a href=#__codelineno-0-785>785</a></span>
<span class=normal><a href=#__codelineno-0-786>786</a></span>
<span class=normal><a href=#__codelineno-0-787>787</a></span>
<span class=normal><a href=#__codelineno-0-788>788</a></span>
<span class=normal><a href=#__codelineno-0-789>789</a></span>
<span class=normal><a href=#__codelineno-0-790>790</a></span>
<span class=normal><a href=#__codelineno-0-791>791</a></span>
<span class=normal><a href=#__codelineno-0-792>792</a></span>
<span class=normal><a href=#__codelineno-0-793>793</a></span>
<span class=normal><a href=#__codelineno-0-794>794</a></span>
<span class=normal><a href=#__codelineno-0-795>795</a></span>
<span class=normal><a href=#__codelineno-0-796>796</a></span>
<span class=normal><a href=#__codelineno-0-797>797</a></span>
<span class=normal><a href=#__codelineno-0-798>798</a></span>
<span class=normal><a href=#__codelineno-0-799>799</a></span>
<span class=normal><a href=#__codelineno-0-800>800</a></span>
<span class=normal><a href=#__codelineno-0-801>801</a></span>
<span class=normal><a href=#__codelineno-0-802>802</a></span>
<span class=normal><a href=#__codelineno-0-803>803</a></span>
<span class=normal><a href=#__codelineno-0-804>804</a></span>
<span class=normal><a href=#__codelineno-0-805>805</a></span>
<span class=normal><a href=#__codelineno-0-806>806</a></span>
<span class=normal><a href=#__codelineno-0-807>807</a></span>
<span class=normal><a href=#__codelineno-0-808>808</a></span>
<span class=normal><a href=#__codelineno-0-809>809</a></span>
<span class=normal><a href=#__codelineno-0-810>810</a></span>
<span class=normal><a href=#__codelineno-0-811>811</a></span>
<span class=normal><a href=#__codelineno-0-812>812</a></span>
<span class=normal><a href=#__codelineno-0-813>813</a></span>
<span class=normal><a href=#__codelineno-0-814>814</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-758><a id=__codelineno-0-758 name=__codelineno-0-758></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-759><a id=__codelineno-0-759 name=__codelineno-0-759></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-760><a id=__codelineno-0-760 name=__codelineno-0-760></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span><span class=p>,</span>
</span><span id=__span-0-761><a id=__codelineno-0-761 name=__codelineno-0-761></a>    <span class=n>channels</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]],</span>
</span><span id=__span-0-762><a id=__codelineno-0-762 name=__codelineno-0-762></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span><span class=p>,</span>
</span><span id=__span-0-763><a id=__codelineno-0-763 name=__codelineno-0-763></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span><span id=__span-0-764><a id=__codelineno-0-764 name=__codelineno-0-764></a>    <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span>
</span><span id=__span-0-765><a id=__codelineno-0-765 name=__codelineno-0-765></a>    <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span>
</span><span id=__span-0-766><a id=__codelineno-0-766 name=__codelineno-0-766></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-767><a id=__codelineno-0-767 name=__codelineno-0-767></a>    <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-768><a id=__codelineno-0-768 name=__codelineno-0-768></a>    <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-769><a id=__codelineno-0-769 name=__codelineno-0-769></a><span class=p>):</span>
</span><span id=__span-0-770><a id=__codelineno-0-770 name=__codelineno-0-770></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-771><a id=__codelineno-0-771 name=__codelineno-0-771></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-772><a id=__codelineno-0-772 name=__codelineno-0-772></a>
</span><span id=__span-0-773><a id=__codelineno-0-773 name=__codelineno-0-773></a><span class=sd>    nonlin: Callable, optional</span>
</span><span id=__span-0-774><a id=__codelineno-0-774 name=__codelineno-0-774></a><span class=sd>        The non-linearity function used in the block. Default is `nn.LeakyReLU`.</span>
</span><span id=__span-0-775><a id=__codelineno-0-775 name=__codelineno-0-775></a><span class=sd>    channels: Union[int, Iterable[int]]</span>
</span><span id=__span-0-776><a id=__codelineno-0-776 name=__codelineno-0-776></a><span class=sd>        The number of channels used in the convolutional blocks of this layer.</span>
</span><span id=__span-0-777><a id=__codelineno-0-777 name=__codelineno-0-777></a><span class=sd>        If it is an `int`:</span>
</span><span id=__span-0-778><a id=__codelineno-0-778 name=__codelineno-0-778></a><span class=sd>            - 1st 1x1 Conv2d: in_channels=2*channels, out_channels=channels</span>
</span><span id=__span-0-779><a id=__codelineno-0-779 name=__codelineno-0-779></a><span class=sd>            - (Optional) ResBlock: in_channels=channels, out_channels=channels</span>
</span><span id=__span-0-780><a id=__codelineno-0-780 name=__codelineno-0-780></a><span class=sd>        If it is an Iterable (must have `len(channels)==3`):</span>
</span><span id=__span-0-781><a id=__codelineno-0-781 name=__codelineno-0-781></a><span class=sd>            - 1st 1x1 Conv2d: in_channels=sum(channels[:-1]), out_channels=channels[-1]</span>
</span><span id=__span-0-782><a id=__codelineno-0-782 name=__codelineno-0-782></a><span class=sd>            - (Optional) ResBlock: in_channels=channels[-1], out_channels=channels[-1]</span>
</span><span id=__span-0-783><a id=__codelineno-0-783 name=__codelineno-0-783></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-784><a id=__codelineno-0-784 name=__codelineno-0-784></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-785><a id=__codelineno-0-785 name=__codelineno-0-785></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-786><a id=__codelineno-0-786 name=__codelineno-0-786></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-787><a id=__codelineno-0-787 name=__codelineno-0-787></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-788><a id=__codelineno-0-788 name=__codelineno-0-788></a><span class=sd>    res_block_type: str, optional</span>
</span><span id=__span-0-789><a id=__codelineno-0-789 name=__codelineno-0-789></a><span class=sd>        A string specifying the structure of residual block.</span>
</span><span id=__span-0-790><a id=__codelineno-0-790 name=__codelineno-0-790></a><span class=sd>        Check `ResidualBlock` doscstring for more information.</span>
</span><span id=__span-0-791><a id=__codelineno-0-791 name=__codelineno-0-791></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-792><a id=__codelineno-0-792 name=__codelineno-0-792></a><span class=sd>    merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;]</span>
</span><span id=__span-0-793><a id=__codelineno-0-793 name=__codelineno-0-793></a><span class=sd>        The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;, and &quot;residual_ungated&quot;.</span>
</span><span id=__span-0-794><a id=__codelineno-0-794 name=__codelineno-0-794></a><span class=sd>        Check the class docstring for more information about the behaviour of different merge modalities.</span>
</span><span id=__span-0-795><a id=__codelineno-0-795 name=__codelineno-0-795></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-796><a id=__codelineno-0-796 name=__codelineno-0-796></a><span class=sd>        Whether to use bias term in convolutions. Default is `True`.</span>
</span><span id=__span-0-797><a id=__codelineno-0-797 name=__codelineno-0-797></a><span class=sd>    res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-798><a id=__codelineno-0-798 name=__codelineno-0-798></a><span class=sd>        The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-799><a id=__codelineno-0-799 name=__codelineno-0-799></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-800><a id=__codelineno-0-800 name=__codelineno-0-800></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-801><a id=__codelineno-0-801 name=__codelineno-0-801></a><span class=sd>    res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-802><a id=__codelineno-0-802 name=__codelineno-0-802></a><span class=sd>        Whether to skip padding in convolutions in the Residual block. Default is `False`.</span>
</span><span id=__span-0-803><a id=__codelineno-0-803 name=__codelineno-0-803></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-804><a id=__codelineno-0-804 name=__codelineno-0-804></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-805><a id=__codelineno-0-805 name=__codelineno-0-805></a>        <span class=n>channels</span><span class=o>=</span><span class=n>channels</span><span class=p>,</span>
</span><span id=__span-0-806><a id=__codelineno-0-806 name=__codelineno-0-806></a>        <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-807><a id=__codelineno-0-807 name=__codelineno-0-807></a>        <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-808><a id=__codelineno-0-808 name=__codelineno-0-808></a>        <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-809><a id=__codelineno-0-809 name=__codelineno-0-809></a>        <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-810><a id=__codelineno-0-810 name=__codelineno-0-810></a>        <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-811><a id=__codelineno-0-811 name=__codelineno-0-811></a>        <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-812><a id=__codelineno-0-812 name=__codelineno-0-812></a>        <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-813><a id=__codelineno-0-813 name=__codelineno-0-813></a>        <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-814><a id=__codelineno-0-814 name=__codelineno-0-814></a>    <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> <div class="doc doc-object doc-class"> <h2 id=careamics.models.lvae.layers.TopDownLayer class="doc doc-heading"> <code>TopDownLayer</code> <a href=#careamics.models.lvae.layers.TopDownLayer class=headerlink title="Permanent link">#</a></h2> <div class="doc doc-contents "> <p class="doc doc-class-bases"> Bases: <code><span title="torch.nn.Module">Module</span></code></p> <p>Top-down inference layer. It includes: - Stochastic sampling, - Computation of KL divergence, - A small deterministic ResNet that performs upsampling.</p> <p>NOTE 1: The algorithm for generative inference approximately works as follows: - p_params = output of top-down layer above - bu = inferred bottom-up value at this layer - q_params = merge(bu, p_params) - z = stochastic_layer(q_params) - (optional) get and merge skip connection from prev top-down layer - top-down deterministic ResNet</p> <p>NOTE 2: The Top-Down layer can work in two modes: inference and prediction/generative. Depending on the particular mode, it follows distinct behaviours: - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path, by merging outcomes of bottom-up and top-down passes. The exception is the top layer, in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer. - On the contrary in prediciton/generative mode, parameters of q(z_i|z_i+1) can be obtained once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).</p> <p>NOTE 3: When doing unconditional generation, bu_value is not available. Hence the merge layer is not used, and z is sampled directly from p_params.</p> <p>NOTE 4: If this is the top layer, at inference time, the uppermost bottom-up value is used directly as q_params, and p_params are defined in this layer (while they are usually taken from the previous layer), and can be learned.</p> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-817> 817</a></span>
<span class=normal><a href=#__codelineno-0-818> 818</a></span>
<span class=normal><a href=#__codelineno-0-819> 819</a></span>
<span class=normal><a href=#__codelineno-0-820> 820</a></span>
<span class=normal><a href=#__codelineno-0-821> 821</a></span>
<span class=normal><a href=#__codelineno-0-822> 822</a></span>
<span class=normal><a href=#__codelineno-0-823> 823</a></span>
<span class=normal><a href=#__codelineno-0-824> 824</a></span>
<span class=normal><a href=#__codelineno-0-825> 825</a></span>
<span class=normal><a href=#__codelineno-0-826> 826</a></span>
<span class=normal><a href=#__codelineno-0-827> 827</a></span>
<span class=normal><a href=#__codelineno-0-828> 828</a></span>
<span class=normal><a href=#__codelineno-0-829> 829</a></span>
<span class=normal><a href=#__codelineno-0-830> 830</a></span>
<span class=normal><a href=#__codelineno-0-831> 831</a></span>
<span class=normal><a href=#__codelineno-0-832> 832</a></span>
<span class=normal><a href=#__codelineno-0-833> 833</a></span>
<span class=normal><a href=#__codelineno-0-834> 834</a></span>
<span class=normal><a href=#__codelineno-0-835> 835</a></span>
<span class=normal><a href=#__codelineno-0-836> 836</a></span>
<span class=normal><a href=#__codelineno-0-837> 837</a></span>
<span class=normal><a href=#__codelineno-0-838> 838</a></span>
<span class=normal><a href=#__codelineno-0-839> 839</a></span>
<span class=normal><a href=#__codelineno-0-840> 840</a></span>
<span class=normal><a href=#__codelineno-0-841> 841</a></span>
<span class=normal><a href=#__codelineno-0-842> 842</a></span>
<span class=normal><a href=#__codelineno-0-843> 843</a></span>
<span class=normal><a href=#__codelineno-0-844> 844</a></span>
<span class=normal><a href=#__codelineno-0-845> 845</a></span>
<span class=normal><a href=#__codelineno-0-846> 846</a></span>
<span class=normal><a href=#__codelineno-0-847> 847</a></span>
<span class=normal><a href=#__codelineno-0-848> 848</a></span>
<span class=normal><a href=#__codelineno-0-849> 849</a></span>
<span class=normal><a href=#__codelineno-0-850> 850</a></span>
<span class=normal><a href=#__codelineno-0-851> 851</a></span>
<span class=normal><a href=#__codelineno-0-852> 852</a></span>
<span class=normal><a href=#__codelineno-0-853> 853</a></span>
<span class=normal><a href=#__codelineno-0-854> 854</a></span>
<span class=normal><a href=#__codelineno-0-855> 855</a></span>
<span class=normal><a href=#__codelineno-0-856> 856</a></span>
<span class=normal><a href=#__codelineno-0-857> 857</a></span>
<span class=normal><a href=#__codelineno-0-858> 858</a></span>
<span class=normal><a href=#__codelineno-0-859> 859</a></span>
<span class=normal><a href=#__codelineno-0-860> 860</a></span>
<span class=normal><a href=#__codelineno-0-861> 861</a></span>
<span class=normal><a href=#__codelineno-0-862> 862</a></span>
<span class=normal><a href=#__codelineno-0-863> 863</a></span>
<span class=normal><a href=#__codelineno-0-864> 864</a></span>
<span class=normal><a href=#__codelineno-0-865> 865</a></span>
<span class=normal><a href=#__codelineno-0-866> 866</a></span>
<span class=normal><a href=#__codelineno-0-867> 867</a></span>
<span class=normal><a href=#__codelineno-0-868> 868</a></span>
<span class=normal><a href=#__codelineno-0-869> 869</a></span>
<span class=normal><a href=#__codelineno-0-870> 870</a></span>
<span class=normal><a href=#__codelineno-0-871> 871</a></span>
<span class=normal><a href=#__codelineno-0-872> 872</a></span>
<span class=normal><a href=#__codelineno-0-873> 873</a></span>
<span class=normal><a href=#__codelineno-0-874> 874</a></span>
<span class=normal><a href=#__codelineno-0-875> 875</a></span>
<span class=normal><a href=#__codelineno-0-876> 876</a></span>
<span class=normal><a href=#__codelineno-0-877> 877</a></span>
<span class=normal><a href=#__codelineno-0-878> 878</a></span>
<span class=normal><a href=#__codelineno-0-879> 879</a></span>
<span class=normal><a href=#__codelineno-0-880> 880</a></span>
<span class=normal><a href=#__codelineno-0-881> 881</a></span>
<span class=normal><a href=#__codelineno-0-882> 882</a></span>
<span class=normal><a href=#__codelineno-0-883> 883</a></span>
<span class=normal><a href=#__codelineno-0-884> 884</a></span>
<span class=normal><a href=#__codelineno-0-885> 885</a></span>
<span class=normal><a href=#__codelineno-0-886> 886</a></span>
<span class=normal><a href=#__codelineno-0-887> 887</a></span>
<span class=normal><a href=#__codelineno-0-888> 888</a></span>
<span class=normal><a href=#__codelineno-0-889> 889</a></span>
<span class=normal><a href=#__codelineno-0-890> 890</a></span>
<span class=normal><a href=#__codelineno-0-891> 891</a></span>
<span class=normal><a href=#__codelineno-0-892> 892</a></span>
<span class=normal><a href=#__codelineno-0-893> 893</a></span>
<span class=normal><a href=#__codelineno-0-894> 894</a></span>
<span class=normal><a href=#__codelineno-0-895> 895</a></span>
<span class=normal><a href=#__codelineno-0-896> 896</a></span>
<span class=normal><a href=#__codelineno-0-897> 897</a></span>
<span class=normal><a href=#__codelineno-0-898> 898</a></span>
<span class=normal><a href=#__codelineno-0-899> 899</a></span>
<span class=normal><a href=#__codelineno-0-900> 900</a></span>
<span class=normal><a href=#__codelineno-0-901> 901</a></span>
<span class=normal><a href=#__codelineno-0-902> 902</a></span>
<span class=normal><a href=#__codelineno-0-903> 903</a></span>
<span class=normal><a href=#__codelineno-0-904> 904</a></span>
<span class=normal><a href=#__codelineno-0-905> 905</a></span>
<span class=normal><a href=#__codelineno-0-906> 906</a></span>
<span class=normal><a href=#__codelineno-0-907> 907</a></span>
<span class=normal><a href=#__codelineno-0-908> 908</a></span>
<span class=normal><a href=#__codelineno-0-909> 909</a></span>
<span class=normal><a href=#__codelineno-0-910> 910</a></span>
<span class=normal><a href=#__codelineno-0-911> 911</a></span>
<span class=normal><a href=#__codelineno-0-912> 912</a></span>
<span class=normal><a href=#__codelineno-0-913> 913</a></span>
<span class=normal><a href=#__codelineno-0-914> 914</a></span>
<span class=normal><a href=#__codelineno-0-915> 915</a></span>
<span class=normal><a href=#__codelineno-0-916> 916</a></span>
<span class=normal><a href=#__codelineno-0-917> 917</a></span>
<span class=normal><a href=#__codelineno-0-918> 918</a></span>
<span class=normal><a href=#__codelineno-0-919> 919</a></span>
<span class=normal><a href=#__codelineno-0-920> 920</a></span>
<span class=normal><a href=#__codelineno-0-921> 921</a></span>
<span class=normal><a href=#__codelineno-0-922> 922</a></span>
<span class=normal><a href=#__codelineno-0-923> 923</a></span>
<span class=normal><a href=#__codelineno-0-924> 924</a></span>
<span class=normal><a href=#__codelineno-0-925> 925</a></span>
<span class=normal><a href=#__codelineno-0-926> 926</a></span>
<span class=normal><a href=#__codelineno-0-927> 927</a></span>
<span class=normal><a href=#__codelineno-0-928> 928</a></span>
<span class=normal><a href=#__codelineno-0-929> 929</a></span>
<span class=normal><a href=#__codelineno-0-930> 930</a></span>
<span class=normal><a href=#__codelineno-0-931> 931</a></span>
<span class=normal><a href=#__codelineno-0-932> 932</a></span>
<span class=normal><a href=#__codelineno-0-933> 933</a></span>
<span class=normal><a href=#__codelineno-0-934> 934</a></span>
<span class=normal><a href=#__codelineno-0-935> 935</a></span>
<span class=normal><a href=#__codelineno-0-936> 936</a></span>
<span class=normal><a href=#__codelineno-0-937> 937</a></span>
<span class=normal><a href=#__codelineno-0-938> 938</a></span>
<span class=normal><a href=#__codelineno-0-939> 939</a></span>
<span class=normal><a href=#__codelineno-0-940> 940</a></span>
<span class=normal><a href=#__codelineno-0-941> 941</a></span>
<span class=normal><a href=#__codelineno-0-942> 942</a></span>
<span class=normal><a href=#__codelineno-0-943> 943</a></span>
<span class=normal><a href=#__codelineno-0-944> 944</a></span>
<span class=normal><a href=#__codelineno-0-945> 945</a></span>
<span class=normal><a href=#__codelineno-0-946> 946</a></span>
<span class=normal><a href=#__codelineno-0-947> 947</a></span>
<span class=normal><a href=#__codelineno-0-948> 948</a></span>
<span class=normal><a href=#__codelineno-0-949> 949</a></span>
<span class=normal><a href=#__codelineno-0-950> 950</a></span>
<span class=normal><a href=#__codelineno-0-951> 951</a></span>
<span class=normal><a href=#__codelineno-0-952> 952</a></span>
<span class=normal><a href=#__codelineno-0-953> 953</a></span>
<span class=normal><a href=#__codelineno-0-954> 954</a></span>
<span class=normal><a href=#__codelineno-0-955> 955</a></span>
<span class=normal><a href=#__codelineno-0-956> 956</a></span>
<span class=normal><a href=#__codelineno-0-957> 957</a></span>
<span class=normal><a href=#__codelineno-0-958> 958</a></span>
<span class=normal><a href=#__codelineno-0-959> 959</a></span>
<span class=normal><a href=#__codelineno-0-960> 960</a></span>
<span class=normal><a href=#__codelineno-0-961> 961</a></span>
<span class=normal><a href=#__codelineno-0-962> 962</a></span>
<span class=normal><a href=#__codelineno-0-963> 963</a></span>
<span class=normal><a href=#__codelineno-0-964> 964</a></span>
<span class=normal><a href=#__codelineno-0-965> 965</a></span>
<span class=normal><a href=#__codelineno-0-966> 966</a></span>
<span class=normal><a href=#__codelineno-0-967> 967</a></span>
<span class=normal><a href=#__codelineno-0-968> 968</a></span>
<span class=normal><a href=#__codelineno-0-969> 969</a></span>
<span class=normal><a href=#__codelineno-0-970> 970</a></span>
<span class=normal><a href=#__codelineno-0-971> 971</a></span>
<span class=normal><a href=#__codelineno-0-972> 972</a></span>
<span class=normal><a href=#__codelineno-0-973> 973</a></span>
<span class=normal><a href=#__codelineno-0-974> 974</a></span>
<span class=normal><a href=#__codelineno-0-975> 975</a></span>
<span class=normal><a href=#__codelineno-0-976> 976</a></span>
<span class=normal><a href=#__codelineno-0-977> 977</a></span>
<span class=normal><a href=#__codelineno-0-978> 978</a></span>
<span class=normal><a href=#__codelineno-0-979> 979</a></span>
<span class=normal><a href=#__codelineno-0-980> 980</a></span>
<span class=normal><a href=#__codelineno-0-981> 981</a></span>
<span class=normal><a href=#__codelineno-0-982> 982</a></span>
<span class=normal><a href=#__codelineno-0-983> 983</a></span>
<span class=normal><a href=#__codelineno-0-984> 984</a></span>
<span class=normal><a href=#__codelineno-0-985> 985</a></span>
<span class=normal><a href=#__codelineno-0-986> 986</a></span>
<span class=normal><a href=#__codelineno-0-987> 987</a></span>
<span class=normal><a href=#__codelineno-0-988> 988</a></span>
<span class=normal><a href=#__codelineno-0-989> 989</a></span>
<span class=normal><a href=#__codelineno-0-990> 990</a></span>
<span class=normal><a href=#__codelineno-0-991> 991</a></span>
<span class=normal><a href=#__codelineno-0-992> 992</a></span>
<span class=normal><a href=#__codelineno-0-993> 993</a></span>
<span class=normal><a href=#__codelineno-0-994> 994</a></span>
<span class=normal><a href=#__codelineno-0-995> 995</a></span>
<span class=normal><a href=#__codelineno-0-996> 996</a></span>
<span class=normal><a href=#__codelineno-0-997> 997</a></span>
<span class=normal><a href=#__codelineno-0-998> 998</a></span>
<span class=normal><a href=#__codelineno-0-999> 999</a></span>
<span class=normal><a href=#__codelineno-0-1000>1000</a></span>
<span class=normal><a href=#__codelineno-0-1001>1001</a></span>
<span class=normal><a href=#__codelineno-0-1002>1002</a></span>
<span class=normal><a href=#__codelineno-0-1003>1003</a></span>
<span class=normal><a href=#__codelineno-0-1004>1004</a></span>
<span class=normal><a href=#__codelineno-0-1005>1005</a></span>
<span class=normal><a href=#__codelineno-0-1006>1006</a></span>
<span class=normal><a href=#__codelineno-0-1007>1007</a></span>
<span class=normal><a href=#__codelineno-0-1008>1008</a></span>
<span class=normal><a href=#__codelineno-0-1009>1009</a></span>
<span class=normal><a href=#__codelineno-0-1010>1010</a></span>
<span class=normal><a href=#__codelineno-0-1011>1011</a></span>
<span class=normal><a href=#__codelineno-0-1012>1012</a></span>
<span class=normal><a href=#__codelineno-0-1013>1013</a></span>
<span class=normal><a href=#__codelineno-0-1014>1014</a></span>
<span class=normal><a href=#__codelineno-0-1015>1015</a></span>
<span class=normal><a href=#__codelineno-0-1016>1016</a></span>
<span class=normal><a href=#__codelineno-0-1017>1017</a></span>
<span class=normal><a href=#__codelineno-0-1018>1018</a></span>
<span class=normal><a href=#__codelineno-0-1019>1019</a></span>
<span class=normal><a href=#__codelineno-0-1020>1020</a></span>
<span class=normal><a href=#__codelineno-0-1021>1021</a></span>
<span class=normal><a href=#__codelineno-0-1022>1022</a></span>
<span class=normal><a href=#__codelineno-0-1023>1023</a></span>
<span class=normal><a href=#__codelineno-0-1024>1024</a></span>
<span class=normal><a href=#__codelineno-0-1025>1025</a></span>
<span class=normal><a href=#__codelineno-0-1026>1026</a></span>
<span class=normal><a href=#__codelineno-0-1027>1027</a></span>
<span class=normal><a href=#__codelineno-0-1028>1028</a></span>
<span class=normal><a href=#__codelineno-0-1029>1029</a></span>
<span class=normal><a href=#__codelineno-0-1030>1030</a></span>
<span class=normal><a href=#__codelineno-0-1031>1031</a></span>
<span class=normal><a href=#__codelineno-0-1032>1032</a></span>
<span class=normal><a href=#__codelineno-0-1033>1033</a></span>
<span class=normal><a href=#__codelineno-0-1034>1034</a></span>
<span class=normal><a href=#__codelineno-0-1035>1035</a></span>
<span class=normal><a href=#__codelineno-0-1036>1036</a></span>
<span class=normal><a href=#__codelineno-0-1037>1037</a></span>
<span class=normal><a href=#__codelineno-0-1038>1038</a></span>
<span class=normal><a href=#__codelineno-0-1039>1039</a></span>
<span class=normal><a href=#__codelineno-0-1040>1040</a></span>
<span class=normal><a href=#__codelineno-0-1041>1041</a></span>
<span class=normal><a href=#__codelineno-0-1042>1042</a></span>
<span class=normal><a href=#__codelineno-0-1043>1043</a></span>
<span class=normal><a href=#__codelineno-0-1044>1044</a></span>
<span class=normal><a href=#__codelineno-0-1045>1045</a></span>
<span class=normal><a href=#__codelineno-0-1046>1046</a></span>
<span class=normal><a href=#__codelineno-0-1047>1047</a></span>
<span class=normal><a href=#__codelineno-0-1048>1048</a></span>
<span class=normal><a href=#__codelineno-0-1049>1049</a></span>
<span class=normal><a href=#__codelineno-0-1050>1050</a></span>
<span class=normal><a href=#__codelineno-0-1051>1051</a></span>
<span class=normal><a href=#__codelineno-0-1052>1052</a></span>
<span class=normal><a href=#__codelineno-0-1053>1053</a></span>
<span class=normal><a href=#__codelineno-0-1054>1054</a></span>
<span class=normal><a href=#__codelineno-0-1055>1055</a></span>
<span class=normal><a href=#__codelineno-0-1056>1056</a></span>
<span class=normal><a href=#__codelineno-0-1057>1057</a></span>
<span class=normal><a href=#__codelineno-0-1058>1058</a></span>
<span class=normal><a href=#__codelineno-0-1059>1059</a></span>
<span class=normal><a href=#__codelineno-0-1060>1060</a></span>
<span class=normal><a href=#__codelineno-0-1061>1061</a></span>
<span class=normal><a href=#__codelineno-0-1062>1062</a></span>
<span class=normal><a href=#__codelineno-0-1063>1063</a></span>
<span class=normal><a href=#__codelineno-0-1064>1064</a></span>
<span class=normal><a href=#__codelineno-0-1065>1065</a></span>
<span class=normal><a href=#__codelineno-0-1066>1066</a></span>
<span class=normal><a href=#__codelineno-0-1067>1067</a></span>
<span class=normal><a href=#__codelineno-0-1068>1068</a></span>
<span class=normal><a href=#__codelineno-0-1069>1069</a></span>
<span class=normal><a href=#__codelineno-0-1070>1070</a></span>
<span class=normal><a href=#__codelineno-0-1071>1071</a></span>
<span class=normal><a href=#__codelineno-0-1072>1072</a></span>
<span class=normal><a href=#__codelineno-0-1073>1073</a></span>
<span class=normal><a href=#__codelineno-0-1074>1074</a></span>
<span class=normal><a href=#__codelineno-0-1075>1075</a></span>
<span class=normal><a href=#__codelineno-0-1076>1076</a></span>
<span class=normal><a href=#__codelineno-0-1077>1077</a></span>
<span class=normal><a href=#__codelineno-0-1078>1078</a></span>
<span class=normal><a href=#__codelineno-0-1079>1079</a></span>
<span class=normal><a href=#__codelineno-0-1080>1080</a></span>
<span class=normal><a href=#__codelineno-0-1081>1081</a></span>
<span class=normal><a href=#__codelineno-0-1082>1082</a></span>
<span class=normal><a href=#__codelineno-0-1083>1083</a></span>
<span class=normal><a href=#__codelineno-0-1084>1084</a></span>
<span class=normal><a href=#__codelineno-0-1085>1085</a></span>
<span class=normal><a href=#__codelineno-0-1086>1086</a></span>
<span class=normal><a href=#__codelineno-0-1087>1087</a></span>
<span class=normal><a href=#__codelineno-0-1088>1088</a></span>
<span class=normal><a href=#__codelineno-0-1089>1089</a></span>
<span class=normal><a href=#__codelineno-0-1090>1090</a></span>
<span class=normal><a href=#__codelineno-0-1091>1091</a></span>
<span class=normal><a href=#__codelineno-0-1092>1092</a></span>
<span class=normal><a href=#__codelineno-0-1093>1093</a></span>
<span class=normal><a href=#__codelineno-0-1094>1094</a></span>
<span class=normal><a href=#__codelineno-0-1095>1095</a></span>
<span class=normal><a href=#__codelineno-0-1096>1096</a></span>
<span class=normal><a href=#__codelineno-0-1097>1097</a></span>
<span class=normal><a href=#__codelineno-0-1098>1098</a></span>
<span class=normal><a href=#__codelineno-0-1099>1099</a></span>
<span class=normal><a href=#__codelineno-0-1100>1100</a></span>
<span class=normal><a href=#__codelineno-0-1101>1101</a></span>
<span class=normal><a href=#__codelineno-0-1102>1102</a></span>
<span class=normal><a href=#__codelineno-0-1103>1103</a></span>
<span class=normal><a href=#__codelineno-0-1104>1104</a></span>
<span class=normal><a href=#__codelineno-0-1105>1105</a></span>
<span class=normal><a href=#__codelineno-0-1106>1106</a></span>
<span class=normal><a href=#__codelineno-0-1107>1107</a></span>
<span class=normal><a href=#__codelineno-0-1108>1108</a></span>
<span class=normal><a href=#__codelineno-0-1109>1109</a></span>
<span class=normal><a href=#__codelineno-0-1110>1110</a></span>
<span class=normal><a href=#__codelineno-0-1111>1111</a></span>
<span class=normal><a href=#__codelineno-0-1112>1112</a></span>
<span class=normal><a href=#__codelineno-0-1113>1113</a></span>
<span class=normal><a href=#__codelineno-0-1114>1114</a></span>
<span class=normal><a href=#__codelineno-0-1115>1115</a></span>
<span class=normal><a href=#__codelineno-0-1116>1116</a></span>
<span class=normal><a href=#__codelineno-0-1117>1117</a></span>
<span class=normal><a href=#__codelineno-0-1118>1118</a></span>
<span class=normal><a href=#__codelineno-0-1119>1119</a></span>
<span class=normal><a href=#__codelineno-0-1120>1120</a></span>
<span class=normal><a href=#__codelineno-0-1121>1121</a></span>
<span class=normal><a href=#__codelineno-0-1122>1122</a></span>
<span class=normal><a href=#__codelineno-0-1123>1123</a></span>
<span class=normal><a href=#__codelineno-0-1124>1124</a></span>
<span class=normal><a href=#__codelineno-0-1125>1125</a></span>
<span class=normal><a href=#__codelineno-0-1126>1126</a></span>
<span class=normal><a href=#__codelineno-0-1127>1127</a></span>
<span class=normal><a href=#__codelineno-0-1128>1128</a></span>
<span class=normal><a href=#__codelineno-0-1129>1129</a></span>
<span class=normal><a href=#__codelineno-0-1130>1130</a></span>
<span class=normal><a href=#__codelineno-0-1131>1131</a></span>
<span class=normal><a href=#__codelineno-0-1132>1132</a></span>
<span class=normal><a href=#__codelineno-0-1133>1133</a></span>
<span class=normal><a href=#__codelineno-0-1134>1134</a></span>
<span class=normal><a href=#__codelineno-0-1135>1135</a></span>
<span class=normal><a href=#__codelineno-0-1136>1136</a></span>
<span class=normal><a href=#__codelineno-0-1137>1137</a></span>
<span class=normal><a href=#__codelineno-0-1138>1138</a></span>
<span class=normal><a href=#__codelineno-0-1139>1139</a></span>
<span class=normal><a href=#__codelineno-0-1140>1140</a></span>
<span class=normal><a href=#__codelineno-0-1141>1141</a></span>
<span class=normal><a href=#__codelineno-0-1142>1142</a></span>
<span class=normal><a href=#__codelineno-0-1143>1143</a></span>
<span class=normal><a href=#__codelineno-0-1144>1144</a></span>
<span class=normal><a href=#__codelineno-0-1145>1145</a></span>
<span class=normal><a href=#__codelineno-0-1146>1146</a></span>
<span class=normal><a href=#__codelineno-0-1147>1147</a></span>
<span class=normal><a href=#__codelineno-0-1148>1148</a></span>
<span class=normal><a href=#__codelineno-0-1149>1149</a></span>
<span class=normal><a href=#__codelineno-0-1150>1150</a></span>
<span class=normal><a href=#__codelineno-0-1151>1151</a></span>
<span class=normal><a href=#__codelineno-0-1152>1152</a></span>
<span class=normal><a href=#__codelineno-0-1153>1153</a></span>
<span class=normal><a href=#__codelineno-0-1154>1154</a></span>
<span class=normal><a href=#__codelineno-0-1155>1155</a></span>
<span class=normal><a href=#__codelineno-0-1156>1156</a></span>
<span class=normal><a href=#__codelineno-0-1157>1157</a></span>
<span class=normal><a href=#__codelineno-0-1158>1158</a></span>
<span class=normal><a href=#__codelineno-0-1159>1159</a></span>
<span class=normal><a href=#__codelineno-0-1160>1160</a></span>
<span class=normal><a href=#__codelineno-0-1161>1161</a></span>
<span class=normal><a href=#__codelineno-0-1162>1162</a></span>
<span class=normal><a href=#__codelineno-0-1163>1163</a></span>
<span class=normal><a href=#__codelineno-0-1164>1164</a></span>
<span class=normal><a href=#__codelineno-0-1165>1165</a></span>
<span class=normal><a href=#__codelineno-0-1166>1166</a></span>
<span class=normal><a href=#__codelineno-0-1167>1167</a></span>
<span class=normal><a href=#__codelineno-0-1168>1168</a></span>
<span class=normal><a href=#__codelineno-0-1169>1169</a></span>
<span class=normal><a href=#__codelineno-0-1170>1170</a></span>
<span class=normal><a href=#__codelineno-0-1171>1171</a></span>
<span class=normal><a href=#__codelineno-0-1172>1172</a></span>
<span class=normal><a href=#__codelineno-0-1173>1173</a></span>
<span class=normal><a href=#__codelineno-0-1174>1174</a></span>
<span class=normal><a href=#__codelineno-0-1175>1175</a></span>
<span class=normal><a href=#__codelineno-0-1176>1176</a></span>
<span class=normal><a href=#__codelineno-0-1177>1177</a></span>
<span class=normal><a href=#__codelineno-0-1178>1178</a></span>
<span class=normal><a href=#__codelineno-0-1179>1179</a></span>
<span class=normal><a href=#__codelineno-0-1180>1180</a></span>
<span class=normal><a href=#__codelineno-0-1181>1181</a></span>
<span class=normal><a href=#__codelineno-0-1182>1182</a></span>
<span class=normal><a href=#__codelineno-0-1183>1183</a></span>
<span class=normal><a href=#__codelineno-0-1184>1184</a></span>
<span class=normal><a href=#__codelineno-0-1185>1185</a></span>
<span class=normal><a href=#__codelineno-0-1186>1186</a></span>
<span class=normal><a href=#__codelineno-0-1187>1187</a></span>
<span class=normal><a href=#__codelineno-0-1188>1188</a></span>
<span class=normal><a href=#__codelineno-0-1189>1189</a></span>
<span class=normal><a href=#__codelineno-0-1190>1190</a></span>
<span class=normal><a href=#__codelineno-0-1191>1191</a></span>
<span class=normal><a href=#__codelineno-0-1192>1192</a></span>
<span class=normal><a href=#__codelineno-0-1193>1193</a></span>
<span class=normal><a href=#__codelineno-0-1194>1194</a></span>
<span class=normal><a href=#__codelineno-0-1195>1195</a></span>
<span class=normal><a href=#__codelineno-0-1196>1196</a></span>
<span class=normal><a href=#__codelineno-0-1197>1197</a></span>
<span class=normal><a href=#__codelineno-0-1198>1198</a></span>
<span class=normal><a href=#__codelineno-0-1199>1199</a></span>
<span class=normal><a href=#__codelineno-0-1200>1200</a></span>
<span class=normal><a href=#__codelineno-0-1201>1201</a></span>
<span class=normal><a href=#__codelineno-0-1202>1202</a></span>
<span class=normal><a href=#__codelineno-0-1203>1203</a></span>
<span class=normal><a href=#__codelineno-0-1204>1204</a></span>
<span class=normal><a href=#__codelineno-0-1205>1205</a></span>
<span class=normal><a href=#__codelineno-0-1206>1206</a></span>
<span class=normal><a href=#__codelineno-0-1207>1207</a></span>
<span class=normal><a href=#__codelineno-0-1208>1208</a></span>
<span class=normal><a href=#__codelineno-0-1209>1209</a></span>
<span class=normal><a href=#__codelineno-0-1210>1210</a></span>
<span class=normal><a href=#__codelineno-0-1211>1211</a></span>
<span class=normal><a href=#__codelineno-0-1212>1212</a></span>
<span class=normal><a href=#__codelineno-0-1213>1213</a></span>
<span class=normal><a href=#__codelineno-0-1214>1214</a></span>
<span class=normal><a href=#__codelineno-0-1215>1215</a></span>
<span class=normal><a href=#__codelineno-0-1216>1216</a></span>
<span class=normal><a href=#__codelineno-0-1217>1217</a></span>
<span class=normal><a href=#__codelineno-0-1218>1218</a></span>
<span class=normal><a href=#__codelineno-0-1219>1219</a></span>
<span class=normal><a href=#__codelineno-0-1220>1220</a></span>
<span class=normal><a href=#__codelineno-0-1221>1221</a></span>
<span class=normal><a href=#__codelineno-0-1222>1222</a></span>
<span class=normal><a href=#__codelineno-0-1223>1223</a></span>
<span class=normal><a href=#__codelineno-0-1224>1224</a></span>
<span class=normal><a href=#__codelineno-0-1225>1225</a></span>
<span class=normal><a href=#__codelineno-0-1226>1226</a></span>
<span class=normal><a href=#__codelineno-0-1227>1227</a></span>
<span class=normal><a href=#__codelineno-0-1228>1228</a></span>
<span class=normal><a href=#__codelineno-0-1229>1229</a></span>
<span class=normal><a href=#__codelineno-0-1230>1230</a></span>
<span class=normal><a href=#__codelineno-0-1231>1231</a></span>
<span class=normal><a href=#__codelineno-0-1232>1232</a></span>
<span class=normal><a href=#__codelineno-0-1233>1233</a></span>
<span class=normal><a href=#__codelineno-0-1234>1234</a></span>
<span class=normal><a href=#__codelineno-0-1235>1235</a></span>
<span class=normal><a href=#__codelineno-0-1236>1236</a></span>
<span class=normal><a href=#__codelineno-0-1237>1237</a></span>
<span class=normal><a href=#__codelineno-0-1238>1238</a></span>
<span class=normal><a href=#__codelineno-0-1239>1239</a></span>
<span class=normal><a href=#__codelineno-0-1240>1240</a></span>
<span class=normal><a href=#__codelineno-0-1241>1241</a></span>
<span class=normal><a href=#__codelineno-0-1242>1242</a></span>
<span class=normal><a href=#__codelineno-0-1243>1243</a></span>
<span class=normal><a href=#__codelineno-0-1244>1244</a></span>
<span class=normal><a href=#__codelineno-0-1245>1245</a></span>
<span class=normal><a href=#__codelineno-0-1246>1246</a></span>
<span class=normal><a href=#__codelineno-0-1247>1247</a></span>
<span class=normal><a href=#__codelineno-0-1248>1248</a></span>
<span class=normal><a href=#__codelineno-0-1249>1249</a></span>
<span class=normal><a href=#__codelineno-0-1250>1250</a></span>
<span class=normal><a href=#__codelineno-0-1251>1251</a></span>
<span class=normal><a href=#__codelineno-0-1252>1252</a></span>
<span class=normal><a href=#__codelineno-0-1253>1253</a></span>
<span class=normal><a href=#__codelineno-0-1254>1254</a></span>
<span class=normal><a href=#__codelineno-0-1255>1255</a></span>
<span class=normal><a href=#__codelineno-0-1256>1256</a></span>
<span class=normal><a href=#__codelineno-0-1257>1257</a></span>
<span class=normal><a href=#__codelineno-0-1258>1258</a></span>
<span class=normal><a href=#__codelineno-0-1259>1259</a></span>
<span class=normal><a href=#__codelineno-0-1260>1260</a></span>
<span class=normal><a href=#__codelineno-0-1261>1261</a></span>
<span class=normal><a href=#__codelineno-0-1262>1262</a></span>
<span class=normal><a href=#__codelineno-0-1263>1263</a></span>
<span class=normal><a href=#__codelineno-0-1264>1264</a></span>
<span class=normal><a href=#__codelineno-0-1265>1265</a></span>
<span class=normal><a href=#__codelineno-0-1266>1266</a></span>
<span class=normal><a href=#__codelineno-0-1267>1267</a></span>
<span class=normal><a href=#__codelineno-0-1268>1268</a></span>
<span class=normal><a href=#__codelineno-0-1269>1269</a></span>
<span class=normal><a href=#__codelineno-0-1270>1270</a></span>
<span class=normal><a href=#__codelineno-0-1271>1271</a></span>
<span class=normal><a href=#__codelineno-0-1272>1272</a></span>
<span class=normal><a href=#__codelineno-0-1273>1273</a></span>
<span class=normal><a href=#__codelineno-0-1274>1274</a></span>
<span class=normal><a href=#__codelineno-0-1275>1275</a></span>
<span class=normal><a href=#__codelineno-0-1276>1276</a></span>
<span class=normal><a href=#__codelineno-0-1277>1277</a></span>
<span class=normal><a href=#__codelineno-0-1278>1278</a></span>
<span class=normal><a href=#__codelineno-0-1279>1279</a></span>
<span class=normal><a href=#__codelineno-0-1280>1280</a></span>
<span class=normal><a href=#__codelineno-0-1281>1281</a></span>
<span class=normal><a href=#__codelineno-0-1282>1282</a></span>
<span class=normal><a href=#__codelineno-0-1283>1283</a></span>
<span class=normal><a href=#__codelineno-0-1284>1284</a></span>
<span class=normal><a href=#__codelineno-0-1285>1285</a></span>
<span class=normal><a href=#__codelineno-0-1286>1286</a></span>
<span class=normal><a href=#__codelineno-0-1287>1287</a></span>
<span class=normal><a href=#__codelineno-0-1288>1288</a></span>
<span class=normal><a href=#__codelineno-0-1289>1289</a></span>
<span class=normal><a href=#__codelineno-0-1290>1290</a></span>
<span class=normal><a href=#__codelineno-0-1291>1291</a></span>
<span class=normal><a href=#__codelineno-0-1292>1292</a></span>
<span class=normal><a href=#__codelineno-0-1293>1293</a></span>
<span class=normal><a href=#__codelineno-0-1294>1294</a></span>
<span class=normal><a href=#__codelineno-0-1295>1295</a></span>
<span class=normal><a href=#__codelineno-0-1296>1296</a></span>
<span class=normal><a href=#__codelineno-0-1297>1297</a></span>
<span class=normal><a href=#__codelineno-0-1298>1298</a></span>
<span class=normal><a href=#__codelineno-0-1299>1299</a></span>
<span class=normal><a href=#__codelineno-0-1300>1300</a></span>
<span class=normal><a href=#__codelineno-0-1301>1301</a></span>
<span class=normal><a href=#__codelineno-0-1302>1302</a></span>
<span class=normal><a href=#__codelineno-0-1303>1303</a></span>
<span class=normal><a href=#__codelineno-0-1304>1304</a></span>
<span class=normal><a href=#__codelineno-0-1305>1305</a></span>
<span class=normal><a href=#__codelineno-0-1306>1306</a></span>
<span class=normal><a href=#__codelineno-0-1307>1307</a></span>
<span class=normal><a href=#__codelineno-0-1308>1308</a></span>
<span class=normal><a href=#__codelineno-0-1309>1309</a></span>
<span class=normal><a href=#__codelineno-0-1310>1310</a></span>
<span class=normal><a href=#__codelineno-0-1311>1311</a></span>
<span class=normal><a href=#__codelineno-0-1312>1312</a></span>
<span class=normal><a href=#__codelineno-0-1313>1313</a></span>
<span class=normal><a href=#__codelineno-0-1314>1314</a></span>
<span class=normal><a href=#__codelineno-0-1315>1315</a></span>
<span class=normal><a href=#__codelineno-0-1316>1316</a></span>
<span class=normal><a href=#__codelineno-0-1317>1317</a></span>
<span class=normal><a href=#__codelineno-0-1318>1318</a></span>
<span class=normal><a href=#__codelineno-0-1319>1319</a></span>
<span class=normal><a href=#__codelineno-0-1320>1320</a></span>
<span class=normal><a href=#__codelineno-0-1321>1321</a></span>
<span class=normal><a href=#__codelineno-0-1322>1322</a></span>
<span class=normal><a href=#__codelineno-0-1323>1323</a></span>
<span class=normal><a href=#__codelineno-0-1324>1324</a></span>
<span class=normal><a href=#__codelineno-0-1325>1325</a></span>
<span class=normal><a href=#__codelineno-0-1326>1326</a></span>
<span class=normal><a href=#__codelineno-0-1327>1327</a></span>
<span class=normal><a href=#__codelineno-0-1328>1328</a></span>
<span class=normal><a href=#__codelineno-0-1329>1329</a></span>
<span class=normal><a href=#__codelineno-0-1330>1330</a></span>
<span class=normal><a href=#__codelineno-0-1331>1331</a></span>
<span class=normal><a href=#__codelineno-0-1332>1332</a></span>
<span class=normal><a href=#__codelineno-0-1333>1333</a></span>
<span class=normal><a href=#__codelineno-0-1334>1334</a></span>
<span class=normal><a href=#__codelineno-0-1335>1335</a></span>
<span class=normal><a href=#__codelineno-0-1336>1336</a></span>
<span class=normal><a href=#__codelineno-0-1337>1337</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-817><a id=__codelineno-0-817 name=__codelineno-0-817></a><span class=k>class</span> <span class=nc>TopDownLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span><span id=__span-0-818><a id=__codelineno-0-818 name=__codelineno-0-818></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-819><a id=__codelineno-0-819 name=__codelineno-0-819></a><span class=sd>    Top-down inference layer.</span>
</span><span id=__span-0-820><a id=__codelineno-0-820 name=__codelineno-0-820></a><span class=sd>    It includes:</span>
</span><span id=__span-0-821><a id=__codelineno-0-821 name=__codelineno-0-821></a><span class=sd>        - Stochastic sampling,</span>
</span><span id=__span-0-822><a id=__codelineno-0-822 name=__codelineno-0-822></a><span class=sd>        - Computation of KL divergence,</span>
</span><span id=__span-0-823><a id=__codelineno-0-823 name=__codelineno-0-823></a><span class=sd>        - A small deterministic ResNet that performs upsampling.</span>
</span><span id=__span-0-824><a id=__codelineno-0-824 name=__codelineno-0-824></a>
</span><span id=__span-0-825><a id=__codelineno-0-825 name=__codelineno-0-825></a><span class=sd>    NOTE 1:</span>
</span><span id=__span-0-826><a id=__codelineno-0-826 name=__codelineno-0-826></a><span class=sd>            The algorithm for generative inference approximately works as follows:</span>
</span><span id=__span-0-827><a id=__codelineno-0-827 name=__codelineno-0-827></a><span class=sd>                - p_params = output of top-down layer above</span>
</span><span id=__span-0-828><a id=__codelineno-0-828 name=__codelineno-0-828></a><span class=sd>                - bu = inferred bottom-up value at this layer</span>
</span><span id=__span-0-829><a id=__codelineno-0-829 name=__codelineno-0-829></a><span class=sd>                - q_params = merge(bu, p_params)</span>
</span><span id=__span-0-830><a id=__codelineno-0-830 name=__codelineno-0-830></a><span class=sd>                - z = stochastic_layer(q_params)</span>
</span><span id=__span-0-831><a id=__codelineno-0-831 name=__codelineno-0-831></a><span class=sd>                - (optional) get and merge skip connection from prev top-down layer</span>
</span><span id=__span-0-832><a id=__codelineno-0-832 name=__codelineno-0-832></a><span class=sd>                - top-down deterministic ResNet</span>
</span><span id=__span-0-833><a id=__codelineno-0-833 name=__codelineno-0-833></a>
</span><span id=__span-0-834><a id=__codelineno-0-834 name=__codelineno-0-834></a><span class=sd>    NOTE 2:</span>
</span><span id=__span-0-835><a id=__codelineno-0-835 name=__codelineno-0-835></a><span class=sd>        The Top-Down layer can work in two modes: inference and prediction/generative.</span>
</span><span id=__span-0-836><a id=__codelineno-0-836 name=__codelineno-0-836></a><span class=sd>        Depending on the particular mode, it follows distinct behaviours:</span>
</span><span id=__span-0-837><a id=__codelineno-0-837 name=__codelineno-0-837></a><span class=sd>        - In inference mode, parameters of q(z_i|z_i+1) are obtained from the inference path,</span>
</span><span id=__span-0-838><a id=__codelineno-0-838 name=__codelineno-0-838></a><span class=sd>        by merging outcomes of bottom-up and top-down passes. The exception is the top layer,</span>
</span><span id=__span-0-839><a id=__codelineno-0-839 name=__codelineno-0-839></a><span class=sd>        in which the parameters of q(z_L|x) are set as the output of the topmost bottom-up layer.</span>
</span><span id=__span-0-840><a id=__codelineno-0-840 name=__codelineno-0-840></a><span class=sd>        - On the contrary in prediciton/generative mode, parameters of q(z_i|z_i+1) can be obtained</span>
</span><span id=__span-0-841><a id=__codelineno-0-841 name=__codelineno-0-841></a><span class=sd>        once again by merging bottom-up and top-down outputs (CONDITIONAL GENERATION), or it is</span>
</span><span id=__span-0-842><a id=__codelineno-0-842 name=__codelineno-0-842></a><span class=sd>        possible to directly sample from the prior p(z_i|z_i+1) (UNCONDITIONAL GENERATION).</span>
</span><span id=__span-0-843><a id=__codelineno-0-843 name=__codelineno-0-843></a>
</span><span id=__span-0-844><a id=__codelineno-0-844 name=__codelineno-0-844></a><span class=sd>    NOTE 3:</span>
</span><span id=__span-0-845><a id=__codelineno-0-845 name=__codelineno-0-845></a><span class=sd>        When doing unconditional generation, bu_value is not available. Hence the</span>
</span><span id=__span-0-846><a id=__codelineno-0-846 name=__codelineno-0-846></a><span class=sd>        merge layer is not used, and z is sampled directly from p_params.</span>
</span><span id=__span-0-847><a id=__codelineno-0-847 name=__codelineno-0-847></a>
</span><span id=__span-0-848><a id=__codelineno-0-848 name=__codelineno-0-848></a><span class=sd>    NOTE 4:</span>
</span><span id=__span-0-849><a id=__codelineno-0-849 name=__codelineno-0-849></a><span class=sd>        If this is the top layer, at inference time, the uppermost bottom-up value</span>
</span><span id=__span-0-850><a id=__codelineno-0-850 name=__codelineno-0-850></a><span class=sd>        is used directly as q_params, and p_params are defined in this layer</span>
</span><span id=__span-0-851><a id=__codelineno-0-851 name=__codelineno-0-851></a><span class=sd>        (while they are usually taken from the previous layer), and can be learned.</span>
</span><span id=__span-0-852><a id=__codelineno-0-852 name=__codelineno-0-852></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-853><a id=__codelineno-0-853 name=__codelineno-0-853></a>
</span><span id=__span-0-854><a id=__codelineno-0-854 name=__codelineno-0-854></a>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-855><a id=__codelineno-0-855 name=__codelineno-0-855></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-856><a id=__codelineno-0-856 name=__codelineno-0-856></a>        <span class=n>z_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-857><a id=__codelineno-0-857 name=__codelineno-0-857></a>        <span class=n>n_res_blocks</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-858><a id=__codelineno-0-858 name=__codelineno-0-858></a>        <span class=n>n_filters</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-859><a id=__codelineno-0-859 name=__codelineno-0-859></a>        <span class=n>is_top_layer</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-860><a id=__codelineno-0-860 name=__codelineno-0-860></a>        <span class=n>downsampling_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-861><a id=__codelineno-0-861 name=__codelineno-0-861></a>        <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-862><a id=__codelineno-0-862 name=__codelineno-0-862></a>        <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-863><a id=__codelineno-0-863 name=__codelineno-0-863></a>        <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-864><a id=__codelineno-0-864 name=__codelineno-0-864></a>        <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-865><a id=__codelineno-0-865 name=__codelineno-0-865></a>        <span class=n>stochastic_skip</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-866><a id=__codelineno-0-866 name=__codelineno-0-866></a>        <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-867><a id=__codelineno-0-867 name=__codelineno-0-867></a>        <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-868><a id=__codelineno-0-868 name=__codelineno-0-868></a>        <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-869><a id=__codelineno-0-869 name=__codelineno-0-869></a>        <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-870><a id=__codelineno-0-870 name=__codelineno-0-870></a>        <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-871><a id=__codelineno-0-871 name=__codelineno-0-871></a>        <span class=n>learn_top_prior</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-872><a id=__codelineno-0-872 name=__codelineno-0-872></a>        <span class=n>top_prior_param_shape</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-873><a id=__codelineno-0-873 name=__codelineno-0-873></a>        <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-874><a id=__codelineno-0-874 name=__codelineno-0-874></a>        <span class=n>bottomup_no_padding_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-875><a id=__codelineno-0-875 name=__codelineno-0-875></a>        <span class=n>topdown_no_padding_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-876><a id=__codelineno-0-876 name=__codelineno-0-876></a>        <span class=n>retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-877><a id=__codelineno-0-877 name=__codelineno-0-877></a>        <span class=n>restricted_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-878><a id=__codelineno-0-878 name=__codelineno-0-878></a>        <span class=n>vanilla_latent_hw</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-879><a id=__codelineno-0-879 name=__codelineno-0-879></a>        <span class=n>non_stochastic_version</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-880><a id=__codelineno-0-880 name=__codelineno-0-880></a>        <span class=n>input_image_shape</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-881><a id=__codelineno-0-881 name=__codelineno-0-881></a>        <span class=n>normalize_latent_factor</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
</span><span id=__span-0-882><a id=__codelineno-0-882 name=__codelineno-0-882></a>        <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-883><a id=__codelineno-0-883 name=__codelineno-0-883></a>        <span class=n>stochastic_use_naive_exponential</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-884><a id=__codelineno-0-884 name=__codelineno-0-884></a>    <span class=p>):</span>
</span><span id=__span-0-885><a id=__codelineno-0-885 name=__codelineno-0-885></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-886><a id=__codelineno-0-886 name=__codelineno-0-886></a><span class=sd>        Constructor.</span>
</span><span id=__span-0-887><a id=__codelineno-0-887 name=__codelineno-0-887></a>
</span><span id=__span-0-888><a id=__codelineno-0-888 name=__codelineno-0-888></a><span class=sd>        Parameters</span>
</span><span id=__span-0-889><a id=__codelineno-0-889 name=__codelineno-0-889></a><span class=sd>        ----------</span>
</span><span id=__span-0-890><a id=__codelineno-0-890 name=__codelineno-0-890></a><span class=sd>        z_dim: int</span>
</span><span id=__span-0-891><a id=__codelineno-0-891 name=__codelineno-0-891></a><span class=sd>            The size of the latent space.</span>
</span><span id=__span-0-892><a id=__codelineno-0-892 name=__codelineno-0-892></a><span class=sd>        n_res_blocks: int</span>
</span><span id=__span-0-893><a id=__codelineno-0-893 name=__codelineno-0-893></a><span class=sd>            The number of TopDownDeterministicResBlock blocks</span>
</span><span id=__span-0-894><a id=__codelineno-0-894 name=__codelineno-0-894></a><span class=sd>        n_filters: int</span>
</span><span id=__span-0-895><a id=__codelineno-0-895 name=__codelineno-0-895></a><span class=sd>            The number of channels present through out the layers of this block.</span>
</span><span id=__span-0-896><a id=__codelineno-0-896 name=__codelineno-0-896></a><span class=sd>        is_top_layer: bool, optional</span>
</span><span id=__span-0-897><a id=__codelineno-0-897 name=__codelineno-0-897></a><span class=sd>            Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.</span>
</span><span id=__span-0-898><a id=__codelineno-0-898 name=__codelineno-0-898></a><span class=sd>        downsampling_steps: int, optional</span>
</span><span id=__span-0-899><a id=__codelineno-0-899 name=__codelineno-0-899></a><span class=sd>            The number of downsampling steps that has to be done in this layer (typically 1).</span>
</span><span id=__span-0-900><a id=__codelineno-0-900 name=__codelineno-0-900></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-901><a id=__codelineno-0-901 name=__codelineno-0-901></a><span class=sd>        nonlin: Callable, optional</span>
</span><span id=__span-0-902><a id=__codelineno-0-902 name=__codelineno-0-902></a><span class=sd>            The non-linearity function used in the block (e.g., `nn.ReLU`). Deafault is `None`.</span>
</span><span id=__span-0-903><a id=__codelineno-0-903 name=__codelineno-0-903></a><span class=sd>        merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;], optional</span>
</span><span id=__span-0-904><a id=__codelineno-0-904 name=__codelineno-0-904></a><span class=sd>            The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;,</span>
</span><span id=__span-0-905><a id=__codelineno-0-905 name=__codelineno-0-905></a><span class=sd>            and &quot;residual_ungated&quot;. Check the `MergeLayer` class docstring for more information</span>
</span><span id=__span-0-906><a id=__codelineno-0-906 name=__codelineno-0-906></a><span class=sd>            about the behaviour of different merging modalities. Default is `None`.</span>
</span><span id=__span-0-907><a id=__codelineno-0-907 name=__codelineno-0-907></a><span class=sd>        batchnorm: bool, optional</span>
</span><span id=__span-0-908><a id=__codelineno-0-908 name=__codelineno-0-908></a><span class=sd>            Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-909><a id=__codelineno-0-909 name=__codelineno-0-909></a><span class=sd>        dropout: float, optional</span>
</span><span id=__span-0-910><a id=__codelineno-0-910 name=__codelineno-0-910></a><span class=sd>            The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-911><a id=__codelineno-0-911 name=__codelineno-0-911></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-912><a id=__codelineno-0-912 name=__codelineno-0-912></a><span class=sd>        stochastic_skip: bool, optional</span>
</span><span id=__span-0-913><a id=__codelineno-0-913 name=__codelineno-0-913></a><span class=sd>            Whether to use skip connections between previous top-down layer&#39;s output and this layer&#39;s stochastic output.</span>
</span><span id=__span-0-914><a id=__codelineno-0-914 name=__codelineno-0-914></a><span class=sd>            Stochastic skip connection allows the previous layer&#39;s output has a way to directly reach this hierarchical</span>
</span><span id=__span-0-915><a id=__codelineno-0-915 name=__codelineno-0-915></a><span class=sd>            level, hence facilitating the gradient flow during backpropagation. Default is `False`.</span>
</span><span id=__span-0-916><a id=__codelineno-0-916 name=__codelineno-0-916></a><span class=sd>        res_block_type: str, optional</span>
</span><span id=__span-0-917><a id=__codelineno-0-917 name=__codelineno-0-917></a><span class=sd>            A string specifying the structure of residual block.</span>
</span><span id=__span-0-918><a id=__codelineno-0-918 name=__codelineno-0-918></a><span class=sd>            Check `ResidualBlock` documentation for more information.</span>
</span><span id=__span-0-919><a id=__codelineno-0-919 name=__codelineno-0-919></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-920><a id=__codelineno-0-920 name=__codelineno-0-920></a><span class=sd>        res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-921><a id=__codelineno-0-921 name=__codelineno-0-921></a><span class=sd>            The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-922><a id=__codelineno-0-922 name=__codelineno-0-922></a><span class=sd>            It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-923><a id=__codelineno-0-923 name=__codelineno-0-923></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-924><a id=__codelineno-0-924 name=__codelineno-0-924></a><span class=sd>        res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-925><a id=__codelineno-0-925 name=__codelineno-0-925></a><span class=sd>            Whether to skip padding in convolutions in the Residual block. Default is `None`.</span>
</span><span id=__span-0-926><a id=__codelineno-0-926 name=__codelineno-0-926></a><span class=sd>        groups: int, optional</span>
</span><span id=__span-0-927><a id=__codelineno-0-927 name=__codelineno-0-927></a><span class=sd>            The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-928><a id=__codelineno-0-928 name=__codelineno-0-928></a><span class=sd>        gated: bool, optional</span>
</span><span id=__span-0-929><a id=__codelineno-0-929 name=__codelineno-0-929></a><span class=sd>            Whether to use gated layer in `ResidualBlock`. Default is `None`.</span>
</span><span id=__span-0-930><a id=__codelineno-0-930 name=__codelineno-0-930></a><span class=sd>        learn_top_prior:</span>
</span><span id=__span-0-931><a id=__codelineno-0-931 name=__codelineno-0-931></a><span class=sd>            Whether to set the top prior as learnable.</span>
</span><span id=__span-0-932><a id=__codelineno-0-932 name=__codelineno-0-932></a><span class=sd>            If this is set to `False`, in the top-most layer the prior will be N(0,1).</span>
</span><span id=__span-0-933><a id=__codelineno-0-933 name=__codelineno-0-933></a><span class=sd>            Otherwise, we will still have a normal distribution whose parameters will be learnt.</span>
</span><span id=__span-0-934><a id=__codelineno-0-934 name=__codelineno-0-934></a><span class=sd>            Deafult is `False`.</span>
</span><span id=__span-0-935><a id=__codelineno-0-935 name=__codelineno-0-935></a><span class=sd>        top_prior_param_shape: Iterable[int], optional</span>
</span><span id=__span-0-936><a id=__codelineno-0-936 name=__codelineno-0-936></a><span class=sd>            The size of the tensor which expresses the mean and the variance</span>
</span><span id=__span-0-937><a id=__codelineno-0-937 name=__codelineno-0-937></a><span class=sd>            of the prior for the top most layer. Default is `None`.</span>
</span><span id=__span-0-938><a id=__codelineno-0-938 name=__codelineno-0-938></a><span class=sd>        analytical_kl: bool, optional</span>
</span><span id=__span-0-939><a id=__codelineno-0-939 name=__codelineno-0-939></a><span class=sd>            If True, KL divergence is calculated according to the analytical formula.</span>
</span><span id=__span-0-940><a id=__codelineno-0-940 name=__codelineno-0-940></a><span class=sd>            Otherwise, an MC approximation using sampled latents is calculated.</span>
</span><span id=__span-0-941><a id=__codelineno-0-941 name=__codelineno-0-941></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-942><a id=__codelineno-0-942 name=__codelineno-0-942></a><span class=sd>        bottomup_no_padding_mode: bool, optional</span>
</span><span id=__span-0-943><a id=__codelineno-0-943 name=__codelineno-0-943></a><span class=sd>            Whether padding is used in the different layers of the bottom-up pass.</span>
</span><span id=__span-0-944><a id=__codelineno-0-944 name=__codelineno-0-944></a><span class=sd>            It is meaningful to know this in advance in order to assess whether before</span>
</span><span id=__span-0-945><a id=__codelineno-0-945 name=__codelineno-0-945></a><span class=sd>            merging `bu_values` and `p_params` tensors any alignment is needed.</span>
</span><span id=__span-0-946><a id=__codelineno-0-946 name=__codelineno-0-946></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-947><a id=__codelineno-0-947 name=__codelineno-0-947></a><span class=sd>        topdown_no_padding_mode: bool, optional</span>
</span><span id=__span-0-948><a id=__codelineno-0-948 name=__codelineno-0-948></a><span class=sd>            Whether padding is used in the different layers of the top-down pass.</span>
</span><span id=__span-0-949><a id=__codelineno-0-949 name=__codelineno-0-949></a><span class=sd>            It is meaningful to know this in advance in order to assess whether before</span>
</span><span id=__span-0-950><a id=__codelineno-0-950 name=__codelineno-0-950></a><span class=sd>            merging `bu_values` and `p_params` tensors any alignment is needed.</span>
</span><span id=__span-0-951><a id=__codelineno-0-951 name=__codelineno-0-951></a><span class=sd>            The same information is also needed in handling the skip connections between</span>
</span><span id=__span-0-952><a id=__codelineno-0-952 name=__codelineno-0-952></a><span class=sd>            top-down layers. Default is `False`.</span>
</span><span id=__span-0-953><a id=__codelineno-0-953 name=__codelineno-0-953></a><span class=sd>        retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-954><a id=__codelineno-0-954 name=__codelineno-0-954></a><span class=sd>            If `True`, the size of Encoder&#39;s latent space is kept to `input_image_shape` within the topdown layer.</span>
</span><span id=__span-0-955><a id=__codelineno-0-955 name=__codelineno-0-955></a><span class=sd>            This implies that the oput spatial size equals the input spatial size.</span>
</span><span id=__span-0-956><a id=__codelineno-0-956 name=__codelineno-0-956></a><span class=sd>            To achieve this, we centercrop the intermediate representation.</span>
</span><span id=__span-0-957><a id=__codelineno-0-957 name=__codelineno-0-957></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-958><a id=__codelineno-0-958 name=__codelineno-0-958></a><span class=sd>        restricted_kl: bool, optional</span>
</span><span id=__span-0-959><a id=__codelineno-0-959 name=__codelineno-0-959></a><span class=sd>            Whether to compute the restricted version of KL Divergence.</span>
</span><span id=__span-0-960><a id=__codelineno-0-960 name=__codelineno-0-960></a><span class=sd>            See `NormalStochasticBlock2d` module for more information about its computation.</span>
</span><span id=__span-0-961><a id=__codelineno-0-961 name=__codelineno-0-961></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-962><a id=__codelineno-0-962 name=__codelineno-0-962></a><span class=sd>        vanilla_latent_hw: Iterable[int], optional</span>
</span><span id=__span-0-963><a id=__codelineno-0-963 name=__codelineno-0-963></a><span class=sd>            The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).</span>
</span><span id=__span-0-964><a id=__codelineno-0-964 name=__codelineno-0-964></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-965><a id=__codelineno-0-965 name=__codelineno-0-965></a><span class=sd>        non_stochastic_version: bool, optional</span>
</span><span id=__span-0-966><a id=__codelineno-0-966 name=__codelineno-0-966></a><span class=sd>            Whether to replace the stochastic layer that samples a latent variable from the latent distribiution with</span>
</span><span id=__span-0-967><a id=__codelineno-0-967 name=__codelineno-0-967></a><span class=sd>            a non-stochastic layer that simply drwas a sample as the mode of the latent distribution.</span>
</span><span id=__span-0-968><a id=__codelineno-0-968 name=__codelineno-0-968></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-969><a id=__codelineno-0-969 name=__codelineno-0-969></a><span class=sd>        input_image_shape: Tuple[int, int], optionalut</span>
</span><span id=__span-0-970><a id=__codelineno-0-970 name=__codelineno-0-970></a><span class=sd>            The shape of the input image tensor.</span>
</span><span id=__span-0-971><a id=__codelineno-0-971 name=__codelineno-0-971></a><span class=sd>            When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer</span>
</span><span id=__span-0-972><a id=__codelineno-0-972 name=__codelineno-0-972></a><span class=sd>            output has the same shape as the input. Default is `None`.</span>
</span><span id=__span-0-973><a id=__codelineno-0-973 name=__codelineno-0-973></a><span class=sd>        normalize_latent_factor: float, optional</span>
</span><span id=__span-0-974><a id=__codelineno-0-974 name=__codelineno-0-974></a><span class=sd>            A factor used to normalize the latent tensors `q_params`.</span>
</span><span id=__span-0-975><a id=__codelineno-0-975 name=__codelineno-0-975></a><span class=sd>            Specifically, normalization is done by dividing the latent tensor by this factor.</span>
</span><span id=__span-0-976><a id=__codelineno-0-976 name=__codelineno-0-976></a><span class=sd>            Default is 1.0.</span>
</span><span id=__span-0-977><a id=__codelineno-0-977 name=__codelineno-0-977></a><span class=sd>        conv2d_bias: bool, optional</span>
</span><span id=__span-0-978><a id=__codelineno-0-978 name=__codelineno-0-978></a><span class=sd>            Whether to use bias term is the convolutional blocks of this layer.</span>
</span><span id=__span-0-979><a id=__codelineno-0-979 name=__codelineno-0-979></a><span class=sd>            Default is `True`.</span>
</span><span id=__span-0-980><a id=__codelineno-0-980 name=__codelineno-0-980></a><span class=sd>        stochastic_use_naive_exponential: bool, optional</span>
</span><span id=__span-0-981><a id=__codelineno-0-981 name=__codelineno-0-981></a><span class=sd>            If `False`, in the NormalStochasticBlock2d exponentials are computed according</span>
</span><span id=__span-0-982><a id=__codelineno-0-982 name=__codelineno-0-982></a><span class=sd>            to the alternative definition provided by `StableExponential` class.</span>
</span><span id=__span-0-983><a id=__codelineno-0-983 name=__codelineno-0-983></a><span class=sd>            This should improve numerical stability in the training process.</span>
</span><span id=__span-0-984><a id=__codelineno-0-984 name=__codelineno-0-984></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-985><a id=__codelineno-0-985 name=__codelineno-0-985></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-986><a id=__codelineno-0-986 name=__codelineno-0-986></a>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-987><a id=__codelineno-0-987 name=__codelineno-0-987></a>
</span><span id=__span-0-988><a id=__codelineno-0-988 name=__codelineno-0-988></a>        <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span> <span class=o>=</span> <span class=n>is_top_layer</span>
</span><span id=__span-0-989><a id=__codelineno-0-989 name=__codelineno-0-989></a>        <span class=bp>self</span><span class=o>.</span><span class=n>z_dim</span> <span class=o>=</span> <span class=n>z_dim</span>
</span><span id=__span-0-990><a id=__codelineno-0-990 name=__codelineno-0-990></a>        <span class=bp>self</span><span class=o>.</span><span class=n>stochastic_skip</span> <span class=o>=</span> <span class=n>stochastic_skip</span>
</span><span id=__span-0-991><a id=__codelineno-0-991 name=__codelineno-0-991></a>        <span class=bp>self</span><span class=o>.</span><span class=n>learn_top_prior</span> <span class=o>=</span> <span class=n>learn_top_prior</span>
</span><span id=__span-0-992><a id=__codelineno-0-992 name=__codelineno-0-992></a>        <span class=bp>self</span><span class=o>.</span><span class=n>analytical_kl</span> <span class=o>=</span> <span class=n>analytical_kl</span>
</span><span id=__span-0-993><a id=__codelineno-0-993 name=__codelineno-0-993></a>        <span class=bp>self</span><span class=o>.</span><span class=n>bottomup_no_padding_mode</span> <span class=o>=</span> <span class=n>bottomup_no_padding_mode</span>
</span><span id=__span-0-994><a id=__codelineno-0-994 name=__codelineno-0-994></a>        <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=o>=</span> <span class=n>topdown_no_padding_mode</span>
</span><span id=__span-0-995><a id=__codelineno-0-995 name=__codelineno-0-995></a>        <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span> <span class=o>=</span> <span class=n>retain_spatial_dims</span>
</span><span id=__span-0-996><a id=__codelineno-0-996 name=__codelineno-0-996></a>        <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span> <span class=o>=</span> <span class=n>input_image_shape</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span> <span class=k>else</span> <span class=kc>None</span>
</span><span id=__span-0-997><a id=__codelineno-0-997 name=__codelineno-0-997></a>        <span class=bp>self</span><span class=o>.</span><span class=n>non_stochastic_version</span> <span class=o>=</span> <span class=n>non_stochastic_version</span>
</span><span id=__span-0-998><a id=__codelineno-0-998 name=__codelineno-0-998></a>        <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span> <span class=o>=</span> <span class=n>normalize_latent_factor</span>
</span><span id=__span-0-999><a id=__codelineno-0-999 name=__codelineno-0-999></a>        <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span> <span class=o>=</span> <span class=n>vanilla_latent_hw</span>
</span><span id=__span-0-1000><a id=__codelineno-0-1000 name=__codelineno-0-1000></a>
</span><span id=__span-0-1001><a id=__codelineno-0-1001 name=__codelineno-0-1001></a>        <span class=c1># Define top layer prior parameters, possibly learnable</span>
</span><span id=__span-0-1002><a id=__codelineno-0-1002 name=__codelineno-0-1002></a>        <span class=k>if</span> <span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1003><a id=__codelineno-0-1003 name=__codelineno-0-1003></a>            <span class=bp>self</span><span class=o>.</span><span class=n>top_prior_params</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span>
</span><span id=__span-0-1004><a id=__codelineno-0-1004 name=__codelineno-0-1004></a>                <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>top_prior_param_shape</span><span class=p>),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=n>learn_top_prior</span>
</span><span id=__span-0-1005><a id=__codelineno-0-1005 name=__codelineno-0-1005></a>            <span class=p>)</span>
</span><span id=__span-0-1006><a id=__codelineno-0-1006 name=__codelineno-0-1006></a>
</span><span id=__span-0-1007><a id=__codelineno-0-1007 name=__codelineno-0-1007></a>        <span class=c1># Downsampling steps left to do in this layer</span>
</span><span id=__span-0-1008><a id=__codelineno-0-1008 name=__codelineno-0-1008></a>        <span class=n>dws_left</span> <span class=o>=</span> <span class=n>downsampling_steps</span>
</span><span id=__span-0-1009><a id=__codelineno-0-1009 name=__codelineno-0-1009></a>
</span><span id=__span-0-1010><a id=__codelineno-0-1010 name=__codelineno-0-1010></a>        <span class=c1># Define deterministic top-down block, which is a sequence of deterministic</span>
</span><span id=__span-0-1011><a id=__codelineno-0-1011 name=__codelineno-0-1011></a>        <span class=c1># residual blocks with (optional) downsampling.</span>
</span><span id=__span-0-1012><a id=__codelineno-0-1012 name=__codelineno-0-1012></a>        <span class=n>block_list</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-1013><a id=__codelineno-0-1013 name=__codelineno-0-1013></a>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_res_blocks</span><span class=p>):</span>
</span><span id=__span-0-1014><a id=__codelineno-0-1014 name=__codelineno-0-1014></a>            <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-1015><a id=__codelineno-0-1015 name=__codelineno-0-1015></a>            <span class=k>if</span> <span class=n>dws_left</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-1016><a id=__codelineno-0-1016 name=__codelineno-0-1016></a>                <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>True</span>
</span><span id=__span-0-1017><a id=__codelineno-0-1017 name=__codelineno-0-1017></a>                <span class=n>dws_left</span> <span class=o>-=</span> <span class=mi>1</span>
</span><span id=__span-0-1018><a id=__codelineno-0-1018 name=__codelineno-0-1018></a>            <span class=n>block_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span><span id=__span-0-1019><a id=__codelineno-0-1019 name=__codelineno-0-1019></a>                <span class=n>TopDownDeterministicResBlock</span><span class=p>(</span>
</span><span id=__span-0-1020><a id=__codelineno-0-1020 name=__codelineno-0-1020></a>                    <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1021><a id=__codelineno-0-1021 name=__codelineno-0-1021></a>                    <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1022><a id=__codelineno-0-1022 name=__codelineno-0-1022></a>                    <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1023><a id=__codelineno-0-1023 name=__codelineno-0-1023></a>                    <span class=n>upsample</span><span class=o>=</span><span class=n>do_resample</span><span class=p>,</span>
</span><span id=__span-0-1024><a id=__codelineno-0-1024 name=__codelineno-0-1024></a>                    <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1025><a id=__codelineno-0-1025 name=__codelineno-0-1025></a>                    <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1026><a id=__codelineno-0-1026 name=__codelineno-0-1026></a>                    <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1027><a id=__codelineno-0-1027 name=__codelineno-0-1027></a>                    <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1028><a id=__codelineno-0-1028 name=__codelineno-0-1028></a>                    <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-1029><a id=__codelineno-0-1029 name=__codelineno-0-1029></a>                    <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-1030><a id=__codelineno-0-1030 name=__codelineno-0-1030></a>                    <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1031><a id=__codelineno-0-1031 name=__codelineno-0-1031></a>                    <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-1032><a id=__codelineno-0-1032 name=__codelineno-0-1032></a>                <span class=p>)</span>
</span><span id=__span-0-1033><a id=__codelineno-0-1033 name=__codelineno-0-1033></a>            <span class=p>)</span>
</span><span id=__span-0-1034><a id=__codelineno-0-1034 name=__codelineno-0-1034></a>        <span class=bp>self</span><span class=o>.</span><span class=n>deterministic_block</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>block_list</span><span class=p>)</span>
</span><span id=__span-0-1035><a id=__codelineno-0-1035 name=__codelineno-0-1035></a>
</span><span id=__span-0-1036><a id=__codelineno-0-1036 name=__codelineno-0-1036></a>        <span class=c1># Define stochastic block with 2D convolutions</span>
</span><span id=__span-0-1037><a id=__codelineno-0-1037 name=__codelineno-0-1037></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>non_stochastic_version</span><span class=p>:</span>
</span><span id=__span-0-1038><a id=__codelineno-0-1038 name=__codelineno-0-1038></a>            <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span> <span class=o>=</span> <span class=n>NonStochasticBlock2d</span><span class=p>(</span>
</span><span id=__span-0-1039><a id=__codelineno-0-1039 name=__codelineno-0-1039></a>                <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1040><a id=__codelineno-0-1040 name=__codelineno-0-1040></a>                <span class=n>c_vars</span><span class=o>=</span><span class=n>z_dim</span><span class=p>,</span>
</span><span id=__span-0-1041><a id=__codelineno-0-1041 name=__codelineno-0-1041></a>                <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1042><a id=__codelineno-0-1042 name=__codelineno-0-1042></a>                <span class=n>transform_p_params</span><span class=o>=</span><span class=p>(</span><span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>),</span>
</span><span id=__span-0-1043><a id=__codelineno-0-1043 name=__codelineno-0-1043></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-1044><a id=__codelineno-0-1044 name=__codelineno-0-1044></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1045><a id=__codelineno-0-1045 name=__codelineno-0-1045></a>            <span class=p>)</span>
</span><span id=__span-0-1046><a id=__codelineno-0-1046 name=__codelineno-0-1046></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1047><a id=__codelineno-0-1047 name=__codelineno-0-1047></a>            <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span> <span class=o>=</span> <span class=n>NormalStochasticBlock2d</span><span class=p>(</span>
</span><span id=__span-0-1048><a id=__codelineno-0-1048 name=__codelineno-0-1048></a>                <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1049><a id=__codelineno-0-1049 name=__codelineno-0-1049></a>                <span class=n>c_vars</span><span class=o>=</span><span class=n>z_dim</span><span class=p>,</span>
</span><span id=__span-0-1050><a id=__codelineno-0-1050 name=__codelineno-0-1050></a>                <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1051><a id=__codelineno-0-1051 name=__codelineno-0-1051></a>                <span class=n>transform_p_params</span><span class=o>=</span><span class=p>(</span><span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>),</span>
</span><span id=__span-0-1052><a id=__codelineno-0-1052 name=__codelineno-0-1052></a>                <span class=n>vanilla_latent_hw</span><span class=o>=</span><span class=n>vanilla_latent_hw</span><span class=p>,</span>
</span><span id=__span-0-1053><a id=__codelineno-0-1053 name=__codelineno-0-1053></a>                <span class=n>restricted_kl</span><span class=o>=</span><span class=n>restricted_kl</span><span class=p>,</span>
</span><span id=__span-0-1054><a id=__codelineno-0-1054 name=__codelineno-0-1054></a>                <span class=n>use_naive_exponential</span><span class=o>=</span><span class=n>stochastic_use_naive_exponential</span><span class=p>,</span>
</span><span id=__span-0-1055><a id=__codelineno-0-1055 name=__codelineno-0-1055></a>            <span class=p>)</span>
</span><span id=__span-0-1056><a id=__codelineno-0-1056 name=__codelineno-0-1056></a>
</span><span id=__span-0-1057><a id=__codelineno-0-1057 name=__codelineno-0-1057></a>        <span class=k>if</span> <span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1058><a id=__codelineno-0-1058 name=__codelineno-0-1058></a>            <span class=c1># Merge layer: it combines bottom-up inference and top-down</span>
</span><span id=__span-0-1059><a id=__codelineno-0-1059 name=__codelineno-0-1059></a>            <span class=c1># generative outcomes to give posterior parameters</span>
</span><span id=__span-0-1060><a id=__codelineno-0-1060 name=__codelineno-0-1060></a>            <span class=bp>self</span><span class=o>.</span><span class=n>merge</span> <span class=o>=</span> <span class=n>MergeLayer</span><span class=p>(</span>
</span><span id=__span-0-1061><a id=__codelineno-0-1061 name=__codelineno-0-1061></a>                <span class=n>channels</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1062><a id=__codelineno-0-1062 name=__codelineno-0-1062></a>                <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-1063><a id=__codelineno-0-1063 name=__codelineno-0-1063></a>                <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1064><a id=__codelineno-0-1064 name=__codelineno-0-1064></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1065><a id=__codelineno-0-1065 name=__codelineno-0-1065></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1066><a id=__codelineno-0-1066 name=__codelineno-0-1066></a>                <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1067><a id=__codelineno-0-1067 name=__codelineno-0-1067></a>                <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1068><a id=__codelineno-0-1068 name=__codelineno-0-1068></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1069><a id=__codelineno-0-1069 name=__codelineno-0-1069></a>            <span class=p>)</span>
</span><span id=__span-0-1070><a id=__codelineno-0-1070 name=__codelineno-0-1070></a>
</span><span id=__span-0-1071><a id=__codelineno-0-1071 name=__codelineno-0-1071></a>            <span class=c1># Skip connection that goes around the stochastic top-down layer</span>
</span><span id=__span-0-1072><a id=__codelineno-0-1072 name=__codelineno-0-1072></a>            <span class=k>if</span> <span class=n>stochastic_skip</span><span class=p>:</span>
</span><span id=__span-0-1073><a id=__codelineno-0-1073 name=__codelineno-0-1073></a>                <span class=bp>self</span><span class=o>.</span><span class=n>skip_connection_merger</span> <span class=o>=</span> <span class=n>SkipConnectionMerger</span><span class=p>(</span>
</span><span id=__span-0-1074><a id=__codelineno-0-1074 name=__codelineno-0-1074></a>                    <span class=n>channels</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1075><a id=__codelineno-0-1075 name=__codelineno-0-1075></a>                    <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1076><a id=__codelineno-0-1076 name=__codelineno-0-1076></a>                    <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1077><a id=__codelineno-0-1077 name=__codelineno-0-1077></a>                    <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1078><a id=__codelineno-0-1078 name=__codelineno-0-1078></a>                    <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1079><a id=__codelineno-0-1079 name=__codelineno-0-1079></a>                    <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-1080><a id=__codelineno-0-1080 name=__codelineno-0-1080></a>                    <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1081><a id=__codelineno-0-1081 name=__codelineno-0-1081></a>                    <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1082><a id=__codelineno-0-1082 name=__codelineno-0-1082></a>                    <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-1083><a id=__codelineno-0-1083 name=__codelineno-0-1083></a>                <span class=p>)</span>
</span><span id=__span-0-1084><a id=__codelineno-0-1084 name=__codelineno-0-1084></a>
</span><span id=__span-0-1085><a id=__codelineno-0-1085 name=__codelineno-0-1085></a>        <span class=c1># print(f&#39;[{self.__class__.__name__}] normalize_latent_factor:{self.normalize_latent_factor}&#39;)</span>
</span><span id=__span-0-1086><a id=__codelineno-0-1086 name=__codelineno-0-1086></a>
</span><span id=__span-0-1087><a id=__codelineno-0-1087 name=__codelineno-0-1087></a>    <span class=k>def</span> <span class=nf>sample_from_q</span><span class=p>(</span>
</span><span id=__span-0-1088><a id=__codelineno-0-1088 name=__codelineno-0-1088></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1089><a id=__codelineno-0-1089 name=__codelineno-0-1089></a>        <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1090><a id=__codelineno-0-1090 name=__codelineno-0-1090></a>        <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1091><a id=__codelineno-0-1091 name=__codelineno-0-1091></a>        <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1092><a id=__codelineno-0-1092 name=__codelineno-0-1092></a>        <span class=n>mask</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1093><a id=__codelineno-0-1093 name=__codelineno-0-1093></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1094><a id=__codelineno-0-1094 name=__codelineno-0-1094></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1095><a id=__codelineno-0-1095 name=__codelineno-0-1095></a><span class=sd>        This method computes the latent inference distribution q(z_i|z_{i+1}) amd samples a latent tensor from it.</span>
</span><span id=__span-0-1096><a id=__codelineno-0-1096 name=__codelineno-0-1096></a>
</span><span id=__span-0-1097><a id=__codelineno-0-1097 name=__codelineno-0-1097></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1098><a id=__codelineno-0-1098 name=__codelineno-0-1098></a><span class=sd>        ----------</span>
</span><span id=__span-0-1099><a id=__codelineno-0-1099 name=__codelineno-0-1099></a><span class=sd>        input_: torch.Tensor</span>
</span><span id=__span-0-1100><a id=__codelineno-0-1100 name=__codelineno-0-1100></a><span class=sd>            The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1101><a id=__codelineno-0-1101 name=__codelineno-0-1101></a><span class=sd>        bu_value: torch.Tensor</span>
</span><span id=__span-0-1102><a id=__codelineno-0-1102 name=__codelineno-0-1102></a><span class=sd>            The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1103><a id=__codelineno-0-1103 name=__codelineno-0-1103></a><span class=sd>            at the correspondent hierarchical layer.</span>
</span><span id=__span-0-1104><a id=__codelineno-0-1104 name=__codelineno-0-1104></a><span class=sd>        var_clip_max: float, optional</span>
</span><span id=__span-0-1105><a id=__codelineno-0-1105 name=__codelineno-0-1105></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1106><a id=__codelineno-0-1106 name=__codelineno-0-1106></a><span class=sd>            Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1107><a id=__codelineno-0-1107 name=__codelineno-0-1107></a><span class=sd>        mask: Union[None, torch.Tensor], optional</span>
</span><span id=__span-0-1108><a id=__codelineno-0-1108 name=__codelineno-0-1108></a><span class=sd>            A tensor that is used to mask the sampled latent tensor. Default is `None`.</span>
</span><span id=__span-0-1109><a id=__codelineno-0-1109 name=__codelineno-0-1109></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1110><a id=__codelineno-0-1110 name=__codelineno-0-1110></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>  <span class=c1># In top layer, we don&#39;t merge bu_value with p_params</span>
</span><span id=__span-0-1111><a id=__codelineno-0-1111 name=__codelineno-0-1111></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=n>bu_value</span>
</span><span id=__span-0-1112><a id=__codelineno-0-1112 name=__codelineno-0-1112></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1113><a id=__codelineno-0-1113 name=__codelineno-0-1113></a>            <span class=c1># NOTE: Here the assumption is that the vampprior is only applied on the top layer.</span>
</span><span id=__span-0-1114><a id=__codelineno-0-1114 name=__codelineno-0-1114></a>            <span class=n>n_img_prior</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1115><a id=__codelineno-0-1115 name=__codelineno-0-1115></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_p_params</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=p>)</span>
</span><span id=__span-0-1116><a id=__codelineno-0-1116 name=__codelineno-0-1116></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1117><a id=__codelineno-0-1117 name=__codelineno-0-1117></a>
</span><span id=__span-0-1118><a id=__codelineno-0-1118 name=__codelineno-0-1118></a>        <span class=n>sample</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span><span class=o>.</span><span class=n>sample_from_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1119><a id=__codelineno-0-1119 name=__codelineno-0-1119></a>
</span><span id=__span-0-1120><a id=__codelineno-0-1120 name=__codelineno-0-1120></a>        <span class=k>if</span> <span class=n>mask</span><span class=p>:</span>
</span><span id=__span-0-1121><a id=__codelineno-0-1121 name=__codelineno-0-1121></a>            <span class=k>return</span> <span class=n>sample</span><span class=p>[</span><span class=n>mask</span><span class=p>]</span>
</span><span id=__span-0-1122><a id=__codelineno-0-1122 name=__codelineno-0-1122></a>
</span><span id=__span-0-1123><a id=__codelineno-0-1123 name=__codelineno-0-1123></a>        <span class=k>return</span> <span class=n>sample</span>
</span><span id=__span-0-1124><a id=__codelineno-0-1124 name=__codelineno-0-1124></a>
</span><span id=__span-0-1125><a id=__codelineno-0-1125 name=__codelineno-0-1125></a>    <span class=k>def</span> <span class=nf>get_p_params</span><span class=p>(</span>
</span><span id=__span-0-1126><a id=__codelineno-0-1126 name=__codelineno-0-1126></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1127><a id=__codelineno-0-1127 name=__codelineno-0-1127></a>        <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1128><a id=__codelineno-0-1128 name=__codelineno-0-1128></a>        <span class=n>n_img_prior</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1129><a id=__codelineno-0-1129 name=__codelineno-0-1129></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1130><a id=__codelineno-0-1130 name=__codelineno-0-1130></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1131><a id=__codelineno-0-1131 name=__codelineno-0-1131></a><span class=sd>        This method returns the parameters of the prior distribution p(z_i|z_{i+1}) for the latent tensor</span>
</span><span id=__span-0-1132><a id=__codelineno-0-1132 name=__codelineno-0-1132></a><span class=sd>        depending on the hierarchical level of the layer and other specific conditions.</span>
</span><span id=__span-0-1133><a id=__codelineno-0-1133 name=__codelineno-0-1133></a>
</span><span id=__span-0-1134><a id=__codelineno-0-1134 name=__codelineno-0-1134></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1135><a id=__codelineno-0-1135 name=__codelineno-0-1135></a><span class=sd>        ----------</span>
</span><span id=__span-0-1136><a id=__codelineno-0-1136 name=__codelineno-0-1136></a><span class=sd>        input_: torch.Tensor</span>
</span><span id=__span-0-1137><a id=__codelineno-0-1137 name=__codelineno-0-1137></a><span class=sd>            The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1138><a id=__codelineno-0-1138 name=__codelineno-0-1138></a><span class=sd>        n_img_prior: int</span>
</span><span id=__span-0-1139><a id=__codelineno-0-1139 name=__codelineno-0-1139></a><span class=sd>            The number of images to be generated from the unconditional prior distribution p(z_L).</span>
</span><span id=__span-0-1140><a id=__codelineno-0-1140 name=__codelineno-0-1140></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1141><a id=__codelineno-0-1141 name=__codelineno-0-1141></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1142><a id=__codelineno-0-1142 name=__codelineno-0-1142></a>
</span><span id=__span-0-1143><a id=__codelineno-0-1143 name=__codelineno-0-1143></a>        <span class=c1># If top layer, define p_params as the ones of the prior p(z_L)</span>
</span><span id=__span-0-1144><a id=__codelineno-0-1144 name=__codelineno-0-1144></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1145><a id=__codelineno-0-1145 name=__codelineno-0-1145></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>top_prior_params</span>
</span><span id=__span-0-1146><a id=__codelineno-0-1146 name=__codelineno-0-1146></a>
</span><span id=__span-0-1147><a id=__codelineno-0-1147 name=__codelineno-0-1147></a>            <span class=c1># Sample specific number of images by expanding the prior</span>
</span><span id=__span-0-1148><a id=__codelineno-0-1148 name=__codelineno-0-1148></a>            <span class=k>if</span> <span class=n>n_img_prior</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1149><a id=__codelineno-0-1149 name=__codelineno-0-1149></a>                <span class=n>p_params</span> <span class=o>=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>n_img_prior</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1150><a id=__codelineno-0-1150 name=__codelineno-0-1150></a>
</span><span id=__span-0-1151><a id=__codelineno-0-1151 name=__codelineno-0-1151></a>        <span class=c1># Else the input from the layer above is p_params itself</span>
</span><span id=__span-0-1152><a id=__codelineno-0-1152 name=__codelineno-0-1152></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1153><a id=__codelineno-0-1153 name=__codelineno-0-1153></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=n>input_</span>
</span><span id=__span-0-1154><a id=__codelineno-0-1154 name=__codelineno-0-1154></a>
</span><span id=__span-0-1155><a id=__codelineno-0-1155 name=__codelineno-0-1155></a>        <span class=k>return</span> <span class=n>p_params</span>
</span><span id=__span-0-1156><a id=__codelineno-0-1156 name=__codelineno-0-1156></a>
</span><span id=__span-0-1157><a id=__codelineno-0-1157 name=__codelineno-0-1157></a>    <span class=k>def</span> <span class=nf>align_pparams_buvalue</span><span class=p>(</span>
</span><span id=__span-0-1158><a id=__codelineno-0-1158 name=__codelineno-0-1158></a>        <span class=bp>self</span><span class=p>,</span> <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span>
</span><span id=__span-0-1159><a id=__codelineno-0-1159 name=__codelineno-0-1159></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-1160><a id=__codelineno-0-1160 name=__codelineno-0-1160></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1161><a id=__codelineno-0-1161 name=__codelineno-0-1161></a><span class=sd>        In case the padding is not used either (or both) in encoder and decoder, we could have a shape mismatch</span>
</span><span id=__span-0-1162><a id=__codelineno-0-1162 name=__codelineno-0-1162></a><span class=sd>        in the spatial dimensions (usually, dim=2 &amp; dim=3).</span>
</span><span id=__span-0-1163><a id=__codelineno-0-1163 name=__codelineno-0-1163></a><span class=sd>        This method performs a centercrop to ensure that both remain aligned.</span>
</span><span id=__span-0-1164><a id=__codelineno-0-1164 name=__codelineno-0-1164></a>
</span><span id=__span-0-1165><a id=__codelineno-0-1165 name=__codelineno-0-1165></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1166><a id=__codelineno-0-1166 name=__codelineno-0-1166></a><span class=sd>        ----------</span>
</span><span id=__span-0-1167><a id=__codelineno-0-1167 name=__codelineno-0-1167></a><span class=sd>        p_params: torch.Tensor</span>
</span><span id=__span-0-1168><a id=__codelineno-0-1168 name=__codelineno-0-1168></a><span class=sd>            The tensor defining the parameters /mu_p and /sigma_p for the latent distribution p(z_i|z_{i+1}).</span>
</span><span id=__span-0-1169><a id=__codelineno-0-1169 name=__codelineno-0-1169></a><span class=sd>        bu_value: torch.Tensor</span>
</span><span id=__span-0-1170><a id=__codelineno-0-1170 name=__codelineno-0-1170></a><span class=sd>            The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1171><a id=__codelineno-0-1171 name=__codelineno-0-1171></a><span class=sd>            at the correspondent hierarchical layer.</span>
</span><span id=__span-0-1172><a id=__codelineno-0-1172 name=__codelineno-0-1172></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1173><a id=__codelineno-0-1173 name=__codelineno-0-1173></a>        <span class=k>if</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span> <span class=o>!=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]:</span>
</span><span id=__span-0-1174><a id=__codelineno-0-1174 name=__codelineno-0-1174></a>            <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>bottomup_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1175><a id=__codelineno-0-1175 name=__codelineno-0-1175></a>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1176><a id=__codelineno-0-1176 name=__codelineno-0-1176></a>                <span class=k>assert</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1177><a id=__codelineno-0-1177 name=__codelineno-0-1177></a>                <span class=n>bu_value</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1178><a id=__codelineno-0-1178 name=__codelineno-0-1178></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1179><a id=__codelineno-0-1179 name=__codelineno-0-1179></a>                <span class=k>if</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span><span id=__span-0-1180><a id=__codelineno-0-1180 name=__codelineno-0-1180></a>                    <span class=n>bu_value</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1181><a id=__codelineno-0-1181 name=__codelineno-0-1181></a>                <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1182><a id=__codelineno-0-1182 name=__codelineno-0-1182></a>                    <span class=n>p_params</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1183><a id=__codelineno-0-1183 name=__codelineno-0-1183></a>        <span class=k>return</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span>
</span><span id=__span-0-1184><a id=__codelineno-0-1184 name=__codelineno-0-1184></a>
</span><span id=__span-0-1185><a id=__codelineno-0-1185 name=__codelineno-0-1185></a>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1186><a id=__codelineno-0-1186 name=__codelineno-0-1186></a>        <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1187><a id=__codelineno-0-1187 name=__codelineno-0-1187></a>        <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1188><a id=__codelineno-0-1188 name=__codelineno-0-1188></a>        <span class=n>skip_connection_input</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1189><a id=__codelineno-0-1189 name=__codelineno-0-1189></a>        <span class=n>inference_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1190><a id=__codelineno-0-1190 name=__codelineno-0-1190></a>        <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1191><a id=__codelineno-0-1191 name=__codelineno-0-1191></a>        <span class=n>n_img_prior</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1192><a id=__codelineno-0-1192 name=__codelineno-0-1192></a>        <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1193><a id=__codelineno-0-1193 name=__codelineno-0-1193></a>        <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1194><a id=__codelineno-0-1194 name=__codelineno-0-1194></a>        <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1195><a id=__codelineno-0-1195 name=__codelineno-0-1195></a>        <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1196><a id=__codelineno-0-1196 name=__codelineno-0-1196></a>        <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1197><a id=__codelineno-0-1197 name=__codelineno-0-1197></a>        <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1198><a id=__codelineno-0-1198 name=__codelineno-0-1198></a>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1199><a id=__codelineno-0-1199 name=__codelineno-0-1199></a><span class=w>        </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1200><a id=__codelineno-0-1200 name=__codelineno-0-1200></a><span class=sd>        Parameters</span>
</span><span id=__span-0-1201><a id=__codelineno-0-1201 name=__codelineno-0-1201></a><span class=sd>        ----------</span>
</span><span id=__span-0-1202><a id=__codelineno-0-1202 name=__codelineno-0-1202></a><span class=sd>        input_: torch.Tensor, optional</span>
</span><span id=__span-0-1203><a id=__codelineno-0-1203 name=__codelineno-0-1203></a><span class=sd>            The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1204><a id=__codelineno-0-1204 name=__codelineno-0-1204></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-1205><a id=__codelineno-0-1205 name=__codelineno-0-1205></a><span class=sd>        skip_connection_input: torch.Tensor, optional</span>
</span><span id=__span-0-1206><a id=__codelineno-0-1206 name=__codelineno-0-1206></a><span class=sd>            The tensor brought by the skip connection between the current and the previous top-down layer.</span>
</span><span id=__span-0-1207><a id=__codelineno-0-1207 name=__codelineno-0-1207></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-1208><a id=__codelineno-0-1208 name=__codelineno-0-1208></a><span class=sd>        inference_mode: bool, optional</span>
</span><span id=__span-0-1209><a id=__codelineno-0-1209 name=__codelineno-0-1209></a><span class=sd>            Whether the layer is in inference mode. See NOTE 2 in class description for more info.</span>
</span><span id=__span-0-1210><a id=__codelineno-0-1210 name=__codelineno-0-1210></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1211><a id=__codelineno-0-1211 name=__codelineno-0-1211></a><span class=sd>        bu_value: torch.Tensor, optional</span>
</span><span id=__span-0-1212><a id=__codelineno-0-1212 name=__codelineno-0-1212></a><span class=sd>            The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1213><a id=__codelineno-0-1213 name=__codelineno-0-1213></a><span class=sd>            at the correspondent hierarchical layer. Default is `None`.</span>
</span><span id=__span-0-1214><a id=__codelineno-0-1214 name=__codelineno-0-1214></a><span class=sd>        n_img_prior: int, optional</span>
</span><span id=__span-0-1215><a id=__codelineno-0-1215 name=__codelineno-0-1215></a><span class=sd>            The number of images to be generated from the unconditional prior distribution p(z_L).</span>
</span><span id=__span-0-1216><a id=__codelineno-0-1216 name=__codelineno-0-1216></a><span class=sd>            Default is `None`.</span>
</span><span id=__span-0-1217><a id=__codelineno-0-1217 name=__codelineno-0-1217></a><span class=sd>        forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1218><a id=__codelineno-0-1218 name=__codelineno-0-1218></a><span class=sd>            A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,</span>
</span><span id=__span-0-1219><a id=__codelineno-0-1219 name=__codelineno-0-1219></a><span class=sd>            hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1220><a id=__codelineno-0-1220 name=__codelineno-0-1220></a><span class=sd>        use_mode: bool, optional</span>
</span><span id=__span-0-1221><a id=__codelineno-0-1221 name=__codelineno-0-1221></a><span class=sd>            Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1222><a id=__codelineno-0-1222 name=__codelineno-0-1222></a><span class=sd>            In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1223><a id=__codelineno-0-1223 name=__codelineno-0-1223></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1224><a id=__codelineno-0-1224 name=__codelineno-0-1224></a><span class=sd>        force_constant_output: bool, optional</span>
</span><span id=__span-0-1225><a id=__codelineno-0-1225 name=__codelineno-0-1225></a><span class=sd>            Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1226><a id=__codelineno-0-1226 name=__codelineno-0-1226></a><span class=sd>            This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1227><a id=__codelineno-0-1227 name=__codelineno-0-1227></a><span class=sd>            Default is `False`.</span>
</span><span id=__span-0-1228><a id=__codelineno-0-1228 name=__codelineno-0-1228></a><span class=sd>        mode_pred: bool, optional</span>
</span><span id=__span-0-1229><a id=__codelineno-0-1229 name=__codelineno-0-1229></a><span class=sd>            Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1230><a id=__codelineno-0-1230 name=__codelineno-0-1230></a><span class=sd>        use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1231><a id=__codelineno-0-1231 name=__codelineno-0-1231></a><span class=sd>            Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1232><a id=__codelineno-0-1232 name=__codelineno-0-1232></a><span class=sd>        var_clip_max: float</span>
</span><span id=__span-0-1233><a id=__codelineno-0-1233 name=__codelineno-0-1233></a><span class=sd>            The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1234><a id=__codelineno-0-1234 name=__codelineno-0-1234></a><span class=sd>            Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1235><a id=__codelineno-0-1235 name=__codelineno-0-1235></a><span class=sd>        &quot;&quot;&quot;</span>
</span><span id=__span-0-1236><a id=__codelineno-0-1236 name=__codelineno-0-1236></a>        <span class=c1># Check consistency of arguments</span>
</span><span id=__span-0-1237><a id=__codelineno-0-1237 name=__codelineno-0-1237></a>        <span class=n>inputs_none</span> <span class=o>=</span> <span class=n>input_</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>skip_connection_input</span> <span class=ow>is</span> <span class=kc>None</span>
</span><span id=__span-0-1238><a id=__codelineno-0-1238 name=__codelineno-0-1238></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>inputs_none</span><span class=p>:</span>
</span><span id=__span-0-1239><a id=__codelineno-0-1239 name=__codelineno-0-1239></a>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;In top layer, inputs should be None&quot;</span><span class=p>)</span>
</span><span id=__span-0-1240><a id=__codelineno-0-1240 name=__codelineno-0-1240></a>
</span><span id=__span-0-1241><a id=__codelineno-0-1241 name=__codelineno-0-1241></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_p_params</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=p>)</span>
</span><span id=__span-0-1242><a id=__codelineno-0-1242 name=__codelineno-0-1242></a>
</span><span id=__span-0-1243><a id=__codelineno-0-1243 name=__codelineno-0-1243></a>        <span class=c1># Get the parameters for the latent distribution to sample from</span>
</span><span id=__span-0-1244><a id=__codelineno-0-1244 name=__codelineno-0-1244></a>        <span class=k>if</span> <span class=n>inference_mode</span><span class=p>:</span>
</span><span id=__span-0-1245><a id=__codelineno-0-1245 name=__codelineno-0-1245></a>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1246><a id=__codelineno-0-1246 name=__codelineno-0-1246></a>                <span class=n>q_params</span> <span class=o>=</span> <span class=n>bu_value</span>
</span><span id=__span-0-1247><a id=__codelineno-0-1247 name=__codelineno-0-1247></a>                <span class=k>if</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1248><a id=__codelineno-0-1248 name=__codelineno-0-1248></a>                    <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>align_pparams_buvalue</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>)</span>
</span><span id=__span-0-1249><a id=__codelineno-0-1249 name=__codelineno-0-1249></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1250><a id=__codelineno-0-1250 name=__codelineno-0-1250></a>                <span class=k>if</span> <span class=n>use_uncond_mode</span><span class=p>:</span>
</span><span id=__span-0-1251><a id=__codelineno-0-1251 name=__codelineno-0-1251></a>                    <span class=n>q_params</span> <span class=o>=</span> <span class=n>p_params</span>
</span><span id=__span-0-1252><a id=__codelineno-0-1252 name=__codelineno-0-1252></a>                <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1253><a id=__codelineno-0-1253 name=__codelineno-0-1253></a>                    <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>align_pparams_buvalue</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>)</span>
</span><span id=__span-0-1254><a id=__codelineno-0-1254 name=__codelineno-0-1254></a>                    <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1255><a id=__codelineno-0-1255 name=__codelineno-0-1255></a>        <span class=c1># In generative mode, q is not used</span>
</span><span id=__span-0-1256><a id=__codelineno-0-1256 name=__codelineno-0-1256></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1257><a id=__codelineno-0-1257 name=__codelineno-0-1257></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1258><a id=__codelineno-0-1258 name=__codelineno-0-1258></a>
</span><span id=__span-0-1259><a id=__codelineno-0-1259 name=__codelineno-0-1259></a>        <span class=c1># NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})</span>
</span><span id=__span-0-1260><a id=__codelineno-0-1260 name=__codelineno-0-1260></a>        <span class=c1># depending on the mode (hence, in practice, by checking whether q_params is None).</span>
</span><span id=__span-0-1261><a id=__codelineno-0-1261 name=__codelineno-0-1261></a>
</span><span id=__span-0-1262><a id=__codelineno-0-1262 name=__codelineno-0-1262></a>        <span class=c1># Normalization of latent space parameters:</span>
</span><span id=__span-0-1263><a id=__codelineno-0-1263 name=__codelineno-0-1263></a>        <span class=c1># it is done, purely for stablity. See Very deep VAEs generalize autoregressive models.</span>
</span><span id=__span-0-1264><a id=__codelineno-0-1264 name=__codelineno-0-1264></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span><span class=p>:</span>
</span><span id=__span-0-1265><a id=__codelineno-0-1265 name=__codelineno-0-1265></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=n>q_params</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span>
</span><span id=__span-0-1266><a id=__codelineno-0-1266 name=__codelineno-0-1266></a>
</span><span id=__span-0-1267><a id=__codelineno-0-1267 name=__codelineno-0-1267></a>        <span class=c1># Sample (and process) a latent tensor in the stochastic layer</span>
</span><span id=__span-0-1268><a id=__codelineno-0-1268 name=__codelineno-0-1268></a>        <span class=n>x</span><span class=p>,</span> <span class=n>data_stoch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span><span class=p>(</span>
</span><span id=__span-0-1269><a id=__codelineno-0-1269 name=__codelineno-0-1269></a>            <span class=n>p_params</span><span class=o>=</span><span class=n>p_params</span><span class=p>,</span>
</span><span id=__span-0-1270><a id=__codelineno-0-1270 name=__codelineno-0-1270></a>            <span class=n>q_params</span><span class=o>=</span><span class=n>q_params</span><span class=p>,</span>
</span><span id=__span-0-1271><a id=__codelineno-0-1271 name=__codelineno-0-1271></a>            <span class=n>forced_latent</span><span class=o>=</span><span class=n>forced_latent</span><span class=p>,</span>
</span><span id=__span-0-1272><a id=__codelineno-0-1272 name=__codelineno-0-1272></a>            <span class=n>use_mode</span><span class=o>=</span><span class=n>use_mode</span><span class=p>,</span>
</span><span id=__span-0-1273><a id=__codelineno-0-1273 name=__codelineno-0-1273></a>            <span class=n>force_constant_output</span><span class=o>=</span><span class=n>force_constant_output</span><span class=p>,</span>
</span><span id=__span-0-1274><a id=__codelineno-0-1274 name=__codelineno-0-1274></a>            <span class=n>analytical_kl</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>analytical_kl</span><span class=p>,</span>
</span><span id=__span-0-1275><a id=__codelineno-0-1275 name=__codelineno-0-1275></a>            <span class=n>mode_pred</span><span class=o>=</span><span class=n>mode_pred</span><span class=p>,</span>
</span><span id=__span-0-1276><a id=__codelineno-0-1276 name=__codelineno-0-1276></a>            <span class=n>use_uncond_mode</span><span class=o>=</span><span class=n>use_uncond_mode</span><span class=p>,</span>
</span><span id=__span-0-1277><a id=__codelineno-0-1277 name=__codelineno-0-1277></a>            <span class=n>var_clip_max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>,</span>
</span><span id=__span-0-1278><a id=__codelineno-0-1278 name=__codelineno-0-1278></a>        <span class=p>)</span>
</span><span id=__span-0-1279><a id=__codelineno-0-1279 name=__codelineno-0-1279></a>
</span><span id=__span-0-1280><a id=__codelineno-0-1280 name=__codelineno-0-1280></a>        <span class=c1># Merge skip connection from previous layer</span>
</span><span id=__span-0-1281><a id=__codelineno-0-1281 name=__codelineno-0-1281></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic_skip</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1282><a id=__codelineno-0-1282 name=__codelineno-0-1282></a>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span><span class=p>:</span>
</span><span id=__span-0-1283><a id=__codelineno-0-1283 name=__codelineno-0-1283></a>                <span class=c1># If no padding is done in the current top-down pass, there may be a shape mismatch between current tensor and skip connection input.</span>
</span><span id=__span-0-1284><a id=__codelineno-0-1284 name=__codelineno-0-1284></a>                <span class=c1># As an example, if the output of last TopDownLayer was of size 64*64, due to lack of padding in the current layer, the current tensor</span>
</span><span id=__span-0-1285><a id=__codelineno-0-1285 name=__codelineno-0-1285></a>                <span class=c1># might become different in shape, say 60*60.</span>
</span><span id=__span-0-1286><a id=__codelineno-0-1286 name=__codelineno-0-1286></a>                <span class=c1># In order to avoid shape mismatch, we do central crop of the skip connection input.</span>
</span><span id=__span-0-1287><a id=__codelineno-0-1287 name=__codelineno-0-1287></a>                <span class=n>skip_connection_input</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span>
</span><span id=__span-0-1288><a id=__codelineno-0-1288 name=__codelineno-0-1288></a>                    <span class=n>skip_connection_input</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-0-1289><a id=__codelineno-0-1289 name=__codelineno-0-1289></a>                <span class=p>)</span>
</span><span id=__span-0-1290><a id=__codelineno-0-1290 name=__codelineno-0-1290></a>
</span><span id=__span-0-1291><a id=__codelineno-0-1291 name=__codelineno-0-1291></a>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>skip_connection_merger</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>skip_connection_input</span><span class=p>)</span>
</span><span id=__span-0-1292><a id=__codelineno-0-1292 name=__codelineno-0-1292></a>
</span><span id=__span-0-1293><a id=__codelineno-0-1293 name=__codelineno-0-1293></a>        <span class=c1># Save activation before residual block as it can be the skip connection input in the next layer</span>
</span><span id=__span-0-1294><a id=__codelineno-0-1294 name=__codelineno-0-1294></a>        <span class=n>x_pre_residual</span> <span class=o>=</span> <span class=n>x</span>
</span><span id=__span-0-1295><a id=__codelineno-0-1295 name=__codelineno-0-1295></a>
</span><span id=__span-0-1296><a id=__codelineno-0-1296 name=__codelineno-0-1296></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span><span class=p>:</span>
</span><span id=__span-0-1297><a id=__codelineno-0-1297 name=__codelineno-0-1297></a>            <span class=c1># when we don&#39;t want to do padding in topdown as well, we need to spare some boundary pixels which would be used up.</span>
</span><span id=__span-0-1298><a id=__codelineno-0-1298 name=__codelineno-0-1298></a>            <span class=n>extra_len</span> <span class=o>=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-1299><a id=__codelineno-0-1299 name=__codelineno-0-1299></a>
</span><span id=__span-0-1300><a id=__codelineno-0-1300 name=__codelineno-0-1300></a>            <span class=c1># this means that x should be of the same size as config.data.image_size. So, we have to centercrop by a factor of 2 at this point.</span>
</span><span id=__span-0-1301><a id=__codelineno-0-1301 name=__codelineno-0-1301></a>            <span class=c1># assert x.shape[-1] &gt;= self.latent_shape[-1] // 2 + extra_len</span>
</span><span id=__span-0-1302><a id=__codelineno-0-1302 name=__codelineno-0-1302></a>            <span class=c1># we assume that one topdown layer will have exactly one upscaling layer.</span>
</span><span id=__span-0-1303><a id=__codelineno-0-1303 name=__codelineno-0-1303></a>            <span class=n>new_latent_shape</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1304><a id=__codelineno-0-1304 name=__codelineno-0-1304></a>                <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>extra_len</span><span class=p>,</span>
</span><span id=__span-0-1305><a id=__codelineno-0-1305 name=__codelineno-0-1305></a>                <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>extra_len</span><span class=p>,</span>
</span><span id=__span-0-1306><a id=__codelineno-0-1306 name=__codelineno-0-1306></a>            <span class=p>)</span>
</span><span id=__span-0-1307><a id=__codelineno-0-1307 name=__codelineno-0-1307></a>
</span><span id=__span-0-1308><a id=__codelineno-0-1308 name=__codelineno-0-1308></a>            <span class=c1># If the LC is not applied on all layers, then this can happen.</span>
</span><span id=__span-0-1309><a id=__codelineno-0-1309 name=__codelineno-0-1309></a>            <span class=k>if</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>new_latent_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span><span id=__span-0-1310><a id=__codelineno-0-1310 name=__codelineno-0-1310></a>                <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>new_latent_shape</span><span class=p>)</span>
</span><span id=__span-0-1311><a id=__codelineno-0-1311 name=__codelineno-0-1311></a>
</span><span id=__span-0-1312><a id=__codelineno-0-1312 name=__codelineno-0-1312></a>        <span class=c1># Last top-down block (sequence of residual blocks)</span>
</span><span id=__span-0-1313><a id=__codelineno-0-1313 name=__codelineno-0-1313></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>deterministic_block</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-1314><a id=__codelineno-0-1314 name=__codelineno-0-1314></a>
</span><span id=__span-0-1315><a id=__codelineno-0-1315 name=__codelineno-0-1315></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span><span class=p>:</span>
</span><span id=__span-0-1316><a id=__codelineno-0-1316 name=__codelineno-0-1316></a>            <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>)</span>
</span><span id=__span-0-1317><a id=__codelineno-0-1317 name=__codelineno-0-1317></a>
</span><span id=__span-0-1318><a id=__codelineno-0-1318 name=__codelineno-0-1318></a>        <span class=c1># Save some metrics that will be used in the loss computation</span>
</span><span id=__span-0-1319><a id=__codelineno-0-1319 name=__codelineno-0-1319></a>        <span class=n>keys</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-0-1320><a id=__codelineno-0-1320 name=__codelineno-0-1320></a>            <span class=s2>&quot;z&quot;</span><span class=p>,</span>
</span><span id=__span-0-1321><a id=__codelineno-0-1321 name=__codelineno-0-1321></a>            <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>,</span>
</span><span id=__span-0-1322><a id=__codelineno-0-1322 name=__codelineno-0-1322></a>            <span class=s2>&quot;kl_samplewise_restricted&quot;</span><span class=p>,</span>
</span><span id=__span-0-1323><a id=__codelineno-0-1323 name=__codelineno-0-1323></a>            <span class=s2>&quot;kl_spatial&quot;</span><span class=p>,</span>
</span><span id=__span-0-1324><a id=__codelineno-0-1324 name=__codelineno-0-1324></a>            <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>,</span>
</span><span id=__span-0-1325><a id=__codelineno-0-1325 name=__codelineno-0-1325></a>            <span class=c1># &#39;logprob_p&#39;,</span>
</span><span id=__span-0-1326><a id=__codelineno-0-1326 name=__codelineno-0-1326></a>            <span class=s2>&quot;logprob_q&quot;</span><span class=p>,</span>
</span><span id=__span-0-1327><a id=__codelineno-0-1327 name=__codelineno-0-1327></a>            <span class=s2>&quot;qvar_max&quot;</span><span class=p>,</span>
</span><span id=__span-0-1328><a id=__codelineno-0-1328 name=__codelineno-0-1328></a>        <span class=p>]</span>
</span><span id=__span-0-1329><a id=__codelineno-0-1329 name=__codelineno-0-1329></a>        <span class=n>data</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>data_stoch</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>keys</span><span class=p>}</span>
</span><span id=__span-0-1330><a id=__codelineno-0-1330 name=__codelineno-0-1330></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_mu&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1331><a id=__codelineno-0-1331 name=__codelineno-0-1331></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_lv&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1332><a id=__codelineno-0-1332 name=__codelineno-0-1332></a>        <span class=k>if</span> <span class=n>data_stoch</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1333><a id=__codelineno-0-1333 name=__codelineno-0-1333></a>            <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span> <span class=o>=</span> <span class=n>data_stoch</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span>
</span><span id=__span-0-1334><a id=__codelineno-0-1334 name=__codelineno-0-1334></a>            <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_mu&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_mu</span>
</span><span id=__span-0-1335><a id=__codelineno-0-1335 name=__codelineno-0-1335></a>            <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_lv&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_lv</span>
</span><span id=__span-0-1336><a id=__codelineno-0-1336 name=__codelineno-0-1336></a>
</span><span id=__span-0-1337><a id=__codelineno-0-1337 name=__codelineno-0-1337></a>        <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>x_pre_residual</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> <div class="doc doc-children"> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.TopDownLayer.__init__ class="doc doc-heading"> <code class="highlight language-python"><span class=fm>__init__</span><span class=p>(</span><span class=n>z_dim</span><span class=p>,</span> <span class=n>n_res_blocks</span><span class=p>,</span> <span class=n>n_filters</span><span class=p>,</span> <span class=n>is_top_layer</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>downsampling_steps</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>nonlin</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>merge_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>batchnorm</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>stochastic_skip</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>res_block_type</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_kernel</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>groups</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>gated</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>learn_top_prior</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>top_prior_param_shape</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>analytical_kl</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>bottomup_no_padding_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>topdown_no_padding_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>retain_spatial_dims</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>restricted_kl</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>vanilla_latent_hw</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>non_stochastic_version</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>input_image_shape</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>normalize_latent_factor</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>conv2d_bias</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>stochastic_use_naive_exponential</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.TopDownLayer.__init__ class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>Constructor.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>z_dim</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The size of the latent space.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>n_res_blocks</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of TopDownDeterministicResBlock blocks</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>n_filters</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of channels present through out the layers of this block.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>is_top_layer</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the current layer is at the top of the Decoder hierarchy. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>downsampling_steps</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of downsampling steps that has to be done in this layer (typically 1). Default is <code>False</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>nonlin</code></td> <td> <code><span title="typing.Callable">Callable</span></code> </td> <td> <div class=doc-md-description> <p>The non-linearity function used in the block (e.g., <code>nn.ReLU</code>). Deafault is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>merge_type</code></td> <td> <code><span title="typing.Literal">Literal</span>[&#39;linear&#39;, &#39;residual&#39;, &#39;residual_ungated&#39;]</code> </td> <td> <div class=doc-md-description> <p>The type of merge done in the layer. It can be chosen between "linear", "residual", and "residual_ungated". Check the <code>MergeLayer</code> class docstring for more information about the behaviour of different merging modalities. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>batchnorm</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use batchnorm layers. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>dropout</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The dropout probability in dropout layers. If <code>None</code> dropout is not used. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>stochastic_skip</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use skip connections between previous top-down layer's output and this layer's stochastic output. Stochastic skip connection allows the previous layer's output has a way to directly reach this hierarchical level, hence facilitating the gradient flow during backpropagation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_type</code></td> <td> <code>str</code> </td> <td> <div class=doc-md-description> <p>A string specifying the structure of residual block. Check <code>ResidualBlock</code> documentation for more information. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_kernel</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The kernel size used in the convolutions of the residual block. It can be either a single integer or a pair of integers defining the squared kernel. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>res_block_skip_padding</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to skip padding in convolutions in the Residual block. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>groups</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of groups to consider in the convolutions. Default is 1.</p> </div> </td> <td> <code>1</code> </td> </tr> <tr class=doc-section-item> <td><code>gated</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use gated layer in <code>ResidualBlock</code>. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>learn_top_prior</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to set the top prior as learnable. If this is set to <code>False</code>, in the top-most layer the prior will be N(0,1). Otherwise, we will still have a normal distribution whose parameters will be learnt. Deafult is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>top_prior_param_shape</code></td> <td> <code><span title="typing.Iterable">Iterable</span>[int]</code> </td> <td> <div class=doc-md-description> <p>The size of the tensor which expresses the mean and the variance of the prior for the top most layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>analytical_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>If True, KL divergence is calculated according to the analytical formula. Otherwise, an MC approximation using sampled latents is calculated. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>bottomup_no_padding_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether padding is used in the different layers of the bottom-up pass. It is meaningful to know this in advance in order to assess whether before merging <code>bu_values</code> and <code>p_params</code> tensors any alignment is needed. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>topdown_no_padding_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether padding is used in the different layers of the top-down pass. It is meaningful to know this in advance in order to assess whether before merging <code>bu_values</code> and <code>p_params</code> tensors any alignment is needed. The same information is also needed in handling the skip connections between top-down layers. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>retain_spatial_dims</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>If <code>True</code>, the size of Encoder's latent space is kept to <code>input_image_shape</code> within the topdown layer. This implies that the oput spatial size equals the input spatial size. To achieve this, we centercrop the intermediate representation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>restricted_kl</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to compute the restricted version of KL Divergence. See <code>NormalStochasticBlock2d</code> module for more information about its computation. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>vanilla_latent_hw</code></td> <td> <code><span title="typing.Iterable">Iterable</span>[int]</code> </td> <td> <div class=doc-md-description> <p>The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL). Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>non_stochastic_version</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to replace the stochastic layer that samples a latent variable from the latent distribiution with a non-stochastic layer that simply drwas a sample as the mode of the latent distribution. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>input_image_shape</code></td> <td> <code><span title="typing.Union">Union</span>[None, <span title="typing.Tuple">Tuple</span>[int, int]]</code> </td> <td> <div class=doc-md-description> <p>The shape of the input image tensor. When <code>retain_spatial_dims</code> is set to <code>True</code>, this is used to ensure that the shape of this layer output has the same shape as the input. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>normalize_latent_factor</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>A factor used to normalize the latent tensors <code>q_params</code>. Specifically, normalization is done by dividing the latent tensor by this factor. Default is 1.0.</p> </div> </td> <td> <code>1.0</code> </td> </tr> <tr class=doc-section-item> <td><code>conv2d_bias</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use bias term is the convolutional blocks of this layer. Default is <code>True</code>.</p> </div> </td> <td> <code>True</code> </td> </tr> <tr class=doc-section-item> <td><code>stochastic_use_naive_exponential</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>If <code>False</code>, in the NormalStochasticBlock2d exponentials are computed according to the alternative definition provided by <code>StableExponential</code> class. This should improve numerical stability in the training process. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-854> 854</a></span>
<span class=normal><a href=#__codelineno-0-855> 855</a></span>
<span class=normal><a href=#__codelineno-0-856> 856</a></span>
<span class=normal><a href=#__codelineno-0-857> 857</a></span>
<span class=normal><a href=#__codelineno-0-858> 858</a></span>
<span class=normal><a href=#__codelineno-0-859> 859</a></span>
<span class=normal><a href=#__codelineno-0-860> 860</a></span>
<span class=normal><a href=#__codelineno-0-861> 861</a></span>
<span class=normal><a href=#__codelineno-0-862> 862</a></span>
<span class=normal><a href=#__codelineno-0-863> 863</a></span>
<span class=normal><a href=#__codelineno-0-864> 864</a></span>
<span class=normal><a href=#__codelineno-0-865> 865</a></span>
<span class=normal><a href=#__codelineno-0-866> 866</a></span>
<span class=normal><a href=#__codelineno-0-867> 867</a></span>
<span class=normal><a href=#__codelineno-0-868> 868</a></span>
<span class=normal><a href=#__codelineno-0-869> 869</a></span>
<span class=normal><a href=#__codelineno-0-870> 870</a></span>
<span class=normal><a href=#__codelineno-0-871> 871</a></span>
<span class=normal><a href=#__codelineno-0-872> 872</a></span>
<span class=normal><a href=#__codelineno-0-873> 873</a></span>
<span class=normal><a href=#__codelineno-0-874> 874</a></span>
<span class=normal><a href=#__codelineno-0-875> 875</a></span>
<span class=normal><a href=#__codelineno-0-876> 876</a></span>
<span class=normal><a href=#__codelineno-0-877> 877</a></span>
<span class=normal><a href=#__codelineno-0-878> 878</a></span>
<span class=normal><a href=#__codelineno-0-879> 879</a></span>
<span class=normal><a href=#__codelineno-0-880> 880</a></span>
<span class=normal><a href=#__codelineno-0-881> 881</a></span>
<span class=normal><a href=#__codelineno-0-882> 882</a></span>
<span class=normal><a href=#__codelineno-0-883> 883</a></span>
<span class=normal><a href=#__codelineno-0-884> 884</a></span>
<span class=normal><a href=#__codelineno-0-885> 885</a></span>
<span class=normal><a href=#__codelineno-0-886> 886</a></span>
<span class=normal><a href=#__codelineno-0-887> 887</a></span>
<span class=normal><a href=#__codelineno-0-888> 888</a></span>
<span class=normal><a href=#__codelineno-0-889> 889</a></span>
<span class=normal><a href=#__codelineno-0-890> 890</a></span>
<span class=normal><a href=#__codelineno-0-891> 891</a></span>
<span class=normal><a href=#__codelineno-0-892> 892</a></span>
<span class=normal><a href=#__codelineno-0-893> 893</a></span>
<span class=normal><a href=#__codelineno-0-894> 894</a></span>
<span class=normal><a href=#__codelineno-0-895> 895</a></span>
<span class=normal><a href=#__codelineno-0-896> 896</a></span>
<span class=normal><a href=#__codelineno-0-897> 897</a></span>
<span class=normal><a href=#__codelineno-0-898> 898</a></span>
<span class=normal><a href=#__codelineno-0-899> 899</a></span>
<span class=normal><a href=#__codelineno-0-900> 900</a></span>
<span class=normal><a href=#__codelineno-0-901> 901</a></span>
<span class=normal><a href=#__codelineno-0-902> 902</a></span>
<span class=normal><a href=#__codelineno-0-903> 903</a></span>
<span class=normal><a href=#__codelineno-0-904> 904</a></span>
<span class=normal><a href=#__codelineno-0-905> 905</a></span>
<span class=normal><a href=#__codelineno-0-906> 906</a></span>
<span class=normal><a href=#__codelineno-0-907> 907</a></span>
<span class=normal><a href=#__codelineno-0-908> 908</a></span>
<span class=normal><a href=#__codelineno-0-909> 909</a></span>
<span class=normal><a href=#__codelineno-0-910> 910</a></span>
<span class=normal><a href=#__codelineno-0-911> 911</a></span>
<span class=normal><a href=#__codelineno-0-912> 912</a></span>
<span class=normal><a href=#__codelineno-0-913> 913</a></span>
<span class=normal><a href=#__codelineno-0-914> 914</a></span>
<span class=normal><a href=#__codelineno-0-915> 915</a></span>
<span class=normal><a href=#__codelineno-0-916> 916</a></span>
<span class=normal><a href=#__codelineno-0-917> 917</a></span>
<span class=normal><a href=#__codelineno-0-918> 918</a></span>
<span class=normal><a href=#__codelineno-0-919> 919</a></span>
<span class=normal><a href=#__codelineno-0-920> 920</a></span>
<span class=normal><a href=#__codelineno-0-921> 921</a></span>
<span class=normal><a href=#__codelineno-0-922> 922</a></span>
<span class=normal><a href=#__codelineno-0-923> 923</a></span>
<span class=normal><a href=#__codelineno-0-924> 924</a></span>
<span class=normal><a href=#__codelineno-0-925> 925</a></span>
<span class=normal><a href=#__codelineno-0-926> 926</a></span>
<span class=normal><a href=#__codelineno-0-927> 927</a></span>
<span class=normal><a href=#__codelineno-0-928> 928</a></span>
<span class=normal><a href=#__codelineno-0-929> 929</a></span>
<span class=normal><a href=#__codelineno-0-930> 930</a></span>
<span class=normal><a href=#__codelineno-0-931> 931</a></span>
<span class=normal><a href=#__codelineno-0-932> 932</a></span>
<span class=normal><a href=#__codelineno-0-933> 933</a></span>
<span class=normal><a href=#__codelineno-0-934> 934</a></span>
<span class=normal><a href=#__codelineno-0-935> 935</a></span>
<span class=normal><a href=#__codelineno-0-936> 936</a></span>
<span class=normal><a href=#__codelineno-0-937> 937</a></span>
<span class=normal><a href=#__codelineno-0-938> 938</a></span>
<span class=normal><a href=#__codelineno-0-939> 939</a></span>
<span class=normal><a href=#__codelineno-0-940> 940</a></span>
<span class=normal><a href=#__codelineno-0-941> 941</a></span>
<span class=normal><a href=#__codelineno-0-942> 942</a></span>
<span class=normal><a href=#__codelineno-0-943> 943</a></span>
<span class=normal><a href=#__codelineno-0-944> 944</a></span>
<span class=normal><a href=#__codelineno-0-945> 945</a></span>
<span class=normal><a href=#__codelineno-0-946> 946</a></span>
<span class=normal><a href=#__codelineno-0-947> 947</a></span>
<span class=normal><a href=#__codelineno-0-948> 948</a></span>
<span class=normal><a href=#__codelineno-0-949> 949</a></span>
<span class=normal><a href=#__codelineno-0-950> 950</a></span>
<span class=normal><a href=#__codelineno-0-951> 951</a></span>
<span class=normal><a href=#__codelineno-0-952> 952</a></span>
<span class=normal><a href=#__codelineno-0-953> 953</a></span>
<span class=normal><a href=#__codelineno-0-954> 954</a></span>
<span class=normal><a href=#__codelineno-0-955> 955</a></span>
<span class=normal><a href=#__codelineno-0-956> 956</a></span>
<span class=normal><a href=#__codelineno-0-957> 957</a></span>
<span class=normal><a href=#__codelineno-0-958> 958</a></span>
<span class=normal><a href=#__codelineno-0-959> 959</a></span>
<span class=normal><a href=#__codelineno-0-960> 960</a></span>
<span class=normal><a href=#__codelineno-0-961> 961</a></span>
<span class=normal><a href=#__codelineno-0-962> 962</a></span>
<span class=normal><a href=#__codelineno-0-963> 963</a></span>
<span class=normal><a href=#__codelineno-0-964> 964</a></span>
<span class=normal><a href=#__codelineno-0-965> 965</a></span>
<span class=normal><a href=#__codelineno-0-966> 966</a></span>
<span class=normal><a href=#__codelineno-0-967> 967</a></span>
<span class=normal><a href=#__codelineno-0-968> 968</a></span>
<span class=normal><a href=#__codelineno-0-969> 969</a></span>
<span class=normal><a href=#__codelineno-0-970> 970</a></span>
<span class=normal><a href=#__codelineno-0-971> 971</a></span>
<span class=normal><a href=#__codelineno-0-972> 972</a></span>
<span class=normal><a href=#__codelineno-0-973> 973</a></span>
<span class=normal><a href=#__codelineno-0-974> 974</a></span>
<span class=normal><a href=#__codelineno-0-975> 975</a></span>
<span class=normal><a href=#__codelineno-0-976> 976</a></span>
<span class=normal><a href=#__codelineno-0-977> 977</a></span>
<span class=normal><a href=#__codelineno-0-978> 978</a></span>
<span class=normal><a href=#__codelineno-0-979> 979</a></span>
<span class=normal><a href=#__codelineno-0-980> 980</a></span>
<span class=normal><a href=#__codelineno-0-981> 981</a></span>
<span class=normal><a href=#__codelineno-0-982> 982</a></span>
<span class=normal><a href=#__codelineno-0-983> 983</a></span>
<span class=normal><a href=#__codelineno-0-984> 984</a></span>
<span class=normal><a href=#__codelineno-0-985> 985</a></span>
<span class=normal><a href=#__codelineno-0-986> 986</a></span>
<span class=normal><a href=#__codelineno-0-987> 987</a></span>
<span class=normal><a href=#__codelineno-0-988> 988</a></span>
<span class=normal><a href=#__codelineno-0-989> 989</a></span>
<span class=normal><a href=#__codelineno-0-990> 990</a></span>
<span class=normal><a href=#__codelineno-0-991> 991</a></span>
<span class=normal><a href=#__codelineno-0-992> 992</a></span>
<span class=normal><a href=#__codelineno-0-993> 993</a></span>
<span class=normal><a href=#__codelineno-0-994> 994</a></span>
<span class=normal><a href=#__codelineno-0-995> 995</a></span>
<span class=normal><a href=#__codelineno-0-996> 996</a></span>
<span class=normal><a href=#__codelineno-0-997> 997</a></span>
<span class=normal><a href=#__codelineno-0-998> 998</a></span>
<span class=normal><a href=#__codelineno-0-999> 999</a></span>
<span class=normal><a href=#__codelineno-0-1000>1000</a></span>
<span class=normal><a href=#__codelineno-0-1001>1001</a></span>
<span class=normal><a href=#__codelineno-0-1002>1002</a></span>
<span class=normal><a href=#__codelineno-0-1003>1003</a></span>
<span class=normal><a href=#__codelineno-0-1004>1004</a></span>
<span class=normal><a href=#__codelineno-0-1005>1005</a></span>
<span class=normal><a href=#__codelineno-0-1006>1006</a></span>
<span class=normal><a href=#__codelineno-0-1007>1007</a></span>
<span class=normal><a href=#__codelineno-0-1008>1008</a></span>
<span class=normal><a href=#__codelineno-0-1009>1009</a></span>
<span class=normal><a href=#__codelineno-0-1010>1010</a></span>
<span class=normal><a href=#__codelineno-0-1011>1011</a></span>
<span class=normal><a href=#__codelineno-0-1012>1012</a></span>
<span class=normal><a href=#__codelineno-0-1013>1013</a></span>
<span class=normal><a href=#__codelineno-0-1014>1014</a></span>
<span class=normal><a href=#__codelineno-0-1015>1015</a></span>
<span class=normal><a href=#__codelineno-0-1016>1016</a></span>
<span class=normal><a href=#__codelineno-0-1017>1017</a></span>
<span class=normal><a href=#__codelineno-0-1018>1018</a></span>
<span class=normal><a href=#__codelineno-0-1019>1019</a></span>
<span class=normal><a href=#__codelineno-0-1020>1020</a></span>
<span class=normal><a href=#__codelineno-0-1021>1021</a></span>
<span class=normal><a href=#__codelineno-0-1022>1022</a></span>
<span class=normal><a href=#__codelineno-0-1023>1023</a></span>
<span class=normal><a href=#__codelineno-0-1024>1024</a></span>
<span class=normal><a href=#__codelineno-0-1025>1025</a></span>
<span class=normal><a href=#__codelineno-0-1026>1026</a></span>
<span class=normal><a href=#__codelineno-0-1027>1027</a></span>
<span class=normal><a href=#__codelineno-0-1028>1028</a></span>
<span class=normal><a href=#__codelineno-0-1029>1029</a></span>
<span class=normal><a href=#__codelineno-0-1030>1030</a></span>
<span class=normal><a href=#__codelineno-0-1031>1031</a></span>
<span class=normal><a href=#__codelineno-0-1032>1032</a></span>
<span class=normal><a href=#__codelineno-0-1033>1033</a></span>
<span class=normal><a href=#__codelineno-0-1034>1034</a></span>
<span class=normal><a href=#__codelineno-0-1035>1035</a></span>
<span class=normal><a href=#__codelineno-0-1036>1036</a></span>
<span class=normal><a href=#__codelineno-0-1037>1037</a></span>
<span class=normal><a href=#__codelineno-0-1038>1038</a></span>
<span class=normal><a href=#__codelineno-0-1039>1039</a></span>
<span class=normal><a href=#__codelineno-0-1040>1040</a></span>
<span class=normal><a href=#__codelineno-0-1041>1041</a></span>
<span class=normal><a href=#__codelineno-0-1042>1042</a></span>
<span class=normal><a href=#__codelineno-0-1043>1043</a></span>
<span class=normal><a href=#__codelineno-0-1044>1044</a></span>
<span class=normal><a href=#__codelineno-0-1045>1045</a></span>
<span class=normal><a href=#__codelineno-0-1046>1046</a></span>
<span class=normal><a href=#__codelineno-0-1047>1047</a></span>
<span class=normal><a href=#__codelineno-0-1048>1048</a></span>
<span class=normal><a href=#__codelineno-0-1049>1049</a></span>
<span class=normal><a href=#__codelineno-0-1050>1050</a></span>
<span class=normal><a href=#__codelineno-0-1051>1051</a></span>
<span class=normal><a href=#__codelineno-0-1052>1052</a></span>
<span class=normal><a href=#__codelineno-0-1053>1053</a></span>
<span class=normal><a href=#__codelineno-0-1054>1054</a></span>
<span class=normal><a href=#__codelineno-0-1055>1055</a></span>
<span class=normal><a href=#__codelineno-0-1056>1056</a></span>
<span class=normal><a href=#__codelineno-0-1057>1057</a></span>
<span class=normal><a href=#__codelineno-0-1058>1058</a></span>
<span class=normal><a href=#__codelineno-0-1059>1059</a></span>
<span class=normal><a href=#__codelineno-0-1060>1060</a></span>
<span class=normal><a href=#__codelineno-0-1061>1061</a></span>
<span class=normal><a href=#__codelineno-0-1062>1062</a></span>
<span class=normal><a href=#__codelineno-0-1063>1063</a></span>
<span class=normal><a href=#__codelineno-0-1064>1064</a></span>
<span class=normal><a href=#__codelineno-0-1065>1065</a></span>
<span class=normal><a href=#__codelineno-0-1066>1066</a></span>
<span class=normal><a href=#__codelineno-0-1067>1067</a></span>
<span class=normal><a href=#__codelineno-0-1068>1068</a></span>
<span class=normal><a href=#__codelineno-0-1069>1069</a></span>
<span class=normal><a href=#__codelineno-0-1070>1070</a></span>
<span class=normal><a href=#__codelineno-0-1071>1071</a></span>
<span class=normal><a href=#__codelineno-0-1072>1072</a></span>
<span class=normal><a href=#__codelineno-0-1073>1073</a></span>
<span class=normal><a href=#__codelineno-0-1074>1074</a></span>
<span class=normal><a href=#__codelineno-0-1075>1075</a></span>
<span class=normal><a href=#__codelineno-0-1076>1076</a></span>
<span class=normal><a href=#__codelineno-0-1077>1077</a></span>
<span class=normal><a href=#__codelineno-0-1078>1078</a></span>
<span class=normal><a href=#__codelineno-0-1079>1079</a></span>
<span class=normal><a href=#__codelineno-0-1080>1080</a></span>
<span class=normal><a href=#__codelineno-0-1081>1081</a></span>
<span class=normal><a href=#__codelineno-0-1082>1082</a></span>
<span class=normal><a href=#__codelineno-0-1083>1083</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-854><a id=__codelineno-0-854 name=__codelineno-0-854></a><span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span><span id=__span-0-855><a id=__codelineno-0-855 name=__codelineno-0-855></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-856><a id=__codelineno-0-856 name=__codelineno-0-856></a>    <span class=n>z_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-857><a id=__codelineno-0-857 name=__codelineno-0-857></a>    <span class=n>n_res_blocks</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-858><a id=__codelineno-0-858 name=__codelineno-0-858></a>    <span class=n>n_filters</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-859><a id=__codelineno-0-859 name=__codelineno-0-859></a>    <span class=n>is_top_layer</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-860><a id=__codelineno-0-860 name=__codelineno-0-860></a>    <span class=n>downsampling_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-861><a id=__codelineno-0-861 name=__codelineno-0-861></a>    <span class=n>nonlin</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-862><a id=__codelineno-0-862 name=__codelineno-0-862></a>    <span class=n>merge_type</span><span class=p>:</span> <span class=n>Literal</span><span class=p>[</span><span class=s2>&quot;linear&quot;</span><span class=p>,</span> <span class=s2>&quot;residual&quot;</span><span class=p>,</span> <span class=s2>&quot;residual_ungated&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-863><a id=__codelineno-0-863 name=__codelineno-0-863></a>    <span class=n>batchnorm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-864><a id=__codelineno-0-864 name=__codelineno-0-864></a>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-865><a id=__codelineno-0-865 name=__codelineno-0-865></a>    <span class=n>stochastic_skip</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-866><a id=__codelineno-0-866 name=__codelineno-0-866></a>    <span class=n>res_block_type</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-867><a id=__codelineno-0-867 name=__codelineno-0-867></a>    <span class=n>res_block_kernel</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-868><a id=__codelineno-0-868 name=__codelineno-0-868></a>    <span class=n>res_block_skip_padding</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-869><a id=__codelineno-0-869 name=__codelineno-0-869></a>    <span class=n>groups</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span><span class=p>,</span>
</span><span id=__span-0-870><a id=__codelineno-0-870 name=__codelineno-0-870></a>    <span class=n>gated</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-871><a id=__codelineno-0-871 name=__codelineno-0-871></a>    <span class=n>learn_top_prior</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-872><a id=__codelineno-0-872 name=__codelineno-0-872></a>    <span class=n>top_prior_param_shape</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-873><a id=__codelineno-0-873 name=__codelineno-0-873></a>    <span class=n>analytical_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-874><a id=__codelineno-0-874 name=__codelineno-0-874></a>    <span class=n>bottomup_no_padding_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-875><a id=__codelineno-0-875 name=__codelineno-0-875></a>    <span class=n>topdown_no_padding_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-876><a id=__codelineno-0-876 name=__codelineno-0-876></a>    <span class=n>retain_spatial_dims</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-877><a id=__codelineno-0-877 name=__codelineno-0-877></a>    <span class=n>restricted_kl</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-878><a id=__codelineno-0-878 name=__codelineno-0-878></a>    <span class=n>vanilla_latent_hw</span><span class=p>:</span> <span class=n>Iterable</span><span class=p>[</span><span class=nb>int</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-879><a id=__codelineno-0-879 name=__codelineno-0-879></a>    <span class=n>non_stochastic_version</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-880><a id=__codelineno-0-880 name=__codelineno-0-880></a>    <span class=n>input_image_shape</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>Tuple</span><span class=p>[</span><span class=nb>int</span><span class=p>,</span> <span class=nb>int</span><span class=p>]]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-881><a id=__codelineno-0-881 name=__codelineno-0-881></a>    <span class=n>normalize_latent_factor</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1.0</span><span class=p>,</span>
</span><span id=__span-0-882><a id=__codelineno-0-882 name=__codelineno-0-882></a>    <span class=n>conv2d_bias</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>True</span><span class=p>,</span>
</span><span id=__span-0-883><a id=__codelineno-0-883 name=__codelineno-0-883></a>    <span class=n>stochastic_use_naive_exponential</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-884><a id=__codelineno-0-884 name=__codelineno-0-884></a><span class=p>):</span>
</span><span id=__span-0-885><a id=__codelineno-0-885 name=__codelineno-0-885></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-886><a id=__codelineno-0-886 name=__codelineno-0-886></a><span class=sd>    Constructor.</span>
</span><span id=__span-0-887><a id=__codelineno-0-887 name=__codelineno-0-887></a>
</span><span id=__span-0-888><a id=__codelineno-0-888 name=__codelineno-0-888></a><span class=sd>    Parameters</span>
</span><span id=__span-0-889><a id=__codelineno-0-889 name=__codelineno-0-889></a><span class=sd>    ----------</span>
</span><span id=__span-0-890><a id=__codelineno-0-890 name=__codelineno-0-890></a><span class=sd>    z_dim: int</span>
</span><span id=__span-0-891><a id=__codelineno-0-891 name=__codelineno-0-891></a><span class=sd>        The size of the latent space.</span>
</span><span id=__span-0-892><a id=__codelineno-0-892 name=__codelineno-0-892></a><span class=sd>    n_res_blocks: int</span>
</span><span id=__span-0-893><a id=__codelineno-0-893 name=__codelineno-0-893></a><span class=sd>        The number of TopDownDeterministicResBlock blocks</span>
</span><span id=__span-0-894><a id=__codelineno-0-894 name=__codelineno-0-894></a><span class=sd>    n_filters: int</span>
</span><span id=__span-0-895><a id=__codelineno-0-895 name=__codelineno-0-895></a><span class=sd>        The number of channels present through out the layers of this block.</span>
</span><span id=__span-0-896><a id=__codelineno-0-896 name=__codelineno-0-896></a><span class=sd>    is_top_layer: bool, optional</span>
</span><span id=__span-0-897><a id=__codelineno-0-897 name=__codelineno-0-897></a><span class=sd>        Whether the current layer is at the top of the Decoder hierarchy. Default is `False`.</span>
</span><span id=__span-0-898><a id=__codelineno-0-898 name=__codelineno-0-898></a><span class=sd>    downsampling_steps: int, optional</span>
</span><span id=__span-0-899><a id=__codelineno-0-899 name=__codelineno-0-899></a><span class=sd>        The number of downsampling steps that has to be done in this layer (typically 1).</span>
</span><span id=__span-0-900><a id=__codelineno-0-900 name=__codelineno-0-900></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-901><a id=__codelineno-0-901 name=__codelineno-0-901></a><span class=sd>    nonlin: Callable, optional</span>
</span><span id=__span-0-902><a id=__codelineno-0-902 name=__codelineno-0-902></a><span class=sd>        The non-linearity function used in the block (e.g., `nn.ReLU`). Deafault is `None`.</span>
</span><span id=__span-0-903><a id=__codelineno-0-903 name=__codelineno-0-903></a><span class=sd>    merge_type: Literal[&quot;linear&quot;, &quot;residual&quot;, &quot;residual_ungated&quot;], optional</span>
</span><span id=__span-0-904><a id=__codelineno-0-904 name=__codelineno-0-904></a><span class=sd>        The type of merge done in the layer. It can be chosen between &quot;linear&quot;, &quot;residual&quot;,</span>
</span><span id=__span-0-905><a id=__codelineno-0-905 name=__codelineno-0-905></a><span class=sd>        and &quot;residual_ungated&quot;. Check the `MergeLayer` class docstring for more information</span>
</span><span id=__span-0-906><a id=__codelineno-0-906 name=__codelineno-0-906></a><span class=sd>        about the behaviour of different merging modalities. Default is `None`.</span>
</span><span id=__span-0-907><a id=__codelineno-0-907 name=__codelineno-0-907></a><span class=sd>    batchnorm: bool, optional</span>
</span><span id=__span-0-908><a id=__codelineno-0-908 name=__codelineno-0-908></a><span class=sd>        Whether to use batchnorm layers. Default is `True`.</span>
</span><span id=__span-0-909><a id=__codelineno-0-909 name=__codelineno-0-909></a><span class=sd>    dropout: float, optional</span>
</span><span id=__span-0-910><a id=__codelineno-0-910 name=__codelineno-0-910></a><span class=sd>        The dropout probability in dropout layers. If `None` dropout is not used.</span>
</span><span id=__span-0-911><a id=__codelineno-0-911 name=__codelineno-0-911></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-912><a id=__codelineno-0-912 name=__codelineno-0-912></a><span class=sd>    stochastic_skip: bool, optional</span>
</span><span id=__span-0-913><a id=__codelineno-0-913 name=__codelineno-0-913></a><span class=sd>        Whether to use skip connections between previous top-down layer&#39;s output and this layer&#39;s stochastic output.</span>
</span><span id=__span-0-914><a id=__codelineno-0-914 name=__codelineno-0-914></a><span class=sd>        Stochastic skip connection allows the previous layer&#39;s output has a way to directly reach this hierarchical</span>
</span><span id=__span-0-915><a id=__codelineno-0-915 name=__codelineno-0-915></a><span class=sd>        level, hence facilitating the gradient flow during backpropagation. Default is `False`.</span>
</span><span id=__span-0-916><a id=__codelineno-0-916 name=__codelineno-0-916></a><span class=sd>    res_block_type: str, optional</span>
</span><span id=__span-0-917><a id=__codelineno-0-917 name=__codelineno-0-917></a><span class=sd>        A string specifying the structure of residual block.</span>
</span><span id=__span-0-918><a id=__codelineno-0-918 name=__codelineno-0-918></a><span class=sd>        Check `ResidualBlock` documentation for more information.</span>
</span><span id=__span-0-919><a id=__codelineno-0-919 name=__codelineno-0-919></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-920><a id=__codelineno-0-920 name=__codelineno-0-920></a><span class=sd>    res_block_kernel: Union[int, Iterable[int]], optional</span>
</span><span id=__span-0-921><a id=__codelineno-0-921 name=__codelineno-0-921></a><span class=sd>        The kernel size used in the convolutions of the residual block.</span>
</span><span id=__span-0-922><a id=__codelineno-0-922 name=__codelineno-0-922></a><span class=sd>        It can be either a single integer or a pair of integers defining the squared kernel.</span>
</span><span id=__span-0-923><a id=__codelineno-0-923 name=__codelineno-0-923></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-924><a id=__codelineno-0-924 name=__codelineno-0-924></a><span class=sd>    res_block_skip_padding: bool, optional</span>
</span><span id=__span-0-925><a id=__codelineno-0-925 name=__codelineno-0-925></a><span class=sd>        Whether to skip padding in convolutions in the Residual block. Default is `None`.</span>
</span><span id=__span-0-926><a id=__codelineno-0-926 name=__codelineno-0-926></a><span class=sd>    groups: int, optional</span>
</span><span id=__span-0-927><a id=__codelineno-0-927 name=__codelineno-0-927></a><span class=sd>        The number of groups to consider in the convolutions. Default is 1.</span>
</span><span id=__span-0-928><a id=__codelineno-0-928 name=__codelineno-0-928></a><span class=sd>    gated: bool, optional</span>
</span><span id=__span-0-929><a id=__codelineno-0-929 name=__codelineno-0-929></a><span class=sd>        Whether to use gated layer in `ResidualBlock`. Default is `None`.</span>
</span><span id=__span-0-930><a id=__codelineno-0-930 name=__codelineno-0-930></a><span class=sd>    learn_top_prior:</span>
</span><span id=__span-0-931><a id=__codelineno-0-931 name=__codelineno-0-931></a><span class=sd>        Whether to set the top prior as learnable.</span>
</span><span id=__span-0-932><a id=__codelineno-0-932 name=__codelineno-0-932></a><span class=sd>        If this is set to `False`, in the top-most layer the prior will be N(0,1).</span>
</span><span id=__span-0-933><a id=__codelineno-0-933 name=__codelineno-0-933></a><span class=sd>        Otherwise, we will still have a normal distribution whose parameters will be learnt.</span>
</span><span id=__span-0-934><a id=__codelineno-0-934 name=__codelineno-0-934></a><span class=sd>        Deafult is `False`.</span>
</span><span id=__span-0-935><a id=__codelineno-0-935 name=__codelineno-0-935></a><span class=sd>    top_prior_param_shape: Iterable[int], optional</span>
</span><span id=__span-0-936><a id=__codelineno-0-936 name=__codelineno-0-936></a><span class=sd>        The size of the tensor which expresses the mean and the variance</span>
</span><span id=__span-0-937><a id=__codelineno-0-937 name=__codelineno-0-937></a><span class=sd>        of the prior for the top most layer. Default is `None`.</span>
</span><span id=__span-0-938><a id=__codelineno-0-938 name=__codelineno-0-938></a><span class=sd>    analytical_kl: bool, optional</span>
</span><span id=__span-0-939><a id=__codelineno-0-939 name=__codelineno-0-939></a><span class=sd>        If True, KL divergence is calculated according to the analytical formula.</span>
</span><span id=__span-0-940><a id=__codelineno-0-940 name=__codelineno-0-940></a><span class=sd>        Otherwise, an MC approximation using sampled latents is calculated.</span>
</span><span id=__span-0-941><a id=__codelineno-0-941 name=__codelineno-0-941></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-942><a id=__codelineno-0-942 name=__codelineno-0-942></a><span class=sd>    bottomup_no_padding_mode: bool, optional</span>
</span><span id=__span-0-943><a id=__codelineno-0-943 name=__codelineno-0-943></a><span class=sd>        Whether padding is used in the different layers of the bottom-up pass.</span>
</span><span id=__span-0-944><a id=__codelineno-0-944 name=__codelineno-0-944></a><span class=sd>        It is meaningful to know this in advance in order to assess whether before</span>
</span><span id=__span-0-945><a id=__codelineno-0-945 name=__codelineno-0-945></a><span class=sd>        merging `bu_values` and `p_params` tensors any alignment is needed.</span>
</span><span id=__span-0-946><a id=__codelineno-0-946 name=__codelineno-0-946></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-947><a id=__codelineno-0-947 name=__codelineno-0-947></a><span class=sd>    topdown_no_padding_mode: bool, optional</span>
</span><span id=__span-0-948><a id=__codelineno-0-948 name=__codelineno-0-948></a><span class=sd>        Whether padding is used in the different layers of the top-down pass.</span>
</span><span id=__span-0-949><a id=__codelineno-0-949 name=__codelineno-0-949></a><span class=sd>        It is meaningful to know this in advance in order to assess whether before</span>
</span><span id=__span-0-950><a id=__codelineno-0-950 name=__codelineno-0-950></a><span class=sd>        merging `bu_values` and `p_params` tensors any alignment is needed.</span>
</span><span id=__span-0-951><a id=__codelineno-0-951 name=__codelineno-0-951></a><span class=sd>        The same information is also needed in handling the skip connections between</span>
</span><span id=__span-0-952><a id=__codelineno-0-952 name=__codelineno-0-952></a><span class=sd>        top-down layers. Default is `False`.</span>
</span><span id=__span-0-953><a id=__codelineno-0-953 name=__codelineno-0-953></a><span class=sd>    retain_spatial_dims: bool, optional</span>
</span><span id=__span-0-954><a id=__codelineno-0-954 name=__codelineno-0-954></a><span class=sd>        If `True`, the size of Encoder&#39;s latent space is kept to `input_image_shape` within the topdown layer.</span>
</span><span id=__span-0-955><a id=__codelineno-0-955 name=__codelineno-0-955></a><span class=sd>        This implies that the oput spatial size equals the input spatial size.</span>
</span><span id=__span-0-956><a id=__codelineno-0-956 name=__codelineno-0-956></a><span class=sd>        To achieve this, we centercrop the intermediate representation.</span>
</span><span id=__span-0-957><a id=__codelineno-0-957 name=__codelineno-0-957></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-958><a id=__codelineno-0-958 name=__codelineno-0-958></a><span class=sd>    restricted_kl: bool, optional</span>
</span><span id=__span-0-959><a id=__codelineno-0-959 name=__codelineno-0-959></a><span class=sd>        Whether to compute the restricted version of KL Divergence.</span>
</span><span id=__span-0-960><a id=__codelineno-0-960 name=__codelineno-0-960></a><span class=sd>        See `NormalStochasticBlock2d` module for more information about its computation.</span>
</span><span id=__span-0-961><a id=__codelineno-0-961 name=__codelineno-0-961></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-962><a id=__codelineno-0-962 name=__codelineno-0-962></a><span class=sd>    vanilla_latent_hw: Iterable[int], optional</span>
</span><span id=__span-0-963><a id=__codelineno-0-963 name=__codelineno-0-963></a><span class=sd>        The shape of the latent tensor used for prediction (i.e., it influences the computation of restricted KL).</span>
</span><span id=__span-0-964><a id=__codelineno-0-964 name=__codelineno-0-964></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-965><a id=__codelineno-0-965 name=__codelineno-0-965></a><span class=sd>    non_stochastic_version: bool, optional</span>
</span><span id=__span-0-966><a id=__codelineno-0-966 name=__codelineno-0-966></a><span class=sd>        Whether to replace the stochastic layer that samples a latent variable from the latent distribiution with</span>
</span><span id=__span-0-967><a id=__codelineno-0-967 name=__codelineno-0-967></a><span class=sd>        a non-stochastic layer that simply drwas a sample as the mode of the latent distribution.</span>
</span><span id=__span-0-968><a id=__codelineno-0-968 name=__codelineno-0-968></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-969><a id=__codelineno-0-969 name=__codelineno-0-969></a><span class=sd>    input_image_shape: Tuple[int, int], optionalut</span>
</span><span id=__span-0-970><a id=__codelineno-0-970 name=__codelineno-0-970></a><span class=sd>        The shape of the input image tensor.</span>
</span><span id=__span-0-971><a id=__codelineno-0-971 name=__codelineno-0-971></a><span class=sd>        When `retain_spatial_dims` is set to `True`, this is used to ensure that the shape of this layer</span>
</span><span id=__span-0-972><a id=__codelineno-0-972 name=__codelineno-0-972></a><span class=sd>        output has the same shape as the input. Default is `None`.</span>
</span><span id=__span-0-973><a id=__codelineno-0-973 name=__codelineno-0-973></a><span class=sd>    normalize_latent_factor: float, optional</span>
</span><span id=__span-0-974><a id=__codelineno-0-974 name=__codelineno-0-974></a><span class=sd>        A factor used to normalize the latent tensors `q_params`.</span>
</span><span id=__span-0-975><a id=__codelineno-0-975 name=__codelineno-0-975></a><span class=sd>        Specifically, normalization is done by dividing the latent tensor by this factor.</span>
</span><span id=__span-0-976><a id=__codelineno-0-976 name=__codelineno-0-976></a><span class=sd>        Default is 1.0.</span>
</span><span id=__span-0-977><a id=__codelineno-0-977 name=__codelineno-0-977></a><span class=sd>    conv2d_bias: bool, optional</span>
</span><span id=__span-0-978><a id=__codelineno-0-978 name=__codelineno-0-978></a><span class=sd>        Whether to use bias term is the convolutional blocks of this layer.</span>
</span><span id=__span-0-979><a id=__codelineno-0-979 name=__codelineno-0-979></a><span class=sd>        Default is `True`.</span>
</span><span id=__span-0-980><a id=__codelineno-0-980 name=__codelineno-0-980></a><span class=sd>    stochastic_use_naive_exponential: bool, optional</span>
</span><span id=__span-0-981><a id=__codelineno-0-981 name=__codelineno-0-981></a><span class=sd>        If `False`, in the NormalStochasticBlock2d exponentials are computed according</span>
</span><span id=__span-0-982><a id=__codelineno-0-982 name=__codelineno-0-982></a><span class=sd>        to the alternative definition provided by `StableExponential` class.</span>
</span><span id=__span-0-983><a id=__codelineno-0-983 name=__codelineno-0-983></a><span class=sd>        This should improve numerical stability in the training process.</span>
</span><span id=__span-0-984><a id=__codelineno-0-984 name=__codelineno-0-984></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-985><a id=__codelineno-0-985 name=__codelineno-0-985></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-986><a id=__codelineno-0-986 name=__codelineno-0-986></a>    <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span><span id=__span-0-987><a id=__codelineno-0-987 name=__codelineno-0-987></a>
</span><span id=__span-0-988><a id=__codelineno-0-988 name=__codelineno-0-988></a>    <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span> <span class=o>=</span> <span class=n>is_top_layer</span>
</span><span id=__span-0-989><a id=__codelineno-0-989 name=__codelineno-0-989></a>    <span class=bp>self</span><span class=o>.</span><span class=n>z_dim</span> <span class=o>=</span> <span class=n>z_dim</span>
</span><span id=__span-0-990><a id=__codelineno-0-990 name=__codelineno-0-990></a>    <span class=bp>self</span><span class=o>.</span><span class=n>stochastic_skip</span> <span class=o>=</span> <span class=n>stochastic_skip</span>
</span><span id=__span-0-991><a id=__codelineno-0-991 name=__codelineno-0-991></a>    <span class=bp>self</span><span class=o>.</span><span class=n>learn_top_prior</span> <span class=o>=</span> <span class=n>learn_top_prior</span>
</span><span id=__span-0-992><a id=__codelineno-0-992 name=__codelineno-0-992></a>    <span class=bp>self</span><span class=o>.</span><span class=n>analytical_kl</span> <span class=o>=</span> <span class=n>analytical_kl</span>
</span><span id=__span-0-993><a id=__codelineno-0-993 name=__codelineno-0-993></a>    <span class=bp>self</span><span class=o>.</span><span class=n>bottomup_no_padding_mode</span> <span class=o>=</span> <span class=n>bottomup_no_padding_mode</span>
</span><span id=__span-0-994><a id=__codelineno-0-994 name=__codelineno-0-994></a>    <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=o>=</span> <span class=n>topdown_no_padding_mode</span>
</span><span id=__span-0-995><a id=__codelineno-0-995 name=__codelineno-0-995></a>    <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span> <span class=o>=</span> <span class=n>retain_spatial_dims</span>
</span><span id=__span-0-996><a id=__codelineno-0-996 name=__codelineno-0-996></a>    <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span> <span class=o>=</span> <span class=n>input_image_shape</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span> <span class=k>else</span> <span class=kc>None</span>
</span><span id=__span-0-997><a id=__codelineno-0-997 name=__codelineno-0-997></a>    <span class=bp>self</span><span class=o>.</span><span class=n>non_stochastic_version</span> <span class=o>=</span> <span class=n>non_stochastic_version</span>
</span><span id=__span-0-998><a id=__codelineno-0-998 name=__codelineno-0-998></a>    <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span> <span class=o>=</span> <span class=n>normalize_latent_factor</span>
</span><span id=__span-0-999><a id=__codelineno-0-999 name=__codelineno-0-999></a>    <span class=bp>self</span><span class=o>.</span><span class=n>_vanilla_latent_hw</span> <span class=o>=</span> <span class=n>vanilla_latent_hw</span>
</span><span id=__span-0-1000><a id=__codelineno-0-1000 name=__codelineno-0-1000></a>
</span><span id=__span-0-1001><a id=__codelineno-0-1001 name=__codelineno-0-1001></a>    <span class=c1># Define top layer prior parameters, possibly learnable</span>
</span><span id=__span-0-1002><a id=__codelineno-0-1002 name=__codelineno-0-1002></a>    <span class=k>if</span> <span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1003><a id=__codelineno-0-1003 name=__codelineno-0-1003></a>        <span class=bp>self</span><span class=o>.</span><span class=n>top_prior_params</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span>
</span><span id=__span-0-1004><a id=__codelineno-0-1004 name=__codelineno-0-1004></a>            <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>top_prior_param_shape</span><span class=p>),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=n>learn_top_prior</span>
</span><span id=__span-0-1005><a id=__codelineno-0-1005 name=__codelineno-0-1005></a>        <span class=p>)</span>
</span><span id=__span-0-1006><a id=__codelineno-0-1006 name=__codelineno-0-1006></a>
</span><span id=__span-0-1007><a id=__codelineno-0-1007 name=__codelineno-0-1007></a>    <span class=c1># Downsampling steps left to do in this layer</span>
</span><span id=__span-0-1008><a id=__codelineno-0-1008 name=__codelineno-0-1008></a>    <span class=n>dws_left</span> <span class=o>=</span> <span class=n>downsampling_steps</span>
</span><span id=__span-0-1009><a id=__codelineno-0-1009 name=__codelineno-0-1009></a>
</span><span id=__span-0-1010><a id=__codelineno-0-1010 name=__codelineno-0-1010></a>    <span class=c1># Define deterministic top-down block, which is a sequence of deterministic</span>
</span><span id=__span-0-1011><a id=__codelineno-0-1011 name=__codelineno-0-1011></a>    <span class=c1># residual blocks with (optional) downsampling.</span>
</span><span id=__span-0-1012><a id=__codelineno-0-1012 name=__codelineno-0-1012></a>    <span class=n>block_list</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-0-1013><a id=__codelineno-0-1013 name=__codelineno-0-1013></a>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_res_blocks</span><span class=p>):</span>
</span><span id=__span-0-1014><a id=__codelineno-0-1014 name=__codelineno-0-1014></a>        <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>False</span>
</span><span id=__span-0-1015><a id=__codelineno-0-1015 name=__codelineno-0-1015></a>        <span class=k>if</span> <span class=n>dws_left</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-0-1016><a id=__codelineno-0-1016 name=__codelineno-0-1016></a>            <span class=n>do_resample</span> <span class=o>=</span> <span class=kc>True</span>
</span><span id=__span-0-1017><a id=__codelineno-0-1017 name=__codelineno-0-1017></a>            <span class=n>dws_left</span> <span class=o>-=</span> <span class=mi>1</span>
</span><span id=__span-0-1018><a id=__codelineno-0-1018 name=__codelineno-0-1018></a>        <span class=n>block_list</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span><span id=__span-0-1019><a id=__codelineno-0-1019 name=__codelineno-0-1019></a>            <span class=n>TopDownDeterministicResBlock</span><span class=p>(</span>
</span><span id=__span-0-1020><a id=__codelineno-0-1020 name=__codelineno-0-1020></a>                <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1021><a id=__codelineno-0-1021 name=__codelineno-0-1021></a>                <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1022><a id=__codelineno-0-1022 name=__codelineno-0-1022></a>                <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1023><a id=__codelineno-0-1023 name=__codelineno-0-1023></a>                <span class=n>upsample</span><span class=o>=</span><span class=n>do_resample</span><span class=p>,</span>
</span><span id=__span-0-1024><a id=__codelineno-0-1024 name=__codelineno-0-1024></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1025><a id=__codelineno-0-1025 name=__codelineno-0-1025></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1026><a id=__codelineno-0-1026 name=__codelineno-0-1026></a>                <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1027><a id=__codelineno-0-1027 name=__codelineno-0-1027></a>                <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1028><a id=__codelineno-0-1028 name=__codelineno-0-1028></a>                <span class=n>skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-1029><a id=__codelineno-0-1029 name=__codelineno-0-1029></a>                <span class=n>gated</span><span class=o>=</span><span class=n>gated</span><span class=p>,</span>
</span><span id=__span-0-1030><a id=__codelineno-0-1030 name=__codelineno-0-1030></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1031><a id=__codelineno-0-1031 name=__codelineno-0-1031></a>                <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-1032><a id=__codelineno-0-1032 name=__codelineno-0-1032></a>            <span class=p>)</span>
</span><span id=__span-0-1033><a id=__codelineno-0-1033 name=__codelineno-0-1033></a>        <span class=p>)</span>
</span><span id=__span-0-1034><a id=__codelineno-0-1034 name=__codelineno-0-1034></a>    <span class=bp>self</span><span class=o>.</span><span class=n>deterministic_block</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>block_list</span><span class=p>)</span>
</span><span id=__span-0-1035><a id=__codelineno-0-1035 name=__codelineno-0-1035></a>
</span><span id=__span-0-1036><a id=__codelineno-0-1036 name=__codelineno-0-1036></a>    <span class=c1># Define stochastic block with 2D convolutions</span>
</span><span id=__span-0-1037><a id=__codelineno-0-1037 name=__codelineno-0-1037></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>non_stochastic_version</span><span class=p>:</span>
</span><span id=__span-0-1038><a id=__codelineno-0-1038 name=__codelineno-0-1038></a>        <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span> <span class=o>=</span> <span class=n>NonStochasticBlock2d</span><span class=p>(</span>
</span><span id=__span-0-1039><a id=__codelineno-0-1039 name=__codelineno-0-1039></a>            <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1040><a id=__codelineno-0-1040 name=__codelineno-0-1040></a>            <span class=n>c_vars</span><span class=o>=</span><span class=n>z_dim</span><span class=p>,</span>
</span><span id=__span-0-1041><a id=__codelineno-0-1041 name=__codelineno-0-1041></a>            <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1042><a id=__codelineno-0-1042 name=__codelineno-0-1042></a>            <span class=n>transform_p_params</span><span class=o>=</span><span class=p>(</span><span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>),</span>
</span><span id=__span-0-1043><a id=__codelineno-0-1043 name=__codelineno-0-1043></a>            <span class=n>groups</span><span class=o>=</span><span class=n>groups</span><span class=p>,</span>
</span><span id=__span-0-1044><a id=__codelineno-0-1044 name=__codelineno-0-1044></a>            <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1045><a id=__codelineno-0-1045 name=__codelineno-0-1045></a>        <span class=p>)</span>
</span><span id=__span-0-1046><a id=__codelineno-0-1046 name=__codelineno-0-1046></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1047><a id=__codelineno-0-1047 name=__codelineno-0-1047></a>        <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span> <span class=o>=</span> <span class=n>NormalStochasticBlock2d</span><span class=p>(</span>
</span><span id=__span-0-1048><a id=__codelineno-0-1048 name=__codelineno-0-1048></a>            <span class=n>c_in</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1049><a id=__codelineno-0-1049 name=__codelineno-0-1049></a>            <span class=n>c_vars</span><span class=o>=</span><span class=n>z_dim</span><span class=p>,</span>
</span><span id=__span-0-1050><a id=__codelineno-0-1050 name=__codelineno-0-1050></a>            <span class=n>c_out</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1051><a id=__codelineno-0-1051 name=__codelineno-0-1051></a>            <span class=n>transform_p_params</span><span class=o>=</span><span class=p>(</span><span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>),</span>
</span><span id=__span-0-1052><a id=__codelineno-0-1052 name=__codelineno-0-1052></a>            <span class=n>vanilla_latent_hw</span><span class=o>=</span><span class=n>vanilla_latent_hw</span><span class=p>,</span>
</span><span id=__span-0-1053><a id=__codelineno-0-1053 name=__codelineno-0-1053></a>            <span class=n>restricted_kl</span><span class=o>=</span><span class=n>restricted_kl</span><span class=p>,</span>
</span><span id=__span-0-1054><a id=__codelineno-0-1054 name=__codelineno-0-1054></a>            <span class=n>use_naive_exponential</span><span class=o>=</span><span class=n>stochastic_use_naive_exponential</span><span class=p>,</span>
</span><span id=__span-0-1055><a id=__codelineno-0-1055 name=__codelineno-0-1055></a>        <span class=p>)</span>
</span><span id=__span-0-1056><a id=__codelineno-0-1056 name=__codelineno-0-1056></a>
</span><span id=__span-0-1057><a id=__codelineno-0-1057 name=__codelineno-0-1057></a>    <span class=k>if</span> <span class=ow>not</span> <span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1058><a id=__codelineno-0-1058 name=__codelineno-0-1058></a>        <span class=c1># Merge layer: it combines bottom-up inference and top-down</span>
</span><span id=__span-0-1059><a id=__codelineno-0-1059 name=__codelineno-0-1059></a>        <span class=c1># generative outcomes to give posterior parameters</span>
</span><span id=__span-0-1060><a id=__codelineno-0-1060 name=__codelineno-0-1060></a>        <span class=bp>self</span><span class=o>.</span><span class=n>merge</span> <span class=o>=</span> <span class=n>MergeLayer</span><span class=p>(</span>
</span><span id=__span-0-1061><a id=__codelineno-0-1061 name=__codelineno-0-1061></a>            <span class=n>channels</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1062><a id=__codelineno-0-1062 name=__codelineno-0-1062></a>            <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-1063><a id=__codelineno-0-1063 name=__codelineno-0-1063></a>            <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1064><a id=__codelineno-0-1064 name=__codelineno-0-1064></a>            <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1065><a id=__codelineno-0-1065 name=__codelineno-0-1065></a>            <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1066><a id=__codelineno-0-1066 name=__codelineno-0-1066></a>            <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1067><a id=__codelineno-0-1067 name=__codelineno-0-1067></a>            <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1068><a id=__codelineno-0-1068 name=__codelineno-0-1068></a>            <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1069><a id=__codelineno-0-1069 name=__codelineno-0-1069></a>        <span class=p>)</span>
</span><span id=__span-0-1070><a id=__codelineno-0-1070 name=__codelineno-0-1070></a>
</span><span id=__span-0-1071><a id=__codelineno-0-1071 name=__codelineno-0-1071></a>        <span class=c1># Skip connection that goes around the stochastic top-down layer</span>
</span><span id=__span-0-1072><a id=__codelineno-0-1072 name=__codelineno-0-1072></a>        <span class=k>if</span> <span class=n>stochastic_skip</span><span class=p>:</span>
</span><span id=__span-0-1073><a id=__codelineno-0-1073 name=__codelineno-0-1073></a>            <span class=bp>self</span><span class=o>.</span><span class=n>skip_connection_merger</span> <span class=o>=</span> <span class=n>SkipConnectionMerger</span><span class=p>(</span>
</span><span id=__span-0-1074><a id=__codelineno-0-1074 name=__codelineno-0-1074></a>                <span class=n>channels</span><span class=o>=</span><span class=n>n_filters</span><span class=p>,</span>
</span><span id=__span-0-1075><a id=__codelineno-0-1075 name=__codelineno-0-1075></a>                <span class=n>nonlin</span><span class=o>=</span><span class=n>nonlin</span><span class=p>,</span>
</span><span id=__span-0-1076><a id=__codelineno-0-1076 name=__codelineno-0-1076></a>                <span class=n>batchnorm</span><span class=o>=</span><span class=n>batchnorm</span><span class=p>,</span>
</span><span id=__span-0-1077><a id=__codelineno-0-1077 name=__codelineno-0-1077></a>                <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span>
</span><span id=__span-0-1078><a id=__codelineno-0-1078 name=__codelineno-0-1078></a>                <span class=n>res_block_type</span><span class=o>=</span><span class=n>res_block_type</span><span class=p>,</span>
</span><span id=__span-0-1079><a id=__codelineno-0-1079 name=__codelineno-0-1079></a>                <span class=n>merge_type</span><span class=o>=</span><span class=n>merge_type</span><span class=p>,</span>
</span><span id=__span-0-1080><a id=__codelineno-0-1080 name=__codelineno-0-1080></a>                <span class=n>conv2d_bias</span><span class=o>=</span><span class=n>conv2d_bias</span><span class=p>,</span>
</span><span id=__span-0-1081><a id=__codelineno-0-1081 name=__codelineno-0-1081></a>                <span class=n>res_block_kernel</span><span class=o>=</span><span class=n>res_block_kernel</span><span class=p>,</span>
</span><span id=__span-0-1082><a id=__codelineno-0-1082 name=__codelineno-0-1082></a>                <span class=n>res_block_skip_padding</span><span class=o>=</span><span class=n>res_block_skip_padding</span><span class=p>,</span>
</span><span id=__span-0-1083><a id=__codelineno-0-1083 name=__codelineno-0-1083></a>            <span class=p>)</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.TopDownLayer.align_pparams_buvalue class="doc doc-heading"> <code class="highlight language-python"><span class=n>align_pparams_buvalue</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.TopDownLayer.align_pparams_buvalue class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>In case the padding is not used either (or both) in encoder and decoder, we could have a shape mismatch in the spatial dimensions (usually, dim=2 &amp; dim=3). This method performs a centercrop to ensure that both remain aligned.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>p_params</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor defining the parameters /mu_p and /sigma_p for the latent distribution p(z_i|z_{i+1}).</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>bu_value</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer.</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1157>1157</a></span>
<span class=normal><a href=#__codelineno-0-1158>1158</a></span>
<span class=normal><a href=#__codelineno-0-1159>1159</a></span>
<span class=normal><a href=#__codelineno-0-1160>1160</a></span>
<span class=normal><a href=#__codelineno-0-1161>1161</a></span>
<span class=normal><a href=#__codelineno-0-1162>1162</a></span>
<span class=normal><a href=#__codelineno-0-1163>1163</a></span>
<span class=normal><a href=#__codelineno-0-1164>1164</a></span>
<span class=normal><a href=#__codelineno-0-1165>1165</a></span>
<span class=normal><a href=#__codelineno-0-1166>1166</a></span>
<span class=normal><a href=#__codelineno-0-1167>1167</a></span>
<span class=normal><a href=#__codelineno-0-1168>1168</a></span>
<span class=normal><a href=#__codelineno-0-1169>1169</a></span>
<span class=normal><a href=#__codelineno-0-1170>1170</a></span>
<span class=normal><a href=#__codelineno-0-1171>1171</a></span>
<span class=normal><a href=#__codelineno-0-1172>1172</a></span>
<span class=normal><a href=#__codelineno-0-1173>1173</a></span>
<span class=normal><a href=#__codelineno-0-1174>1174</a></span>
<span class=normal><a href=#__codelineno-0-1175>1175</a></span>
<span class=normal><a href=#__codelineno-0-1176>1176</a></span>
<span class=normal><a href=#__codelineno-0-1177>1177</a></span>
<span class=normal><a href=#__codelineno-0-1178>1178</a></span>
<span class=normal><a href=#__codelineno-0-1179>1179</a></span>
<span class=normal><a href=#__codelineno-0-1180>1180</a></span>
<span class=normal><a href=#__codelineno-0-1181>1181</a></span>
<span class=normal><a href=#__codelineno-0-1182>1182</a></span>
<span class=normal><a href=#__codelineno-0-1183>1183</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1157><a id=__codelineno-0-1157 name=__codelineno-0-1157></a><span class=k>def</span> <span class=nf>align_pparams_buvalue</span><span class=p>(</span>
</span><span id=__span-0-1158><a id=__codelineno-0-1158 name=__codelineno-0-1158></a>    <span class=bp>self</span><span class=p>,</span> <span class=n>p_params</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span>
</span><span id=__span-0-1159><a id=__codelineno-0-1159 name=__codelineno-0-1159></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span><span id=__span-0-1160><a id=__codelineno-0-1160 name=__codelineno-0-1160></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1161><a id=__codelineno-0-1161 name=__codelineno-0-1161></a><span class=sd>    In case the padding is not used either (or both) in encoder and decoder, we could have a shape mismatch</span>
</span><span id=__span-0-1162><a id=__codelineno-0-1162 name=__codelineno-0-1162></a><span class=sd>    in the spatial dimensions (usually, dim=2 &amp; dim=3).</span>
</span><span id=__span-0-1163><a id=__codelineno-0-1163 name=__codelineno-0-1163></a><span class=sd>    This method performs a centercrop to ensure that both remain aligned.</span>
</span><span id=__span-0-1164><a id=__codelineno-0-1164 name=__codelineno-0-1164></a>
</span><span id=__span-0-1165><a id=__codelineno-0-1165 name=__codelineno-0-1165></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1166><a id=__codelineno-0-1166 name=__codelineno-0-1166></a><span class=sd>    ----------</span>
</span><span id=__span-0-1167><a id=__codelineno-0-1167 name=__codelineno-0-1167></a><span class=sd>    p_params: torch.Tensor</span>
</span><span id=__span-0-1168><a id=__codelineno-0-1168 name=__codelineno-0-1168></a><span class=sd>        The tensor defining the parameters /mu_p and /sigma_p for the latent distribution p(z_i|z_{i+1}).</span>
</span><span id=__span-0-1169><a id=__codelineno-0-1169 name=__codelineno-0-1169></a><span class=sd>    bu_value: torch.Tensor</span>
</span><span id=__span-0-1170><a id=__codelineno-0-1170 name=__codelineno-0-1170></a><span class=sd>        The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1171><a id=__codelineno-0-1171 name=__codelineno-0-1171></a><span class=sd>        at the correspondent hierarchical layer.</span>
</span><span id=__span-0-1172><a id=__codelineno-0-1172 name=__codelineno-0-1172></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1173><a id=__codelineno-0-1173 name=__codelineno-0-1173></a>    <span class=k>if</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span> <span class=o>!=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]:</span>
</span><span id=__span-0-1174><a id=__codelineno-0-1174 name=__codelineno-0-1174></a>        <span class=k>assert</span> <span class=bp>self</span><span class=o>.</span><span class=n>bottomup_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span>
</span><span id=__span-0-1175><a id=__codelineno-0-1175 name=__codelineno-0-1175></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1176><a id=__codelineno-0-1176 name=__codelineno-0-1176></a>            <span class=k>assert</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span><span id=__span-0-1177><a id=__codelineno-0-1177 name=__codelineno-0-1177></a>            <span class=n>bu_value</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1178><a id=__codelineno-0-1178 name=__codelineno-0-1178></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1179><a id=__codelineno-0-1179 name=__codelineno-0-1179></a>            <span class=k>if</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span><span id=__span-0-1180><a id=__codelineno-0-1180 name=__codelineno-0-1180></a>                <span class=n>bu_value</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1181><a id=__codelineno-0-1181 name=__codelineno-0-1181></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1182><a id=__codelineno-0-1182 name=__codelineno-0-1182></a>                <span class=n>p_params</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:])</span>
</span><span id=__span-0-1183><a id=__codelineno-0-1183 name=__codelineno-0-1183></a>    <span class=k>return</span> <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.TopDownLayer.forward class="doc doc-heading"> <code class="highlight language-python"><span class=n>forward</span><span class=p>(</span><span class=n>input_</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>skip_connection_input</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>inference_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>bu_value</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>forced_latent</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>use_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>force_constant_output</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>mode_pred</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>use_uncond_mode</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.TopDownLayer.forward class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>input_</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input tensor to the layer, which is the output of the top-down layer above. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>skip_connection_input</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor brought by the skip connection between the current and the previous top-down layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>inference_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the layer is in inference mode. See NOTE 2 in class description for more info. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>bu_value</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>n_img_prior</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of images to be generated from the unconditional prior distribution p(z_L). Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>forced_latent</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>A pre-defined latent tensor. If it is not <code>None</code>, than it is used as the actual latent tensor and, hence, sampling does not happen. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>use_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Wheteher the latent tensor should be set as the latent distribution mode. In the case of Gaussian, the mode coincides with the mean of the distribution. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>force_constant_output</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to copy the first sample (and rel. distrib parameters) over the whole batch. This is used when doing experiment from the prior - q is not used. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>mode_pred</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether the model is in prediction mode. Default is <code>False</code>.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>use_uncond_mode</code></td> <td> <code>bool</code> </td> <td> <div class=doc-md-description> <p>Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</p> </div> </td> <td> <code>False</code> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1185>1185</a></span>
<span class=normal><a href=#__codelineno-0-1186>1186</a></span>
<span class=normal><a href=#__codelineno-0-1187>1187</a></span>
<span class=normal><a href=#__codelineno-0-1188>1188</a></span>
<span class=normal><a href=#__codelineno-0-1189>1189</a></span>
<span class=normal><a href=#__codelineno-0-1190>1190</a></span>
<span class=normal><a href=#__codelineno-0-1191>1191</a></span>
<span class=normal><a href=#__codelineno-0-1192>1192</a></span>
<span class=normal><a href=#__codelineno-0-1193>1193</a></span>
<span class=normal><a href=#__codelineno-0-1194>1194</a></span>
<span class=normal><a href=#__codelineno-0-1195>1195</a></span>
<span class=normal><a href=#__codelineno-0-1196>1196</a></span>
<span class=normal><a href=#__codelineno-0-1197>1197</a></span>
<span class=normal><a href=#__codelineno-0-1198>1198</a></span>
<span class=normal><a href=#__codelineno-0-1199>1199</a></span>
<span class=normal><a href=#__codelineno-0-1200>1200</a></span>
<span class=normal><a href=#__codelineno-0-1201>1201</a></span>
<span class=normal><a href=#__codelineno-0-1202>1202</a></span>
<span class=normal><a href=#__codelineno-0-1203>1203</a></span>
<span class=normal><a href=#__codelineno-0-1204>1204</a></span>
<span class=normal><a href=#__codelineno-0-1205>1205</a></span>
<span class=normal><a href=#__codelineno-0-1206>1206</a></span>
<span class=normal><a href=#__codelineno-0-1207>1207</a></span>
<span class=normal><a href=#__codelineno-0-1208>1208</a></span>
<span class=normal><a href=#__codelineno-0-1209>1209</a></span>
<span class=normal><a href=#__codelineno-0-1210>1210</a></span>
<span class=normal><a href=#__codelineno-0-1211>1211</a></span>
<span class=normal><a href=#__codelineno-0-1212>1212</a></span>
<span class=normal><a href=#__codelineno-0-1213>1213</a></span>
<span class=normal><a href=#__codelineno-0-1214>1214</a></span>
<span class=normal><a href=#__codelineno-0-1215>1215</a></span>
<span class=normal><a href=#__codelineno-0-1216>1216</a></span>
<span class=normal><a href=#__codelineno-0-1217>1217</a></span>
<span class=normal><a href=#__codelineno-0-1218>1218</a></span>
<span class=normal><a href=#__codelineno-0-1219>1219</a></span>
<span class=normal><a href=#__codelineno-0-1220>1220</a></span>
<span class=normal><a href=#__codelineno-0-1221>1221</a></span>
<span class=normal><a href=#__codelineno-0-1222>1222</a></span>
<span class=normal><a href=#__codelineno-0-1223>1223</a></span>
<span class=normal><a href=#__codelineno-0-1224>1224</a></span>
<span class=normal><a href=#__codelineno-0-1225>1225</a></span>
<span class=normal><a href=#__codelineno-0-1226>1226</a></span>
<span class=normal><a href=#__codelineno-0-1227>1227</a></span>
<span class=normal><a href=#__codelineno-0-1228>1228</a></span>
<span class=normal><a href=#__codelineno-0-1229>1229</a></span>
<span class=normal><a href=#__codelineno-0-1230>1230</a></span>
<span class=normal><a href=#__codelineno-0-1231>1231</a></span>
<span class=normal><a href=#__codelineno-0-1232>1232</a></span>
<span class=normal><a href=#__codelineno-0-1233>1233</a></span>
<span class=normal><a href=#__codelineno-0-1234>1234</a></span>
<span class=normal><a href=#__codelineno-0-1235>1235</a></span>
<span class=normal><a href=#__codelineno-0-1236>1236</a></span>
<span class=normal><a href=#__codelineno-0-1237>1237</a></span>
<span class=normal><a href=#__codelineno-0-1238>1238</a></span>
<span class=normal><a href=#__codelineno-0-1239>1239</a></span>
<span class=normal><a href=#__codelineno-0-1240>1240</a></span>
<span class=normal><a href=#__codelineno-0-1241>1241</a></span>
<span class=normal><a href=#__codelineno-0-1242>1242</a></span>
<span class=normal><a href=#__codelineno-0-1243>1243</a></span>
<span class=normal><a href=#__codelineno-0-1244>1244</a></span>
<span class=normal><a href=#__codelineno-0-1245>1245</a></span>
<span class=normal><a href=#__codelineno-0-1246>1246</a></span>
<span class=normal><a href=#__codelineno-0-1247>1247</a></span>
<span class=normal><a href=#__codelineno-0-1248>1248</a></span>
<span class=normal><a href=#__codelineno-0-1249>1249</a></span>
<span class=normal><a href=#__codelineno-0-1250>1250</a></span>
<span class=normal><a href=#__codelineno-0-1251>1251</a></span>
<span class=normal><a href=#__codelineno-0-1252>1252</a></span>
<span class=normal><a href=#__codelineno-0-1253>1253</a></span>
<span class=normal><a href=#__codelineno-0-1254>1254</a></span>
<span class=normal><a href=#__codelineno-0-1255>1255</a></span>
<span class=normal><a href=#__codelineno-0-1256>1256</a></span>
<span class=normal><a href=#__codelineno-0-1257>1257</a></span>
<span class=normal><a href=#__codelineno-0-1258>1258</a></span>
<span class=normal><a href=#__codelineno-0-1259>1259</a></span>
<span class=normal><a href=#__codelineno-0-1260>1260</a></span>
<span class=normal><a href=#__codelineno-0-1261>1261</a></span>
<span class=normal><a href=#__codelineno-0-1262>1262</a></span>
<span class=normal><a href=#__codelineno-0-1263>1263</a></span>
<span class=normal><a href=#__codelineno-0-1264>1264</a></span>
<span class=normal><a href=#__codelineno-0-1265>1265</a></span>
<span class=normal><a href=#__codelineno-0-1266>1266</a></span>
<span class=normal><a href=#__codelineno-0-1267>1267</a></span>
<span class=normal><a href=#__codelineno-0-1268>1268</a></span>
<span class=normal><a href=#__codelineno-0-1269>1269</a></span>
<span class=normal><a href=#__codelineno-0-1270>1270</a></span>
<span class=normal><a href=#__codelineno-0-1271>1271</a></span>
<span class=normal><a href=#__codelineno-0-1272>1272</a></span>
<span class=normal><a href=#__codelineno-0-1273>1273</a></span>
<span class=normal><a href=#__codelineno-0-1274>1274</a></span>
<span class=normal><a href=#__codelineno-0-1275>1275</a></span>
<span class=normal><a href=#__codelineno-0-1276>1276</a></span>
<span class=normal><a href=#__codelineno-0-1277>1277</a></span>
<span class=normal><a href=#__codelineno-0-1278>1278</a></span>
<span class=normal><a href=#__codelineno-0-1279>1279</a></span>
<span class=normal><a href=#__codelineno-0-1280>1280</a></span>
<span class=normal><a href=#__codelineno-0-1281>1281</a></span>
<span class=normal><a href=#__codelineno-0-1282>1282</a></span>
<span class=normal><a href=#__codelineno-0-1283>1283</a></span>
<span class=normal><a href=#__codelineno-0-1284>1284</a></span>
<span class=normal><a href=#__codelineno-0-1285>1285</a></span>
<span class=normal><a href=#__codelineno-0-1286>1286</a></span>
<span class=normal><a href=#__codelineno-0-1287>1287</a></span>
<span class=normal><a href=#__codelineno-0-1288>1288</a></span>
<span class=normal><a href=#__codelineno-0-1289>1289</a></span>
<span class=normal><a href=#__codelineno-0-1290>1290</a></span>
<span class=normal><a href=#__codelineno-0-1291>1291</a></span>
<span class=normal><a href=#__codelineno-0-1292>1292</a></span>
<span class=normal><a href=#__codelineno-0-1293>1293</a></span>
<span class=normal><a href=#__codelineno-0-1294>1294</a></span>
<span class=normal><a href=#__codelineno-0-1295>1295</a></span>
<span class=normal><a href=#__codelineno-0-1296>1296</a></span>
<span class=normal><a href=#__codelineno-0-1297>1297</a></span>
<span class=normal><a href=#__codelineno-0-1298>1298</a></span>
<span class=normal><a href=#__codelineno-0-1299>1299</a></span>
<span class=normal><a href=#__codelineno-0-1300>1300</a></span>
<span class=normal><a href=#__codelineno-0-1301>1301</a></span>
<span class=normal><a href=#__codelineno-0-1302>1302</a></span>
<span class=normal><a href=#__codelineno-0-1303>1303</a></span>
<span class=normal><a href=#__codelineno-0-1304>1304</a></span>
<span class=normal><a href=#__codelineno-0-1305>1305</a></span>
<span class=normal><a href=#__codelineno-0-1306>1306</a></span>
<span class=normal><a href=#__codelineno-0-1307>1307</a></span>
<span class=normal><a href=#__codelineno-0-1308>1308</a></span>
<span class=normal><a href=#__codelineno-0-1309>1309</a></span>
<span class=normal><a href=#__codelineno-0-1310>1310</a></span>
<span class=normal><a href=#__codelineno-0-1311>1311</a></span>
<span class=normal><a href=#__codelineno-0-1312>1312</a></span>
<span class=normal><a href=#__codelineno-0-1313>1313</a></span>
<span class=normal><a href=#__codelineno-0-1314>1314</a></span>
<span class=normal><a href=#__codelineno-0-1315>1315</a></span>
<span class=normal><a href=#__codelineno-0-1316>1316</a></span>
<span class=normal><a href=#__codelineno-0-1317>1317</a></span>
<span class=normal><a href=#__codelineno-0-1318>1318</a></span>
<span class=normal><a href=#__codelineno-0-1319>1319</a></span>
<span class=normal><a href=#__codelineno-0-1320>1320</a></span>
<span class=normal><a href=#__codelineno-0-1321>1321</a></span>
<span class=normal><a href=#__codelineno-0-1322>1322</a></span>
<span class=normal><a href=#__codelineno-0-1323>1323</a></span>
<span class=normal><a href=#__codelineno-0-1324>1324</a></span>
<span class=normal><a href=#__codelineno-0-1325>1325</a></span>
<span class=normal><a href=#__codelineno-0-1326>1326</a></span>
<span class=normal><a href=#__codelineno-0-1327>1327</a></span>
<span class=normal><a href=#__codelineno-0-1328>1328</a></span>
<span class=normal><a href=#__codelineno-0-1329>1329</a></span>
<span class=normal><a href=#__codelineno-0-1330>1330</a></span>
<span class=normal><a href=#__codelineno-0-1331>1331</a></span>
<span class=normal><a href=#__codelineno-0-1332>1332</a></span>
<span class=normal><a href=#__codelineno-0-1333>1333</a></span>
<span class=normal><a href=#__codelineno-0-1334>1334</a></span>
<span class=normal><a href=#__codelineno-0-1335>1335</a></span>
<span class=normal><a href=#__codelineno-0-1336>1336</a></span>
<span class=normal><a href=#__codelineno-0-1337>1337</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1185><a id=__codelineno-0-1185 name=__codelineno-0-1185></a><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span><span id=__span-0-1186><a id=__codelineno-0-1186 name=__codelineno-0-1186></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1187><a id=__codelineno-0-1187 name=__codelineno-0-1187></a>    <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1188><a id=__codelineno-0-1188 name=__codelineno-0-1188></a>    <span class=n>skip_connection_input</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1189><a id=__codelineno-0-1189 name=__codelineno-0-1189></a>    <span class=n>inference_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1190><a id=__codelineno-0-1190 name=__codelineno-0-1190></a>    <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1191><a id=__codelineno-0-1191 name=__codelineno-0-1191></a>    <span class=n>n_img_prior</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1192><a id=__codelineno-0-1192 name=__codelineno-0-1192></a>    <span class=n>forced_latent</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1193><a id=__codelineno-0-1193 name=__codelineno-0-1193></a>    <span class=n>use_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1194><a id=__codelineno-0-1194 name=__codelineno-0-1194></a>    <span class=n>force_constant_output</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1195><a id=__codelineno-0-1195 name=__codelineno-0-1195></a>    <span class=n>mode_pred</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1196><a id=__codelineno-0-1196 name=__codelineno-0-1196></a>    <span class=n>use_uncond_mode</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span><span id=__span-0-1197><a id=__codelineno-0-1197 name=__codelineno-0-1197></a>    <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1198><a id=__codelineno-0-1198 name=__codelineno-0-1198></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]:</span>
</span><span id=__span-0-1199><a id=__codelineno-0-1199 name=__codelineno-0-1199></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1200><a id=__codelineno-0-1200 name=__codelineno-0-1200></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1201><a id=__codelineno-0-1201 name=__codelineno-0-1201></a><span class=sd>    ----------</span>
</span><span id=__span-0-1202><a id=__codelineno-0-1202 name=__codelineno-0-1202></a><span class=sd>    input_: torch.Tensor, optional</span>
</span><span id=__span-0-1203><a id=__codelineno-0-1203 name=__codelineno-0-1203></a><span class=sd>        The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1204><a id=__codelineno-0-1204 name=__codelineno-0-1204></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-1205><a id=__codelineno-0-1205 name=__codelineno-0-1205></a><span class=sd>    skip_connection_input: torch.Tensor, optional</span>
</span><span id=__span-0-1206><a id=__codelineno-0-1206 name=__codelineno-0-1206></a><span class=sd>        The tensor brought by the skip connection between the current and the previous top-down layer.</span>
</span><span id=__span-0-1207><a id=__codelineno-0-1207 name=__codelineno-0-1207></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-1208><a id=__codelineno-0-1208 name=__codelineno-0-1208></a><span class=sd>    inference_mode: bool, optional</span>
</span><span id=__span-0-1209><a id=__codelineno-0-1209 name=__codelineno-0-1209></a><span class=sd>        Whether the layer is in inference mode. See NOTE 2 in class description for more info.</span>
</span><span id=__span-0-1210><a id=__codelineno-0-1210 name=__codelineno-0-1210></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1211><a id=__codelineno-0-1211 name=__codelineno-0-1211></a><span class=sd>    bu_value: torch.Tensor, optional</span>
</span><span id=__span-0-1212><a id=__codelineno-0-1212 name=__codelineno-0-1212></a><span class=sd>        The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1213><a id=__codelineno-0-1213 name=__codelineno-0-1213></a><span class=sd>        at the correspondent hierarchical layer. Default is `None`.</span>
</span><span id=__span-0-1214><a id=__codelineno-0-1214 name=__codelineno-0-1214></a><span class=sd>    n_img_prior: int, optional</span>
</span><span id=__span-0-1215><a id=__codelineno-0-1215 name=__codelineno-0-1215></a><span class=sd>        The number of images to be generated from the unconditional prior distribution p(z_L).</span>
</span><span id=__span-0-1216><a id=__codelineno-0-1216 name=__codelineno-0-1216></a><span class=sd>        Default is `None`.</span>
</span><span id=__span-0-1217><a id=__codelineno-0-1217 name=__codelineno-0-1217></a><span class=sd>    forced_latent: torch.Tensor, optional</span>
</span><span id=__span-0-1218><a id=__codelineno-0-1218 name=__codelineno-0-1218></a><span class=sd>        A pre-defined latent tensor. If it is not `None`, than it is used as the actual latent tensor and,</span>
</span><span id=__span-0-1219><a id=__codelineno-0-1219 name=__codelineno-0-1219></a><span class=sd>        hence, sampling does not happen. Default is `None`.</span>
</span><span id=__span-0-1220><a id=__codelineno-0-1220 name=__codelineno-0-1220></a><span class=sd>    use_mode: bool, optional</span>
</span><span id=__span-0-1221><a id=__codelineno-0-1221 name=__codelineno-0-1221></a><span class=sd>        Wheteher the latent tensor should be set as the latent distribution mode.</span>
</span><span id=__span-0-1222><a id=__codelineno-0-1222 name=__codelineno-0-1222></a><span class=sd>        In the case of Gaussian, the mode coincides with the mean of the distribution.</span>
</span><span id=__span-0-1223><a id=__codelineno-0-1223 name=__codelineno-0-1223></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1224><a id=__codelineno-0-1224 name=__codelineno-0-1224></a><span class=sd>    force_constant_output: bool, optional</span>
</span><span id=__span-0-1225><a id=__codelineno-0-1225 name=__codelineno-0-1225></a><span class=sd>        Whether to copy the first sample (and rel. distrib parameters) over the whole batch.</span>
</span><span id=__span-0-1226><a id=__codelineno-0-1226 name=__codelineno-0-1226></a><span class=sd>        This is used when doing experiment from the prior - q is not used.</span>
</span><span id=__span-0-1227><a id=__codelineno-0-1227 name=__codelineno-0-1227></a><span class=sd>        Default is `False`.</span>
</span><span id=__span-0-1228><a id=__codelineno-0-1228 name=__codelineno-0-1228></a><span class=sd>    mode_pred: bool, optional</span>
</span><span id=__span-0-1229><a id=__codelineno-0-1229 name=__codelineno-0-1229></a><span class=sd>        Whether the model is in prediction mode. Default is `False`.</span>
</span><span id=__span-0-1230><a id=__codelineno-0-1230 name=__codelineno-0-1230></a><span class=sd>    use_uncond_mode: bool, optional</span>
</span><span id=__span-0-1231><a id=__codelineno-0-1231 name=__codelineno-0-1231></a><span class=sd>        Whether to use the uncoditional distribution p(z) to sample latents in prediction mode.</span>
</span><span id=__span-0-1232><a id=__codelineno-0-1232 name=__codelineno-0-1232></a><span class=sd>    var_clip_max: float</span>
</span><span id=__span-0-1233><a id=__codelineno-0-1233 name=__codelineno-0-1233></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1234><a id=__codelineno-0-1234 name=__codelineno-0-1234></a><span class=sd>        Values exceeding this threshold are clipped.</span>
</span><span id=__span-0-1235><a id=__codelineno-0-1235 name=__codelineno-0-1235></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1236><a id=__codelineno-0-1236 name=__codelineno-0-1236></a>    <span class=c1># Check consistency of arguments</span>
</span><span id=__span-0-1237><a id=__codelineno-0-1237 name=__codelineno-0-1237></a>    <span class=n>inputs_none</span> <span class=o>=</span> <span class=n>input_</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>skip_connection_input</span> <span class=ow>is</span> <span class=kc>None</span>
</span><span id=__span-0-1238><a id=__codelineno-0-1238 name=__codelineno-0-1238></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>inputs_none</span><span class=p>:</span>
</span><span id=__span-0-1239><a id=__codelineno-0-1239 name=__codelineno-0-1239></a>        <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span><span class=s2>&quot;In top layer, inputs should be None&quot;</span><span class=p>)</span>
</span><span id=__span-0-1240><a id=__codelineno-0-1240 name=__codelineno-0-1240></a>
</span><span id=__span-0-1241><a id=__codelineno-0-1241 name=__codelineno-0-1241></a>    <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_p_params</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=p>)</span>
</span><span id=__span-0-1242><a id=__codelineno-0-1242 name=__codelineno-0-1242></a>
</span><span id=__span-0-1243><a id=__codelineno-0-1243 name=__codelineno-0-1243></a>    <span class=c1># Get the parameters for the latent distribution to sample from</span>
</span><span id=__span-0-1244><a id=__codelineno-0-1244 name=__codelineno-0-1244></a>    <span class=k>if</span> <span class=n>inference_mode</span><span class=p>:</span>
</span><span id=__span-0-1245><a id=__codelineno-0-1245 name=__codelineno-0-1245></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1246><a id=__codelineno-0-1246 name=__codelineno-0-1246></a>            <span class=n>q_params</span> <span class=o>=</span> <span class=n>bu_value</span>
</span><span id=__span-0-1247><a id=__codelineno-0-1247 name=__codelineno-0-1247></a>            <span class=k>if</span> <span class=n>mode_pred</span> <span class=ow>is</span> <span class=kc>False</span><span class=p>:</span>
</span><span id=__span-0-1248><a id=__codelineno-0-1248 name=__codelineno-0-1248></a>                <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>align_pparams_buvalue</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>)</span>
</span><span id=__span-0-1249><a id=__codelineno-0-1249 name=__codelineno-0-1249></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1250><a id=__codelineno-0-1250 name=__codelineno-0-1250></a>            <span class=k>if</span> <span class=n>use_uncond_mode</span><span class=p>:</span>
</span><span id=__span-0-1251><a id=__codelineno-0-1251 name=__codelineno-0-1251></a>                <span class=n>q_params</span> <span class=o>=</span> <span class=n>p_params</span>
</span><span id=__span-0-1252><a id=__codelineno-0-1252 name=__codelineno-0-1252></a>            <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1253><a id=__codelineno-0-1253 name=__codelineno-0-1253></a>                <span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>align_pparams_buvalue</span><span class=p>(</span><span class=n>p_params</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>)</span>
</span><span id=__span-0-1254><a id=__codelineno-0-1254 name=__codelineno-0-1254></a>                <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1255><a id=__codelineno-0-1255 name=__codelineno-0-1255></a>    <span class=c1># In generative mode, q is not used</span>
</span><span id=__span-0-1256><a id=__codelineno-0-1256 name=__codelineno-0-1256></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1257><a id=__codelineno-0-1257 name=__codelineno-0-1257></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1258><a id=__codelineno-0-1258 name=__codelineno-0-1258></a>
</span><span id=__span-0-1259><a id=__codelineno-0-1259 name=__codelineno-0-1259></a>    <span class=c1># NOTE: Sampling is done either from q(z_i | z_{i+1}, x) or p(z_i | z_{i+1})</span>
</span><span id=__span-0-1260><a id=__codelineno-0-1260 name=__codelineno-0-1260></a>    <span class=c1># depending on the mode (hence, in practice, by checking whether q_params is None).</span>
</span><span id=__span-0-1261><a id=__codelineno-0-1261 name=__codelineno-0-1261></a>
</span><span id=__span-0-1262><a id=__codelineno-0-1262 name=__codelineno-0-1262></a>    <span class=c1># Normalization of latent space parameters:</span>
</span><span id=__span-0-1263><a id=__codelineno-0-1263 name=__codelineno-0-1263></a>    <span class=c1># it is done, purely for stablity. See Very deep VAEs generalize autoregressive models.</span>
</span><span id=__span-0-1264><a id=__codelineno-0-1264 name=__codelineno-0-1264></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span><span class=p>:</span>
</span><span id=__span-0-1265><a id=__codelineno-0-1265 name=__codelineno-0-1265></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=n>q_params</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>normalize_latent_factor</span>
</span><span id=__span-0-1266><a id=__codelineno-0-1266 name=__codelineno-0-1266></a>
</span><span id=__span-0-1267><a id=__codelineno-0-1267 name=__codelineno-0-1267></a>    <span class=c1># Sample (and process) a latent tensor in the stochastic layer</span>
</span><span id=__span-0-1268><a id=__codelineno-0-1268 name=__codelineno-0-1268></a>    <span class=n>x</span><span class=p>,</span> <span class=n>data_stoch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span><span class=p>(</span>
</span><span id=__span-0-1269><a id=__codelineno-0-1269 name=__codelineno-0-1269></a>        <span class=n>p_params</span><span class=o>=</span><span class=n>p_params</span><span class=p>,</span>
</span><span id=__span-0-1270><a id=__codelineno-0-1270 name=__codelineno-0-1270></a>        <span class=n>q_params</span><span class=o>=</span><span class=n>q_params</span><span class=p>,</span>
</span><span id=__span-0-1271><a id=__codelineno-0-1271 name=__codelineno-0-1271></a>        <span class=n>forced_latent</span><span class=o>=</span><span class=n>forced_latent</span><span class=p>,</span>
</span><span id=__span-0-1272><a id=__codelineno-0-1272 name=__codelineno-0-1272></a>        <span class=n>use_mode</span><span class=o>=</span><span class=n>use_mode</span><span class=p>,</span>
</span><span id=__span-0-1273><a id=__codelineno-0-1273 name=__codelineno-0-1273></a>        <span class=n>force_constant_output</span><span class=o>=</span><span class=n>force_constant_output</span><span class=p>,</span>
</span><span id=__span-0-1274><a id=__codelineno-0-1274 name=__codelineno-0-1274></a>        <span class=n>analytical_kl</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>analytical_kl</span><span class=p>,</span>
</span><span id=__span-0-1275><a id=__codelineno-0-1275 name=__codelineno-0-1275></a>        <span class=n>mode_pred</span><span class=o>=</span><span class=n>mode_pred</span><span class=p>,</span>
</span><span id=__span-0-1276><a id=__codelineno-0-1276 name=__codelineno-0-1276></a>        <span class=n>use_uncond_mode</span><span class=o>=</span><span class=n>use_uncond_mode</span><span class=p>,</span>
</span><span id=__span-0-1277><a id=__codelineno-0-1277 name=__codelineno-0-1277></a>        <span class=n>var_clip_max</span><span class=o>=</span><span class=n>var_clip_max</span><span class=p>,</span>
</span><span id=__span-0-1278><a id=__codelineno-0-1278 name=__codelineno-0-1278></a>    <span class=p>)</span>
</span><span id=__span-0-1279><a id=__codelineno-0-1279 name=__codelineno-0-1279></a>
</span><span id=__span-0-1280><a id=__codelineno-0-1280 name=__codelineno-0-1280></a>    <span class=c1># Merge skip connection from previous layer</span>
</span><span id=__span-0-1281><a id=__codelineno-0-1281 name=__codelineno-0-1281></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic_skip</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1282><a id=__codelineno-0-1282 name=__codelineno-0-1282></a>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span><span class=p>:</span>
</span><span id=__span-0-1283><a id=__codelineno-0-1283 name=__codelineno-0-1283></a>            <span class=c1># If no padding is done in the current top-down pass, there may be a shape mismatch between current tensor and skip connection input.</span>
</span><span id=__span-0-1284><a id=__codelineno-0-1284 name=__codelineno-0-1284></a>            <span class=c1># As an example, if the output of last TopDownLayer was of size 64*64, due to lack of padding in the current layer, the current tensor</span>
</span><span id=__span-0-1285><a id=__codelineno-0-1285 name=__codelineno-0-1285></a>            <span class=c1># might become different in shape, say 60*60.</span>
</span><span id=__span-0-1286><a id=__codelineno-0-1286 name=__codelineno-0-1286></a>            <span class=c1># In order to avoid shape mismatch, we do central crop of the skip connection input.</span>
</span><span id=__span-0-1287><a id=__codelineno-0-1287 name=__codelineno-0-1287></a>            <span class=n>skip_connection_input</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span>
</span><span id=__span-0-1288><a id=__codelineno-0-1288 name=__codelineno-0-1288></a>                <span class=n>skip_connection_input</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>:]</span>
</span><span id=__span-0-1289><a id=__codelineno-0-1289 name=__codelineno-0-1289></a>            <span class=p>)</span>
</span><span id=__span-0-1290><a id=__codelineno-0-1290 name=__codelineno-0-1290></a>
</span><span id=__span-0-1291><a id=__codelineno-0-1291 name=__codelineno-0-1291></a>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>skip_connection_merger</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>skip_connection_input</span><span class=p>)</span>
</span><span id=__span-0-1292><a id=__codelineno-0-1292 name=__codelineno-0-1292></a>
</span><span id=__span-0-1293><a id=__codelineno-0-1293 name=__codelineno-0-1293></a>    <span class=c1># Save activation before residual block as it can be the skip connection input in the next layer</span>
</span><span id=__span-0-1294><a id=__codelineno-0-1294 name=__codelineno-0-1294></a>    <span class=n>x_pre_residual</span> <span class=o>=</span> <span class=n>x</span>
</span><span id=__span-0-1295><a id=__codelineno-0-1295 name=__codelineno-0-1295></a>
</span><span id=__span-0-1296><a id=__codelineno-0-1296 name=__codelineno-0-1296></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>retain_spatial_dims</span><span class=p>:</span>
</span><span id=__span-0-1297><a id=__codelineno-0-1297 name=__codelineno-0-1297></a>        <span class=c1># when we don&#39;t want to do padding in topdown as well, we need to spare some boundary pixels which would be used up.</span>
</span><span id=__span-0-1298><a id=__codelineno-0-1298 name=__codelineno-0-1298></a>        <span class=n>extra_len</span> <span class=o>=</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span> <span class=ow>is</span> <span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=mi>3</span>
</span><span id=__span-0-1299><a id=__codelineno-0-1299 name=__codelineno-0-1299></a>
</span><span id=__span-0-1300><a id=__codelineno-0-1300 name=__codelineno-0-1300></a>        <span class=c1># this means that x should be of the same size as config.data.image_size. So, we have to centercrop by a factor of 2 at this point.</span>
</span><span id=__span-0-1301><a id=__codelineno-0-1301 name=__codelineno-0-1301></a>        <span class=c1># assert x.shape[-1] &gt;= self.latent_shape[-1] // 2 + extra_len</span>
</span><span id=__span-0-1302><a id=__codelineno-0-1302 name=__codelineno-0-1302></a>        <span class=c1># we assume that one topdown layer will have exactly one upscaling layer.</span>
</span><span id=__span-0-1303><a id=__codelineno-0-1303 name=__codelineno-0-1303></a>        <span class=n>new_latent_shape</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-0-1304><a id=__codelineno-0-1304 name=__codelineno-0-1304></a>            <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>extra_len</span><span class=p>,</span>
</span><span id=__span-0-1305><a id=__codelineno-0-1305 name=__codelineno-0-1305></a>            <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>//</span> <span class=mi>2</span> <span class=o>+</span> <span class=n>extra_len</span><span class=p>,</span>
</span><span id=__span-0-1306><a id=__codelineno-0-1306 name=__codelineno-0-1306></a>        <span class=p>)</span>
</span><span id=__span-0-1307><a id=__codelineno-0-1307 name=__codelineno-0-1307></a>
</span><span id=__span-0-1308><a id=__codelineno-0-1308 name=__codelineno-0-1308></a>        <span class=c1># If the LC is not applied on all layers, then this can happen.</span>
</span><span id=__span-0-1309><a id=__codelineno-0-1309 name=__codelineno-0-1309></a>        <span class=k>if</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>new_latent_shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]:</span>
</span><span id=__span-0-1310><a id=__codelineno-0-1310 name=__codelineno-0-1310></a>            <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>new_latent_shape</span><span class=p>)</span>
</span><span id=__span-0-1311><a id=__codelineno-0-1311 name=__codelineno-0-1311></a>
</span><span id=__span-0-1312><a id=__codelineno-0-1312 name=__codelineno-0-1312></a>    <span class=c1># Last top-down block (sequence of residual blocks)</span>
</span><span id=__span-0-1313><a id=__codelineno-0-1313 name=__codelineno-0-1313></a>    <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>deterministic_block</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span><span id=__span-0-1314><a id=__codelineno-0-1314 name=__codelineno-0-1314></a>
</span><span id=__span-0-1315><a id=__codelineno-0-1315 name=__codelineno-0-1315></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>topdown_no_padding_mode</span><span class=p>:</span>
</span><span id=__span-0-1316><a id=__codelineno-0-1316 name=__codelineno-0-1316></a>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>center_crop</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>latent_shape</span><span class=p>)</span>
</span><span id=__span-0-1317><a id=__codelineno-0-1317 name=__codelineno-0-1317></a>
</span><span id=__span-0-1318><a id=__codelineno-0-1318 name=__codelineno-0-1318></a>    <span class=c1># Save some metrics that will be used in the loss computation</span>
</span><span id=__span-0-1319><a id=__codelineno-0-1319 name=__codelineno-0-1319></a>    <span class=n>keys</span> <span class=o>=</span> <span class=p>[</span>
</span><span id=__span-0-1320><a id=__codelineno-0-1320 name=__codelineno-0-1320></a>        <span class=s2>&quot;z&quot;</span><span class=p>,</span>
</span><span id=__span-0-1321><a id=__codelineno-0-1321 name=__codelineno-0-1321></a>        <span class=s2>&quot;kl_samplewise&quot;</span><span class=p>,</span>
</span><span id=__span-0-1322><a id=__codelineno-0-1322 name=__codelineno-0-1322></a>        <span class=s2>&quot;kl_samplewise_restricted&quot;</span><span class=p>,</span>
</span><span id=__span-0-1323><a id=__codelineno-0-1323 name=__codelineno-0-1323></a>        <span class=s2>&quot;kl_spatial&quot;</span><span class=p>,</span>
</span><span id=__span-0-1324><a id=__codelineno-0-1324 name=__codelineno-0-1324></a>        <span class=s2>&quot;kl_channelwise&quot;</span><span class=p>,</span>
</span><span id=__span-0-1325><a id=__codelineno-0-1325 name=__codelineno-0-1325></a>        <span class=c1># &#39;logprob_p&#39;,</span>
</span><span id=__span-0-1326><a id=__codelineno-0-1326 name=__codelineno-0-1326></a>        <span class=s2>&quot;logprob_q&quot;</span><span class=p>,</span>
</span><span id=__span-0-1327><a id=__codelineno-0-1327 name=__codelineno-0-1327></a>        <span class=s2>&quot;qvar_max&quot;</span><span class=p>,</span>
</span><span id=__span-0-1328><a id=__codelineno-0-1328 name=__codelineno-0-1328></a>    <span class=p>]</span>
</span><span id=__span-0-1329><a id=__codelineno-0-1329 name=__codelineno-0-1329></a>    <span class=n>data</span> <span class=o>=</span> <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>data_stoch</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>keys</span><span class=p>}</span>
</span><span id=__span-0-1330><a id=__codelineno-0-1330 name=__codelineno-0-1330></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_mu&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1331><a id=__codelineno-0-1331 name=__codelineno-0-1331></a>    <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_lv&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1332><a id=__codelineno-0-1332 name=__codelineno-0-1332></a>    <span class=k>if</span> <span class=n>data_stoch</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1333><a id=__codelineno-0-1333 name=__codelineno-0-1333></a>        <span class=n>q_mu</span><span class=p>,</span> <span class=n>q_lv</span> <span class=o>=</span> <span class=n>data_stoch</span><span class=p>[</span><span class=s2>&quot;q_params&quot;</span><span class=p>]</span>
</span><span id=__span-0-1334><a id=__codelineno-0-1334 name=__codelineno-0-1334></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_mu&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_mu</span>
</span><span id=__span-0-1335><a id=__codelineno-0-1335 name=__codelineno-0-1335></a>        <span class=n>data</span><span class=p>[</span><span class=s2>&quot;q_lv&quot;</span><span class=p>]</span> <span class=o>=</span> <span class=n>q_lv</span>
</span><span id=__span-0-1336><a id=__codelineno-0-1336 name=__codelineno-0-1336></a>
</span><span id=__span-0-1337><a id=__codelineno-0-1337 name=__codelineno-0-1337></a>    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>x_pre_residual</span><span class=p>,</span> <span class=n>data</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.TopDownLayer.get_p_params class="doc doc-heading"> <code class="highlight language-python"><span class=n>get_p_params</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.TopDownLayer.get_p_params class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>This method returns the parameters of the prior distribution p(z_i|z_{i+1}) for the latent tensor depending on the hierarchical level of the layer and other specific conditions.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>input_</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input tensor to the layer, which is the output of the top-down layer above.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>n_img_prior</code></td> <td> <code>int</code> </td> <td> <div class=doc-md-description> <p>The number of images to be generated from the unconditional prior distribution p(z_L).</p> </div> </td> <td> <em>required</em> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1125>1125</a></span>
<span class=normal><a href=#__codelineno-0-1126>1126</a></span>
<span class=normal><a href=#__codelineno-0-1127>1127</a></span>
<span class=normal><a href=#__codelineno-0-1128>1128</a></span>
<span class=normal><a href=#__codelineno-0-1129>1129</a></span>
<span class=normal><a href=#__codelineno-0-1130>1130</a></span>
<span class=normal><a href=#__codelineno-0-1131>1131</a></span>
<span class=normal><a href=#__codelineno-0-1132>1132</a></span>
<span class=normal><a href=#__codelineno-0-1133>1133</a></span>
<span class=normal><a href=#__codelineno-0-1134>1134</a></span>
<span class=normal><a href=#__codelineno-0-1135>1135</a></span>
<span class=normal><a href=#__codelineno-0-1136>1136</a></span>
<span class=normal><a href=#__codelineno-0-1137>1137</a></span>
<span class=normal><a href=#__codelineno-0-1138>1138</a></span>
<span class=normal><a href=#__codelineno-0-1139>1139</a></span>
<span class=normal><a href=#__codelineno-0-1140>1140</a></span>
<span class=normal><a href=#__codelineno-0-1141>1141</a></span>
<span class=normal><a href=#__codelineno-0-1142>1142</a></span>
<span class=normal><a href=#__codelineno-0-1143>1143</a></span>
<span class=normal><a href=#__codelineno-0-1144>1144</a></span>
<span class=normal><a href=#__codelineno-0-1145>1145</a></span>
<span class=normal><a href=#__codelineno-0-1146>1146</a></span>
<span class=normal><a href=#__codelineno-0-1147>1147</a></span>
<span class=normal><a href=#__codelineno-0-1148>1148</a></span>
<span class=normal><a href=#__codelineno-0-1149>1149</a></span>
<span class=normal><a href=#__codelineno-0-1150>1150</a></span>
<span class=normal><a href=#__codelineno-0-1151>1151</a></span>
<span class=normal><a href=#__codelineno-0-1152>1152</a></span>
<span class=normal><a href=#__codelineno-0-1153>1153</a></span>
<span class=normal><a href=#__codelineno-0-1154>1154</a></span>
<span class=normal><a href=#__codelineno-0-1155>1155</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1125><a id=__codelineno-0-1125 name=__codelineno-0-1125></a><span class=k>def</span> <span class=nf>get_p_params</span><span class=p>(</span>
</span><span id=__span-0-1126><a id=__codelineno-0-1126 name=__codelineno-0-1126></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1127><a id=__codelineno-0-1127 name=__codelineno-0-1127></a>    <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1128><a id=__codelineno-0-1128 name=__codelineno-0-1128></a>    <span class=n>n_img_prior</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span><span id=__span-0-1129><a id=__codelineno-0-1129 name=__codelineno-0-1129></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1130><a id=__codelineno-0-1130 name=__codelineno-0-1130></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1131><a id=__codelineno-0-1131 name=__codelineno-0-1131></a><span class=sd>    This method returns the parameters of the prior distribution p(z_i|z_{i+1}) for the latent tensor</span>
</span><span id=__span-0-1132><a id=__codelineno-0-1132 name=__codelineno-0-1132></a><span class=sd>    depending on the hierarchical level of the layer and other specific conditions.</span>
</span><span id=__span-0-1133><a id=__codelineno-0-1133 name=__codelineno-0-1133></a>
</span><span id=__span-0-1134><a id=__codelineno-0-1134 name=__codelineno-0-1134></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1135><a id=__codelineno-0-1135 name=__codelineno-0-1135></a><span class=sd>    ----------</span>
</span><span id=__span-0-1136><a id=__codelineno-0-1136 name=__codelineno-0-1136></a><span class=sd>    input_: torch.Tensor</span>
</span><span id=__span-0-1137><a id=__codelineno-0-1137 name=__codelineno-0-1137></a><span class=sd>        The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1138><a id=__codelineno-0-1138 name=__codelineno-0-1138></a><span class=sd>    n_img_prior: int</span>
</span><span id=__span-0-1139><a id=__codelineno-0-1139 name=__codelineno-0-1139></a><span class=sd>        The number of images to be generated from the unconditional prior distribution p(z_L).</span>
</span><span id=__span-0-1140><a id=__codelineno-0-1140 name=__codelineno-0-1140></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1141><a id=__codelineno-0-1141 name=__codelineno-0-1141></a>    <span class=n>p_params</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1142><a id=__codelineno-0-1142 name=__codelineno-0-1142></a>
</span><span id=__span-0-1143><a id=__codelineno-0-1143 name=__codelineno-0-1143></a>    <span class=c1># If top layer, define p_params as the ones of the prior p(z_L)</span>
</span><span id=__span-0-1144><a id=__codelineno-0-1144 name=__codelineno-0-1144></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>
</span><span id=__span-0-1145><a id=__codelineno-0-1145 name=__codelineno-0-1145></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>top_prior_params</span>
</span><span id=__span-0-1146><a id=__codelineno-0-1146 name=__codelineno-0-1146></a>
</span><span id=__span-0-1147><a id=__codelineno-0-1147 name=__codelineno-0-1147></a>        <span class=c1># Sample specific number of images by expanding the prior</span>
</span><span id=__span-0-1148><a id=__codelineno-0-1148 name=__codelineno-0-1148></a>        <span class=k>if</span> <span class=n>n_img_prior</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span><span id=__span-0-1149><a id=__codelineno-0-1149 name=__codelineno-0-1149></a>            <span class=n>p_params</span> <span class=o>=</span> <span class=n>p_params</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>n_img_prior</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span><span id=__span-0-1150><a id=__codelineno-0-1150 name=__codelineno-0-1150></a>
</span><span id=__span-0-1151><a id=__codelineno-0-1151 name=__codelineno-0-1151></a>    <span class=c1># Else the input from the layer above is p_params itself</span>
</span><span id=__span-0-1152><a id=__codelineno-0-1152 name=__codelineno-0-1152></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1153><a id=__codelineno-0-1153 name=__codelineno-0-1153></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=n>input_</span>
</span><span id=__span-0-1154><a id=__codelineno-0-1154 name=__codelineno-0-1154></a>
</span><span id=__span-0-1155><a id=__codelineno-0-1155 name=__codelineno-0-1155></a>    <span class=k>return</span> <span class=n>p_params</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> <div class="doc doc-object doc-function"> <h3 id=careamics.models.lvae.layers.TopDownLayer.sample_from_q class="doc doc-heading"> <code class="highlight language-python"><span class=n>sample_from_q</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>bu_value</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></code> <a href=#careamics.models.lvae.layers.TopDownLayer.sample_from_q class=headerlink title="Permanent link">#</a></h3> <div class="doc doc-contents "> <p>This method computes the latent inference distribution q(z_i|z_{i+1}) amd samples a latent tensor from it.</p> <p><span class=doc-section-title>Parameters:</span></p> <table> <thead> <tr> <th>Name</th> <th>Type</th> <th>Description</th> <th>Default</th> </tr> </thead> <tbody> <tr class=doc-section-item> <td><code>input_</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The input tensor to the layer, which is the output of the top-down layer above.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>bu_value</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass at the correspondent hierarchical layer.</p> </div> </td> <td> <em>required</em> </td> </tr> <tr class=doc-section-item> <td><code>var_clip_max</code></td> <td> <code>float</code> </td> <td> <div class=doc-md-description> <p>The maximum value reachable by the log-variance of the latent distribtion. Values exceeding this threshold are clipped. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> <tr class=doc-section-item> <td><code>mask</code></td> <td> <code><span title="torch.Tensor">Tensor</span></code> </td> <td> <div class=doc-md-description> <p>A tensor that is used to mask the sampled latent tensor. Default is <code>None</code>.</p> </div> </td> <td> <code>None</code> </td> </tr> </tbody> </table> <details class=quote> <summary>Source code in <code>src/careamics/models/lvae/layers.py</code></summary> <div class="language-python highlight"><table class=highlighttable><tr><td class=linenos><div class=linenodiv><pre><span></span><span class=normal><a href=#__codelineno-0-1087>1087</a></span>
<span class=normal><a href=#__codelineno-0-1088>1088</a></span>
<span class=normal><a href=#__codelineno-0-1089>1089</a></span>
<span class=normal><a href=#__codelineno-0-1090>1090</a></span>
<span class=normal><a href=#__codelineno-0-1091>1091</a></span>
<span class=normal><a href=#__codelineno-0-1092>1092</a></span>
<span class=normal><a href=#__codelineno-0-1093>1093</a></span>
<span class=normal><a href=#__codelineno-0-1094>1094</a></span>
<span class=normal><a href=#__codelineno-0-1095>1095</a></span>
<span class=normal><a href=#__codelineno-0-1096>1096</a></span>
<span class=normal><a href=#__codelineno-0-1097>1097</a></span>
<span class=normal><a href=#__codelineno-0-1098>1098</a></span>
<span class=normal><a href=#__codelineno-0-1099>1099</a></span>
<span class=normal><a href=#__codelineno-0-1100>1100</a></span>
<span class=normal><a href=#__codelineno-0-1101>1101</a></span>
<span class=normal><a href=#__codelineno-0-1102>1102</a></span>
<span class=normal><a href=#__codelineno-0-1103>1103</a></span>
<span class=normal><a href=#__codelineno-0-1104>1104</a></span>
<span class=normal><a href=#__codelineno-0-1105>1105</a></span>
<span class=normal><a href=#__codelineno-0-1106>1106</a></span>
<span class=normal><a href=#__codelineno-0-1107>1107</a></span>
<span class=normal><a href=#__codelineno-0-1108>1108</a></span>
<span class=normal><a href=#__codelineno-0-1109>1109</a></span>
<span class=normal><a href=#__codelineno-0-1110>1110</a></span>
<span class=normal><a href=#__codelineno-0-1111>1111</a></span>
<span class=normal><a href=#__codelineno-0-1112>1112</a></span>
<span class=normal><a href=#__codelineno-0-1113>1113</a></span>
<span class=normal><a href=#__codelineno-0-1114>1114</a></span>
<span class=normal><a href=#__codelineno-0-1115>1115</a></span>
<span class=normal><a href=#__codelineno-0-1116>1116</a></span>
<span class=normal><a href=#__codelineno-0-1117>1117</a></span>
<span class=normal><a href=#__codelineno-0-1118>1118</a></span>
<span class=normal><a href=#__codelineno-0-1119>1119</a></span>
<span class=normal><a href=#__codelineno-0-1120>1120</a></span>
<span class=normal><a href=#__codelineno-0-1121>1121</a></span>
<span class=normal><a href=#__codelineno-0-1122>1122</a></span>
<span class=normal><a href=#__codelineno-0-1123>1123</a></span></pre></div></td><td class=code><div><pre><span></span><code><span id=__span-0-1087><a id=__codelineno-0-1087 name=__codelineno-0-1087></a><span class=k>def</span> <span class=nf>sample_from_q</span><span class=p>(</span>
</span><span id=__span-0-1088><a id=__codelineno-0-1088 name=__codelineno-0-1088></a>    <span class=bp>self</span><span class=p>,</span>
</span><span id=__span-0-1089><a id=__codelineno-0-1089 name=__codelineno-0-1089></a>    <span class=n>input_</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1090><a id=__codelineno-0-1090 name=__codelineno-0-1090></a>    <span class=n>bu_value</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span><span id=__span-0-1091><a id=__codelineno-0-1091 name=__codelineno-0-1091></a>    <span class=n>var_clip_max</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1092><a id=__codelineno-0-1092 name=__codelineno-0-1092></a>    <span class=n>mask</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span><span id=__span-0-1093><a id=__codelineno-0-1093 name=__codelineno-0-1093></a><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span><span id=__span-0-1094><a id=__codelineno-0-1094 name=__codelineno-0-1094></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
</span><span id=__span-0-1095><a id=__codelineno-0-1095 name=__codelineno-0-1095></a><span class=sd>    This method computes the latent inference distribution q(z_i|z_{i+1}) amd samples a latent tensor from it.</span>
</span><span id=__span-0-1096><a id=__codelineno-0-1096 name=__codelineno-0-1096></a>
</span><span id=__span-0-1097><a id=__codelineno-0-1097 name=__codelineno-0-1097></a><span class=sd>    Parameters</span>
</span><span id=__span-0-1098><a id=__codelineno-0-1098 name=__codelineno-0-1098></a><span class=sd>    ----------</span>
</span><span id=__span-0-1099><a id=__codelineno-0-1099 name=__codelineno-0-1099></a><span class=sd>    input_: torch.Tensor</span>
</span><span id=__span-0-1100><a id=__codelineno-0-1100 name=__codelineno-0-1100></a><span class=sd>        The input tensor to the layer, which is the output of the top-down layer above.</span>
</span><span id=__span-0-1101><a id=__codelineno-0-1101 name=__codelineno-0-1101></a><span class=sd>    bu_value: torch.Tensor</span>
</span><span id=__span-0-1102><a id=__codelineno-0-1102 name=__codelineno-0-1102></a><span class=sd>        The tensor defining the parameters /mu_q and /sigma_q computed during the bottom-up deterministic pass</span>
</span><span id=__span-0-1103><a id=__codelineno-0-1103 name=__codelineno-0-1103></a><span class=sd>        at the correspondent hierarchical layer.</span>
</span><span id=__span-0-1104><a id=__codelineno-0-1104 name=__codelineno-0-1104></a><span class=sd>    var_clip_max: float, optional</span>
</span><span id=__span-0-1105><a id=__codelineno-0-1105 name=__codelineno-0-1105></a><span class=sd>        The maximum value reachable by the log-variance of the latent distribtion.</span>
</span><span id=__span-0-1106><a id=__codelineno-0-1106 name=__codelineno-0-1106></a><span class=sd>        Values exceeding this threshold are clipped. Default is `None`.</span>
</span><span id=__span-0-1107><a id=__codelineno-0-1107 name=__codelineno-0-1107></a><span class=sd>    mask: Union[None, torch.Tensor], optional</span>
</span><span id=__span-0-1108><a id=__codelineno-0-1108 name=__codelineno-0-1108></a><span class=sd>        A tensor that is used to mask the sampled latent tensor. Default is `None`.</span>
</span><span id=__span-0-1109><a id=__codelineno-0-1109 name=__codelineno-0-1109></a><span class=sd>    &quot;&quot;&quot;</span>
</span><span id=__span-0-1110><a id=__codelineno-0-1110 name=__codelineno-0-1110></a>    <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_top_layer</span><span class=p>:</span>  <span class=c1># In top layer, we don&#39;t merge bu_value with p_params</span>
</span><span id=__span-0-1111><a id=__codelineno-0-1111 name=__codelineno-0-1111></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=n>bu_value</span>
</span><span id=__span-0-1112><a id=__codelineno-0-1112 name=__codelineno-0-1112></a>    <span class=k>else</span><span class=p>:</span>
</span><span id=__span-0-1113><a id=__codelineno-0-1113 name=__codelineno-0-1113></a>        <span class=c1># NOTE: Here the assumption is that the vampprior is only applied on the top layer.</span>
</span><span id=__span-0-1114><a id=__codelineno-0-1114 name=__codelineno-0-1114></a>        <span class=n>n_img_prior</span> <span class=o>=</span> <span class=kc>None</span>
</span><span id=__span-0-1115><a id=__codelineno-0-1115 name=__codelineno-0-1115></a>        <span class=n>p_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_p_params</span><span class=p>(</span><span class=n>input_</span><span class=p>,</span> <span class=n>n_img_prior</span><span class=p>)</span>
</span><span id=__span-0-1116><a id=__codelineno-0-1116 name=__codelineno-0-1116></a>        <span class=n>q_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>merge</span><span class=p>(</span><span class=n>bu_value</span><span class=p>,</span> <span class=n>p_params</span><span class=p>)</span>
</span><span id=__span-0-1117><a id=__codelineno-0-1117 name=__codelineno-0-1117></a>
</span><span id=__span-0-1118><a id=__codelineno-0-1118 name=__codelineno-0-1118></a>    <span class=n>sample</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>stochastic</span><span class=o>.</span><span class=n>sample_from_q</span><span class=p>(</span><span class=n>q_params</span><span class=p>,</span> <span class=n>var_clip_max</span><span class=p>)</span>
</span><span id=__span-0-1119><a id=__codelineno-0-1119 name=__codelineno-0-1119></a>
</span><span id=__span-0-1120><a id=__codelineno-0-1120 name=__codelineno-0-1120></a>    <span class=k>if</span> <span class=n>mask</span><span class=p>:</span>
</span><span id=__span-0-1121><a id=__codelineno-0-1121 name=__codelineno-0-1121></a>        <span class=k>return</span> <span class=n>sample</span><span class=p>[</span><span class=n>mask</span><span class=p>]</span>
</span><span id=__span-0-1122><a id=__codelineno-0-1122 name=__codelineno-0-1122></a>
</span><span id=__span-0-1123><a id=__codelineno-0-1123 name=__codelineno-0-1123></a>    <span class=k>return</span> <span class=n>sample</span>
</span></code></pre></div></td></tr></table></div> </details> </div> </div> </div> </div> </div> </div> </div> </div> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../../layers/ class="md-footer__link md-footer__link--prev" aria-label="Previous: layers" rel=prev> <div class="md-footer__button md-icon"><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg></div> <kbd class="key-control prev">P</kbd> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> layers </div> </div> </a> <a href=../likelihoods/ class="md-footer__link md-footer__link--next" aria-label="Next: likelihoods" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> likelihoods </div> </div> <kbd class=key-control>N</kbd> <div class="md-footer__button md-icon"><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg></div> </a> </nav> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/CAREamics/careamics target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../../..", "features": ["content.code.copy", "content.code.annotate", "content.action.edit", "navigation.content_next", "navigation.indexes", "search.highlight", "search.share", "search.suggest", "navigation.icons"], "search": "../../../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../../../assets/javascripts/bundle.fe8b6f2b.min.js></script> <script src=../../../../../javascripts/termynal.js></script> <script src=../../../../../javascripts/extra.js></script> <script src=../../../../../javascripts/katex.js></script> <script src=https://unpkg.com/katex@0/dist/katex.min.js></script> <script src=https://unpkg.com/katex@0/dist/contrib/auto-render.min.js></script> </body> </html>